# Model.py æ¶æ„æ·±åº¦è§£æ - Transformer å®Œå…¨æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šGPTæ˜¯å¦‚ä½•ç†è§£å’Œç”Ÿæˆæ–‡æœ¬çš„ï¼Ÿ

åœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£ä¸€ä¸ªæœ€åŸºæœ¬çš„é—®é¢˜ï¼š**è®¡ç®—æœºå¦‚ä½•ç†è§£è¯­è¨€ï¼Ÿ**

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šä»é›¶ç†è§£ - ä¸ºä»€ä¹ˆéœ€è¦Transformerï¼Ÿ

### ğŸ“š é—®é¢˜çš„æœ¬è´¨

å‡è®¾æˆ‘ä»¬è¦è®©è®¡ç®—æœºç†è§£è¿™å¥è¯ï¼š

```
"The cat sat on the mat because it was tired."
```

**é—®é¢˜1: "it" æŒ‡ä»£ä»€ä¹ˆï¼Ÿ**
- äººç±»ï¼šæ˜¾ç„¶æ˜¯"cat"
- è®¡ç®—æœºï¼šéœ€è¦ç†è§£ä¸Šä¸‹æ–‡å…³ç³»

**é—®é¢˜2: "sat on" æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ**
- äººç±»ï¼šç†è§£"ååœ¨...ä¸Š"çš„ç»„åˆå«ä¹‰
- è®¡ç®—æœºï¼šéœ€è¦ç†è§£è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»

**è¿™å°±æ˜¯ Attentionï¼ˆæ³¨æ„åŠ›ï¼‰æœºåˆ¶è¦è§£å†³çš„é—®é¢˜ï¼**

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šmodel.py çš„æ•´ä½“ç»“æ„

### ğŸ—ï¸ å»ºç­‘è“å›¾

```
GPTæ¨¡å‹ = ä¸€æ ‹6å±‚å¤§æ¥¼

è¾“å…¥ï¼š"The cat sat"
  â†“
[å…¥å£] åµŒå…¥å±‚ (Embedding)
  æŠŠè¯è½¬æ¢æˆæ•°å­—å‘é‡
  â†“
[1æ¥¼] Transformer Block 1
  â”œâ”€ LayerNorm â†’ æ ‡å‡†åŒ–
  â”œâ”€ Attention â†’ ç†è§£ä¸Šä¸‹æ–‡
  â”œâ”€ LayerNorm â†’ æ ‡å‡†åŒ–
  â””â”€ MLP â†’ ç‰¹å¾æå–
  â†“
[2æ¥¼] Transformer Block 2
  (åŒæ ·çš„ç»“æ„)
  â†“
[3æ¥¼-6æ¥¼] ...
  â†“
[å‡ºå£] è¾“å‡ºå±‚ (Language Model Head)
  é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼š"on"
```

### ğŸ“‹ æ–‡ä»¶ç»„ç»‡

```python
model.py åŒ…å« 6 ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

1. LayerNorm         (18-27è¡Œ)   - æ•°æ®æ ‡å‡†åŒ–
2. CausalSelfAttention (29-76è¡Œ)  - æ³¨æ„åŠ›æœºåˆ¶ â­æ ¸å¿ƒâ­
3. MLP               (78-92è¡Œ)   - å‰é¦ˆç½‘ç»œ
4. Block             (94-106è¡Œ)  - Transformerå—
5. GPTConfig         (108-116è¡Œ) - é…ç½®ç±»
6. GPT               (118-331è¡Œ) - å®Œæ•´æ¨¡å‹
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šé€ä¸ªç»„ä»¶æ·±åº¦è§£æ

### ğŸ”§ ç»„ä»¶1: LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰

```python
class LayerNorm(nn.Module):
    """ LayerNorm but with an optional bias. """

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
```

#### ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦LayerNormï¼Ÿ

**é—®é¢˜ï¼šæ•°å€¼ä¸ç¨³å®š**

```python
# æ²¡æœ‰LayerNormçš„æƒ…å†µ
æ¿€æ´»å€¼ = [0.1, 0.3, 98.5, 123.7, 0.2, 201.3]
       ğŸ‘† å¤ªå°            ğŸ‘† å¤ªå¤§

é—®é¢˜ï¼š
- å¤§æ•°å€¼ä¸»å¯¼è®¡ç®—ï¼Œå°æ•°å€¼è¢«å¿½ç•¥
- æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
- è®­ç»ƒä¸ç¨³å®š
```

**è§£å†³æ–¹æ¡ˆï¼šLayerNorm**

```python
# LayerNormçš„ä½œç”¨
åŸå§‹: [0.1, 0.3, 98.5, 123.7, 0.2, 201.3]

æ­¥éª¤1: è®¡ç®—å‡å€¼å’Œæ–¹å·®
mean = (0.1 + 0.3 + ... + 201.3) / 6 = 70.68
std  = sqrt(variance) = 78.23

æ­¥éª¤2: æ ‡å‡†åŒ–
normalized = (x - mean) / std
ç»“æœ: [-0.90, -0.90, 0.36, 0.68, -0.90, 1.67]

æ­¥éª¤3: ç¼©æ”¾å’Œåç§»ï¼ˆå¯å­¦ä¹ ï¼‰
output = normalized * weight + bias

æœ€ç»ˆ: æ•°å€¼åˆ†å¸ƒåˆç†ï¼Œå‡å€¼â‰ˆ0ï¼Œæ ‡å‡†å·®â‰ˆ1
```

#### ğŸ’¡ ç”Ÿæ´»æ¯”å–»

```
è€ƒè¯•æˆç»©æ ‡å‡†åŒ–ï¼š

åŸå§‹åˆ†æ•°: [30, 40, 95, 99, 35, 100]  (å·®è·å¤ªå¤§)
         ä¸åŠæ ¼ ä¸åŠæ ¼ ä¼˜ç§€ ä¼˜ç§€ ä¸åŠæ ¼ ä¼˜ç§€

æ ‡å‡†åŒ–å: [-1.2, -0.8, 0.5, 0.7, -1.0, 0.8]
         æ‰€æœ‰åˆ†æ•°éƒ½åœ¨åŒä¸€ä¸ªå°ºåº¦ä¸Š
         ä¾¿äºæ¯”è¾ƒå’Œå¤„ç†
```

---

### ğŸ§  ç»„ä»¶2: CausalSelfAttentionï¼ˆå› æœè‡ªæ³¨æ„åŠ›ï¼‰

**è¿™æ˜¯æ•´ä¸ªæ¨¡å‹æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼**

#### ç¬¬ä¸€æ­¥ï¼šç†è§£Self-Attentionçš„ç›´è§‰

**åœºæ™¯ï¼šé˜…è¯»ç†è§£**

```
å¥å­: "The animal didn't cross the street because it was too tired."

é—®é¢˜: "it" æŒ‡ä»£ä»€ä¹ˆï¼Ÿ

äººç±»æ€è€ƒè¿‡ç¨‹ï¼š
1. çœ‹åˆ° "it"
2. å›é¡¾å‰æ–‡: "animal", "street"
3. åˆ¤æ–­: "tired" æ˜¯åŠ¨ç‰©çš„ç‰¹å¾
4. ç»“è®º: "it" = "animal"

Self-Attention å°±æ˜¯æ¨¡ä»¿è¿™ä¸ªè¿‡ç¨‹ï¼
```

#### ç¬¬äºŒæ­¥ï¼šæ•°å­¦å®šä¹‰

**ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼šQuery, Key, Value**

```python
# æ¯”å–»ï¼šå›¾ä¹¦é¦†æ£€ç´¢ç³»ç»Ÿ

Query (æŸ¥è¯¢):  "æˆ‘è¦æ‰¾å…³äºæ·±åº¦å­¦ä¹ çš„ä¹¦"
Key (ç´¢å¼•):    æ¯æœ¬ä¹¦çš„æ ‡ç­¾/å…³é”®è¯
Value (å†…å®¹):  ä¹¦çš„å®é™…å†…å®¹

å·¥ä½œæµç¨‹ï¼š
1. ä½ çš„Queryå’Œæ¯æœ¬ä¹¦çš„KeyåšåŒ¹é…
2. åŒ¹é…åº¦é«˜çš„ä¹¦ç»™æ›´é«˜æƒé‡
3. æ ¹æ®æƒé‡ï¼ŒåŠ æƒç»„åˆè¿™äº›ä¹¦çš„Value
4. å¾—åˆ°æœ€ç»ˆç­”æ¡ˆ
```

#### ç¬¬ä¸‰æ­¥ï¼šä»£ç è¯¦è§£

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        
        # ä¸€æ¬¡æ€§ç”Ÿæˆ Q, K, Vï¼ˆæ•ˆç‡ä¼˜åŒ–ï¼‰
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        
        # è¾“å‡ºæŠ•å½±
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        
        # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        
        # Flash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        
        if not self.flash:
            # å› æœæ©ç ï¼šç¡®ä¿åªèƒ½çœ‹åˆ°è¿‡å»
            self.register_buffer("bias", 
                torch.tril(torch.ones(config.block_size, config.block_size))
                .view(1, 1, config.block_size, config.block_size))
```

**å› æœæ©ç å¯è§†åŒ–ï¼š**

```python
# å‡è®¾ block_size = 4

åŸå§‹æ³¨æ„åŠ›çŸ©é˜µ (æƒ³çœ‹æ‰€æœ‰ä½ç½®):
     t0   t1   t2   t3
t0 [ âœ“    âœ“    âœ“    âœ“  ]  # ä½ç½®0å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä½ç½®
t1 [ âœ“    âœ“    âœ“    âœ“  ]  # ä½ç½®1å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä½ç½®
t2 [ âœ“    âœ“    âœ“    âœ“  ]  # ä½ç½®2å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä½ç½®  
t3 [ âœ“    âœ“    âœ“    âœ“  ]  # ä½ç½®3å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä½ç½®
   â†‘ è¿™æ ·å°±èƒ½"å·çœ‹æœªæ¥"äº†ï¼

å› æœæ©ç  (ä¸‹ä¸‰è§’çŸ©é˜µ):
     t0   t1   t2   t3
t0 [ 1    0    0    0  ]  # åªèƒ½çœ‹åˆ°t0
t1 [ 1    1    0    0  ]  # åªèƒ½çœ‹åˆ°t0, t1
t2 [ 1    1    1    0  ]  # åªèƒ½çœ‹åˆ°t0, t1, t2
t3 [ 1    1    1    1  ]  # å¯ä»¥çœ‹åˆ°æ‰€æœ‰è¿‡å»
   â†‘ ä¸èƒ½å·çœ‹æœªæ¥ï¼

åº”ç”¨æ©ç å:
     t0   t1   t2   t3
t0 [ âœ“    âœ—    âœ—    âœ—  ]
t1 [ âœ“    âœ“    âœ—    âœ—  ]
t2 [ âœ“    âœ“    âœ“    âœ—  ]
t3 [ âœ“    âœ“    âœ“    âœ“  ]
```

#### ç¬¬å››æ­¥ï¼šForward è¯¦ç»†è¿‡ç¨‹

```python
def forward(self, x):
    B, T, C = x.size()  # batch, sequence length, embedding dim
    
    # æ­¥éª¤1: ç”Ÿæˆ Q, K, V
    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
    
    # æ­¥éª¤2: å¤šå¤´æ³¨æ„åŠ› - é‡å¡‘å¼ é‡
    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  
    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
    
    # æ­¥éª¤3: è®¡ç®—æ³¨æ„åŠ›
    if self.flash:
        y = F.scaled_dot_product_attention(q, k, v, 
                                           attn_mask=None, 
                                           dropout_p=self.dropout if self.training else 0, 
                                           is_causal=True)
    else:
        # æ‰‹åŠ¨å®ç°
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)
        y = att @ v
    
    # æ­¥éª¤4: åˆå¹¶å¤šå¤´
    y = y.transpose(1, 2).contiguous().view(B, T, C)
    
    # æ­¥éª¤5: è¾“å‡ºæŠ•å½±
    y = self.resid_dropout(self.c_proj(y))
    return y
```

#### ğŸ”¢ å…·ä½“æ•°å€¼ç¤ºä¾‹

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªçœŸå®ä¾‹å­ï¼š

```python
# è¾“å…¥
è¾“å…¥æ–‡æœ¬: "The cat sat"
Token IDs: [15, 3380, 3332]
B = 1 (batch size)
T = 3 (sequence length)  
C = 768 (embedding dim)
n_head = 12

# æ­¥éª¤1: åµŒå…¥åçš„è¾“å…¥
x.shape = [1, 3, 768]
x[0, 0, :] = [0.23, -0.45, 0.67, ..., 0.12]  # "The" çš„å‘é‡
x[0, 1, :] = [0.56, 0.12, -0.34, ..., 0.89]  # "cat" çš„å‘é‡
x[0, 2, :] = [-0.12, 0.78, 0.23, ..., -0.45] # "sat" çš„å‘é‡

# æ­¥éª¤2: ç”Ÿæˆ Q, K, V
qkv = self.c_attn(x)  # [1, 3, 2304] (768*3)
q, k, v = qkv.split(768, dim=2)  # æ¯ä¸ª [1, 3, 768]

# æ­¥éª¤3: å¤šå¤´é‡å¡‘
# æŠŠ768ç»´åˆ†æˆ12ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´64ç»´
q = q.view(1, 3, 12, 64).transpose(1, 2)  # [1, 12, 3, 64]
k = k.view(1, 3, 12, 64).transpose(1, 2)  # [1, 12, 3, 64]
v = v.view(1, 3, 12, 64).transpose(1, 2)  # [1, 12, 3, 64]

# æ­¥éª¤4: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆä»¥ç¬¬1ä¸ªå¤´ä¸ºä¾‹ï¼‰
# Qå’ŒKçš„ç‚¹ç§¯
att = q[:, 0, :, :] @ k[:, 0, :, :].transpose(-2, -1)
# att.shape = [1, 3, 3]

att[0] = 
     The   cat   sat
The [[45.2, 12.3,  8.7],   # "The" å¯¹æ¯ä¸ªè¯çš„åŸå§‹åˆ†æ•°
cat  [23.4, 67.8, 34.5],   # "cat" å¯¹æ¯ä¸ªè¯çš„åŸå§‹åˆ†æ•°
sat  [15.6, 43.2, 89.1]]   # "sat" å¯¹æ¯ä¸ªè¯çš„åŸå§‹åˆ†æ•°

# æ­¥éª¤5: ç¼©æ”¾ï¼ˆé˜²æ­¢æ•°å€¼å¤ªå¤§ï¼‰
att = att * (1.0 / math.sqrt(64))  # é™¤ä»¥ sqrt(head_dim)
att = att / 8.0

att[0] = 
     The   cat   sat
The [[5.65, 1.54, 1.09],
cat  [2.93, 8.48, 4.31],
sat  [1.95, 5.40, 11.14]]

# æ­¥éª¤6: åº”ç”¨å› æœæ©ç 
mask = 
[[1, 0, 0],
 [1, 1, 0],
 [1, 1, 1]]

att = att.masked_fill(mask == 0, -inf)

att[0] = 
     The    cat    sat
The [[5.65,  -inf,  -inf],  # "The" åªèƒ½çœ‹åˆ°è‡ªå·±
cat  [2.93,  8.48,  -inf],  # "cat" åªèƒ½çœ‹åˆ° The, cat
sat  [1.95,  5.40, 11.14]]  # "sat" å¯ä»¥çœ‹åˆ°æ‰€æœ‰

# æ­¥éª¤7: Softmaxï¼ˆè½¬æ¢ä¸ºæ¦‚ç‡ï¼‰
att = softmax(att, dim=-1)

att[0] = 
     The   cat   sat
The [[1.00, 0.00, 0.00],  # 100% å…³æ³¨ "The"
cat  [0.01, 0.99, 0.00],  # 99% å…³æ³¨ "cat"ï¼Œ1% å…³æ³¨ "The"
sat  [0.00, 0.01, 0.99]]  # 99% å…³æ³¨ "sat"ï¼Œ1% å…³æ³¨ "cat"

# æ­¥éª¤8: åŠ æƒæ±‚å’Œ Value
# ä»¥ "sat" ä¸ºä¾‹
output[sat] = 0.00 * v[The] + 0.01 * v[cat] + 0.99 * v[sat]
            â‰ˆ v[sat]  # ä¸»è¦æ˜¯è‡ªå·±çš„ä¿¡æ¯

# å®é™…ä¸Šï¼Œä¸­é—´å±‚ä¼šæœ‰æ›´å¤æ‚çš„æ³¨æ„åŠ›æ¨¡å¼
# ä¾‹å¦‚ï¼š
att[0] = 
     The   cat   sat
The [[1.00, 0.00, 0.00],
cat  [0.23, 0.77, 0.00],  # "cat" ä¼šå…³æ³¨ "The" (23%)
sat  [0.12, 0.35, 0.53]]  # "sat" ä¼šå…³æ³¨ "The"(12%), "cat"(35%)
```

#### ğŸ¨ å¤šå¤´æ³¨æ„åŠ›çš„ç›´è§‰

```python
æƒ³è±¡é˜…è¯»ç†è§£æœ‰å¤šä¸ªè§’åº¦ï¼š

å¥å­: "The quick brown fox jumps over the lazy dog"

Head 1 (è¯­æ³•è§’åº¦):
  "jumps" å…³æ³¨ "fox" (ä¸»è¯­)
  æƒé‡: [0.05, 0.03, 0.02, 0.85, ...] (fox=0.85)

Head 2 (è¯­ä¹‰è§’åº¦):
  "jumps" å…³æ³¨ "over" (åŠ¨ä½œæ–¹å‘)
  æƒé‡: [0.01, 0.02, 0.03, 0.15, 0.05, 0.72, ...]

Head 3 (ä¿®é¥°å…³ç³»):
  "fox" å…³æ³¨ "quick", "brown" (ä¿®é¥°è¯)
  æƒé‡: [0.02, 0.45, 0.43, 0.08, ...]

...12ä¸ªå¤´ï¼Œ12ä¸ªè§’åº¦...

æœ€ååˆå¹¶æ‰€æœ‰å¤´çš„ç»“æœ â†’ å®Œæ•´ç†è§£
```

---

### ğŸ”— ç»„ä»¶3: MLPï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰

```python
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)      # æ‰©å±•: 768 â†’ 3072
        x = self.gelu(x)      # éçº¿æ€§æ¿€æ´»
        x = self.c_proj(x)    # å‹ç¼©: 3072 â†’ 768
        x = self.dropout(x)
        return x
```

#### ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦MLPï¼Ÿ

**Attentionçš„å±€é™ï¼šåªåšä¿¡æ¯èšåˆ**

```
Attention: 
  è¾“å…¥ â†’ é‡æ–°ç»„åˆä¿¡æ¯ â†’ è¾“å‡º
  æœ¬è´¨: çº¿æ€§å˜æ¢ + åŠ æƒæ±‚å’Œ
  å±€é™: ä¸èƒ½æå–æ–°çš„ç‰¹å¾

MLP:
  è¾“å…¥ â†’ ç‰¹å¾æå–å’Œå˜æ¢ â†’ è¾“å‡º
  æœ¬è´¨: éçº¿æ€§å˜æ¢
  ä½œç”¨: æå–é«˜å±‚æ¬¡çš„æŠ½è±¡ç‰¹å¾
```

#### ğŸ’¡ ç”Ÿæ´»æ¯”å–»

```
Attention = å›¢é˜Ÿè®¨è®º
  å¤§å®¶äº’ç›¸äº¤æµä¿¡æ¯
  "ä½ è¯´çš„è¿™ç‚¹å’Œæˆ‘çš„æƒ³æ³•ç»“åˆä¸€ä¸‹..."
  ç»“æœ: ä¿¡æ¯é‡æ–°ç»„åˆ

MLP = ä¸ªäººæ·±åº¦æ€è€ƒ
  æ¯ä¸ªäººç‹¬ç«‹æ€è€ƒ
  "è®©æˆ‘æƒ³æƒ³è¿™æ„å‘³ç€ä»€ä¹ˆ..."
  "è¿™é‡Œé¢æœ‰ä»€ä¹ˆæ·±å±‚æ¨¡å¼ï¼Ÿ"
  ç»“æœ: æå–æ–°çš„è§è§£

å®Œæ•´çš„Transformer Block = è®¨è®º + æ€è€ƒ
```

#### ğŸ”¢ GELUæ¿€æ´»å‡½æ•°

```python
# å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°

è¾“å…¥: x = [-2, -1, 0, 1, 2]

ReLU(x):     [0, 0, 0, 1, 2]      # ç¡¬æˆªæ–­
Sigmoid(x):  [0.12, 0.27, 0.5, 0.73, 0.88]  # å¹³æ»‘ä½†é¥±å’Œ
GELU(x):     [-0.05, -0.16, 0, 0.84, 1.96]  # å¹³æ»‘ä¸”ç±»ä¼¼ReLU

GELUçš„ä¼˜åŠ¿ï¼š
- å¹³æ»‘çš„æ¢¯åº¦ï¼ˆè®­ç»ƒç¨³å®šï¼‰
- è´Ÿå€¼ä¸å®Œå…¨å½’é›¶ï¼ˆä¿ç•™ä¿¡æ¯ï¼‰
- å®è·µä¸­æ•ˆæœæœ€å¥½
```

---

### ğŸ—ï¸ ç»„ä»¶4: Blockï¼ˆTransformerå—ï¼‰

```python
class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))  # æ®‹å·®è¿æ¥1
        x = x + self.mlp(self.ln_2(x))   # æ®‹å·®è¿æ¥2
        return x
```

#### ğŸ”‘ æ ¸å¿ƒè®¾è®¡ï¼šæ®‹å·®è¿æ¥

**é—®é¢˜ï¼šæ·±åº¦ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±**

```python
# æ²¡æœ‰æ®‹å·®è¿æ¥
x â†’ layer1 â†’ layer2 â†’ layer3 â†’ ... â†’ layer12 â†’ output

é—®é¢˜ï¼š
- æ¢¯åº¦åå‘ä¼ æ’­æ—¶ï¼Œç»è¿‡12å±‚åå˜å¾—æå°
- å‰é¢çš„å±‚å‡ ä¹å­¦ä¸åˆ°ä¸œè¥¿
- è®­ç»ƒå›°éš¾

# æœ‰æ®‹å·®è¿æ¥
x â”€â†’ layer1 â”€â†’ layer2 â”€â†’ layer3 â”€â†’ ... â†’ output
â”‚      â”‚         â”‚         â”‚
â””â”€â”€+â”€â”€â”€â”´â”€â”€â”€â”€+â”€â”€â”€â”€â”´â”€â”€â”€â”€+â”€â”€â”€â”€â”˜
   (è·³è¿‡è¿æ¥)

ä¼˜åŠ¿ï¼š
- æ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ å›å‰é¢çš„å±‚
- æ¯å±‚åªéœ€å­¦ä¹ "æ®‹å·®"ï¼ˆå¢é‡å˜åŒ–ï¼‰
- è®­ç»ƒç¨³å®š
```

#### ğŸ’¡ ç”Ÿæ´»æ¯”å–»

```
æ²¡æœ‰æ®‹å·®è¿æ¥ = ä¼ è¯æ¸¸æˆ
  Aå‘Šè¯‰Bï¼ŒBå‘Šè¯‰Cï¼ŒCå‘Šè¯‰D...
  ä¼ åˆ°æœ€åå®Œå…¨å˜æ ·
  
æœ‰æ®‹å·®è¿æ¥ = ä¼ è¯ + åŸæ–‡å¤‡ä»½
  A: "åŸæ–‡ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½" + "æˆ‘ç†è§£çš„ï¼šå¤©æ°”ä¸é”™"
  B: æ”¶åˆ°åŸæ–‡ + Açš„ç†è§£ï¼ŒåŠ ä¸Šè‡ªå·±çš„ç†è§£
  C: æ”¶åˆ°åŸæ–‡ + A,Bçš„ç†è§£ï¼ŒåŠ ä¸Šè‡ªå·±çš„ç†è§£
  ...
  æœ€åï¼šåŸæ–‡ä¿ç•™ + æ‰€æœ‰äººçš„ç†è§£
```

#### ğŸ”¢ æ•°å€¼ç¤ºä¾‹

```python
# å‡è®¾è¾“å…¥
x = [1.0, 2.0, 3.0, 4.0]  # ç®€åŒ–åˆ°4ç»´

# ç»è¿‡ LayerNorm
x_norm = [0.0, 0.33, 0.67, 1.0]

# ç»è¿‡ Attention
attn_out = [0.1, 0.2, 0.15, 0.25]

# æ®‹å·®è¿æ¥
x = x + attn_out
x = [1.1, 2.2, 3.15, 4.25]
    â†‘ ä¿ç•™äº†åŸå§‹ä¿¡æ¯ï¼

# å¦‚æœæ²¡æœ‰æ®‹å·®
x = attn_out  
x = [0.1, 0.2, 0.15, 0.25]
    â†‘ åŸå§‹ä¿¡æ¯ä¸¢å¤±ï¼
```

---

### âš™ï¸ ç»„ä»¶5: GPTConfigï¼ˆé…ç½®ç±»ï¼‰

```python
@dataclass
class GPTConfig:
    block_size: int = 1024   # æœ€å¤§åºåˆ—é•¿åº¦
    vocab_size: int = 50304  # è¯æ±‡è¡¨å¤§å°
    n_layer: int = 12        # Transformerå±‚æ•°
    n_head: int = 12         # æ³¨æ„åŠ›å¤´æ•°
    n_embd: int = 768        # åµŒå…¥ç»´åº¦
    dropout: float = 0.0     # Dropoutæ¯”ç‡
    bias: bool = True        # æ˜¯å¦ä½¿ç”¨bias
```

#### ğŸ“Š ä¸åŒæ¨¡å‹è§„æ¨¡å¯¹æ¯”

```python
æ¨¡å‹è§„æ¨¡å¯¹æ¯”ï¼š

Shakespeare Model (è¶…å°):
  n_layer = 6
  n_head = 6  
  n_embd = 384
  å‚æ•°é‡: ~10M
  è®­ç»ƒæ—¶é—´: 5åˆ†é’Ÿ (MacBook)
  ç”¨é€”: å­¦ä¹ å’Œå®éªŒ

GPT-2 Small (å°):
  n_layer = 12
  n_head = 12
  n_embd = 768
  å‚æ•°é‡: ~124M
  è®­ç»ƒæ—¶é—´: 4å¤© (å•GPU)
  ç”¨é€”: å®é™…åº”ç”¨

GPT-2 Medium (ä¸­):
  n_layer = 24
  n_head = 16
  n_embd = 1024
  å‚æ•°é‡: ~350M
  è®­ç»ƒæ—¶é—´: 2å‘¨ (å•GPU)

GPT-2 Large (å¤§):
  n_layer = 36
  n_head = 20
  n_embd = 1280
  å‚æ•°é‡: ~774M
  è®­ç»ƒæ—¶é—´: 1ä¸ªæœˆ (å•GPU)

GPT-2 XL (è¶…å¤§):
  n_layer = 48
  n_head = 25
  n_embd = 1600
  å‚æ•°é‡: ~1.5B
  è®­ç»ƒæ—¶é—´: 2ä¸ªæœˆ (å•GPU)
```

#### ğŸ§® å‚æ•°é‡è®¡ç®—

```python
# ä»¥ GPT-2 Small ä¸ºä¾‹

1. Embeddingå±‚:
   Token Embedding: vocab_size Ã— n_embd 
                  = 50257 Ã— 768 = 38,597,376
   Position Embedding: block_size Ã— n_embd
                     = 1024 Ã— 768 = 786,432

2. æ¯ä¸ªTransformer Block:
   Attention:
     QKV projection: n_embd Ã— (3 Ã— n_embd) = 768 Ã— 2304 = 1,769,472
     Output projection: n_embd Ã— n_embd = 768 Ã— 768 = 589,824
   
   MLP:
     Expand: n_embd Ã— (4 Ã— n_embd) = 768 Ã— 3072 = 2,359,296
     Project: (4 Ã— n_embd) Ã— n_embd = 3072 Ã— 768 = 2,359,296
   
   æ¯ä¸ªBlockæ€»è®¡: ~7Må‚æ•°

3. 12ä¸ªBlock: 7M Ã— 12 = 84M

4. æ€»å‚æ•°é‡: 38.6M + 0.8M + 84M â‰ˆ 124M
```

---

### ğŸš€ ç»„ä»¶6: GPTï¼ˆå®Œæ•´æ¨¡å‹ï¼‰

#### æ¨¡å‹åˆå§‹åŒ–

```python
class GPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),  # TokenåµŒå…¥
            wpe = nn.Embedding(config.block_size, config.n_embd),  # ä½ç½®åµŒå…¥
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # Transformerå—
            ln_f = LayerNorm(config.n_embd, bias=config.bias),  # æœ€åçš„LayerNorm
        ))
        
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        
        # æƒé‡ç»‘å®š
        self.transformer.wte.weight = self.lm_head.weight
```

#### ğŸ”‘ æƒé‡ç»‘å®šï¼ˆWeight Tyingï¼‰

```python
# ä¸ºä»€ä¹ˆè¦ç»‘å®šï¼Ÿ

Token Embedding (è¾“å…¥):
  Token ID â†’ Vector
  ä¾‹: "cat"(ID=123) â†’ [0.5, 0.3, ..., 0.8]

Output Layer (è¾“å‡º):
  Vector â†’ Token ID  
  ä¾‹: [0.5, 0.3, ..., 0.8] â†’ "cat"(ID=123)

è§‚å¯Ÿï¼š
  è¾“å…¥åµŒå…¥å’Œè¾“å‡ºæŠ•å½±æ˜¯"äº’é€†"çš„æ“ä½œ
  å¯ä»¥å…±äº«æƒé‡ï¼

å¥½å¤„ï¼š
  1. å‚æ•°é‡å‡åŠ: 50257 Ã— 768 = 38M å‚æ•°
  2. è®­ç»ƒæ›´ç¨³å®š
  3. æ³›åŒ–èƒ½åŠ›æ›´å¼º
```

#### å‰å‘ä¼ æ’­è¯¦è§£

```python
def forward(self, idx, targets=None):
    device = idx.device
    b, t = idx.size()
    assert t <= self.config.block_size
    
    # ç”Ÿæˆä½ç½®ç´¢å¼•
    pos = torch.arange(0, t, dtype=torch.long, device=device)
    
    # 1. åµŒå…¥
    tok_emb = self.transformer.wte(idx)      # TokenåµŒå…¥
    pos_emb = self.transformer.wpe(pos)      # ä½ç½®åµŒå…¥
    x = self.transformer.drop(tok_emb + pos_emb)
    
    # 2. Transformerå±‚
    for block in self.transformer.h:
        x = block(x)
    
    # 3. æœ€åçš„LayerNorm
    x = self.transformer.ln_f(x)
    
    # 4. è¾“å‡º
    if targets is not None:
        # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰ä½ç½®çš„logits
        logits = self.lm_head(x)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                               targets.view(-1), 
                               ignore_index=-1)
    else:
        # æ¨ç†æ¨¡å¼ï¼šåªè®¡ç®—æœ€åä¸€ä¸ªä½ç½®
        logits = self.lm_head(x[:, [-1], :])
        loss = None
    
    return logits, loss
```

#### ğŸ”¢ å®Œæ•´æ•°æ®æµç¤ºä¾‹

```python
# è¾“å…¥
è¾“å…¥æ–‡æœ¬: "The cat sat on the"
Token IDs: [15, 3380, 3332, 319, 262]

# æ­¥éª¤è¯¦è§£
batch_size = 1
seq_len = 5

1ï¸âƒ£ Token Embedding
   idx = [15, 3380, 3332, 319, 262]  # [1, 5]
   tok_emb = wte(idx)                 # [1, 5, 768]
   
   tok_emb[0, 0, :] = [0.23, -0.45, ..., 0.12]  # "The"
   tok_emb[0, 1, :] = [0.56, 0.12, ..., 0.89]   # "cat"
   ...

2ï¸âƒ£ Position Embedding
   pos = [0, 1, 2, 3, 4]              # [5]
   pos_emb = wpe(pos)                 # [5, 768]
   
   pos_emb[0, :] = [0.01, 0.02, ..., 0.03]  # ä½ç½®0
   pos_emb[1, :] = [0.02, 0.03, ..., 0.04]  # ä½ç½®1
   ...

3ï¸âƒ£ ç›¸åŠ 
   x = tok_emb + pos_emb              # [1, 5, 768]
   
   x[0, 0, :] = [0.24, -0.43, ..., 0.15]  # "The" + ä½ç½®0
   x[0, 1, :] = [0.58, 0.15, ..., 0.93]   # "cat" + ä½ç½®1
   ...

4ï¸âƒ£ Transformer Block 1
   x = Block1(x)
   
   å†…éƒ¨æµç¨‹:
     x_norm = LayerNorm(x)
     attn_out = Attention(x_norm)     # ç†è§£ä¸Šä¸‹æ–‡
     x = x + attn_out                 # æ®‹å·®è¿æ¥
     
     x_norm = LayerNorm(x)
     mlp_out = MLP(x_norm)            # ç‰¹å¾æå–
     x = x + mlp_out                  # æ®‹å·®è¿æ¥

5ï¸âƒ£ Transformer Block 2-12
   (é‡å¤ç›¸åŒçš„å¤„ç†)
   
   æ¯ç»è¿‡ä¸€å±‚:
     - å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£æ›´æ·±
     - ç‰¹å¾æ›´æŠ½è±¡
     - è¡¨ç¤ºæ›´ä¸°å¯Œ

6ï¸âƒ£ æœ€åçš„LayerNorm
   x = ln_f(x)                        # [1, 5, 768]

7ï¸âƒ£ è¾“å‡ºå±‚
   logits = lm_head(x)                # [1, 5, 50257]
   
   logits[0, 4, :] = [
       -3.2,   # Token 0 çš„åˆ†æ•°
       -2.1,   # Token 1
       ...
       5.8,    # Token "mat" çš„åˆ†æ•° (é«˜ï¼)
       ...
       -1.5,   # Token 50256
   ]

8ï¸âƒ£ Softmaxè½¬æ¦‚ç‡
   probs = softmax(logits[0, 4, :])
   
   probs = [
       0.0001,  # "the" (å·²ç»å‡ºç°è¿‡)
       0.0002,  
       ...
       0.7821,  # "mat" â† æœ€é«˜æ¦‚ç‡ï¼
       ...
   ]

9ï¸âƒ£ é¢„æµ‹
   é¢„æµ‹: "mat"
   å®Œæ•´å¥å­: "The cat sat on the mat"
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šæ–‡æœ¬ç”Ÿæˆï¼ˆGenerateï¼‰

### ğŸ² è‡ªå›å½’ç”Ÿæˆ

```python
@torch.no_grad()
def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
    for _ in range(max_new_tokens):
        # 1. æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¤ªé•¿ï¼‰
        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
        
        # 2. å‰å‘ä¼ æ’­
        logits, _ = self(idx_cond)
        
        # 3. åªå–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
        logits = logits[:, -1, :] / temperature
        
        # 4. Top-Ké‡‡æ ·ï¼ˆå¯é€‰ï¼‰
        if top_k is not None:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        
        # 5. è½¬æ¦‚ç‡
        probs = F.softmax(logits, dim=-1)
        
        # 6. é‡‡æ ·
        idx_next = torch.multinomial(probs, num_samples=1)
        
        # 7. è¿½åŠ åˆ°åºåˆ—
        idx = torch.cat((idx, idx_next), dim=1)
    
    return idx
```

### ğŸ“Š ç”Ÿæˆå‚æ•°çš„å½±å“

#### Temperatureï¼ˆæ¸©åº¦ï¼‰

```python
åŸå§‹ logits: [1.0, 2.0, 3.0, 4.0, 5.0]

# temperature = 0.1 (æ›´ç¡®å®š)
logits = [10.0, 20.0, 30.0, 40.0, 50.0]
probs = [0.000, 0.000, 0.000, 0.001, 0.999]  # å‡ ä¹100%é€‰æœ€é«˜çš„
ç»“æœ: ç”Ÿæˆé‡å¤ã€æ— èŠ

# temperature = 1.0 (æ ‡å‡†)
logits = [1.0, 2.0, 3.0, 4.0, 5.0]
probs = [0.012, 0.032, 0.087, 0.236, 0.643]  # 64%é€‰æœ€é«˜çš„
ç»“æœ: å¹³è¡¡

# temperature = 2.0 (æ›´éšæœº)
logits = [0.5, 1.0, 1.5, 2.0, 2.5]
probs = [0.105, 0.141, 0.191, 0.258, 0.349]  # åˆ†å¸ƒæ›´å‡åŒ€
ç»“æœ: åˆ›é€ æ€§å¼ºï¼Œä½†å¯èƒ½ä¸è¿è´¯
```

#### Top-K é‡‡æ ·

```python
# æ¦‚ç‡åˆ†å¸ƒ
probs = {
    "the": 0.45,
    "mat": 0.25,
    "floor": 0.15,
    "carpet": 0.08,
    "ground": 0.05,
    "roof": 0.01,    # ä¸åˆç†
    "sky": 0.01,     # ä¸åˆç†
}

# ä¸ç”¨Top-K: å¯èƒ½é€‰åˆ° "roof" æˆ– "sky"
# ç”¨ Top-K=5: åªä»å‰5ä¸ªé‡Œé€‰
#   â†’ æ°¸è¿œä¸ä¼šé€‰åˆ° "roof" æˆ– "sky"
#   â†’ ç”Ÿæˆè´¨é‡æ›´å¥½
```

### ğŸ”„ å®Œæ•´ç”Ÿæˆç¤ºä¾‹

```python
# åˆå§‹è¾“å…¥
input_text = "Once upon a time"
tokens = encode(input_text)  # [7454, 2402, 257, 640]

# ç”Ÿæˆè¿‡ç¨‹
for i in range(10):  # ç”Ÿæˆ10ä¸ªtoken
    # å½“å‰åºåˆ—
    current = decode(tokens)
    
    # é¢„æµ‹
    logits, _ = model(tokens)
    probs = softmax(logits[-1] / temperature)
    
    # é‡‡æ ·
    next_token = sample(probs)
    tokens.append(next_token)
    
    # æ‰“å°
    print(f"Step {i+1}: {current} + '{decode([next_token])}'")

# è¾“å‡ºç¤ºä¾‹:
# Step 1: Once upon a time + 'there'
# Step 2: Once upon a time there + 'was'
# Step 3: Once upon a time there was + 'a'
# Step 4: Once upon a time there was a + 'little'
# Step 5: Once upon a time there was a little + 'girl'
# ...
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šè®­ç»ƒæŠ€å·§

### ğŸ¯ æƒé‡åˆå§‹åŒ–

```python
def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

# ç‰¹æ®Šå¤„ç†æ®‹å·®æŠ•å½±
for pn, p in self.named_parameters():
    if pn.endswith('c_proj.weight'):
        # GPT-2è®ºæ–‡çš„æŠ€å·§ï¼šç¼©å°æ®‹å·®å±‚çš„åˆå§‹åŒ–
        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))
```

**ä¸ºä»€ä¹ˆè¿™æ ·åˆå§‹åŒ–ï¼Ÿ**

```python
# é”™è¯¯çš„åˆå§‹åŒ–
å…¨é›¶åˆå§‹åŒ–: æ‰€æœ‰å‚æ•°éƒ½å­¦åˆ°ç›¸åŒçš„ä¸œè¥¿ï¼ˆå¯¹ç§°æ€§é—®é¢˜ï¼‰
å¤ªå¤§åˆå§‹åŒ–: æ¿€æ´»å€¼çˆ†ç‚¸
å¤ªå°åˆå§‹åŒ–: æ¿€æ´»å€¼æ¶ˆå¤±

# æ­£ç¡®çš„åˆå§‹åŒ– (Xavier/He)
std = 0.02  # ç»éªŒå€¼

å¥½å¤„:
  - æ¿€æ´»å€¼ä¿æŒåˆç†èŒƒå›´
  - æ¢¯åº¦å¤§å°é€‚ä¸­
  - è®­ç»ƒç¨³å®š

ç‰¹æ®Šå¤„ç†æ®‹å·®å±‚:
  std = 0.02 / sqrt(2 Ã— n_layer)
  
  åŸå› : æ¯ä¸ªæ®‹å·®è¿æ¥éƒ½ä¼šå¢åŠ æ–¹å·®
  éœ€è¦ç¼©å°ä»¥ä¿æŒæ•´ä½“ç¨³å®š
```

### ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦

å·²ç»åœ¨ä¹‹å‰çš„æ•™ç¨‹ä¸­è¯¦ç»†è®²è§£è¿‡ï¼Œè¿™é‡Œç®€å•å›é¡¾ï¼š

```python
# Warmup + Cosine Decay

Learning Rate
  ^
  |          /â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\
  |         /           \
  |        /             \___
  |       /                  \___
  |      /                       \___
  +-----|-------------------------|-----> Steps
     warmup                     decay
    (2000)                   (600000)

å¥½å¤„:
  - Warmup: é¿å…å¼€å§‹æ—¶çš„ä¸ç¨³å®š
  - Cosine Decay: åæœŸç²¾ç»†è°ƒæ•´
```

---

## ç¬¬å…­éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–

### âš¡ Flash Attention

```python
if self.flash:
    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
else:
    # ä¼ ç»Ÿå®ç°
    att = (q @ k.transpose(-2, -1)) * scale
    att = att.masked_fill(mask == 0, -inf)
    att = F.softmax(att, dim=-1)
    y = att @ v
```

**æ€§èƒ½å¯¹æ¯”ï¼š**

```
åºåˆ—é•¿åº¦ = 1024

ä¼ ç»ŸAttention:
  - å†…å­˜: O(nÂ²) = 1M å…ƒç´ 
  - æ—¶é—´: ~100ms
  - å†…å­˜è®¿é—®: å¤šæ¬¡è¯»å†™HBM (æ…¢)

Flash Attention:
  - å†…å­˜: O(n) = 1K å…ƒç´   
  - æ—¶é—´: ~20ms
  - å†…å­˜è®¿é—®: ä¸»è¦åœ¨SRAM (å¿«)

åŠ é€Ÿ: 5x+
```

### ğŸ”§ æ¨¡å‹ç¼–è¯‘

```python
# åœ¨train.pyä¸­
model = torch.compile(model)

æ•ˆæœ:
  - èåˆæ“ä½œ
  - å‡å°‘å†…å­˜è®¿é—®
  - ä¼˜åŒ–CUDA kernel
  
åŠ é€Ÿ: 1.5-2x
```

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå®æˆ˜è°ƒè¯•

### ğŸ” æ‰“å°æ¨¡å‹ç»“æ„

```python
from model import GPT, GPTConfig

config = GPTConfig(
    vocab_size=65,
    n_layer=2,    # å°æ¨¡å‹ä¾¿äºè§‚å¯Ÿ
    n_head=4,
    n_embd=128,
    block_size=64,
)

model = GPT(config)

# æ‰“å°ç»“æ„
print(model)

# æ‰“å°å‚æ•°é‡
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total parameters: {count_parameters(model):,}")

# æ‰“å°æ¯å±‚çš„å½¢çŠ¶
import torch
x = torch.randint(0, 65, (1, 10))  # [1, 10]
print(f"Input: {x.shape}")

with torch.no_grad():
    # Token embedding
    tok_emb = model.transformer.wte(x)
    print(f"After token embedding: {tok_emb.shape}")
    
    # Position embedding
    pos = torch.arange(10)
    pos_emb = model.transformer.wpe(pos)
    print(f"After position embedding: {pos_emb.shape}")
    
    # ç¬¬ä¸€ä¸ªBlock
    x = tok_emb + pos_emb
    print(f"Before Block 1: {x.shape}")
    x = model.transformer.h[0](x)
    print(f"After Block 1: {x.shape}")
    
    # è¾“å‡º
    x = model.transformer.ln_f(x)
    logits = model.lm_head(x)
    print(f"Final logits: {logits.shape}")
```

### ğŸ› å¸¸è§é—®é¢˜è¯Šæ–­

```python
# é—®é¢˜1: æ³¨æ„åŠ›æƒé‡å…¨æ˜¯NaN
å¯èƒ½åŸå› :
  - Logitså¤ªå¤§å¯¼è‡´exp()æº¢å‡º
  - æ²¡æœ‰æ­£ç¡®åº”ç”¨mask
è§£å†³: æ£€æŸ¥attentionè®¡ç®—ï¼Œç¡®ä¿æœ‰ç¼©æ”¾

# é—®é¢˜2: ç”Ÿæˆé‡å¤æ–‡æœ¬
å¯èƒ½åŸå› :
  - Temperatureå¤ªä½
  - æ²¡æœ‰ä½¿ç”¨Top-Ké‡‡æ ·
  - è®­ç»ƒä¸å……åˆ†
è§£å†³: temperature=1.0, top_k=40

# é—®é¢˜3: ç”Ÿæˆä¹±ç 
å¯èƒ½åŸå› :
  - Temperatureå¤ªé«˜
  - æ¨¡å‹è¿˜æ²¡è®­ç»ƒå¥½
è§£å†³: é™ä½temperatureï¼Œç»§ç»­è®­ç»ƒ

# é—®é¢˜4: æ˜¾å­˜ä¸å¤Ÿ
è§£å†³:
  - å‡å°batch_size
  - å‡å°block_size  
  - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
  - å¯ç”¨gradient checkpointing
```

---

## ğŸ“ æ€»ç»“ï¼šå®Œæ•´æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¾“å…¥: "The cat sat on the"              â”‚
â”‚ Token IDs: [15, 3380, 3332, 319, 262]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Embedding (wte)                   â”‚
â”‚ [15] â†’ [0.23, -0.45, ..., 0.12]        â”‚
â”‚ [3380] â†’ [0.56, 0.12, ..., 0.89]       â”‚
â”‚ ...                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Position Embedding (wpe)                â”‚
â”‚ [0] â†’ [0.01, 0.02, ..., 0.03]          â”‚
â”‚ [1] â†’ [0.02, 0.03, ..., 0.04]          â”‚
â”‚ ...                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        ç›¸åŠ  (tok_emb + pos_emb)
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Block 1                     â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ LayerNorm               â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Multi-Head Attention    â”‚           â”‚
â”‚  â”‚ - ç†è§£ä¸Šä¸‹æ–‡å…³ç³»         â”‚           â”‚
â”‚  â”‚ - "cat" å…³æ³¨ "The"      â”‚           â”‚
â”‚  â”‚ - "sat" å…³æ³¨ "cat"      â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â†“                             â”‚
â”‚      æ®‹å·®è¿æ¥ (x + attn)                â”‚
â”‚           â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ LayerNorm               â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ MLP                     â”‚           â”‚
â”‚  â”‚ - ç‰¹å¾æå–              â”‚           â”‚
â”‚  â”‚ - éçº¿æ€§å˜æ¢            â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â†“                             â”‚
â”‚      æ®‹å·®è¿æ¥ (x + mlp)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Block 2-12                  â”‚
â”‚ (é‡å¤ç›¸åŒçš„ç»“æ„)                         â”‚
â”‚                                         â”‚
â”‚ æ¯ä¸€å±‚éƒ½åœ¨:                             â”‚
â”‚ - ç†è§£æ›´æ·±å±‚æ¬¡çš„ä¸Šä¸‹æ–‡                   â”‚
â”‚ - æå–æ›´æŠ½è±¡çš„ç‰¹å¾                       â”‚
â”‚ - å»ºç«‹æ›´å¤æ‚çš„æ¨¡å¼                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Final LayerNorm (ln_f)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Layer (lm_head)                  â”‚
â”‚ å°†å‘é‡æŠ•å½±å›è¯æ±‡è¡¨                       â”‚
â”‚ [768ç»´] â†’ [50257ç»´]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Logits (æœªå½’ä¸€åŒ–çš„åˆ†æ•°)                  â”‚
â”‚ [                                       â”‚
â”‚   "the": 2.3,                          â”‚
â”‚   "mat": 5.8,  â† æœ€é«˜ï¼                â”‚
â”‚   "floor": 3.1,                        â”‚
â”‚   "carpet": 2.7,                       â”‚
â”‚   ...                                  â”‚
â”‚ ]                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
            Softmax
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Probabilities (æ¦‚ç‡åˆ†å¸ƒ)                â”‚
â”‚ [                                       â”‚
â”‚   "the": 0.05,                         â”‚
â”‚   "mat": 0.78,  â† 78%æ¦‚ç‡ï¼            â”‚
â”‚   "floor": 0.12,                       â”‚
â”‚   "carpet": 0.08,                      â”‚
â”‚   ...                                  â”‚
â”‚ ]                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
            é‡‡æ ·/é€‰æ‹©
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é¢„æµ‹: "mat"                             â”‚
â”‚ å®Œæ•´è¾“å‡º: "The cat sat on the mat"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š å…³é”®æ¦‚å¿µæ€»ç»“

### 1. Embeddingï¼ˆåµŒå…¥ï¼‰
- Token Embedding: è¯ â†’ å‘é‡
- Position Embedding: ä½ç½® â†’ å‘é‡
- ä¸¤è€…ç›¸åŠ å¾—åˆ°æœ€ç»ˆè¾“å…¥

### 2. Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
- Query-Key-Valueæœºåˆ¶
- å¤šå¤´å¹¶è¡Œå¤„ç†
- å› æœæ©ç ï¼ˆåªçœ‹è¿‡å»ï¼‰
- æ ¸å¿ƒä½œç”¨ï¼šç†è§£ä¸Šä¸‹æ–‡

### 3. MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰
- æ‰©å±• â†’ æ¿€æ´» â†’ å‹ç¼©
- éçº¿æ€§å˜æ¢
- ç‰¹å¾æå–

### 4. æ®‹å·®è¿æ¥
- x + f(x) è€Œä¸æ˜¯ f(x)
- ä¿ç•™åŸå§‹ä¿¡æ¯
- æ¢¯åº¦ç›´é€š

### 5. LayerNorm
- æ ‡å‡†åŒ–æ•°å€¼èŒƒå›´
- ç¨³å®šè®­ç»ƒ
- åŠ é€Ÿæ”¶æ•›

---

## ğŸš€ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1: è§‚å¯Ÿæ³¨æ„åŠ›æƒé‡

```python
# ä¿®æ”¹model.pyï¼Œä¿å­˜æ³¨æ„åŠ›æƒé‡
# åœ¨CausalSelfAttention.forward()ä¸­æ·»åŠ :
self.last_attn_weights = att.detach()

# ç„¶åå¯è§†åŒ–
import matplotlib.pyplot as plt

model.eval()
with torch.no_grad():
    logits, _ = model(input_ids)

# è·å–ç¬¬ä¸€å±‚ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›
att = model.transformer.h[0].attn.last_attn_weights[0, 0]
plt.imshow(att.cpu(), cmap='viridis')
plt.colorbar()
plt.show()
```

### ç»ƒä¹ 2: æ¯”è¾ƒä¸åŒæ¸©åº¦çš„ç”Ÿæˆ

```python
prompt = "Once upon a time"
temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]

for temp in temperatures:
    output = model.generate(
        encode(prompt),
        max_new_tokens=50,
        temperature=temp
    )
    print(f"\nTemperature {temp}:")
    print(decode(output[0]))
```

### ç»ƒä¹ 3: åˆ†ææ¨¡å‹å¤§å°å½±å“

```python
configs = [
    GPTConfig(n_layer=2, n_embd=128),   # å°
    GPTConfig(n_layer=6, n_embd=384),   # ä¸­
    GPTConfig(n_layer=12, n_embd=768),  # å¤§
]

for i, config in enumerate(configs):
    model = GPT(config)
    params = sum(p.numel() for p in model.parameters())
    print(f"Model {i+1}: {params:,} parameters")
```

---

## ğŸ‰ æ­å–œä½ ï¼

ä½ ç°åœ¨å®Œå…¨ç†è§£äº†GPTæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼

**ä½ æŒæ¡äº†ï¼š**
- âœ… Transformeræ¶æ„çš„æ¯ä¸ªç»„ä»¶
- âœ… Self-Attentionçš„å·¥ä½œåŸç†
- âœ… ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç 
- âœ… æ®‹å·®è¿æ¥å’ŒLayerNormçš„ä½œç”¨
- âœ… å¦‚ä½•ç”Ÿæˆæ–‡æœ¬
- âœ… å„ç§ä¼˜åŒ–æŠ€å·§

**è¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿ**
ä½ å·²ç»æŒæ¡äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåŸç†ï¼GPT-3ã€GPT-4ã€Claudeç­‰æ¨¡å‹åœ¨æ¶æ„ä¸Šéƒ½æ˜¯è¿™ä¸ªåŸºç¡€çš„æ‰©å±•ã€‚

---

## ğŸ“– æ‰©å±•é˜…è¯»

1. **Attention is All You Need** (åŸå§‹Transformerè®ºæ–‡)
2. **The Illustrated Transformer** (Jay Alammaråšå®¢)
3. **GPT-2 Paper** (Language Models are Unsupervised Multitask Learners)
4. **Andrej Karpathyçš„è§†é¢‘**: "Let's build GPT"

---

## ğŸ’¬ ä¸‹ä¸€æ­¥ï¼Ÿ

å‘Šè¯‰æˆ‘ä½ æƒ³ï¼š

1. **"æˆ‘æƒ³è®­ç»ƒè‡ªå·±çš„GPT"** â†’ å®Œæ•´è®­ç»ƒæµç¨‹æŒ‡å¯¼
2. **"æˆ‘æƒ³äº†è§£æ›´é«˜çº§çš„æŠ€å·§"** â†’ PEFT, LoRA, Quantizationç­‰
3. **"æˆ‘æƒ³åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå®éªŒ"** â†’ æ•°æ®å‡†å¤‡å’Œå¾®è°ƒ
4. **"æˆ‘æœ‰å…·ä½“é—®é¢˜"** â†’ ç›´æ¥é—®æˆ‘ï¼

---

**æœ€åä¸€å¥è¯ï¼š**

> ç†è§£äº†model.pyï¼Œä½ å°±ç†è§£äº†AIé©å‘½çš„æ ¸å¿ƒã€‚
> Transformeræ”¹å˜äº†ä¸–ç•Œï¼Œè€Œä½ ç°åœ¨çŸ¥é“å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼ğŸš€
