# ç¬¬05ç« ï¼šæ¨¡åž‹æž¶æž„æ·±åº¦è§£æž - ä»Žé›¶ç†è§£ Transformer

> **å­¦ä¹ ç›®æ ‡**ï¼šå®Œå…¨ç†è§£GPTæ¨¡åž‹å†…éƒ¨æ˜¯å¦‚ä½•å·¥ä½œçš„  
> **éš¾åº¦ç­‰çº§**ï¼šðŸŒ¿ðŸŒ¿ðŸŒ¿ è¿›é˜¶  
> **é¢„è®¡æ—¶é—´**ï¼š3-4å°æ—¶ï¼ˆå»ºè®®åˆ†2å¤©å­¦ä¹ ï¼‰  
> **å‰ç½®çŸ¥è¯†**ï¼š01-04ç« åŸºç¡€çŸ¥è¯†

---

## ðŸŽ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£Transformerçš„æ¯ä¸ªç»„ä»¶ä¸ºä»€ä¹ˆå­˜åœ¨
- âœ… ç”¨æœ€ç®€å•çš„è¯­è¨€è§£é‡ŠSelf-Attentionæœºåˆ¶
- âœ… çŸ¥é“ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ã€æ®‹å·®è¿žæŽ¥ã€LayerNorm
- âœ… èƒ½å¤Ÿè¯»æ‡‚å¹¶ä¿®æ”¹model.pyæºä»£ç 
- âœ… ç†è§£GPTç”Ÿæˆæ–‡æœ¬çš„å®Œæ•´è¿‡ç¨‹
- âœ… æŽŒæ¡å¸¸è§çš„æ€§èƒ½ä¼˜åŒ–æŠ€å·§

---

## ðŸ’­ å¼€å§‹ä¹‹å‰ï¼šä¸ºä»€ä¹ˆè¦å­¦æ¨¡åž‹æž¶æž„ï¼Ÿ

**ä½ å¯èƒ½ä¼šæƒ³ï¼š**
- "æˆ‘å·²ç»èƒ½è®­ç»ƒæ¨¡åž‹äº†ï¼Œä¸ºä»€ä¹ˆè¿˜è¦å­¦å†…éƒ¨ç»“æž„ï¼Ÿ"
- "è¿™äº›çœ‹èµ·æ¥å¾ˆå¤æ‚ï¼Œæˆ‘çœŸçš„éœ€è¦æ‡‚å—ï¼Ÿ"

**ç­”æ¡ˆï¼šè¿™æ˜¯æœ€å€¼å¾—æŠ•èµ„çš„å­¦ä¹ ï¼**

### ðŸš— ç”Ÿæ´»æ¯”å–»

```
å­¦ä¼šå¼€è½¦ï¼ˆèƒ½è®­ç»ƒæ¨¡åž‹ï¼‰ï¼š
  âœ… ä½ èƒ½ä»ŽAå¼€åˆ°B
  âŒ ä½†è½¦åäº†ä¸çŸ¥é“æ€Žä¹ˆä¿®
  âŒ ä¸çŸ¥é“å¦‚ä½•æ”¹è£…æå‡æ€§èƒ½
  
æ‡‚å‘åŠ¨æœºåŽŸç†ï¼ˆæ‡‚æ¨¡åž‹æž¶æž„ï¼‰ï¼š
  âœ… è½¦åäº†èƒ½è‡ªå·±ä¿®
  âœ… èƒ½æ”¹è£…æå‡æ€§èƒ½
  âœ… èƒ½è®¾è®¡æ›´å¥½çš„å¼•æ“Ž
  âœ… ç†è§£ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡
```

### ðŸŽ å­¦å®Œä¹‹åŽä½ èƒ½åšä»€ä¹ˆ

**ç«‹å³èƒ½åšçš„ï¼š**
- âœ… çœ‹æ‡‚GPT-2ã€GPT-3ã€LLaMAç­‰æ‰€æœ‰Transformeræ¨¡åž‹çš„ä»£ç 
- âœ… è‡ªå·±ä¿®æ”¹æ¨¡åž‹ç»“æž„ï¼ˆæ¯”å¦‚æ”¹è¿›Attentionæœºåˆ¶ï¼‰
- âœ… è°ƒè¯•æ¨¡åž‹é—®é¢˜ï¼ˆæ¯”å¦‚ä¸ºä»€ä¹ˆç”Ÿæˆè´¨é‡å·®ï¼‰
- âœ… ä¼˜åŒ–æ¨¡åž‹æ€§èƒ½ï¼ˆæ¯”å¦‚å‡å°‘æ˜¾å­˜å ç”¨ï¼‰

**æ·±å…¥èƒ½åšçš„ï¼š**
- âœ… ç†è§£æœ€æ–°è®ºæ–‡çš„åˆ›æ–°ç‚¹ï¼ˆFlash Attentionã€RoPEç­‰ï¼‰
- âœ… è®¾è®¡è‡ªå·±çš„æ¨¡åž‹æž¶æž„
- âœ… é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–æ¨¡åž‹
- âœ… æˆä¸ºçœŸæ­£çš„AIæž¶æž„å¸ˆ

---

## ðŸŽ¯ æ ¸å¿ƒé—®é¢˜ï¼šè®¡ç®—æœºå¦‚ä½•ç†è§£è¯­è¨€ï¼Ÿ

åœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç†è§£ä¸€ä¸ªæœ€æ ¹æœ¬çš„é—®é¢˜ã€‚

### ðŸ§  äººç±»å¦‚ä½•ç†è§£è¯­è¨€

çœ‹è¿™å¥è¯ï¼š

```
"The cat sat on the mat because it was tired."
```

**é—®é¢˜1: "it" æŒ‡ä»£ä»€ä¹ˆï¼Ÿ**
- ðŸ‘¨ äººç±»ï¼šæ˜¾ç„¶æ˜¯"cat"ï¼ˆçŒ«ä¼šç´¯ï¼Œåž«å­ä¸ä¼šç´¯ï¼‰
- ðŸ¤– è®¡ç®—æœºï¼šï¼Ÿï¼Ÿï¼Ÿï¼ˆéœ€è¦ç†è§£ä¸Šä¸‹æ–‡ï¼‰

**é—®é¢˜2: "sat on" æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ**
- ðŸ‘¨ äººç±»ï¼šç†è§£"ååœ¨...ä¸Š"çš„ç»„åˆå«ä¹‰
- ðŸ¤– è®¡ç®—æœºï¼šï¼Ÿï¼Ÿï¼Ÿï¼ˆéœ€è¦ç†è§£è¯ä¹‹é—´çš„å…³ç³»ï¼‰

### ðŸ’¡ è¿™å°±æ˜¯Attentionè¦è§£å†³çš„é—®é¢˜ï¼

**æ ¸å¿ƒæ€æƒ³ï¼šè®©æ¨¡åž‹èƒ½å¤Ÿ"å…³æ³¨"å¥å­ä¸­çš„ç›¸å…³è¯**

```
è¾“å…¥ï¼š"The cat sat on the mat because it was tired."

å½“å¤„ç†"it"æ—¶ï¼š
  æ¨¡åž‹éœ€è¦å›žå¤´çœ‹ï¼š
  - "The"   â†’ å…³æ³¨åº¦ 5%
  - "cat"   â†’ å…³æ³¨åº¦ 80%  âœ… é‡ç‚¹å…³æ³¨ï¼
  - "sat"   â†’ å…³æ³¨åº¦ 10%
  - "mat"   â†’ å…³æ³¨åº¦ 5%
  
ç»“è®ºï¼š"it" æŒ‡ä»£ "cat"
```

**è¿™å°±æ˜¯Self-Attentionæœºåˆ¶ï¼** è®©æ¨¡åž‹èƒ½å¤Ÿï¼š
- ç†è§£è¯ä¸Žè¯ä¹‹é—´çš„å…³ç³»
- è‡ªåŠ¨å†³å®š"å…³æ³¨"å“ªäº›è¯
- æ•æ‰é•¿è·ç¦»çš„ä¾èµ–å…³ç³»

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šTransformer æ•´ä½“æž¶æž„ - å…ˆçœ‹å…¨è²Œ

### ðŸ—ï¸ å»ºç­‘è“å›¾ï¼šGPTæ¨¡åž‹çš„å®Œæ•´ç»“æž„

æƒ³è±¡GPTæ¨¡åž‹æ˜¯ä¸€æ ‹6å±‚å¤§æ¥¼ï¼Œæ–‡æœ¬ä»Žåº•å±‚è¿›å…¥ï¼Œä»Žé¡¶å±‚è¾“å‡ºï¼š

```
ðŸ“¥ è¾“å…¥ï¼š"The cat sat"
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸšª å…¥å£ï¼šåµŒå…¥å±‚ (Embedding)         â”‚
â”‚  ä½œç”¨ï¼šæŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—å‘é‡          â”‚
â”‚  "The" â†’ [0.23, -0.45, ..., 0.12]  â”‚
â”‚  "cat" â†’ [0.56, 0.12, ..., 0.89]   â”‚
â”‚  "sat" â†’ [-0.12, 0.78, ..., -0.45] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ¢ 1æ¥¼ï¼šTransformer Block 1        â”‚
â”‚  â”œâ”€ ðŸ“ LayerNorm (æ ‡å‡†åŒ–æ•°æ®)       â”‚
â”‚  â”œâ”€ ðŸ§  Attention (ç†è§£ä¸Šä¸‹æ–‡)       â”‚
â”‚  â”œâ”€ âž• æ®‹å·®è¿žæŽ¥ (ä¿ç•™ä¿¡æ¯)           â”‚
â”‚  â”œâ”€ ðŸ“ LayerNorm (å†æ¬¡æ ‡å‡†åŒ–)       â”‚
â”‚  â”œâ”€ ðŸ”§ MLP (ç‰¹å¾æå–)               â”‚
â”‚  â””â”€ âž• æ®‹å·®è¿žæŽ¥ (å†æ¬¡ä¿ç•™)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ¢ 2-6æ¥¼ï¼šé‡å¤ç›¸åŒç»“æž„              â”‚
â”‚  æ¯ä¸€å±‚éƒ½åœ¨ï¼š                        â”‚
â”‚  - æ›´æ·±å…¥åœ°ç†è§£æ–‡æœ¬                  â”‚
â”‚  - æå–æ›´æŠ½è±¡çš„ç‰¹å¾                  â”‚
â”‚  - å»ºç«‹æ›´å¤æ‚çš„å…³è”                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸšª å‡ºå£ï¼šè¾“å‡ºå±‚ (LM Head)          â”‚
â”‚  ä½œç”¨ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯                  â”‚
â”‚  è¾“å‡ºï¼š"on" (æ¦‚çŽ‡æœ€é«˜çš„è¯)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
ðŸ“¤ è¾“å‡ºï¼š"The cat sat on"
```

### ðŸ“‹ model.py æ–‡ä»¶ç»„ç»‡

æ•´ä¸ªæ¨¡åž‹ç”±6ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼š

```python
model.py çš„ç»“æž„ï¼š

1ï¸âƒ£ LayerNorm          (18-27è¡Œ)
   - æ•°æ®æ ‡å‡†åŒ–
   - è®©è®­ç»ƒæ›´ç¨³å®š
   
2ï¸âƒ£ CausalSelfAttention (29-76è¡Œ) â­æ ¸å¿ƒâ­
   - æ³¨æ„åŠ›æœºåˆ¶
   - ç†è§£ä¸Šä¸‹æ–‡å…³ç³»
   - è¿™æ˜¯æ•´ä¸ªæ¨¡åž‹æœ€é‡è¦çš„éƒ¨åˆ†ï¼
   
3ï¸âƒ£ MLP                (78-92è¡Œ)
   - å‰é¦ˆç¥žç»ç½‘ç»œ
   - ç‰¹å¾æå–å’Œå˜æ¢
   
4ï¸âƒ£ Block              (94-106è¡Œ)
   - ç»„åˆAttentionå’ŒMLP
   - æž„æˆTransformerçš„åŸºæœ¬å•å…ƒ
   
5ï¸âƒ£ GPTConfig          (108-116è¡Œ)
   - é…ç½®å‚æ•°
   - æŽ§åˆ¶æ¨¡åž‹å¤§å°å’Œè¡Œä¸º
   
6ï¸âƒ£ GPT                (118-331è¡Œ)
   - å®Œæ•´æ¨¡åž‹
   - ç»„åˆæ‰€æœ‰ç»„ä»¶
```

### ðŸŽ¯ å­¦ä¹ ç­–ç•¥

æˆ‘ä»¬å°†æŒ‰ç…§ä»¥ä¸‹é¡ºåºå­¦ä¹ ï¼š

```
ç¬¬ä¸€æ­¥ï¼šç†è§£ç®€å•ç»„ä»¶
  â””â”€ LayerNorm (æ•°æ®æ ‡å‡†åŒ–)
  
ç¬¬äºŒæ­¥ï¼šç†è§£æ ¸å¿ƒç»„ä»¶ â­
  â””â”€ Attention (æœ€é‡è¦ï¼)
  
ç¬¬ä¸‰æ­¥ï¼šç†è§£å…¶ä»–ç»„ä»¶
  â”œâ”€ MLP (ç‰¹å¾æå–)
  â””â”€ Block (ç»„åˆå•å…ƒ)
  
ç¬¬å››æ­¥ï¼šç†è§£å®Œæ•´æ¨¡åž‹
  â””â”€ GPT (æ•´ä½“ç»“æž„)
  
ç¬¬äº”æ­¥ï¼šç†è§£æ–‡æœ¬ç”Ÿæˆ
  â””â”€ Generate (å¦‚ä½•ç”Ÿæˆæ–‡æœ¬)
```

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬ä»Žæœ€ç®€å•çš„ç»„ä»¶å¼€å§‹ï¼**

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šç»„ä»¶1 - LayerNormï¼ˆæ•°æ®æ ‡å‡†åŒ–ï¼‰

### ðŸŒ± æœ€ç®€å•çš„ç»„ä»¶ï¼šLayerNorm

æˆ‘ä»¬ä»Žæœ€ç®€å•çš„ç»„ä»¶å¼€å§‹å­¦ä¹ ã€‚è¿™ä¸ªç»„ä»¶è™½ç„¶ç®€å•ï¼Œä½†éžå¸¸é‡è¦ï¼

#### ðŸ’¡ ç›´è§‚ç†è§£ï¼šä¸ºä»€ä¹ˆéœ€è¦LayerNormï¼Ÿ

**ç”Ÿæ´»åœºæ™¯ï¼šè€ƒè¯•æˆç»©**

```
ç­çº§è€ƒè¯•æˆç»©ï¼š

âŒ ä¸æ ‡å‡†åŒ–çš„é—®é¢˜ï¼š
  è¯­æ–‡: 30, 40, 35     (å¹³å‡ 35åˆ†ï¼Œæ»¡åˆ†100)
  æ•°å­¦: 95, 99, 100    (å¹³å‡ 98åˆ†ï¼Œæ»¡åˆ†100)
  
  é—®é¢˜ï¼š
  - è¯­æ–‡æˆç»©å¤ªä½Žï¼Œæ•°å­¦æˆç»©å¤ªé«˜
  - å¦‚æžœç›´æŽ¥æ¯”è¾ƒï¼Œæ•°å­¦çš„å½±å“ä¼šåŽ‹å€’è¯­æ–‡
  - ä¸å…¬å¹³ï¼

âœ… æ ‡å‡†åŒ–åŽï¼š
  è¯­æ–‡: [-0.8, 0.5, -0.3]   (æ ‡å‡†åŒ–åˆ°ç›¸åŒå°ºåº¦)
  æ•°å­¦: [-0.9, 0.3, 0.6]    (æ ‡å‡†åŒ–åˆ°ç›¸åŒå°ºåº¦)
  
  å¥½å¤„ï¼š
  - æ‰€æœ‰ç§‘ç›®åœ¨åŒä¸€å°ºåº¦ä¸Š
  - å¯ä»¥å…¬å¹³æ¯”è¾ƒ
  - å®¹æ˜“å¤„ç†
```

#### ðŸ¤” ç¥žç»ç½‘ç»œä¸­çš„é—®é¢˜

**æ²¡æœ‰LayerNormæ—¶çš„é—®é¢˜ï¼š**

```python
# å‡è®¾æŸä¸€å±‚çš„è¾“å‡º
æ¿€æ´»å€¼ = [0.1, 0.3, 98.5, 123.7, 0.2, 201.3]
         ðŸ‘† å¾ˆå°      ðŸ‘† å¾ˆå¤§     ðŸ‘† è¶…å¤§

é—®é¢˜1ï¼šæ•°å€¼ä¸å¹³è¡¡
  - å¤§æ•°å€¼ï¼ˆ201.3ï¼‰ä¸»å¯¼è®¡ç®—
  - å°æ•°å€¼ï¼ˆ0.1, 0.3ï¼‰è¢«å¿½ç•¥
  - æ¨¡åž‹å­¦ä¹ å›°éš¾

é—®é¢˜2ï¼šæ¢¯åº¦ä¸ç¨³å®š
  - æ¢¯åº¦çˆ†ç‚¸ï¼ˆæ•°å€¼å¤ªå¤§ï¼‰
  - æ¢¯åº¦æ¶ˆå¤±ï¼ˆæ•°å€¼å¤ªå°ï¼‰
  - è®­ç»ƒå´©æºƒ

é—®é¢˜3ï¼šè®­ç»ƒæ…¢
  - å‚æ•°æ›´æ–°ä¸å‡åŒ€
  - æ”¶æ•›å›°éš¾
```

#### ðŸ“ LayerNormçš„å·¥ä½œåŽŸç†

**ä¸‰ä¸ªç®€å•æ­¥éª¤ï¼š**

```python
# åŽŸå§‹æ•°æ®
x = [0.1, 0.3, 98.5, 123.7, 0.2, 201.3]

# æ­¥éª¤1ï¼šè®¡ç®—å‡å€¼å’Œæ–¹å·®
mean = (0.1 + 0.3 + 98.5 + 123.7 + 0.2 + 201.3) / 6
     = 70.68

variance = ((0.1-70.68)Â² + (0.3-70.68)Â² + ... ) / 6
std = âˆšvariance = 78.23

# æ­¥éª¤2ï¼šæ ‡å‡†åŒ–ï¼ˆå‡å‡å€¼ï¼Œé™¤æ ‡å‡†å·®ï¼‰
x_normalized = (x - mean) / std
             = [
                 (0.1 - 70.68) / 78.23 = -0.90,
                 (0.3 - 70.68) / 78.23 = -0.90,
                 (98.5 - 70.68) / 78.23 = 0.36,
                 (123.7 - 70.68) / 78.23 = 0.68,
                 (0.2 - 70.68) / 78.23 = -0.90,
                 (201.3 - 70.68) / 78.23 = 1.67
               ]

# æ­¥éª¤3ï¼šç¼©æ”¾å’Œå¹³ç§»ï¼ˆå¯å­¦ä¹ çš„å‚æ•°ï¼‰
# weightå’Œbiasæ˜¯æ¨¡åž‹å­¦ä¹ çš„å‚æ•°
output = x_normalized * weight + bias

# æœ€ç»ˆç»“æžœï¼š
# - å‡å€¼ â‰ˆ 0
# - æ ‡å‡†å·® â‰ˆ 1
# - æ•°å€¼åˆ†å¸ƒåˆç†
```

#### ðŸ“Š æ•ˆæžœå¯¹æ¯”

```
åŽŸå§‹æ•°æ®ï¼š
[0.1, 0.3, 98.5, 123.7, 0.2, 201.3]
æœ€å°å€¼: 0.1  |  æœ€å¤§å€¼: 201.3  |  èŒƒå›´: 201.2
âŒ æ•°å€¼èŒƒå›´å¤ªå¤§ï¼Œä¸ç¨³å®š

æ ‡å‡†åŒ–åŽï¼š
[-0.90, -0.90, 0.36, 0.68, -0.90, 1.67]
æœ€å°å€¼: -0.90  |  æœ€å¤§å€¼: 1.67  |  èŒƒå›´: 2.57
âœ… æ•°å€¼èŒƒå›´åˆç†ï¼Œç¨³å®š
```

#### ðŸ’» ä»£ç å®žçŽ°

```python
class LayerNorm(nn.Module):
    """å±‚å½’ä¸€åŒ–"""
    
    def __init__(self, ndim, bias):
        super().__init__()
        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼ˆåˆå§‹åŒ–ä¸º1ï¼‰
        self.weight = nn.Parameter(torch.ones(ndim))
        # å¯å­¦ä¹ çš„å¹³ç§»å‚æ•°ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None
    
    def forward(self, input):
        # è°ƒç”¨PyTorchçš„layer_normå‡½æ•°
        # 1e-5 æ˜¯é˜²æ­¢é™¤ä»¥0çš„å°å¸¸æ•°
        return F.layer_norm(
            input,
            self.weight.shape,
            self.weight,
            self.bias,
            1e-5
        )
```

#### ðŸŽ¯ LayerNormçš„ä½œç”¨æ€»ç»“

```
ä¸ºä»€ä¹ˆéœ€è¦LayerNormï¼Ÿ

1. æ•°å€¼ç¨³å®š âœ…
   - é˜²æ­¢æ•°å€¼è¿‡å¤§æˆ–è¿‡å°
   - é¿å…æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
   
2. è®­ç»ƒç¨³å®š âœ…
   - å‚æ•°æ›´æ–°æ›´å‡åŒ€
   - æ”¶æ•›æ›´å¿«
   - å¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ çŽ‡
   
3. æ€§èƒ½æå‡ âœ…
   - è®­ç»ƒé€Ÿåº¦æ›´å¿«
   - æœ€ç»ˆæ•ˆæžœæ›´å¥½
   
4. æ·±å±‚ç½‘ç»œå¿…å¤‡ âœ…
   - æ²¡æœ‰LayerNormï¼Œæ·±å±‚ç½‘ç»œå¾ˆéš¾è®­ç»ƒ
   - Transformerå¿…é¡»ä½¿ç”¨LayerNorm
```

#### ðŸ” LayerNorm vs BatchNorm

ä½ å¯èƒ½å¬è¯´è¿‡BatchNormï¼Œå®ƒä»¬æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

```python
BatchNormï¼ˆåœ¨batchç»´åº¦ä¸Šæ ‡å‡†åŒ–ï¼‰ï¼š
  batch_size = 3
  feature_dim = 4
  
  æ•°æ®å½¢çŠ¶: [3, 4]
  [
    [1, 2, 3, 4],      # æ ·æœ¬1
    [5, 6, 7, 8],      # æ ·æœ¬2
    [9, 10, 11, 12]    # æ ·æœ¬3
  ]
  
  æ ‡å‡†åŒ–æ–¹å¼ï¼šå¯¹æ¯ä¸€åˆ—ï¼ˆç‰¹å¾ï¼‰æ ‡å‡†åŒ–
  mean = [5, 6, 7, 8]  # æ¯ä¸€åˆ—çš„å‡å€¼
  âœ… é€‚åˆCNN
  âŒ ä¸é€‚åˆNLPï¼ˆåºåˆ—é•¿åº¦ä¸å›ºå®šï¼‰

LayerNormï¼ˆåœ¨featureç»´åº¦ä¸Šæ ‡å‡†åŒ–ï¼‰ï¼š
  æ•°æ®å½¢çŠ¶: [3, 4]
  [
    [1, 2, 3, 4],      # æ ·æœ¬1
    [5, 6, 7, 8],      # æ ·æœ¬2
    [9, 10, 11, 12]    # æ ·æœ¬3
  ]
  
  æ ‡å‡†åŒ–æ–¹å¼ï¼šå¯¹æ¯ä¸€è¡Œï¼ˆæ ·æœ¬ï¼‰æ ‡å‡†åŒ–
  æ ·æœ¬1: mean=[2.5], std=[1.12]
  æ ·æœ¬2: mean=[6.5], std=[1.12]
  æ ·æœ¬3: mean=[10.5], std=[1.12]
  âœ… é€‚åˆNLP
  âœ… é€‚åˆTransformer
```

**ä¸ºä»€ä¹ˆTransformerç”¨LayerNormï¼Ÿ**
- NLPä¸­åºåˆ—é•¿åº¦ä¸å›ºå®š
- BatchNormä¾èµ–batchç»Ÿè®¡ï¼Œä¸ç¨³å®š
- LayerNormåªä¾èµ–å•ä¸ªæ ·æœ¬ï¼Œç¨³å®š

---

**âœ… LayerNormæ£€æŸ¥ç‚¹**

å­¦å®Œè¿™éƒ¨åˆ†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- [ ] è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦LayerNorm
- [ ] è¯´å‡ºLayerNormçš„ä¸‰ä¸ªæ­¥éª¤
- [ ] ç†è§£LayerNormå¦‚ä½•ç¨³å®šè®­ç»ƒ
- [ ] çŸ¥é“LayerNormå’ŒBatchNormçš„åŒºåˆ«

**ä¸‹ä¸€æ­¥ï¼šå­¦ä¹ æœ€é‡è¦çš„ç»„ä»¶ - Attentionï¼** ðŸš€

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»„ä»¶2 - Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰â­æ ¸å¿ƒâ­

### ðŸ§  æœ€é‡è¦çš„ç»„ä»¶ï¼šSelf-Attention

**è¿™æ˜¯æ•´ä¸ªTransformeræœ€æ ¸å¿ƒã€æœ€é‡è¦ã€æœ€ç²¾å¦™çš„éƒ¨åˆ†ï¼**

ç†è§£äº†Attentionï¼Œä½ å°±ç†è§£äº†çŽ°ä»£AIçš„æ ¸å¿ƒç§˜å¯†ã€‚è®©æˆ‘ä»¬ç”¨æœ€ç®€å•çš„æ–¹å¼è®²æ¸…æ¥šå®ƒã€‚

---

### ðŸŽ¯ ç¬¬ä¸€æ­¥ï¼šä¸ºä»€ä¹ˆéœ€è¦Attentionï¼Ÿ

#### ðŸ’¡ ç”Ÿæ´»åœºæ™¯ï¼šå¼€ä¼šè®¨è®º

æƒ³è±¡ä¸€ä¸ª5äººä¼šè®®ï¼š

```
ä¼šè®®ä¸»é¢˜ï¼š"å¦‚ä½•æå‡äº§å“é”€é‡ï¼Ÿ"

å‚ä¸Žè€…ï¼š
  å¼ ä¸‰ï¼ˆäº§å“ç»ç†ï¼‰ï¼š"æˆ‘ä»¬çš„äº§å“å¾ˆå¥½"
  æŽå››ï¼ˆé”€å”®ç»ç†ï¼‰ï¼š"ä½†æ˜¯ä»·æ ¼å¤ªé«˜äº†"     â† é‡è¦ï¼
  çŽ‹äº”ï¼ˆæŠ€æœ¯ï¼‰ï¼š"æˆ‘ä»¬å¯ä»¥é™ä½Žæˆæœ¬"        â† é‡è¦ï¼
  èµµå…­ï¼ˆHRï¼‰ï¼š"æˆ‘ä»¬å›¢é˜Ÿå¾ˆå›¢ç»“"
  å­™ä¸ƒï¼ˆè´¢åŠ¡ï¼‰ï¼š"é¢„ç®—è¿˜å……è¶³"

ä½ åœ¨æ€»ç»“æ—¶ï¼Œä¼šï¼š
  âœ… é‡ç‚¹å…³æ³¨æŽå››å’ŒçŽ‹äº”çš„å‘è¨€ï¼ˆ80%æ³¨æ„åŠ›ï¼‰
  âŒ åŸºæœ¬å¿½ç•¥èµµå…­å’Œå­™ä¸ƒçš„å‘è¨€ï¼ˆ20%æ³¨æ„åŠ›ï¼‰

è¿™å°±æ˜¯Attentionï¼è‡ªåŠ¨å†³å®š"å…³æ³¨"å“ªäº›ä¿¡æ¯ã€‚
```

#### ðŸ¤” NLPä¸­çš„åŒæ ·é—®é¢˜

```
å¥å­ï¼š"The cat sat on the mat because it was tired."

é—®é¢˜ï¼š"it" æŒ‡ä»£ä»€ä¹ˆï¼Ÿ

æ¨¡åž‹éœ€è¦ï¼š
  1. çœ‹åˆ° "it"
  2. å›žå¤´çœ‹å‰é¢çš„è¯
  3. åˆ¤æ–­å“ªäº›è¯æ˜¯é‡è¦çš„
  4. å†³å®š "it" çš„å«ä¹‰

Attentionæƒé‡ï¼ˆæ¨¡åž‹è‡ªåŠ¨å­¦ä¹ çš„ï¼‰ï¼š
  "The"    â†’ 5%   (ä¸å¤ªç›¸å…³)
  "cat"    â†’ 80%  âœ… é‡ç‚¹å…³æ³¨ï¼
  "sat"    â†’ 5%   (ä¸å¤ªç›¸å…³)
  "on"     â†’ 3%   (ä¸å¤ªç›¸å…³)
  "the"    â†’ 2%   (ä¸å¤ªç›¸å…³)
  "mat"    â†’ 5%   (å¯èƒ½ç›¸å…³)
  
ç»“è®ºï¼š"it" = "cat" (å› ä¸º80%çš„æ³¨æ„åŠ›åœ¨catä¸Š)
```

---

### ðŸŽ¯ ç¬¬äºŒæ­¥ï¼šQueryã€Keyã€Value - ä¸‰å…„å¼Ÿ

Attentionæœºåˆ¶æœ‰ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼Œè®©æˆ‘ä»¬ç”¨æœ€ç®€å•çš„æ¯”å–»ç†è§£å®ƒä»¬ã€‚

#### ðŸ“š æ¯”å–»1ï¼šå›¾ä¹¦é¦†æ£€ç´¢

```
åœºæ™¯ï¼šä½ åŽ»å›¾ä¹¦é¦†æ‰¾ä¹¦

Queryï¼ˆæŸ¥è¯¢ï¼‰= "æˆ‘è¦æ‰¾å…³äºŽæ·±åº¦å­¦ä¹ çš„ä¹¦"
  â†“ ä½ çš„éœ€æ±‚
  
Keyï¼ˆç´¢å¼•ï¼‰= æ¯æœ¬ä¹¦çš„æ ‡ç­¾/å…³é”®è¯
  ä¹¦1: ã€æ·±åº¦å­¦ä¹ ã€‘ã€ç¥žç»ç½‘ç»œã€‘ã€AIã€‘
  ä¹¦2: ã€åšé¥­ã€‘ã€é£Ÿè°±ã€‘ã€çƒ¹é¥ªã€‘
  ä¹¦3: ã€æœºå™¨å­¦ä¹ ã€‘ã€ç®—æ³•ã€‘
  ä¹¦4: ã€æ·±åº¦å­¦ä¹ ã€‘ã€PyTorchã€‘
  
  â†“ åŒ¹é…è¿‡ç¨‹
  
åŒ¹é…åº¦ï¼ˆAttentionåˆ†æ•°ï¼‰ï¼š
  ä¹¦1: 0.45  âœ… å®Œå…¨åŒ¹é…ï¼
  ä¹¦2: 0.00  âŒ ä¸ç›¸å…³
  ä¹¦3: 0.15  âš ï¸ éƒ¨åˆ†ç›¸å…³
  ä¹¦4: 0.40  âœ… å®Œå…¨åŒ¹é…ï¼
  
  â†“ åŠ æƒç»„åˆ
  
Valueï¼ˆå†…å®¹ï¼‰= ä¹¦çš„å®žé™…å†…å®¹
  è¾“å‡º = 0.45 Ã— ä¹¦1å†…å®¹ + 0.15 Ã— ä¹¦3å†…å®¹ + 0.40 Ã— ä¹¦4å†…å®¹
  
æœ€ç»ˆï¼šä½ å¾—åˆ°äº†å…³äºŽæ·±åº¦å­¦ä¹ çš„ç»¼åˆçŸ¥è¯†
```

#### ðŸ” æ¯”å–»2ï¼šæœç´¢å¼•æ“Ž

```
Query = ä½ åœ¨æœç´¢æ¡†è¾“å…¥çš„å…³é”®è¯
  "Transformeræ¨¡åž‹åŽŸç†"
  
Key = æ¯ä¸ªç½‘é¡µçš„å…³é”®è¯
  ç½‘é¡µ1: Transformer, æ³¨æ„åŠ›, æ·±åº¦å­¦ä¹   â† åŒ¹é…åº¦é«˜ï¼
  ç½‘é¡µ2: ç¾Žé£Ÿ, é¤åŽ…, é¢„è®¢            â† åŒ¹é…åº¦ä½Ž
  ç½‘é¡µ3: ç¥žç»ç½‘ç»œ, NLP, AI          â† åŒ¹é…åº¦ä¸­ç­‰
  
Attentionåˆ†æ•° = ç›¸å…³æ€§æŽ’å
  ç½‘é¡µ1: 95åˆ†
  ç½‘é¡µ2: 5åˆ†
  ç½‘é¡µ3: 60åˆ†
  
Value = ç½‘é¡µçš„å®žé™…å†…å®¹
  
æœ€ç»ˆç»“æžœ = 
  0.70 Ã— ç½‘é¡µ1å†…å®¹ +   â† æœ€ç›¸å…³ï¼Œæƒé‡æœ€å¤§
  0.25 Ã— ç½‘é¡µ3å†…å®¹ +   â† éƒ¨åˆ†ç›¸å…³
  0.05 Ã— ç½‘é¡µ2å†…å®¹     â† åŸºæœ¬æ— å…³
```

#### ðŸ’¡ Self-Attentionä¸­çš„Qã€Kã€V

```python
åœ¨GPTä¸­ï¼š

Queryï¼ˆæˆ‘æƒ³çŸ¥é“ä»€ä¹ˆï¼‰ï¼š
  å½“å‰è¯çš„"é—®é¢˜"
  ä¾‹å¦‚ï¼š"it" æƒ³çŸ¥é“ â†’ "æˆ‘æŒ‡ä»£è°ï¼Ÿ"
  
Keyï¼ˆè°èƒ½å›žç­”æˆ‘ï¼‰ï¼š
  å…¶ä»–è¯çš„"æ ‡ç­¾"
  "cat" çš„æ ‡ç­¾ â†’ "æˆ‘æ˜¯ä¸€ä¸ªåŠ¨ç‰©ï¼Œä¼šç´¯"
  "mat" çš„æ ‡ç­¾ â†’ "æˆ‘æ˜¯ä¸€ä¸ªç‰©ä½“ï¼Œä¸ä¼šç´¯"
  
Valueï¼ˆå…·ä½“ç­”æ¡ˆæ˜¯ä»€ä¹ˆï¼‰ï¼š
  å…¶ä»–è¯çš„"å†…å®¹"
  "cat" çš„å†…å®¹ â†’ [0.56, 0.12, ..., 0.89]  (768ç»´å‘é‡)
  "mat" çš„å†…å®¹ â†’ [0.23, -0.45, ..., 0.12]

è®¡ç®—è¿‡ç¨‹ï¼š
  1. "it"çš„Query å’Œ æ‰€æœ‰è¯çš„Key è®¡ç®—ç›¸ä¼¼åº¦
  2. "cat"çš„Key åŒ¹é…åº¦é«˜ â†’ æƒé‡0.8
  3. "mat"çš„Key åŒ¹é…åº¦ä½Ž â†’ æƒé‡0.05
  4. è¾“å‡º = 0.8 Ã— "cat"çš„Value + 0.05 Ã— "mat"çš„Value + ...
  5. ç»“æžœï¼š"it"èŽ·å¾—äº†ä¸»è¦æ¥è‡ª"cat"çš„ä¿¡æ¯
```

---

### ðŸŽ¯ ç¬¬ä¸‰æ­¥ï¼šAttentionçš„æ•°å­¦å…¬å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰

**ä¸è¦è¢«å“åˆ°ï¼æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ã€‚**

#### ðŸ“ æ ¸å¿ƒå…¬å¼

```python
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd) Â· V

çœ‹èµ·æ¥å“äººï¼Ÿæ‹†å¼€å°±å¾ˆç®€å•ï¼š
```

#### æ­¥éª¤1ï¼šè®¡ç®—ç›¸ä¼¼åº¦

```python
# Qå’ŒKåšç‚¹ç§¯ï¼ˆdot productï¼‰
scores = Q Â· K^T

ä¾‹å­ï¼ˆç®€åŒ–åˆ°2ç»´ï¼‰ï¼š
  Query = [1, 2]      # "it"çš„query
  Key1 = [1, 2]       # "cat"çš„key
  Key2 = [0, 1]       # "mat"çš„key
  
  score1 = Query Â· Key1 = 1Ã—1 + 2Ã—2 = 5  âœ… é«˜åˆ†ï¼
  score2 = Query Â· Key2 = 1Ã—0 + 2Ã—1 = 2  âš ï¸ ä½Žåˆ†
  
ç»“è®ºï¼šQueryå’ŒKey1æ›´ç›¸ä¼¼
```

#### æ­¥éª¤2ï¼šç¼©æ”¾ï¼ˆé˜²æ­¢æ•°å€¼è¿‡å¤§ï¼‰

```python
# é™¤ä»¥âˆšdï¼ˆdæ˜¯ç»´åº¦ï¼‰
scaled_scores = scores / âˆšd

ä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼Ÿ
  å‡è®¾d=768ï¼ˆGPTçš„ç»´åº¦ï¼‰
  åŽŸå§‹scoreå¯èƒ½= 100, 200, 300  (å¤ªå¤§äº†ï¼)
  ç¼©æ”¾åŽ = 100/âˆš768 â‰ˆ 3.6, 7.2, 10.8  (åˆç†)
  
å¥½å¤„ï¼š
  - é˜²æ­¢softmaxé¥±å’Œ
  - æ¢¯åº¦æ›´ç¨³å®š
  - è®­ç»ƒæ›´å®¹æ˜“
```

#### æ­¥éª¤3ï¼šSoftmaxï¼ˆè½¬æ¢æˆæ¦‚çŽ‡ï¼‰

```python
# æŠŠåˆ†æ•°è½¬æ¢æˆæ¦‚çŽ‡ï¼ˆå’Œä¸º1ï¼‰
weights = softmax(scaled_scores)

ä¾‹å­ï¼š
  scaled_scores = [3.6, 7.2, 10.8]
  
  # softmaxè®¡ç®—
  exp_scores = [e^3.6, e^7.2, e^10.8]
             = [36.6, 1339.4, 49020.8]
  
  sum = 36.6 + 1339.4 + 49020.8 = 50396.8
  
  weights = [36.6/50396.8, 1339.4/50396.8, 49020.8/50396.8]
          = [0.0007, 0.0266, 0.9727]
          â‰ˆ [0%, 3%, 97%]
          
ç»“è®ºï¼š97%çš„æ³¨æ„åŠ›åœ¨ç¬¬ä¸‰ä¸ªè¯ä¸Šï¼
```

#### æ­¥éª¤4ï¼šåŠ æƒæ±‚å’ŒValue

```python
# æ ¹æ®æƒé‡ç»„åˆValue
output = weights Â· V

ä¾‹å­ï¼š
  weights = [0.0007, 0.0266, 0.9727]
  Value1 = [1, 2, 3]
  Value2 = [4, 5, 6]
  Value3 = [7, 8, 9]
  
  output = 0.0007Ã—[1,2,3] + 0.0266Ã—[4,5,6] + 0.9727Ã—[7,8,9]
        â‰ˆ [0, 0, 0] + [0.1, 0.13, 0.16] + [6.8, 7.8, 8.75]
        â‰ˆ [6.9, 7.93, 8.91]
        
ç»“è®ºï¼šè¾“å‡ºä¸»è¦æ¥è‡ªValue3ï¼ˆå› ä¸ºæƒé‡æœ€å¤§ï¼‰
```

---

### ðŸŽ¯ ç¬¬å››æ­¥ï¼šå®Œæ•´ç¤ºä¾‹ï¼ˆå…·ä½“æ•°å­—ï¼‰

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªçœŸå®žçš„ä¾‹å­èµ°ä¸€éå®Œæ•´æµç¨‹ã€‚

#### ðŸ“ è¾“å…¥å¥å­

```python
å¥å­ï¼š"cat sat on"
Tokenæ•°ï¼š3ä¸ª
ç»´åº¦ï¼šå‡è®¾ç®€åŒ–ä¸º4ç»´ï¼ˆå®žé™…æ˜¯768ç»´ï¼‰
```

#### Step 1: Token Embedding

```python
# æ¯ä¸ªè¯è½¬æ¢æˆå‘é‡
cat = [1.0, 0.5, 0.3, 0.2]
sat = [0.2, 1.0, 0.4, 0.1]
on  = [0.3, 0.2, 1.0, 0.5]

# å½¢çŠ¶ï¼š[3, 4] (3ä¸ªtokenï¼Œæ¯ä¸ª4ç»´)
X = [
  [1.0, 0.5, 0.3, 0.2],  # cat
  [0.2, 1.0, 0.4, 0.1],  # sat
  [0.3, 0.2, 1.0, 0.5],  # on
]
```

#### Step 2: ç”ŸæˆQã€Kã€V

```python
# é€šè¿‡çº¿æ€§å˜æ¢ç”ŸæˆQã€Kã€V
# å®žé™…ä¸Šæ˜¯çŸ©é˜µä¹˜æ³•ï¼šX @ W_q, X @ W_k, X @ W_v

Q = [
  [0.8, 0.6, 0.4, 0.2],  # catçš„query
  [0.3, 0.9, 0.5, 0.3],  # satçš„query
  [0.4, 0.3, 0.8, 0.6],  # onçš„query
]

K = [
  [0.9, 0.7, 0.5, 0.3],  # catçš„key
  [0.4, 0.8, 0.6, 0.2],  # satçš„key
  [0.5, 0.4, 0.9, 0.7],  # onçš„key
]

V = [
  [1.2, 0.8, 0.6, 0.4],  # catçš„value
  [0.5, 1.3, 0.7, 0.3],  # satçš„value
  [0.6, 0.5, 1.4, 0.9],  # onçš„value
]
```

#### Step 3: è®¡ç®—Attentionåˆ†æ•°

```python
# Q @ K^T (æ¯ä¸ªqueryå’Œæ‰€æœ‰keyè®¡ç®—ç›¸ä¼¼åº¦)
scores = Q @ K^T

è®¡ç®— sat çš„attentionåˆ†æ•°ï¼ˆç¬¬2è¡Œï¼‰ï¼š
  sat_query = [0.3, 0.9, 0.5, 0.3]
  
  score_with_cat = sat_query Â· cat_key
                 = 0.3Ã—0.9 + 0.9Ã—0.7 + 0.5Ã—0.5 + 0.3Ã—0.3
                 = 0.27 + 0.63 + 0.25 + 0.09
                 = 1.24
                 
  score_with_sat = sat_query Â· sat_key
                 = 0.3Ã—0.4 + 0.9Ã—0.8 + 0.5Ã—0.6 + 0.3Ã—0.2
                 = 0.12 + 0.72 + 0.30 + 0.06
                 = 1.20
                 
  score_with_on = sat_query Â· on_key
                = 0.3Ã—0.5 + 0.9Ã—0.4 + 0.5Ã—0.9 + 0.3Ã—0.7
                = 0.15 + 0.36 + 0.45 + 0.21
                = 1.17

æ‰€æœ‰åˆ†æ•°çŸ©é˜µï¼š
scores = [
  [1.32, 1.15, 1.28],  # catå…³æ³¨å„è¯çš„åˆ†æ•°
  [1.24, 1.20, 1.17],  # satå…³æ³¨å„è¯çš„åˆ†æ•°
  [1.30, 1.22, 1.45],  # onå…³æ³¨å„è¯çš„åˆ†æ•°
]
```

#### Step 4: ç¼©æ”¾

```python
# é™¤ä»¥âˆšdï¼ˆd=4ï¼Œæ‰€ä»¥âˆš4=2ï¼‰
scaled_scores = scores / 2

scaled_scores = [
  [0.66, 0.58, 0.64],
  [0.62, 0.60, 0.59],
  [0.65, 0.61, 0.73],
]
```

#### Step 5: åº”ç”¨Causal Maskï¼ˆåªçœ‹è¿‡åŽ»ï¼‰

```python
# å› æžœmaskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
mask = [
  [1, 0, 0],  # catåªèƒ½çœ‹cat
  [1, 1, 0],  # satèƒ½çœ‹catå’Œsat
  [1, 1, 1],  # onèƒ½çœ‹æ‰€æœ‰
]

# æŠŠmask=0çš„ä½ç½®è®¾ä¸º-inf
masked_scores = [
  [0.66,  -inf,  -inf],  # cat
  [0.62,  0.60,  -inf],  # sat
  [0.65,  0.61,  0.73],  # on
]
```

#### Step 6: Softmax

```python
# å¯¹æ¯ä¸€è¡Œåšsoftmax
attention_weights = softmax(masked_scores)

# catè¡Œï¼ˆåªèƒ½çœ‹è‡ªå·±ï¼‰
cat_weights = [1.0, 0.0, 0.0]  # 100%çœ‹cat

# satè¡Œï¼ˆèƒ½çœ‹catå’Œsatï¼‰
sat_weights = softmax([0.62, 0.60])
            = [0.51, 0.49]  # 51%çœ‹catï¼Œ49%çœ‹sat
            åŠ ä¸Šmask = [0.51, 0.49, 0.0]

# onè¡Œï¼ˆèƒ½çœ‹æ‰€æœ‰ï¼‰
on_weights = softmax([0.65, 0.61, 0.73])
           = [0.32, 0.30, 0.38]  # åˆ†æ•£æ³¨æ„åŠ›

æœ€ç»ˆæƒé‡çŸ©é˜µï¼š
attention_weights = [
  [1.00, 0.00, 0.00],  # cat
  [0.51, 0.49, 0.00],  # sat
  [0.32, 0.30, 0.38],  # on
]
```

#### Step 7: åŠ æƒæ±‚å’ŒValue

```python
# æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè¯çš„è¾“å‡º
output = attention_weights @ V

# catçš„è¾“å‡º
cat_output = 1.00 Ã— cat_value + 0.00 Ã— sat_value + 0.00 Ã— on_value
           = 1.00 Ã— [1.2, 0.8, 0.6, 0.4]
           = [1.2, 0.8, 0.6, 0.4]
           # catåªçœ‹è‡ªå·±ï¼Œæ‰€ä»¥è¾“å‡º=è‡ªå·±çš„value

# satçš„è¾“å‡º
sat_output = 0.51 Ã— cat_value + 0.49 Ã— sat_value + 0.00 Ã— on_value
           = 0.51 Ã— [1.2, 0.8, 0.6, 0.4] + 0.49 Ã— [0.5, 1.3, 0.7, 0.3]
           = [0.61, 0.41, 0.31, 0.20] + [0.25, 0.64, 0.34, 0.15]
           = [0.86, 1.05, 0.65, 0.35]
           # satç»¼åˆäº†catå’Œè‡ªå·±çš„ä¿¡æ¯

# onçš„è¾“å‡º
on_output = 0.32Ã—cat_value + 0.30Ã—sat_value + 0.38Ã—on_value
          = [0.38, 0.26, 0.19, 0.13] + [0.15, 0.39, 0.21, 0.09] + [0.23, 0.19, 0.53, 0.34]
          = [0.76, 0.84, 0.93, 0.56]
          # onç»¼åˆäº†æ‰€æœ‰è¯çš„ä¿¡æ¯

æœ€ç»ˆè¾“å‡ºï¼š
output = [
  [1.2, 0.8, 0.6, 0.4],      # catï¼ˆä¸»è¦æ˜¯è‡ªå·±ï¼‰
  [0.86, 1.05, 0.65, 0.35],  # satï¼ˆcat+satï¼‰
  [0.76, 0.84, 0.93, 0.56],  # onï¼ˆæ‰€æœ‰è¯ï¼‰
]
```

#### ðŸ“Š è§‚å¯Ÿç»“æžœ

```
è§‚å¯Ÿ1ï¼šä¿¡æ¯æµåŠ¨
  cat â†’ åªåŒ…å«è‡ªå·±çš„ä¿¡æ¯
  sat â†’ ç»¼åˆäº†cat(51%)å’Œè‡ªå·±(49%)
  on  â†’ ç»¼åˆäº†æ‰€æœ‰è¯çš„ä¿¡æ¯
  
  â†’ åŽé¢çš„è¯èƒ½çœ‹åˆ°æ›´å¤šä¿¡æ¯ï¼

è§‚å¯Ÿ2ï¼šCausal Maskçš„ä½œç”¨
  catä¸èƒ½çœ‹satå’Œonï¼ˆæœªæ¥ï¼‰
  satä¸èƒ½çœ‹onï¼ˆæœªæ¥ï¼‰
  onèƒ½çœ‹æ‰€æœ‰ï¼ˆéƒ½æ˜¯è¿‡åŽ»ï¼‰
  
  â†’ ä¿è¯äº†å› æžœæ€§ï¼ˆä¸èƒ½å·çœ‹æœªæ¥ï¼‰

è§‚å¯Ÿ3ï¼šAttentionçš„æœ¬è´¨
  æ¯ä¸ªè¯çš„è¾“å‡º = å…¶ä»–è¯çš„åŠ æƒç»„åˆ
  æƒé‡è‡ªåŠ¨å­¦ä¹ 
  æ•æ‰äº†è¯ä¹‹é—´çš„å…³ç³»
```

---

### ðŸŽ¯ ç¬¬äº”æ­¥ï¼šMulti-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

#### ðŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªå¤´ï¼Ÿ

**ç”Ÿæ´»æ¯”å–»ï¼šå¤šè§’åº¦åˆ†æž**

```
å•ä¸ªå¤´ = å•ä¸€è§†è§’ï¼š
  åªä»Žè¯­æ³•è§’åº¦åˆ†æžå¥å­
  "The cat sat on the mat"
  â†’ çœ‹åˆ°ï¼šä¸»è¯­ã€è°“è¯­ã€å®¾è¯­
  
å¤šä¸ªå¤´ = å¤šé‡è§†è§’ï¼š
  å¤´1ï¼šè¯­æ³•è§’åº¦ï¼ˆä¸»è°“å®¾ï¼‰
  å¤´2ï¼šè¯­ä¹‰è§’åº¦ï¼ˆåŠ¨ç‰©ã€åŠ¨ä½œã€ç‰©ä½“ï¼‰
  å¤´3ï¼šæƒ…æ„Ÿè§’åº¦ï¼ˆä¸­æ€§æè¿°ï¼‰
  å¤´4ï¼šæ—¶æ€è§’åº¦ï¼ˆè¿‡åŽ»æ—¶ï¼‰
  å¤´5ï¼šè·ç¦»å…³ç³»ï¼ˆcatå’Œmatçš„ç©ºé—´å…³ç³»ï¼‰
  å¤´6ï¼š...
  
æœ€åŽç»¼åˆæ‰€æœ‰è§†è§’ â†’ å®Œæ•´ç†è§£ï¼
```

#### ðŸ”§ Multi-Headçš„å®žçŽ°

```python
# å‡è®¾é…ç½®
n_embd = 768    # æ€»ç»´åº¦
n_head = 12     # 12ä¸ªå¤´
head_dim = 768 / 12 = 64  # æ¯ä¸ªå¤´64ç»´

# æ­¥éª¤1ï¼šæŠŠ768ç»´åˆ†æˆ12ä»½
[768ç»´å‘é‡] 
  â†“ åˆ†å‰²
[64ç»´] [64ç»´] [64ç»´] ... [64ç»´]  (12ä¸ª)
 å¤´1    å¤´2    å¤´3    ...  å¤´12

# æ­¥éª¤2ï¼šæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—attention
å¤´1: Attention([64ç»´Q], [64ç»´K], [64ç»´V])  â†’ [64ç»´è¾“å‡º]
å¤´2: Attention([64ç»´Q], [64ç»´K], [64ç»´V])  â†’ [64ç»´è¾“å‡º]
...
å¤´12: Attention([64ç»´Q], [64ç»´K], [64ç»´V]) â†’ [64ç»´è¾“å‡º]

# æ­¥éª¤3ï¼šæŠŠæ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æŽ¥èµ·æ¥
è¾“å‡º = Concat(å¤´1, å¤´2, ..., å¤´12)
     = [64Ã—12 = 768ç»´]

# æ­¥éª¤4ï¼šé€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚
æœ€ç»ˆè¾“å‡º = Linear(è¾“å‡º)
```

#### ðŸ“Š ä¸åŒå¤´å­¦åˆ°ä»€ä¹ˆï¼Ÿ

```
å®žé™…è®­ç»ƒåŽï¼Œä¸åŒçš„å¤´ä¼šä¸“æ³¨äºŽä¸åŒçš„æ¨¡å¼ï¼š

å¤´1ï¼ˆä½ç½®å…³ç³»ï¼‰ï¼š
  å…³æ³¨ç›¸é‚»çš„è¯
  "quick brown" â†’ é«˜æƒé‡

å¤´2ï¼ˆè¯­æ³•å…³ç³»ï¼‰ï¼š
  å…³æ³¨ä¸»è°“å®¾
  "cat" â†’ "sat" â†’ é«˜æƒé‡

å¤´3ï¼ˆé•¿è·ç¦»ä¾èµ–ï¼‰ï¼š
  å…³æ³¨è¿œè·ç¦»çš„ç›¸å…³è¯
  "it" â†’ "cat"ï¼ˆè·¨è¶Šå¤šä¸ªè¯ï¼‰â†’ é«˜æƒé‡

å¤´4-12ï¼š
  ... å„æœ‰ä¸“é•¿
```

---

### ðŸŽ¯ ç¬¬å…­æ­¥ï¼šä»£ç å®žçŽ°è¯¦è§£

#### ðŸ’» CausalSelfAttentionç±»

```python
class CausalSelfAttention(nn.Module):
    """å› æžœè‡ªæ³¨æ„åŠ›"""
    
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        
        # ä¸€æ¬¡æ€§ç”ŸæˆQã€Kã€Vï¼ˆæ•ˆçŽ‡ä¼˜åŒ–ï¼‰
        # è¾“å…¥768ç»´ â†’ è¾“å‡º768Ã—3=2304ç»´ï¼ˆQã€Kã€Vå„768ç»´ï¼‰
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        
        # Dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        
        # ä¿å­˜é…ç½®
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash Attention
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        
        if not self.flash:
            # å¦‚æžœä¸æ”¯æŒFlash Attentionï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºmask
            # å› æžœmaskï¼šä¸‹ä¸‰è§’çŸ©é˜µ
            self.register_buffer("bias", 
                torch.tril(torch.ones(config.block_size, config.block_size))
                .view(1, 1, config.block_size, config.block_size))
    
    def forward(self, x):
        B, T, C = x.size()  # batch_size, seq_len, n_embd
        
        # æ­¥éª¤1ï¼šç”ŸæˆQã€Kã€V
        qkv = self.c_attn(x)  # [B, T, 3*C]
        q, k, v = qkv.split(self.n_embd, dim=2)  # å„è‡ª [B, T, C]
        
        # æ­¥éª¤2ï¼šé‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        # [B, T, C] â†’ [B, T, n_head, head_dim] â†’ [B, n_head, T, head_dim]
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        
        # æ­¥éª¤3ï¼šè®¡ç®—attention
        if self.flash:
            # ä½¿ç”¨Flash Attentionï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰
            y = F.scaled_dot_product_attention(
                q, k, v, 
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=True  # å› æžœmask
            )
        else:
            # æ‰‹åŠ¨å®žçŽ°ï¼ˆæ ‡å‡†ç‰ˆæœ¬ï¼‰
            # 3.1: Q @ K^T
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            
            # 3.2: åº”ç”¨causal mask
            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
            
            # 3.3: Softmax
            att = F.softmax(att, dim=-1)
            
            # 3.4: Dropout
            att = self.attn_dropout(att)
            
            # 3.5: @ V
            y = att @ v
        
        # æ­¥éª¤4ï¼šåˆå¹¶å¤šå¤´
        # [B, n_head, T, head_dim] â†’ [B, T, n_head, head_dim] â†’ [B, T, C]
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        
        # æ­¥éª¤5ï¼šè¾“å‡ºæŠ•å½±
        y = self.resid_dropout(self.c_proj(y))
        
        return y
```

#### ðŸ” ä»£ç å…³é”®ç‚¹è§£é‡Š

```python
å…³é”®ç‚¹1ï¼šä¸ºä»€ä¹ˆä¸€æ¬¡æ€§ç”ŸæˆQKVï¼Ÿ
  # åˆ†å¼€ç”Ÿæˆï¼ˆæ…¢ï¼‰
  q = self.q_proj(x)  # ä¸€æ¬¡çŸ©é˜µä¹˜æ³•
  k = self.k_proj(x)  # ä¸€æ¬¡çŸ©é˜µä¹˜æ³•
  v = self.v_proj(x)  # ä¸€æ¬¡çŸ©é˜µä¹˜æ³•
  
  # åˆå¹¶ç”Ÿæˆï¼ˆå¿«ï¼‰âœ…
  qkv = self.c_attn(x)  # ä¸€æ¬¡çŸ©é˜µä¹˜æ³•
  q, k, v = qkv.split(...)  # åªæ˜¯åˆ‡åˆ†ï¼Œå¾ˆå¿«
  
  å¥½å¤„ï¼šå‡å°‘kernel launchæ¬¡æ•°ï¼Œæ›´å¿«ï¼

å…³é”®ç‚¹2ï¼šä¸ºä»€ä¹ˆè¦transposeï¼Ÿ
  # é‡å¡‘å‰
  [B, T, C]  # batch, seq, channels
  
  # é‡å¡‘åŽ
  [B, n_head, T, head_dim]
  
  ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºè¦å¯¹æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—attention
  transposeè®©ç»´åº¦æŽ’åˆ—æ›´é€‚åˆçŸ©é˜µè¿ç®—

å…³é”®ç‚¹3ï¼šFlash Attentionçš„ä¼˜åŠ¿
  æ ‡å‡†å®žçŽ°ï¼š
    - æ˜¾å­˜ O(NÂ²)
    - æ…¢
  
  Flash Attentionï¼š
    - æ˜¾å­˜ O(N)
    - å¿« 2-4å€
    - ç»“æžœå®Œå…¨ä¸€æ ·ï¼
  
  åŽŸç†ï¼šä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼

å…³é”®ç‚¹4ï¼šä¸ºä»€ä¹ˆç”¨-infä½œä¸ºmaskï¼Ÿ
  scores = [0.5, 0.3, -inf]
  
  softmaxåŽï¼š
  e^0.5 / (e^0.5 + e^0.3 + e^-inf)
  = e^0.5 / (e^0.5 + e^0.3 + 0)
  
  e^-inf â‰ˆ 0ï¼Œè¢«å®Œå…¨å¿½ç•¥ï¼
```

---

### ðŸŽ¯ ç¬¬ä¸ƒæ­¥ï¼šAttentionæ€»ç»“

#### âœ… æ ¸å¿ƒè¦ç‚¹å›žé¡¾

```
Attentionçš„æœ¬è´¨ï¼š
  æ ¹æ®ç›¸å…³æ€§ï¼Œè‡ªåŠ¨å†³å®šå…³æ³¨å“ªäº›ä¿¡æ¯
  
ä¸‰ä¸ªå…³é”®æ¦‚å¿µï¼š
  Queryï¼šæˆ‘æƒ³çŸ¥é“ä»€ä¹ˆ
  Keyï¼šè°èƒ½å›žç­”æˆ‘
  Valueï¼šå…·ä½“ç­”æ¡ˆæ˜¯ä»€ä¹ˆ
  
å››ä¸ªå…³é”®æ­¥éª¤ï¼š
  1. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆQÂ·K^Tï¼‰
  2. ç¼©æ”¾ï¼ˆ/ âˆšdï¼‰
  3. Softmaxï¼ˆè½¬æ¦‚çŽ‡ï¼‰
  4. åŠ æƒæ±‚å’Œï¼ˆ@ Vï¼‰
  
Multi-Headçš„ä½œç”¨ï¼š
  ä»Žå¤šä¸ªè§’åº¦ç†è§£æ–‡æœ¬
  ä¸åŒå¤´å­¦ä¹ ä¸åŒæ¨¡å¼
  
Causal Maskçš„ä½œç”¨ï¼š
  åªèƒ½çœ‹è¿‡åŽ»ï¼Œä¸èƒ½çœ‹æœªæ¥
  ä¿è¯ç”Ÿæˆçš„åˆæ³•æ€§
```

#### ðŸ“Š Attentionçš„è®¡ç®—å¤æ‚åº¦

```python
å‡è®¾ï¼š
  åºåˆ—é•¿åº¦ = N
  åµŒå…¥ç»´åº¦ = d
  
è®¡ç®—é‡ï¼š
  Q @ K^T:    O(NÂ² Ã— d)  â† ç“¶é¢ˆï¼
  Softmax:    O(NÂ²)
  @ V:        O(NÂ² Ã— d)
  
  æ€»è®¡ï¼šO(NÂ² Ã— d)
  
æ˜¾å­˜å ç”¨ï¼š
  AttentionçŸ©é˜µ: O(NÂ²)  â† ç“¶é¢ˆï¼
  
ä¸ºä»€ä¹ˆæ˜¯ç“¶é¢ˆï¼Ÿ
  N=1024:   1M å…ƒç´ 
  N=2048:   4M å…ƒç´ ï¼ˆ4å€ï¼ï¼‰
  N=4096:   16M å…ƒç´ ï¼ˆ16å€ï¼ï¼‰
  
  â†’ åºåˆ—é•¿åº¦ç¿»å€ï¼Œæ˜¾å­˜Ã—4ï¼
  â†’ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé•¿æ–‡æœ¬å›°éš¾
```

---

**âœ… Attentionæ£€æŸ¥ç‚¹**

å­¦å®Œè¿™éƒ¨åˆ†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- [ ] ç”¨ç”Ÿæ´»æ¯”å–»è§£é‡ŠAttentionæ˜¯ä»€ä¹ˆ
- [ ] è¯´å‡ºQã€Kã€Vå„è‡ªçš„ä½œç”¨
- [ ] ç†è§£Attentionçš„4ä¸ªè®¡ç®—æ­¥éª¤
- [ ] çŸ¥é“Multi-Head Attentionä¸ºä»€ä¹ˆæœ‰ç”¨
- [ ] ç†è§£Causal Maskçš„ä½œç”¨
- [ ] çŸ¥é“Attentionçš„è®¡ç®—ç“¶é¢ˆåœ¨å“ªé‡Œ

**æ­å–œï¼ä½ å·²ç»ç†è§£äº†Transformeræœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼** ðŸŽ‰

**ä¸‹ä¸€æ­¥ï¼šå­¦ä¹ å…¶ä»–ç»„ä»¶** â†’

---

## ç¬¬å››éƒ¨åˆ†ï¼šç»„ä»¶3 - MLPï¼ˆå‰é¦ˆç¥žç»ç½‘ç»œï¼‰

### ðŸ”§ ç®€å•ä½†é‡è¦çš„ç»„ä»¶ï¼šMLP

Attentionä¹‹åŽï¼Œæˆ‘ä»¬æ¥çœ‹å¦ä¸€ä¸ªé‡è¦ç»„ä»¶ï¼šMLPï¼ˆMulti-Layer Perceptronï¼Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰ã€‚

#### ðŸ’¡ ç›´è§‚ç†è§£ï¼šä¸ºä»€ä¹ˆéœ€è¦MLPï¼Ÿ

**é—®é¢˜ï¼šAttentionçš„å±€é™**

```
Attentionåšä»€ä¹ˆï¼Ÿ
  æŠŠä¸åŒè¯çš„ä¿¡æ¯"æ··åˆ"åœ¨ä¸€èµ·
  ä¾‹å¦‚ï¼š
    "it" çš„è¾“å‡º = 0.8Ã—cat + 0.1Ã—mat + 0.1Ã—å…¶ä»–
  
Attentionä¸åšä»€ä¹ˆï¼Ÿ
  âŒ ä¸æ”¹å˜ä¿¡æ¯çš„"æ€§è´¨"
  âŒ ä¸æå–æ–°çš„ç‰¹å¾
  âŒ åªæ˜¯é‡æ–°ç»„åˆå·²æœ‰ä¿¡æ¯
  
å°±åƒï¼š
  Attention = è°ƒé…’å¸ˆ
  æŠŠä¸åŒçš„é…’æ··åˆåœ¨ä¸€èµ·
  ä½†ä¸èƒ½æŠŠè‘¡è„é…’å˜æˆå¨å£«å¿Œ
```

**MLPçš„ä½œç”¨ï¼šç‰¹å¾å˜æ¢**

```
MLPåšä»€ä¹ˆï¼Ÿ
  âœ… æ”¹å˜ä¿¡æ¯çš„æ€§è´¨
  âœ… æå–æ›´é«˜å±‚æ¬¡çš„ç‰¹å¾
  âœ… éžçº¿æ€§å˜æ¢
  
å°±åƒï¼š
  MLP = åŒ–å­¦ååº”
  ä¸åªæ˜¯æ··åˆï¼Œè€Œæ˜¯äº§ç”Ÿæ–°ç‰©è´¨
  
ä¾‹å­ï¼š
  è¾“å…¥ï¼š[0.5, 0.3, 0.8]  (åŽŸå§‹ç‰¹å¾)
  MLPå¤„ç†
  è¾“å‡ºï¼š[0.9, 0.1, 0.6]  (æ–°ç‰¹å¾)
  
  å¯èƒ½å­¦åˆ°äº†ï¼š
  - è¿™æ˜¯ä¸€ä¸ªåŠ¨ç‰©è¯
  - è¿™æ˜¯è¿‡åŽ»æ—¶æ€
  - è¿™æ˜¯å…·ä½“åè¯
  ... æ›´æŠ½è±¡çš„ç‰¹å¾
```

#### ðŸ—ï¸ MLPçš„ç»“æž„

**ä¸¤å±‚å…¨è¿žæŽ¥ç½‘ç»œï¼š**

```python
è¾“å…¥ï¼š768ç»´
  â†“
ç¬¬ä¸€å±‚ï¼šæ‰©å±•åˆ° 768Ã—4 = 3072ç»´
  â†“
GELUæ¿€æ´»å‡½æ•°ï¼ˆéžçº¿æ€§ï¼‰
  â†“
ç¬¬äºŒå±‚ï¼šåŽ‹ç¼©å›ž 768ç»´
  â†“
è¾“å‡ºï¼š768ç»´

ä¸ºä»€ä¹ˆå…ˆæ‰©å±•å†åŽ‹ç¼©ï¼Ÿ
  æ‰©å±•ï¼šæä¾›æ›´å¤§çš„"æ€è€ƒç©ºé—´"
  åŽ‹ç¼©ï¼šæå–æœ€é‡è¦çš„ç‰¹å¾
  
å°±åƒï¼š
  å¸æ°”ï¼ˆæ‰©å±•ï¼‰â†’ æ€è€ƒ â†’ å‘¼æ°”ï¼ˆåŽ‹ç¼©ï¼‰
```

#### ðŸ“ å…·ä½“æ•°å€¼ç¤ºä¾‹

```python
# è¾“å…¥ï¼ˆç®€åŒ–ä¸º4ç»´ï¼‰
x = [0.5, 0.3, 0.8, 0.2]

# ç¬¬ä¸€å±‚ï¼šæ‰©å±•åˆ° 4Ã—4 = 16ç»´
# æƒé‡çŸ©é˜µ W1: [4, 16]
# x @ W1 = [16ç»´]
expanded = [
  0.7, 0.2, 0.9, 0.1,
  0.5, 0.8, 0.3, 0.6,
  0.4, 0.9, 0.2, 0.7,
  0.8, 0.3, 0.5, 0.1
]

# GELUæ¿€æ´»ï¼ˆéžçº¿æ€§å˜æ¢ï¼‰
# GELU(x) â‰ˆ x * sigmoid(1.702 * x)
activated = [
  0.68, 0.19, 0.89, 0.09,
  0.48, 0.78, 0.28, 0.58,
  0.38, 0.88, 0.18, 0.68,
  0.78, 0.28, 0.48, 0.09
]

# ç¬¬äºŒå±‚ï¼šåŽ‹ç¼©å›ž4ç»´
# æƒé‡çŸ©é˜µ W2: [16, 4]
# activated @ W2 = [4ç»´]
output = [0.65, 0.42, 0.71, 0.38]

# å¯¹æ¯”è¾“å…¥è¾“å‡º
è¾“å…¥:  [0.5, 0.3, 0.8, 0.2]
è¾“å‡º:  [0.65, 0.42, 0.71, 0.38]
       â†‘ ç‰¹å¾å·²ç»è¢«å˜æ¢äº†ï¼
```

#### ðŸ’» ä»£ç å®žçŽ°

```python
class MLP(nn.Module):
    """å‰é¦ˆç¥žç»ç½‘ç»œ"""
    
    def __init__(self, config):
        super().__init__()
        # ç¬¬ä¸€å±‚ï¼šæ‰©å±• (768 â†’ 3072)
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        
        # æ¿€æ´»å‡½æ•°ï¼šGELU
        self.gelu = nn.GELU()
        
        # ç¬¬äºŒå±‚ï¼šåŽ‹ç¼© (3072 â†’ 768)
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        
        # Dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x):
        x = self.c_fc(x)      # [B, T, 768] â†’ [B, T, 3072]
        x = self.gelu(x)      # éžçº¿æ€§æ¿€æ´»
        x = self.c_proj(x)    # [B, T, 3072] â†’ [B, T, 768]
        x = self.dropout(x)   # éšæœºä¸¢å¼ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        return x
```

#### ðŸ” GELUæ¿€æ´»å‡½æ•°

**ä¸ºä»€ä¹ˆç”¨GELUè€Œä¸æ˜¯ReLUï¼Ÿ**

```python
# ReLUï¼ˆç¡¬æˆªæ–­ï¼‰
ReLU(x) = max(0, x)

è¾“å…¥:  [-2, -1, 0, 1, 2]
è¾“å‡º:  [0,  0,  0, 1, 2]
       â†‘ è´Ÿå€¼å…¨éƒ¨å½’é›¶

é—®é¢˜ï¼š
  - è´Ÿå€¼ä¿¡æ¯å®Œå…¨ä¸¢å¤±
  - æ¢¯åº¦ä¸º0æˆ–1ï¼ˆä¸å¤Ÿå¹³æ»‘ï¼‰

# GELUï¼ˆå¹³æ»‘ç‰ˆReLUï¼‰
GELU(x) â‰ˆ x * Î¦(x)  (Î¦æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„CDF)

è¾“å…¥:  [-2,   -1,    0,   1,    2]
è¾“å‡º:  [-0.05, -0.16, 0,  0.84, 1.96]
        â†‘ ä¿ç•™äº†ä¸€äº›è´Ÿå€¼ä¿¡æ¯

ä¼˜åŠ¿ï¼š
  âœ… å¹³æ»‘çš„æ¢¯åº¦ï¼ˆè®­ç»ƒæ›´ç¨³å®šï¼‰
  âœ… è´Ÿå€¼ä¸å®Œå…¨å½’é›¶ï¼ˆä¿ç•™ä¿¡æ¯ï¼‰
  âœ… å®žè·µä¸­æ•ˆæžœæœ€å¥½
  âœ… GPTã€BERTéƒ½åœ¨ç”¨
```

#### ðŸ“Š GELUå¯è§†åŒ–

```
   y
   ^
 2 |           /
   |          /
 1 |        /
   |      /
 0 |____/________________> x
   |  /
-1 |/
  -3  -2  -1   0   1   2   3

ç‰¹ç‚¹ï¼š
- åœ¨x>1æ—¶ï¼ŒæŽ¥è¿‘y=xï¼ˆçº¿æ€§ï¼‰
- åœ¨x<-1æ—¶ï¼ŒæŽ¥è¿‘y=0ï¼ˆä½†ä¸å®Œå…¨ä¸º0ï¼‰
- åœ¨xâ‰ˆ0é™„è¿‘ï¼Œå¹³æ»‘è¿‡æ¸¡
```

#### ðŸŽ¯ MLPçš„ä½œç”¨æ€»ç»“

```
Attention vs MLPï¼š

Attentionï¼ˆä¿¡æ¯èšåˆï¼‰ï¼š
  ä½œç”¨ï¼šé‡æ–°ç»„åˆä¿¡æ¯
  ä¾‹å­ï¼š"it" å­¦ä¹ åˆ° "æˆ‘å’Œcatç›¸å…³"
  æœºåˆ¶ï¼šåŠ æƒæ±‚å’Œ
  
MLPï¼ˆç‰¹å¾æå–ï¼‰ï¼š
  ä½œç”¨ï¼šå˜æ¢å’Œæå–ç‰¹å¾
  ä¾‹å­ï¼šå­¦ä¹ åˆ° "catæ˜¯åŠ¨ç‰©ï¼Œå•æ•°ï¼Œç¬¬ä¸‰äººç§°"
  æœºåˆ¶ï¼šéžçº¿æ€§å˜æ¢
  
ä¸¤è€…é…åˆï¼š
  Attention â†’ ä»Žä¸Šä¸‹æ–‡èŽ·å–ä¿¡æ¯
  MLP â†’ ç†è§£è¿™äº›ä¿¡æ¯çš„å«ä¹‰
  
å°±åƒï¼š
  Attention = æ”¶é›†èµ„æ–™
  MLP = åˆ†æžèµ„æ–™
```

#### ðŸ“ å‚æ•°é‡è®¡ç®—

```python
# å‡è®¾ n_embd = 768

ç¬¬ä¸€å±‚ (c_fc):
  è¾“å…¥: 768
  è¾“å‡º: 3072
  å‚æ•°: 768 Ã— 3072 = 2,359,296
  
ç¬¬äºŒå±‚ (c_proj):
  è¾“å…¥: 3072
  è¾“å‡º: 768
  å‚æ•°: 3072 Ã— 768 = 2,359,296
  
æ€»å‚æ•°: 4,718,592 â‰ˆ 4.7M

å¯¹æ¯”ï¼š
  Attentionå‚æ•°: ~2.4M
  MLPå‚æ•°: ~4.7M
  
â†’ MLPå äº†Transformer Blockçº¦2/3çš„å‚æ•°ï¼
```

---

**âœ… MLPæ£€æŸ¥ç‚¹**

å­¦å®Œè¿™éƒ¨åˆ†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- [ ] ç†è§£Attentionå’ŒMLPçš„ä¸åŒä½œç”¨
- [ ] çŸ¥é“ä¸ºä»€ä¹ˆè¦å…ˆæ‰©å±•å†åŽ‹ç¼©
- [ ] ç†è§£GELUæ¯”ReLUå¥½åœ¨å“ªé‡Œ
- [ ] çŸ¥é“MLPå æ¨¡åž‹å‚æ•°çš„å¤§éƒ¨åˆ†

**ä¸‹ä¸€æ­¥ï¼šç»„åˆAttentionå’ŒMLP â†’ Block** â†’

---

## ç¬¬äº”éƒ¨åˆ†ï¼šç»„ä»¶4 - Blockï¼ˆç»„åˆå•å…ƒï¼‰

### ðŸ—ï¸ æŠŠç»„ä»¶ç»„åˆèµ·æ¥ï¼šTransformer Block

çŽ°åœ¨æˆ‘ä»¬å·²ç»ç†è§£äº†LayerNormã€Attentionå’ŒMLPï¼Œæ˜¯æ—¶å€™æŠŠå®ƒä»¬ç»„åˆèµ·æ¥äº†ï¼

#### ðŸ’¡ ç›´è§‚ç†è§£ï¼šBlockçš„ç»“æž„

**ä¸€ä¸ªBlock = ä¸¤ä¸ªå­å±‚**

```
è¾“å…¥
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å­å±‚1ï¼šAttentionå­å±‚             â”‚
â”‚                                 â”‚
â”‚  LayerNorm                      â”‚
â”‚      â†“                          â”‚
â”‚  Attentionï¼ˆç†è§£ä¸Šä¸‹æ–‡ï¼‰         â”‚
â”‚      â†“                          â”‚
â”‚  +ï¼ˆæ®‹å·®è¿žæŽ¥ï¼‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å­å±‚2ï¼šMLPå­å±‚                   â”‚
â”‚                                 â”‚
â”‚  LayerNorm                      â”‚
â”‚      â†“                          â”‚
â”‚  MLPï¼ˆç‰¹å¾æå–ï¼‰                 â”‚
â”‚      â†“                          â”‚
â”‚  +ï¼ˆæ®‹å·®è¿žæŽ¥ï¼‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
è¾“å‡º
```

#### ðŸ”‘ æ ¸å¿ƒè®¾è®¡ï¼šæ®‹å·®è¿žæŽ¥ï¼ˆResidual Connectionï¼‰

**è¿™æ˜¯æ·±åº¦å­¦ä¹ æœ€é‡è¦çš„å‘æ˜Žä¹‹ä¸€ï¼**

##### ðŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿žæŽ¥ï¼Ÿ

**é—®é¢˜ï¼šæ·±åº¦ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±**

```python
æ²¡æœ‰æ®‹å·®è¿žæŽ¥çš„æ·±å±‚ç½‘ç»œï¼š

è¾“å…¥ â†’ Layer1 â†’ Layer2 â†’ ... â†’ Layer12 â†’ è¾“å‡º

åå‘ä¼ æ’­æ—¶ï¼š
  Layer12çš„æ¢¯åº¦ï¼š1.0
  Layer11çš„æ¢¯åº¦ï¼š0.9  (è¡°å‡10%)
  Layer10çš„æ¢¯åº¦ï¼š0.81  (è¡°å‡19%)
  ...
  Layer1çš„æ¢¯åº¦ï¼š0.28  (è¡°å‡72%ï¼)
  
é—®é¢˜ï¼š
  âŒ å‰é¢çš„å±‚å‡ ä¹Žå­¦ä¸åˆ°ä¸œè¥¿
  âŒ è®­ç»ƒéžå¸¸å›°éš¾
  âŒ æ·±å±‚ç½‘ç»œæ•ˆæžœåè€Œå˜å·®
```

**è§£å†³æ–¹æ¡ˆï¼šæ®‹å·®è¿žæŽ¥**

```python
æœ‰æ®‹å·®è¿žæŽ¥ï¼š

è¾“å…¥ â”€â”€â”¬â†’ Layer1 â”€â”€â”¬â†’ Layer2 â”€â”€â”¬â†’ ... â†’ è¾“å‡º
       â”‚           â”‚           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€+â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€+â”€â”€â”€â”€â†’
            (ç›´æŽ¥è·³è¿‡)

æ¯ä¸€å±‚çš„è¾“å‡ºï¼š
  output = input + Layer(input)
           â†‘ ä¿ç•™åŽŸå§‹ä¿¡æ¯

å¥½å¤„ï¼š
  âœ… æ¢¯åº¦å¯ä»¥ç›´æŽ¥ä¼ å›žå‰é¢çš„å±‚
  âœ… æ¯å±‚åªéœ€å­¦ä¹ "å¢žé‡"
  âœ… è®­ç»ƒéžå¸¸ç¨³å®š
  âœ… å¯ä»¥è®­ç»ƒå¾ˆæ·±çš„ç½‘ç»œ
```

##### ðŸŽ¯ ç”Ÿæ´»æ¯”å–»

```
æ²¡æœ‰æ®‹å·®è¿žæŽ¥ = ä¼ è¯æ¸¸æˆï¼š
  Aè¯´ï¼š"ä»Šå¤©å¤©æ°”å¾ˆå¥½"
  Bå¬åˆ°åŽå‘Šè¯‰Cï¼š"ä»Šå¤©å¤©ä¸é”™"
  Cå¬åˆ°åŽå‘Šè¯‰Dï¼š"ä»Šå¤©è¿˜è¡Œ"
  Då¬åˆ°åŽå‘Šè¯‰Eï¼š"è¿˜å¯ä»¥"
  Eå¬åˆ°åŽå‘Šè¯‰Fï¼š"è¡Œ"
  Få¬åˆ°åŽï¼š"ï¼Ÿ"
  
  â†’ ä¼ åˆ°æœ€åŽå®Œå…¨å˜æ ·ï¼

æœ‰æ®‹å·®è¿žæŽ¥ = ä¼ è¯ + åŽŸæ–‡ï¼š
  A â†’ B: "åŽŸæ–‡ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½" + "æˆ‘ç†è§£çš„ï¼šå¤©ä¸é”™"
  B â†’ C: "åŽŸæ–‡ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½" + "Aè¯´å¤©ä¸é”™ï¼Œæˆ‘ä¹Ÿè§‰å¾—ä¸é”™"
  C â†’ D: "åŽŸæ–‡ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½" + "Aå’ŒBéƒ½è¯´ä¸é”™ï¼Œç¡®å®žä¸é”™"
  ...
  
  â†’ åŽŸæ–‡ä¸€ç›´ä¿ç•™ï¼Œç†è§£å±‚å±‚å åŠ ï¼
```

##### ðŸ“ æ•°å­¦æŽ¨å¯¼

```python
# æ²¡æœ‰æ®‹å·®è¿žæŽ¥
y = f(x)

å¦‚æžœ f å¾ˆå¤æ‚ï¼Œå­¦ä¹ å›°éš¾

# æœ‰æ®‹å·®è¿žæŽ¥
y = x + f(x)

çŽ°åœ¨ f åªéœ€å­¦ä¹ "æ®‹å·®"ï¼ˆå¢žé‡å˜åŒ–ï¼‰
å¦‚æžœ f=0ï¼Œåˆ™ y=xï¼ˆè‡³å°‘ä¸ä¼šå˜å·®ï¼‰
â†’ å­¦ä¹ å˜å¾—æ›´å®¹æ˜“ï¼

ä¾‹å­ï¼š
  è¾“å…¥ x = [1, 2, 3]
  
  f(x) = [0.1, 0.2, 0.15]  (åªéœ€å­¦ä¹ å°çš„è°ƒæ•´)
  
  è¾“å‡º y = x + f(x)
        = [1, 2, 3] + [0.1, 0.2, 0.15]
        = [1.1, 2.2, 3.15]
        â†‘ ä¿ç•™äº†åŽŸå§‹ä¿¡æ¯ï¼
```

#### ðŸ’» Blockä»£ç å®žçŽ°

```python
class Block(nn.Module):
    """Transformerå—"""
    
    def __init__(self, config):
        super().__init__()
        # LayerNormï¼ˆåœ¨Attentionä¹‹å‰ï¼‰
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        
        # Attentionå±‚
        self.attn = CausalSelfAttention(config)
        
        # LayerNormï¼ˆåœ¨MLPä¹‹å‰ï¼‰
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        
        # MLPå±‚
        self.mlp = MLP(config)
    
    def forward(self, x):
        # å­å±‚1ï¼šAttention + æ®‹å·®
        x = x + self.attn(self.ln_1(x))
        #   â†‘ æ®‹å·®è¿žæŽ¥
        
        # å­å±‚2ï¼šMLP + æ®‹å·®
        x = x + self.mlp(self.ln_2(x))
        #   â†‘ æ®‹å·®è¿žæŽ¥
        
        return x
```

#### ðŸ” è¯¦ç»†æ•°æ®æµ

```python
# è¾“å…¥ï¼ˆç®€åŒ–ä¸º4ç»´ï¼‰
x_in = [1.0, 2.0, 3.0, 4.0]

# ===== å­å±‚1ï¼šAttention =====

# æ­¥éª¤1ï¼šLayerNorm
x_norm1 = LayerNorm(x_in)
        = [0.0, 0.33, 0.67, 1.0]  # æ ‡å‡†åŒ–

# æ­¥éª¤2ï¼šAttention
attn_out = Attention(x_norm1)
         = [0.1, 0.2, 0.15, 0.25]  # ä»Žä¸Šä¸‹æ–‡èŽ·å–ä¿¡æ¯

# æ­¥éª¤3ï¼šæ®‹å·®è¿žæŽ¥
x_mid = x_in + attn_out
      = [1.0, 2.0, 3.0, 4.0] + [0.1, 0.2, 0.15, 0.25]
      = [1.1, 2.2, 3.15, 4.25]
      â†‘ ä¿ç•™äº†åŽŸå§‹ä¿¡æ¯ï¼

# ===== å­å±‚2ï¼šMLP =====

# æ­¥éª¤4ï¼šLayerNorm
x_norm2 = LayerNorm(x_mid)
        = [0.0, 0.35, 0.68, 1.0]

# æ­¥éª¤5ï¼šMLP
mlp_out = MLP(x_norm2)
        = [0.08, 0.15, 0.12, 0.20]  # ç‰¹å¾æå–

# æ­¥éª¤6ï¼šæ®‹å·®è¿žæŽ¥
x_out = x_mid + mlp_out
      = [1.1, 2.2, 3.15, 4.25] + [0.08, 0.15, 0.12, 0.20]
      = [1.18, 2.35, 3.27, 4.45]
      â†‘ å†æ¬¡ä¿ç•™ä¿¡æ¯ï¼

# å¯¹æ¯”
è¾“å…¥:  [1.0, 2.0, 3.0, 4.0]
è¾“å‡º:  [1.18, 2.35, 3.27, 4.45]
      â†‘ æœ‰æ”¹å˜ï¼Œä½†ä¸å‰§çƒˆ
```

#### ðŸŽ¨ Pre-Norm vs Post-Norm

**ä¸ºä»€ä¹ˆLayerNormåœ¨Attention/MLPä¹‹å‰ï¼Ÿ**

```python
Post-Normï¼ˆä¼ ç»ŸTransformerï¼‰ï¼š
  x = LayerNorm(x + Attention(x))
  
  é—®é¢˜ï¼š
    - æ®‹å·®è·¯å¾„ä¸Šæ²¡æœ‰å½’ä¸€åŒ–
    - è®­ç»ƒä¸å¤Ÿç¨³å®š
    - éœ€è¦warmup

Pre-Normï¼ˆGPT-2åŠä¹‹åŽï¼‰âœ…ï¼š
  x = x + Attention(LayerNorm(x))
  
  ä¼˜åŠ¿ï¼š
    - æ®‹å·®è·¯å¾„ç›´æŽ¥è¿žæŽ¥
    - è®­ç»ƒæ›´ç¨³å®š
    - ä¸éœ€è¦warmup
    - å¯ä»¥è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ
```

#### ðŸ“Š ä¸€ä¸ªBlockçš„å®Œæ•´æµç¨‹å›¾

```
è¾“å…¥ x [B, T, C]
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” (residual)
  â”‚                  â”‚
  â””â†’ LayerNorm       â”‚
      â†“              â”‚
    Attention        â”‚
    (ç†è§£ä¸Šä¸‹æ–‡)     â”‚
      â†“              â”‚
      +â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
  â”Œâ”€â”€â”€â”´â”€â”€â”€â”
  â”‚       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” (residual)
  â”‚       â”‚          â”‚
  â””â†’ LayerNorm       â”‚
      â†“              â”‚
     MLP             â”‚
    (ç‰¹å¾æå–)       â”‚
      â†“              â”‚
      +â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â†“
  è¾“å‡º x [B, T, C]
```

#### ðŸŽ¯ Blockçš„ä½œç”¨æ€»ç»“

```
ä¸€ä¸ªBlockåšäº†ä»€ä¹ˆï¼Ÿ

è¾“å…¥ä¸€ä¸ªè¯çš„è¡¨ç¤ºï¼š
  "cat" = [0.5, 0.3, 0.8, ...]

ç»è¿‡BlockåŽï¼š
  "cat" = [0.58, 0.35, 0.85, ...]
  
å˜åŒ–ï¼š
  âœ… èžåˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆAttentionï¼‰
  âœ… æå–äº†æ›´é«˜å±‚ç‰¹å¾ï¼ˆMLPï¼‰
  âœ… ä¿ç•™äº†åŽŸå§‹ä¿¡æ¯ï¼ˆæ®‹å·®ï¼‰
  âœ… æ•°å€¼ä¿æŒç¨³å®šï¼ˆLayerNormï¼‰

å¤šä¸ªBlockå †å ï¼š
  Block1ï¼šç†è§£åŸºç¡€è¯­æ³•
  Block2ï¼šç†è§£è¯ä¹‰
  Block3ï¼šç†è§£çŸ­è¯­
  Block4ï¼šç†è§£å¥æ³•
  Block5ï¼šç†è§£è¯­ä¹‰
  Block6ï¼šç†è§£æ·±å±‚å«ä¹‰
  
  â†’ å±‚å±‚é€’è¿›ï¼Œç†è§£è¶Šæ¥è¶Šæ·±ï¼
```

---

**âœ… Blockæ£€æŸ¥ç‚¹**

å­¦å®Œè¿™éƒ¨åˆ†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- [ ] ç†è§£æ®‹å·®è¿žæŽ¥ä¸ºä»€ä¹ˆé‡è¦
- [ ] çŸ¥é“Pre-Normçš„ä¼˜åŠ¿
- [ ] èƒ½ç”»å‡ºBlockçš„å®Œæ•´ç»“æž„å›¾
- [ ] ç†è§£ä¸ºä»€ä¹ˆè¦å †å å¤šä¸ªBlock

**ä¸‹ä¸€æ­¥ï¼šæŠŠå¤šä¸ªBlockç»„åˆæˆå®Œæ•´çš„GPTæ¨¡åž‹** â†’

---

## ç¬¬å…­éƒ¨åˆ†ï¼šå®Œæ•´æ¨¡åž‹ - GPT

### ðŸš€ ç»„åˆæ‰€æœ‰ç»„ä»¶ï¼šå®Œæ•´çš„GPTæ¨¡åž‹

çŽ°åœ¨æˆ‘ä»¬è¦æŠŠæ‰€æœ‰ç»„ä»¶ç»„åˆèµ·æ¥ï¼Œæž„å»ºå®Œæ•´çš„GPTæ¨¡åž‹ï¼

#### ðŸ—ï¸ GPTæ¨¡åž‹çš„æ•´ä½“ç»“æž„

```
è¾“å…¥: Token IDs [B, T]
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. åµŒå…¥å±‚ (Embedding)                â”‚
â”‚                                     â”‚
â”‚  Token Embedding                    â”‚
â”‚  + Position Embedding               â”‚
â”‚  + Dropout                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, 768]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Transformer Blocks Ã— N           â”‚
â”‚                                     â”‚
â”‚  Block 1                            â”‚
â”‚  Block 2                            â”‚
â”‚  ...                                â”‚
â”‚  Block 12                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, 768]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. æœ€åŽçš„LayerNorm                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, 768]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. è¾“å‡ºå±‚ (LM Head)                 â”‚
â”‚                                     â”‚
â”‚  Linear: 768 â†’ vocab_size           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, vocab_size]
è¾“å‡º: Logitsï¼ˆæ¯ä¸ªtokençš„åˆ†æ•°ï¼‰
```

#### ðŸ’¡ åµŒå…¥å±‚è¯¦è§£

**Token Embedding + Position Embedding**

```python
# é—®é¢˜ï¼šå¦‚ä½•è¡¨ç¤ºä¸€ä¸ªè¯ï¼Ÿ
"cat" â†’ æ•°å­—ID â†’ å‘é‡è¡¨ç¤º

# Token Embeddingï¼ˆè¯åµŒå…¥ï¼‰
ä½œç”¨ï¼šæŠŠè¯IDè½¬æ¢æˆå‘é‡
  "cat" (ID=123) â†’ [0.5, 0.3, 0.8, ...]  (768ç»´)
  "dog" (ID=456) â†’ [0.6, 0.2, 0.7, ...]  (768ç»´)

# Position Embeddingï¼ˆä½ç½®åµŒå…¥ï¼‰
ä½œç”¨ï¼šæ ‡è®°è¯åœ¨å¥å­ä¸­çš„ä½ç½®
  ä½ç½®0 â†’ [0.01, 0.02, 0.03, ...]
  ä½ç½®1 â†’ [0.02, 0.03, 0.04, ...]
  ä½ç½®2 â†’ [0.03, 0.04, 0.05, ...]

# ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®åµŒå…¥ï¼Ÿ
å› ä¸ºAttentionæ˜¯"å¹¶è¡Œ"å¤„ç†æ‰€æœ‰è¯çš„
ä¸åƒRNNé‚£æ ·"é¡ºåº"å¤„ç†
æ‰€ä»¥éœ€è¦æ˜¾å¼å‘Šè¯‰æ¨¡åž‹è¯çš„ä½ç½®
```

**ç”Ÿæ´»æ¯”å–»ï¼š**

```
æƒ³è±¡ä¸€ä¸ªå¥å­ï¼š"I love cats"

æ²¡æœ‰ä½ç½®åµŒå…¥ï¼š
  æ¨¡åž‹çœ‹åˆ°ï¼š[I, love, cats]
  ä½†ä¸çŸ¥é“é¡ºåº
  å¯èƒ½ç†è§£æˆï¼š"cats love I"
  æˆ–è€…ï¼š"love I cats"
  å®Œå…¨ä¸å¯¹ï¼

æœ‰ä½ç½®åµŒå…¥ï¼š
  æ¨¡åž‹çœ‹åˆ°ï¼š
    "I" + "ä½ç½®1æ ‡ç­¾"
    "love" + "ä½ç½®2æ ‡ç­¾"
    "cats" + "ä½ç½®3æ ‡ç­¾"
  
  çŽ°åœ¨çŸ¥é“æ­£ç¡®é¡ºåºäº†ï¼
```

#### ðŸ”‘ æƒé‡ç»‘å®šï¼ˆWeight Tyingï¼‰

**ä¸€ä¸ªå·§å¦™çš„è®¾è®¡ï¼šå…±äº«åµŒå…¥å±‚å’Œè¾“å‡ºå±‚çš„æƒé‡**

```python
# è§‚å¯Ÿï¼š
è¾“å…¥åµŒå…¥ï¼šToken ID â†’ Vector
  "cat" (ID=123) â†’ [0.5, 0.3, 0.8, ...]

è¾“å‡ºå±‚ï¼šVector â†’ Token ID
  [0.5, 0.3, 0.8, ...] â†’ "cat" (ID=123)

å‘çŽ°ï¼š
  è¿™ä¸¤ä¸ªæ“ä½œæ˜¯"äº’é€†"çš„ï¼
  å¯ä»¥å…±äº«æƒé‡ï¼

å®žçŽ°ï¼š
  self.transformer.wte.weight = self.lm_head.weight
  â†‘ è®©è¾“å‡ºå±‚ç›´æŽ¥ç”¨åµŒå…¥å±‚çš„æƒé‡
```

**å¥½å¤„ï¼š**

```
1. å‚æ•°é‡å‡åŠ
   åŽŸæœ¬ï¼šåµŒå…¥38M + è¾“å‡º38M = 76M
   çŽ°åœ¨ï¼šå…±äº«38M
   çœäº†ï¼š38Må‚æ•°ï¼

2. è®­ç»ƒæ›´ç¨³å®š
   åµŒå…¥å’Œè¾“å‡ºä¿æŒä¸€è‡´
   ä¸ä¼šå‡ºçŽ°"åŒä¸€ä¸ªè¯ï¼Œè¾“å…¥å’Œè¾“å‡ºè¡¨ç¤ºä¸åŒ"

3. æ³›åŒ–èƒ½åŠ›æ›´å¼º
   å‡å°‘å‚æ•° â†’ å‡å°‘è¿‡æ‹Ÿåˆé£Žé™©
```

#### ðŸ’» GPTæ¨¡åž‹ä»£ç 

```python
class GPT(nn.Module):
    """å®Œæ•´çš„GPTæ¨¡åž‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # æ¨¡åž‹çš„ä¸»è¦ç»„ä»¶
        self.transformer = nn.ModuleDict(dict(
            # TokenåµŒå…¥ï¼švocab_size â†’ n_embd
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            
            # ä½ç½®åµŒå…¥ï¼šblock_size â†’ n_embd
            wpe = nn.Embedding(config.block_size, config.n_embd),
            
            # Dropout
            drop = nn.Dropout(config.dropout),
            
            # Transformer Blocks Ã— N
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            
            # æœ€åŽçš„LayerNorm
            ln_f = LayerNorm(config.n_embd, bias=config.bias),
        ))
        
        # è¾“å‡ºå±‚ï¼šn_embd â†’ vocab_size
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        
        # æƒé‡ç»‘å®š
        self.transformer.wte.weight = self.lm_head.weight
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def forward(self, idx, targets=None):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            idx: è¾“å…¥token IDs [B, T]
            targets: ç›®æ ‡token IDs [B, T]ï¼ˆè®­ç»ƒæ—¶æä¾›ï¼‰
        
        è¿”å›ž:
            logits: é¢„æµ‹åˆ†æ•° [B, T, vocab_size]
            loss: æŸå¤±å€¼ï¼ˆå¦‚æžœæä¾›targetsï¼‰
        """
        device = idx.device
        b, t = idx.size()
        assert t <= self.config.block_size
        
        # ç”Ÿæˆä½ç½®ç´¢å¼• [T]
        pos = torch.arange(0, t, dtype=torch.long, device=device)
        
        # 1. åµŒå…¥å±‚
        tok_emb = self.transformer.wte(idx)      # [B, T, n_embd]
        pos_emb = self.transformer.wpe(pos)      # [T, n_embd]
        x = self.transformer.drop(tok_emb + pos_emb)  # [B, T, n_embd]
        
        # 2. Transformer Blocks
        for block in self.transformer.h:
            x = block(x)  # [B, T, n_embd]
        
        # 3. æœ€åŽçš„LayerNorm
        x = self.transformer.ln_f(x)  # [B, T, n_embd]
        
        # 4. è¾“å‡ºå±‚
        if targets is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰ä½ç½®çš„logits
            logits = self.lm_head(x)  # [B, T, vocab_size]
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),  # [B*T, vocab_size]
                targets.view(-1),                   # [B*T]
                ignore_index=-1
            )
        else:
            # æŽ¨ç†æ¨¡å¼ï¼šåªè®¡ç®—æœ€åŽä¸€ä¸ªä½ç½®
            logits = self.lm_head(x[:, [-1], :])  # [B, 1, vocab_size]
            loss = None
        
        return logits, loss
```

#### ðŸ”¢ å®Œæ•´æ•°æ®æµç¤ºä¾‹

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªçœŸå®žä¾‹å­èµ°ä¸€éå®Œæ•´æµç¨‹ï¼š

```python
# ===== è¾“å…¥ =====
è¾“å…¥æ–‡æœ¬: "The cat sat"
Token IDs: [15, 3380, 3332]
batch_size = 1
seq_len = 3

# ===== æ­¥éª¤1ï¼šåµŒå…¥å±‚ =====

# Token Embedding
idx = [15, 3380, 3332]  # [1, 3]
tok_emb = wte(idx)       # [1, 3, 768]

tok_emb[0, 0, :] = [0.23, -0.45, 0.67, ..., 0.12]  # "The"
tok_emb[0, 1, :] = [0.56, 0.12, -0.34, ..., 0.89]  # "cat"
tok_emb[0, 2, :] = [-0.12, 0.78, 0.23, ..., -0.45] # "sat"

# Position Embedding
pos = [0, 1, 2]          # [3]
pos_emb = wpe(pos)       # [3, 768]

pos_emb[0, :] = [0.01, 0.02, ..., 0.03]  # ä½ç½®0
pos_emb[1, :] = [0.02, 0.03, ..., 0.04]  # ä½ç½®1
pos_emb[2, :] = [0.03, 0.04, ..., 0.05]  # ä½ç½®2

# ç›¸åŠ 
x = tok_emb + pos_emb    # [1, 3, 768]

x[0, 0, :] = [0.24, -0.43, ..., 0.15]  # "The" + ä½ç½®0
x[0, 1, :] = [0.58, 0.15, ..., 0.93]   # "cat" + ä½ç½®1
x[0, 2, :] = [-0.09, 0.82, ..., -0.40] # "sat" + ä½ç½®2

# ===== æ­¥éª¤2ï¼šTransformer Blocks =====

# Block 1
x = Block1(x)
# çŽ°åœ¨ x åŒ…å«äº†åŸºç¡€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

# Block 2
x = Block2(x)
# ç†è§£æ›´æ·±äº†

# ... Blocks 3-12
# ç†è§£è¶Šæ¥è¶Šæ·±

# ===== æ­¥éª¤3ï¼šLayerNorm =====
x = ln_f(x)  # [1, 3, 768]

# ===== æ­¥éª¤4ï¼šè¾“å‡ºå±‚ =====
logits = lm_head(x)  # [1, 3, 50257]

# å¯¹äºŽæœ€åŽä¸€ä¸ªä½ç½®ï¼ˆ"sat"ä¹‹åŽï¼‰
logits[0, 2, :] = [
    -3.2,   # Token 0 ("!") çš„åˆ†æ•°
    -2.1,   # Token 1 (".") çš„åˆ†æ•°
    ...
    5.8,    # Token 319 ("on") çš„åˆ†æ•°  â† æœ€é«˜ï¼
    ...
    -1.5,   # Token 50256 çš„åˆ†æ•°
]

# ===== æ­¥éª¤5ï¼šSoftmaxï¼ˆè½¬æ¦‚çŽ‡ï¼‰=====
probs = softmax(logits[0, 2, :])

probs = [
    0.0001,  # "!" çš„æ¦‚çŽ‡
    0.0002,  # "." çš„æ¦‚çŽ‡
    ...
    0.7821,  # "on" çš„æ¦‚çŽ‡  â† æœ€é«˜ï¼78%
    ...
]

# ===== æ­¥éª¤6ï¼šé‡‡æ · =====
next_token = sample(probs)  # é€‰æ‹© "on"

# è¾“å‡º
"The cat sat on"
```

#### ðŸ“Š æ¨¡åž‹è§„æ¨¡å¯¹æ¯”

```python
# Shakespeare Modelï¼ˆå­¦ä¹ ç”¨ï¼‰
config = GPTConfig(
    vocab_size=65,      # å­—ç¬¦çº§
    n_layer=6,
    n_head=6,
    n_embd=384,
    block_size=256,
)
å‚æ•°é‡: ~10M
è®­ç»ƒ: MacBook 5åˆ†é’Ÿ
ç”¨é€”: å¿«é€Ÿå®žéªŒ

# GPT-2 Smallï¼ˆå®žç”¨ï¼‰
config = GPTConfig(
    vocab_size=50257,
    n_layer=12,
    n_head=12,
    n_embd=768,
    block_size=1024,
)
å‚æ•°é‡: ~124M
è®­ç»ƒ: å•GPU 4å¤©
ç”¨é€”: å®žé™…åº”ç”¨

# GPT-3ï¼ˆå¤§è§„æ¨¡ï¼‰
config = GPTConfig(
    vocab_size=50257,
    n_layer=96,
    n_head=96,
    n_embd=12288,
    block_size=2048,
)
å‚æ•°é‡: ~175B
è®­ç»ƒ: é›†ç¾¤æ•°æœˆ
ç”¨é€”: å•†ä¸šæœåŠ¡
```

---

**âœ… GPTæ¨¡åž‹æ£€æŸ¥ç‚¹**

å­¦å®Œè¿™éƒ¨åˆ†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- [ ] ç†è§£GPTæ¨¡åž‹çš„å®Œæ•´ç»“æž„
- [ ] çŸ¥é“Tokenå’ŒPosition Embeddingçš„ä½œç”¨
- [ ] ç†è§£æƒé‡ç»‘å®šçš„å¥½å¤„
- [ ] èƒ½ç”»å‡ºå®Œæ•´çš„æ•°æ®æµå›¾
- [ ] çŸ¥é“ä¸åŒè§„æ¨¡æ¨¡åž‹çš„å·®å¼‚

**ä¸‹ä¸€æ­¥ï¼šå­¦ä¹ å¦‚ä½•ç”Ÿæˆæ–‡æœ¬** â†’

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ–‡æœ¬ç”Ÿæˆ - Generate

### ðŸŽ² å¦‚ä½•è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬ï¼Ÿ

çŽ°åœ¨æˆ‘ä»¬æœ‰äº†å®Œæ•´çš„GPTæ¨¡åž‹ï¼Œæœ€æ¿€åŠ¨äººå¿ƒçš„æ—¶åˆ»åˆ°äº†ï¼š**è®©æ¨¡åž‹ç”Ÿæˆæ–‡æœ¬ï¼**

#### ðŸ’¡ è‡ªå›žå½’ç”Ÿæˆçš„åŽŸç†

**ä»€ä¹ˆæ˜¯è‡ªå›žå½’ç”Ÿæˆï¼Ÿ**

```
è‡ªå›žå½’ (Autoregressive)ï¼š
  ç”¨è‡ªå·±çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥
  ä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°ç”Ÿæˆ

å°±åƒï¼š
  ä½ : "Once upon a"
  GPT: "time"
  ä½ : "Once upon a time"
  GPT: "there"
  ä½ : "Once upon a time there"
  GPT: "was"
  ...
  
  â†’ ä¸æ–­é‡å¤ï¼Œç›´åˆ°ç”Ÿæˆå®Œæ•´æ•…äº‹
```

#### ðŸ”„ ç”Ÿæˆå¾ªçŽ¯

```python
åˆå§‹è¾“å…¥: "Once upon a time"

å¾ªçŽ¯ 1:
  è¾“å…¥: "Once upon a time"
  æ¨¡åž‹é¢„æµ‹: "there" (æ¦‚çŽ‡æœ€é«˜)
  æ–°è¾“å…¥: "Once upon a time there"

å¾ªçŽ¯ 2:
  è¾“å…¥: "Once upon a time there"
  æ¨¡åž‹é¢„æµ‹: "was"
  æ–°è¾“å…¥: "Once upon a time there was"

å¾ªçŽ¯ 3:
  è¾“å…¥: "Once upon a time there was"
  æ¨¡åž‹é¢„æµ‹: "a"
  æ–°è¾“å…¥: "Once upon a time there was a"

... æŒç»­ç”Ÿæˆ ...

å¾ªçŽ¯ N:
  è¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–é‡åˆ°ç»“æŸç¬¦
  åœæ­¢ç”Ÿæˆ
```

#### ðŸ’» ç”Ÿæˆå‡½æ•°ä»£ç 

```python
@torch.no_grad()  # ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ˆæŽ¨ç†æ¨¡å¼ï¼‰
def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
    """
    è‡ªå›žå½’ç”Ÿæˆæ–‡æœ¬
    
    å‚æ•°:
        idx: åˆå§‹tokenåºåˆ— [B, T]
        max_new_tokens: ç”Ÿæˆå¤šå°‘ä¸ªæ–°token
        temperature: æ¸©åº¦å‚æ•°ï¼ˆæŽ§åˆ¶éšæœºæ€§ï¼‰
        top_k: Top-Ké‡‡æ ·ï¼ˆé™åˆ¶é€‰æ‹©èŒƒå›´ï¼‰
    
    è¿”å›ž:
        ç”Ÿæˆçš„å®Œæ•´åºåˆ— [B, T+max_new_tokens]
    """
    for _ in range(max_new_tokens):
        # æ­¥éª¤1ï¼šæˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆå¦‚æžœå¤ªé•¿ï¼‰
        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
        
        # æ­¥éª¤2ï¼šå‰å‘ä¼ æ’­
        logits, _ = self(idx_cond)
        
        # æ­¥éª¤3ï¼šåªå–æœ€åŽä¸€ä¸ªä½ç½®çš„é¢„æµ‹
        logits = logits[:, -1, :] / temperature
        
        # æ­¥éª¤4ï¼šTop-Ké‡‡æ ·ï¼ˆå¯é€‰ï¼‰
        if top_k is not None:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        
        # æ­¥éª¤5ï¼šè½¬æ¦‚çŽ‡
        probs = F.softmax(logits, dim=-1)
        
        # æ­¥éª¤6ï¼šé‡‡æ ·
        idx_next = torch.multinomial(probs, num_samples=1)
        
        # æ­¥éª¤7ï¼šè¿½åŠ åˆ°åºåˆ—
        idx = torch.cat((idx, idx_next), dim=1)
    
    return idx
```

#### ðŸŽ¯ Temperatureï¼ˆæ¸©åº¦ï¼‰å‚æ•°

**æ¸©åº¦æŽ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§**

```python
åŽŸå§‹logits: [1.0, 2.0, 3.0, 4.0, 5.0]

===== Temperature = 0.1 (ä½Žæ¸©ï¼Œæ›´ç¡®å®š) =====
scaled_logits = [10.0, 20.0, 30.0, 40.0, 50.0]
probs = softmax(scaled_logits)
      = [0.000, 0.000, 0.000, 0.001, 0.999]
      
ç»“æžœï¼šå‡ ä¹Ž100%é€‰æ¦‚çŽ‡æœ€é«˜çš„
æ•ˆæžœï¼šç”Ÿæˆé‡å¤ã€æ— èŠã€ç¡®å®šæ€§å¼º

===== Temperature = 1.0 (æ ‡å‡†ï¼Œå¹³è¡¡) =====
scaled_logits = [1.0, 2.0, 3.0, 4.0, 5.0]
probs = softmax(scaled_logits)
      = [0.012, 0.032, 0.087, 0.236, 0.643]
      
ç»“æžœï¼š64%é€‰æœ€é«˜çš„ï¼Œä½†ä¹Ÿæœ‰å˜åŒ–
æ•ˆæžœï¼šå¹³è¡¡çš„ç”Ÿæˆè´¨é‡

===== Temperature = 2.0 (é«˜æ¸©ï¼Œæ›´éšæœº) =====
scaled_logits = [0.5, 1.0, 1.5, 2.0, 2.5]
probs = softmax(scaled_logits)
      = [0.105, 0.141, 0.191, 0.258, 0.349]
      
ç»“æžœï¼šæ¦‚çŽ‡åˆ†å¸ƒæ›´å‡åŒ€
æ•ˆæžœï¼šåˆ›é€ æ€§å¼ºï¼Œä½†å¯èƒ½ä¸è¿žè´¯

===== Temperature â†’ 0 (æžä½Žæ¸©) =====
ç­‰ä»·äºŽè´ªå¿ƒæœç´¢ï¼ˆGreedy Searchï¼‰
æ€»æ˜¯é€‰æ¦‚çŽ‡æœ€é«˜çš„
ç”Ÿæˆå®Œå…¨ç¡®å®š
```

**ç”Ÿæ´»æ¯”å–»ï¼š**

```
Temperature = å†’é™©ç²¾ç¥ž

ä½Žæ¸© (0.1-0.5):
  ä¿å®ˆæ´¾ï¼šæ€»èµ°æœ€å®‰å…¨çš„è·¯
  "Once upon a time there was a cat."
  "Once upon a time there was a dog."
  â†’ å®‰å…¨ä½†æ— èŠ

æ ‡å‡† (1.0):
  å¹³è¡¡æ´¾ï¼šæœ‰ä¸»è§ä½†ä¸æ­»æ¿
  "Once upon a time there was a brave knight."
  â†’ æœ‰è¶£ä¸”åˆç†

é«˜æ¸© (1.5-2.0):
  å†’é™©æ´¾ï¼šç»å¸¸å°è¯•æ–°å¥‡çš„
  "Once upon a time there was a purple singing toaster."
  â†’ åˆ›æ„åè¶³ä½†å¯èƒ½ä¸åˆç†
```

#### ðŸ” Top-Ké‡‡æ ·

**é™åˆ¶é€‰æ‹©èŒƒå›´ï¼Œæé«˜è´¨é‡**

```python
# æ¦‚çŽ‡åˆ†å¸ƒ
all_probs = {
    "the": 0.45,
    "mat": 0.25,
    "floor": 0.15,
    "carpet": 0.08,
    "ground": 0.05,
    "roof": 0.01,      # ä¸åˆç†
    "sky": 0.01,       # ä¸åˆç†
    ... 50250ä¸ªtoken
}

===== ä¸ç”¨Top-K =====
ä»Žå…¨éƒ¨50257ä¸ªtokenä¸­é‡‡æ ·
å¯èƒ½é€‰åˆ°ï¼š
  - "roof" (å±‹é¡¶ï¼ŸçŒ«ååœ¨matä¸Šçš„å±‹é¡¶ï¼Ÿ)
  - "sky" (å¤©ç©ºï¼Ÿæ›´ç¦»è°±)
  
é—®é¢˜ï¼šä½Žæ¦‚çŽ‡ä½†ä¸åˆç†çš„è¯ä¼šè¢«é€‰ä¸­

===== ç”¨Top-K=5 =====
åªä»Žæ¦‚çŽ‡æœ€é«˜çš„5ä¸ªtokenä¸­é‡‡æ ·ï¼š
  choices = ["the", "mat", "floor", "carpet", "ground"]
  
å¥½å¤„ï¼š
  âœ… æ°¸è¿œä¸ä¼šé€‰åˆ°"roof"æˆ–"sky"
  âœ… ç”Ÿæˆè´¨é‡æ›´ç¨³å®š
  âœ… è¿˜ä¿ç•™äº†ä¸€å®šéšæœºæ€§
  
å®žè·µä¸­ï¼š
  Top-K = 40-50 æ•ˆæžœæœ€å¥½
```

**å¯è§†åŒ–Top-Kï¼š**

```
æ‰€æœ‰tokençš„æ¦‚çŽ‡åˆ†å¸ƒï¼š
  
  ^
æ¦‚|  â– 
çŽ‡|  â– 
  |  â–   â– 
  |  â–   â–   â– 
  |  â–   â–   â–   â–   â–   â–   â–“  â–“  â–“  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
  |  â–   â–   â–   â–   â–   â–   â–“  â–“  â–“  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
  +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> tokens
     â†‘â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†‘
     Top-K=5 (é€‰è¿™äº›)    å…¶ä»–(å¿½ç•¥)
     
â–  = é«˜æ¦‚çŽ‡ï¼Œåˆç†
â–“ = ä¸­ç­‰æ¦‚çŽ‡ï¼Œè¿˜è¡Œ
â–‘ = ä½Žæ¦‚çŽ‡ï¼Œç»å¸¸ä¸åˆç†
```

#### ðŸ“Š å®Œæ•´ç”Ÿæˆç¤ºä¾‹

```python
# åˆå§‹åŒ–
model.eval()  # æŽ¨ç†æ¨¡å¼
prompt = "Once upon a time"
tokens = encode(prompt)  # [7454, 2402, 257, 640]

# ç”Ÿæˆå‚æ•°
max_new_tokens = 50
temperature = 0.8
top_k = 40

# ç”Ÿæˆè¿‡ç¨‹ï¼ˆå±•ç¤ºå‰5æ­¥ï¼‰
print(f"Prompt: {prompt}")

for step in range(5):
    # å½“å‰æ–‡æœ¬
    current_text = decode(tokens)
    
    # æ¨¡åž‹é¢„æµ‹
    logits, _ = model(tokens)
    logits = logits[:, -1, :] / temperature
    
    # Top-K
    v, _ = torch.topk(logits, top_k)
    logits[logits < v[:, [-1]]] = -float('Inf')
    
    # é‡‡æ ·
    probs = F.softmax(logits, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    
    # è¿½åŠ 
    tokens = torch.cat((tokens, next_token), dim=1)
    next_word = decode([next_token.item()])
    
    print(f"Step {step+1}: {current_text} + '{next_word}'")
    print(f"  æ¦‚çŽ‡: {probs[0, next_token].item():.2%}")

# å¯èƒ½çš„è¾“å‡ºï¼š
# Prompt: Once upon a time
# Step 1: Once upon a time + 'there'
#   æ¦‚çŽ‡: 68.45%
# Step 2: Once upon a time there + 'was'
#   æ¦‚çŽ‡: 78.23%
# Step 3: Once upon a time there was + 'a'
#   æ¦‚çŽ‡: 85.67%
# Step 4: Once upon a time there was a + 'little'
#   æ¦‚çŽ‡: 34.56%
# Step 5: Once upon a time there was a little + 'girl'
#   æ¦‚çŽ‡: 45.78%
```

#### ðŸŽ® ä¸åŒé‡‡æ ·ç­–ç•¥å¯¹æ¯”

```python
ç­–ç•¥1ï¼šè´ªå¿ƒæœç´¢ (Temperature=0, Top-K=1)
  "Once upon a time there was a cat. The cat was very happy."
  âœ… è¯­æ³•å®Œç¾Ž
  âŒ é‡å¤ã€æ— èŠ

ç­–ç•¥2ï¼šéšæœºé‡‡æ · (Temperature=1.0, Top-K=None)
  "Once upon a sky dragon flew purple singing loudly yesterday."
  âœ… åˆ›æ„åè¶³
  âŒ ä¸è¿žè´¯ã€ä¸åˆç†

ç­–ç•¥3ï¼šå¹³è¡¡é‡‡æ · (Temperature=0.8, Top-K=40) â­æŽ¨è
  "Once upon a time there was a brave knight who lived in a castle."
  âœ… æœ‰è¶£ä¸”åˆç†
  âœ… è´¨é‡ç¨³å®š
  
å®žè·µå»ºè®®ï¼š
  - æ•…äº‹ç”Ÿæˆï¼štemperature=0.8-1.0, top_k=40
  - ä»£ç ç”Ÿæˆï¼štemperature=0.5-0.7, top_k=20
  - æ‘˜è¦æ€»ç»“ï¼štemperature=0.3-0.5, top_k=10
```

---

**âœ… æ–‡æœ¬ç”Ÿæˆæ£€æŸ¥ç‚¹**

å­¦å®Œè¿™éƒ¨åˆ†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- [ ] ç†è§£è‡ªå›žå½’ç”Ÿæˆçš„åŽŸç†
- [ ] çŸ¥é“Temperatureå¦‚ä½•å½±å“ç”Ÿæˆ
- [ ] ç†è§£Top-Ké‡‡æ ·çš„ä½œç”¨
- [ ] èƒ½è°ƒæ•´å‚æ•°æŽ§åˆ¶ç”Ÿæˆè´¨é‡

**æ­å–œï¼ä½ å·²ç»å®Œå…¨ç†è§£GPTçš„æž¶æž„å’Œå·¥ä½œåŽŸç†ï¼** ðŸŽ‰

---

## ðŸŽ“ ç¬¬å…«éƒ¨åˆ†ï¼šæ€»ç»“ä¸Žå±•æœ›

### âœ… ä½ å·²ç»å­¦ä¼šäº†ä»€ä¹ˆ

æ­å–œä½ ï¼ç»è¿‡è¿™ä¸€ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»å®Œå…¨æŽŒæ¡äº†Transformerå’ŒGPTçš„æ ¸å¿ƒåŽŸç†ã€‚

#### ðŸ“š çŸ¥è¯†å›žé¡¾

**æ ¸å¿ƒç»„ä»¶ï¼ˆ6ä¸ªï¼‰**ï¼š

```
1. LayerNorm âœ…
   - ä½œç”¨ï¼šæ•°æ®æ ‡å‡†åŒ–
   - ä½ç½®ï¼šAttentionå’ŒMLPä¹‹å‰ï¼ˆPre-Normï¼‰
   - ä¸ºä»€ä¹ˆï¼šç¨³å®šè®­ç»ƒï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸

2. Attention âœ… ï¼ˆæœ€æ ¸å¿ƒï¼‰
   - ä½œç”¨ï¼šç†è§£ä¸Šä¸‹æ–‡å…³ç³»
   - æœºåˆ¶ï¼šQã€Kã€Vä¸‰å…„å¼Ÿ
   - å…¬å¼ï¼šAttention(Q,K,V) = softmax(QK^T/âˆšd)V
   - ä¸ºä»€ä¹ˆï¼šè®©æ¨¡åž‹è‡ªåŠ¨å†³å®šå…³æ³¨å“ªäº›ä¿¡æ¯

3. MLP âœ…
   - ä½œç”¨ï¼šç‰¹å¾æå–å’Œå˜æ¢
   - ç»“æž„ï¼šæ‰©å±•(4Ã—) â†’ GELU â†’ åŽ‹ç¼©
   - ä¸ºä»€ä¹ˆï¼šAttentionåªæ··åˆä¿¡æ¯ï¼ŒMLPæå–æ–°ç‰¹å¾

4. Block âœ…
   - ä½œç”¨ï¼šç»„åˆAttentionå’ŒMLP
   - å…³é”®ï¼šæ®‹å·®è¿žæŽ¥ï¼ˆx + f(x)ï¼‰
   - ä¸ºä»€ä¹ˆï¼šè®©æ·±å±‚ç½‘ç»œå¯ä»¥è®­ç»ƒ

5. Embedding âœ…
   - Token Embeddingï¼šè¯ â†’ å‘é‡
   - Position Embeddingï¼šä½ç½® â†’ å‘é‡
   - ä¸ºä»€ä¹ˆï¼šAttentionæ˜¯å¹¶è¡Œçš„ï¼Œéœ€è¦ä½ç½®ä¿¡æ¯

6. GPT âœ…
   - å®Œæ•´æ¨¡åž‹ï¼šEmbedding + BlocksÃ—N + LM Head
   - æƒé‡ç»‘å®šï¼šå…±äº«è¾“å…¥è¾“å‡ºæƒé‡
   - ä¸ºä»€ä¹ˆï¼šè¿™å°±æ˜¯çŽ°ä»£AIçš„æ ¸å¿ƒï¼
```

#### ðŸ§® å…³é”®æ•°å­—

```python
GPT-2 Small (124Må‚æ•°):
  n_layer = 12          # 12å±‚
  n_head = 12           # 12ä¸ªå¤´
  n_embd = 768          # 768ç»´
  block_size = 1024     # 1024 tokensä¸Šä¸‹æ–‡
  
å‚æ•°åˆ†å¸ƒï¼š
  Embedding: ~38M (31%)
  Attention: ~29M (23%)
  MLP: ~57M (46%)
  
è®¡ç®—å¤æ‚åº¦ï¼š
  Attention: O(NÂ² Ã— d)  â† ç“¶é¢ˆ
  MLP: O(N Ã— dÂ²)
```

#### ðŸ’¡ æ ¸å¿ƒæ´žå¯Ÿ

**5ä¸ªæœ€é‡è¦çš„ç†è§£ï¼š**

1. **Attentionä¸æ˜¯é­”æ³•**
   ```
   æœ¬è´¨ï¼šæ ¹æ®ç›¸å…³æ€§åŠ æƒæ±‚å’Œ
   QÂ·K^T â†’ è®¡ç®—ç›¸å…³æ€§
   softmax â†’ è½¬æ¦‚çŽ‡
   @V â†’ åŠ æƒç»„åˆ
   å°±è¿™ä¹ˆç®€å•ï¼
   ```

2. **æ®‹å·®è¿žæŽ¥æ˜¯å…³é”®**
   ```
   æ²¡æœ‰æ®‹å·®ï¼šæ·±å±‚ç½‘ç»œè®­ç»ƒå›°éš¾
   æœ‰æ®‹å·®ï¼šå¯ä»¥è®­ç»ƒå‡ ç™¾å±‚
   y = x + f(x)
   â†’ fåªéœ€å­¦ä¹ "å¢žé‡"
   ```

3. **å¤šå¤´=å¤šè§†è§’**
   ```
   12ä¸ªå¤´ = 12ä¸ªä¸“å®¶
   å„è‡ªå…³æ³¨ä¸åŒçš„æ¨¡å¼
   æœ€åŽç»¼åˆæ‰€æœ‰æ„è§
   ```

4. **ä½ç½®ç¼–ç å¿…ä¸å¯å°‘**
   ```
   Attentionæ˜¯æ— åºçš„
   éœ€è¦æ˜¾å¼æ ‡è®°ä½ç½®
   å¦åˆ™ä¸çŸ¥é“è¯çš„é¡ºåº
   ```

5. **ç”Ÿæˆ=å¾ªçŽ¯é¢„æµ‹**
   ```
   æ¯æ¬¡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
   æŠŠé¢„æµ‹åŠ åˆ°è¾“å…¥
   é‡å¤ç›´åˆ°ç»“æŸ
   ```

---

### ðŸŽ¯ å®Œæ•´æµç¨‹æ€»ç»“å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GPTå®Œæ•´æµç¨‹ï¼ˆä»Žè¾“å…¥åˆ°è¾“å‡ºï¼‰                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥: "The cat sat"
  â†“
Token IDs: [15, 3380, 3332]
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. åµŒå…¥å±‚                                                    â”‚
â”‚    - Token Embedding: ID â†’ Vector (768ç»´)                   â”‚
â”‚    - Position Embedding: ä½ç½® â†’ Vector (768ç»´)              â”‚
â”‚    - ç›¸åŠ : å¾—åˆ°å¸¦ä½ç½®ä¿¡æ¯çš„è¯å‘é‡                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, 768]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Transformer Block Ã— 12                                   â”‚
â”‚                                                             â”‚
â”‚    æ¯ä¸ªBlock:                                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ LayerNorm â†’ Attention â†’ æ®‹å·®                        â”‚  â”‚
â”‚    â”‚ ï¼ˆç†è§£ä¸Šä¸‹æ–‡ï¼‰                                        â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ LayerNorm â†’ MLP â†’ æ®‹å·®                              â”‚  â”‚
â”‚    â”‚ ï¼ˆç‰¹å¾æå–ï¼‰                                          â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚    æ¯ç»è¿‡ä¸€å±‚ï¼š                                              â”‚
â”‚    - å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£æ›´æ·±                                       â”‚
â”‚    - ç‰¹å¾æ›´æŠ½è±¡                                             â”‚
â”‚    - æœ€åŽå‡ å±‚å·²ç»ç†è§£æ·±å±‚è¯­ä¹‰                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, 768]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Final LayerNorm                                          â”‚
â”‚    - æœ€åŽä¸€æ¬¡æ ‡å‡†åŒ–                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, 768]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LM Head (è¾“å‡ºå±‚)                                          â”‚
â”‚    - Linear: 768 â†’ 50257 (è¯æ±‡è¡¨å¤§å°)                       â”‚
â”‚    - æ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„åˆ†æ•°                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ [B, T, 50257]
Logits â†’ Softmax â†’ Probabilities
  â†“
é‡‡æ ·/é€‰æ‹©ä¸‹ä¸€ä¸ªè¯
  â†“
è¾“å‡º: "on"
æœ€ç»ˆ: "The cat sat on"
```

---

### ðŸš€ ä½ çŽ°åœ¨èƒ½åšä»€ä¹ˆ

**ç«‹å³èƒ½åšçš„ï¼š**
- âœ… è¯»æ‡‚ä»»ä½•Transformeræ¨¡åž‹çš„ä»£ç ï¼ˆGPT-2/3/4, LLaMA, BERTç­‰ï¼‰
- âœ… ä¿®æ”¹æ¨¡åž‹ç»“æž„ï¼ˆæ”¹å±‚æ•°ã€å¤´æ•°ã€ç»´åº¦ï¼‰
- âœ… è°ƒè¯•è®­ç»ƒé—®é¢˜ï¼ˆçŸ¥é“å“ªé‡Œå¯èƒ½å‡ºé”™ï¼‰
- âœ… ä¼˜åŒ–æ¨¡åž‹æ€§èƒ½ï¼ˆçŸ¥é“ç“¶é¢ˆåœ¨å“ªï¼‰
- âœ… å®žçŽ°è‡ªå·±çš„å˜ä½“ï¼ˆæ”¹è¿›Attentionç­‰ï¼‰

**æ·±å…¥èƒ½åšçš„ï¼š**
- âœ… ç†è§£æœ€æ–°è®ºæ–‡çš„åˆ›æ–°ç‚¹
- âœ… è®¾è®¡æ–°çš„æ¨¡åž‹æž¶æž„
- âœ… é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–
- âœ… æˆä¸ºAIæž¶æž„ä¸“å®¶

---

### ðŸ“– æŽ¨èé˜…è¯»

**å¿…è¯»è®ºæ–‡ï¼š**
1. **Attention Is All You Need** (2017)
   - TransformeråŽŸè®ºæ–‡
   - https://arxiv.org/abs/1706.03762

2. **Language Models are Unsupervised Multitask Learners** (GPT-2, 2019)
   - GPT-2è®ºæ–‡
   
3. **Language Models are Few-Shot Learners** (GPT-3, 2020)
   - GPT-3è®ºæ–‡

4. **Flash Attention** (2022)
   - Attentionä¼˜åŒ–
   - https://arxiv.org/abs/2205.14135

**ä¼˜ç§€æ•™ç¨‹ï¼š**
- **The Illustrated Transformer** (Jay Alammar)
  - https://jalammar.github.io/illustrated-transformer/
  
- **Andrej Karpathy: Let's build GPT**
  - https://www.youtube.com/watch?v=kCc8FmEb1nY

---

### ðŸŽ“ çŸ¥è¯†æ£€æŸ¥æ¸…å•

**åŸºç¡€ç†è§£ï¼ˆå¿…é¡»æŽŒæ¡ï¼‰**ï¼š
- [ ] èƒ½ç”¨ç”Ÿæ´»æ¯”å–»è§£é‡ŠAttention
- [ ] çŸ¥é“Qã€Kã€Vå„æ˜¯ä»€ä¹ˆ
- [ ] ç†è§£æ®‹å·®è¿žæŽ¥çš„ä½œç”¨
- [ ] çŸ¥é“ä¸ºä»€ä¹ˆéœ€è¦LayerNorm
- [ ] ç†è§£MLPçš„ä½œç”¨
- [ ] èƒ½ç”»å‡ºBlockçš„ç»“æž„

**æ·±å…¥ç†è§£ï¼ˆå»ºè®®æŽŒæ¡ï¼‰**ï¼š
- [ ] èƒ½æ‰‹å†™Attentionå…¬å¼
- [ ] ç†è§£Causal Maskçš„å®žçŽ°
- [ ] çŸ¥é“Multi-Headä¸ºä»€ä¹ˆæœ‰æ•ˆ
- [ ] ç†è§£æƒé‡ç»‘å®šçš„å¥½å¤„
- [ ] èƒ½è®¡ç®—æ¨¡åž‹å‚æ•°é‡
- [ ] çŸ¥é“è®¡ç®—ç“¶é¢ˆåœ¨å“ª

**å®žæˆ˜èƒ½åŠ›ï¼ˆé«˜çº§ç›®æ ‡ï¼‰**ï¼š
- [ ] èƒ½ä¿®æ”¹model.pyæ·»åŠ åŠŸèƒ½
- [ ] ä¼šè°ƒè¯•Attentioné—®é¢˜
- [ ] èƒ½å®žçŽ°æ–°çš„ä½ç½®ç¼–ç 
- [ ] ä¼šä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
- [ ] èƒ½å®žçŽ°Attentionå˜ä½“
- [ ] ç†è§£æœ€æ–°ç ”ç©¶è¿›å±•

---

### ðŸ’ª ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

**æ ¹æ®ä½ çš„ç›®æ ‡é€‰æ‹©ï¼š**

#### ðŸŽ¯ ç›®æ ‡1ï¼šæˆä¸ºç ”ç©¶è€…
```
ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š
  â†’ ç¬¬06ç« ï¼šScaling Lawsï¼ˆæ‰©å±•è§„å¾‹ï¼‰
  â†’ ç¬¬07ç« ï¼šæž¶æž„æ”¹è¿›æŠ€æœ¯
  â†’ ç¬¬11ç« ï¼šå¤šæ¨¡æ€æ¨¡åž‹
  â†’ ç¬¬12ç« ï¼šMoEç¨€ç–æ¨¡åž‹
  
é‡ç‚¹ï¼š
  - ç†è§£å‰æ²¿æŠ€æœ¯
  - é˜…è¯»æœ€æ–°è®ºæ–‡
  - å®žçŽ°åˆ›æ–°æƒ³æ³•
```

#### ðŸŽ¯ ç›®æ ‡2ï¼šæˆä¸ºå·¥ç¨‹å¸ˆ
```
ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š
  â†’ ç¬¬08ç« ï¼šåˆ†å¸ƒå¼è®­ç»ƒ
  â†’ ç¬¬09ç« ï¼šæ¨¡åž‹ä¼˜åŒ–
  â†’ ç¬¬10ç« ï¼šç”Ÿäº§éƒ¨ç½²
  
é‡ç‚¹ï¼š
  - è®­ç»ƒå¤§æ¨¡åž‹
  - ä¼˜åŒ–æ€§èƒ½
  - éƒ¨ç½²ä¸Šçº¿
```

#### ðŸŽ¯ ç›®æ ‡3ï¼šæˆä¸ºåº”ç”¨å¼€å‘è€…
```
ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š
  â†’ Fine-tuningæŠ€å·§
  â†’ Prompt Engineering
  â†’ RAGç³»ç»Ÿ
  â†’ Agentå¼€å‘
  
é‡ç‚¹ï¼š
  - åº”ç”¨çŽ°æœ‰æ¨¡åž‹
  - è§£å†³å®žé™…é—®é¢˜
  - äº§å“è½åœ°
```

---

### ðŸŽ‰ æœ€åŽçš„è¯

**ä½ å·²ç»å®Œæˆäº†ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ï¼**

ç†è§£Transformeræ˜¯ç†è§£çŽ°ä»£AIçš„å…³é”®ã€‚ä½ çŽ°åœ¨æŽŒæ¡çš„çŸ¥è¯†æ˜¯ï¼š
- GPT-3ã€GPT-4çš„åŸºç¡€
- Claudeã€Geminiçš„æ ¸å¿ƒ
- æ‰€æœ‰å¤§è¯­è¨€æ¨¡åž‹çš„å…±åŒåŽŸç†

**è¿™äº›çŸ¥è¯†æ°¸ä¸è¿‡æ—¶ï¼Œå› ä¸ºå®ƒæ˜¯åŸºç¡€ï¼**

æ— è®ºæœªæ¥AIå¦‚ä½•å‘å±•ï¼ŒTransformerçš„æ ¸å¿ƒæ€æƒ³ï¼ˆAttentionã€æ®‹å·®ã€å½’ä¸€åŒ–ï¼‰éƒ½ä¼šå­˜åœ¨ã€‚

**ç»§ç»­å‰è¿›ï¼Œæˆä¸ºAIä¸“å®¶ï¼** ðŸš€

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿ** é€‰æ‹©ä½ çš„ä¸‹ä¸€ç« ï¼Œç»§ç»­ä½ çš„AIå­¦ä¹ ä¹‹æ—…ï¼

â†’ [ç¬¬06ç« ï¼šScaling Laws](06_scaling_laws_explained.md)  
â†’ [ç¬¬07ç« ï¼šæž¶æž„æ”¹è¿›æŠ€æœ¯](07_architecture_improvements.md)  
â†’ [ç¬¬08ç« ï¼šåˆ†å¸ƒå¼è®­ç»ƒ](08_distributed_training.md)

**æˆ–è€…ï¼Œå…ˆä¼‘æ¯ä¸€ä¸‹ï¼Œæ¶ˆåŒ–è¿™ç« çš„å†…å®¹ã€‚ä½ å·²ç»å­¦å¾—å¾ˆå¥½äº†ï¼** â˜•

---

**æ–‡æ¡£ç»“æŸ** ðŸŽŠ
