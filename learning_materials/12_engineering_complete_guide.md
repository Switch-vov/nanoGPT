# ğŸš€ ä»è®­ç»ƒåˆ°éƒ¨ç½²ï¼šå·¥ç¨‹å®Œå…¨æŒ‡å—

## ğŸ¯ æ¦‚è§ˆ

è¿™ä¸ªæŒ‡å—æ•´åˆäº†ä¸‰ä¸ªå…³é”®çš„å·¥ç¨‹ä¸»é¢˜ï¼Œå¸¦ä½ èµ°å®Œä»æ¨¡å‹è®­ç»ƒåˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚

```
å®Œæ•´æµç¨‹å›¾ï¼š

è®­ç»ƒé˜¶æ®µ              ä¼˜åŒ–é˜¶æ®µ              éƒ¨ç½²é˜¶æ®µ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                        
ğŸ“š å‡†å¤‡æ•°æ®          âš¡ åˆ†å¸ƒå¼è®­ç»ƒ        ğŸ³ å®¹å™¨åŒ–
    â†“                   â†“                   â†“
ğŸ§  è®­ç»ƒæ¨¡å‹          ğŸ“Š æ¨¡å‹é‡åŒ–          ğŸš€ APIæœåŠ¡
    â†“                   â†“                   â†“
âœ… è¯„ä¼°æ€§èƒ½          ğŸ” ç²¾åº¦éªŒè¯          ğŸ“ˆ ç›‘æ§è¿ç»´
    â†“                   â†“                   â†“
ğŸ’¾ ä¿å­˜checkpoint    ğŸ’¾ å¯¼å‡ºæ¨¡å‹          â˜ï¸ äº‘ç«¯éƒ¨ç½²
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šå®Œæ•´é¡¹ç›®å®æˆ˜

### ğŸ¯ é¡¹ç›®ï¼šéƒ¨ç½²ä¸€ä¸ªç”Ÿäº§çº§çš„ä»£ç è¡¥å…¨åŠ©æ‰‹

#### **é˜¶æ®µ1ï¼šè®­ç»ƒæ¨¡å‹ï¼ˆå•GPUï¼‰**

```bash
# æ­¥éª¤1: å‡†å¤‡æ•°æ®
mkdir -p data/python_code
cd data/python_code

# æ”¶é›†Pythonä»£ç ï¼ˆç¤ºä¾‹ï¼‰
cat > collect_code.py << 'EOF'
import os
import tiktoken

code_files = []
for root, dirs, files in os.walk('/path/to/your/python/projects'):
    for file in files:
        if file.endswith('.py'):
            path = os.path.join(root, file)
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                code_files.append(f.read())

# åˆå¹¶å’Œæ¸…æ´—
data = '\n\n# ================\n\n'.join(code_files)

# åˆ†å‰²
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# Tokenize
enc = tiktoken.get_encoding("gpt2")
train_ids = enc.encode_ordinary(train_data)
val_ids = enc.encode_ordinary(val_data)

# ä¿å­˜
import numpy as np
np.array(train_ids, dtype=np.uint16).tofile('train.bin')
np.array(val_ids, dtype=np.uint16).tofile('val.bin')

print(f"è®­ç»ƒtokens: {len(train_ids):,}")
print(f"éªŒè¯tokens: {len(val_ids):,}")
EOF

python collect_code.py

# æ­¥éª¤2: åˆ›å»ºé…ç½®
cd ../..
cat > config/train_code_assistant.py << 'EOF'
import time

# è¾“å‡ºç›®å½•
out_dir = 'out-code-assistant'
eval_interval = 500
eval_iters = 100
log_interval = 10

# æ•°æ®é›†
dataset = 'python_code'
gradient_accumulation_steps = 4
batch_size = 8
block_size = 512  # ä»£ç éœ€è¦é•¿ä¸Šä¸‹æ–‡

# æ¨¡å‹ï¼ˆä¸­ç­‰å¤§å°ï¼‰
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.1

# å¾®è°ƒè®¾ç½®
init_from = 'gpt2'  # ä»GPT-2å¼€å§‹
learning_rate = 5e-5  # å°å­¦ä¹ ç‡
max_iters = 5000
lr_decay_iters = 5000
min_lr = 5e-6

# ä¼˜åŒ–
weight_decay = 0.1
grad_clip = 1.0
decay_lr = True
warmup_iters = 100
EOF

# æ­¥éª¤3: å¼€å§‹è®­ç»ƒ
python train.py config/train_code_assistant.py

# é¢„æœŸï¼š2-3å°æ—¶å®Œæˆï¼ˆå•GPUï¼‰
```

#### **é˜¶æ®µ2ï¼šåˆ†å¸ƒå¼åŠ é€Ÿï¼ˆ4 GPUï¼‰**

```bash
# ä¿®æ”¹é…ç½®ä»¥åˆ©ç”¨å¤šGPU
cat > config/train_code_assistant_ddp.py << 'EOF'
# ç»§æ‰¿å•GPUé…ç½®
exec(open('config/train_code_assistant.py').read())

# DDPä¼˜åŒ–
batch_size = 16  # æ¯ä¸ªGPUæ›´å¤§batch
gradient_accumulation_steps = 2  # å‡å°‘ç´¯ç§¯æ­¥æ•°

# æ€»batch_size = 16 Ã— 2 Ã— 4 = 128 (å’Œå•GPUçš„8Ã—4Ã—4ä¸€æ ·)
EOF

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --standalone --nproc_per_node=4 \
  train.py config/train_code_assistant_ddp.py

# é¢„æœŸï¼š30-40åˆ†é’Ÿå®Œæˆï¼ˆ4Ã—åŠ é€Ÿï¼‰
```

#### **é˜¶æ®µ3ï¼šæ¨¡å‹é‡åŒ–**

```python
# quantize_code_assistant.py

import torch
from model import GPT, GPTConfig

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
checkpoint = torch.load('out-code-assistant/ckpt.pt')
gptconf = GPTConfig(**checkpoint['model_args'])
model = GPT(gptconf)
model.load_state_dict(checkpoint['model'])
model.eval()

# åŠ¨æ€é‡åŒ–
model_quantized = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# ä¿å­˜
torch.save({
    'model': model_quantized.state_dict(),
    'model_args': checkpoint['model_args'],
    'quantized': True,
}, 'out-code-assistant/ckpt_int8.pt')

print("é‡åŒ–å®Œæˆï¼")

# å¯¹æ¯”å¤§å°
import os
orig_size = os.path.getsize('out-code-assistant/ckpt.pt') / 1e6
quant_size = os.path.getsize('out-code-assistant/ckpt_int8.pt') / 1e6
print(f"åŸå§‹: {orig_size:.2f} MB")
print(f"é‡åŒ–: {quant_size:.2f} MB")
print(f"å‹ç¼©æ¯”: {orig_size/quant_size:.2f}x")
```

è¿è¡Œé‡åŒ–ï¼š

```bash
python quantize_code_assistant.py

# è¾“å‡º:
# é‡åŒ–å®Œæˆï¼
# åŸå§‹: 497.35 MB
# é‡åŒ–: 124.67 MB
# å‹ç¼©æ¯”: 3.99x
```

#### **é˜¶æ®µ4ï¼šåˆ›å»ºAPIæœåŠ¡**

```python
# serve_code_assistant.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from model import GPT, GPTConfig
import tiktoken
from contextlib import asynccontextmanager
from typing import List

model_cache = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åŠ è½½é‡åŒ–æ¨¡å‹"""
    print("ğŸš€ å¯åŠ¨ä»£ç åŠ©æ‰‹æœåŠ¡...")
    
    checkpoint = torch.load('out-code-assistant/ckpt_int8.pt', map_location='cuda')
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    model.to('cuda')
    
    enc = tiktoken.get_encoding("gpt2")
    
    model_cache['model'] = model
    model_cache['tokenizer'] = enc
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
    yield
    model_cache.clear()

app = FastAPI(
    title="Code Assistant API",
    description="åŸºäºGPTçš„ä»£ç è¡¥å…¨åŠ©æ‰‹",
    version="1.0.0",
    lifespan=lifespan
)

class CompletionRequest(BaseModel):
    code: str
    max_tokens: int = 100
    temperature: float = 0.3  # ä»£ç ç”Ÿæˆç”¨ä½æ¸©åº¦
    top_k: int = 50

class CompletionResponse(BaseModel):
    completion: str
    full_code: str

@app.get("/")
async def root():
    return {
        "service": "Code Assistant API",
        "status": "running",
        "version": "1.0.0"
    }

@app.post("/complete", response_model=CompletionResponse)
async def complete_code(request: CompletionRequest):
    """ä»£ç è¡¥å…¨"""
    try:
        model = model_cache['model']
        enc = model_cache['tokenizer']
        
        # ç¼–ç 
        input_ids = torch.tensor([enc.encode(request.code)], dtype=torch.long, device='cuda')
        
        # ç”Ÿæˆ
        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_k=request.top_k
            )
        
        # è§£ç 
        full_text = enc.decode(output[0].tolist())
        completion = full_text[len(request.code):]
        
        return CompletionResponse(
            completion=completion,
            full_code=full_text
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "model_loaded": 'model' in model_cache
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### **é˜¶æ®µ5ï¼šDockeréƒ¨ç½²**

```dockerfile
# Dockerfile

FROM nvidia/cuda:12.1.0-base-ubuntu22.04

WORKDIR /app

# å®‰è£…ä¾èµ–
RUN apt-get update && apt-get install -y python3.10 python3-pip
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY model.py .
COPY serve_code_assistant.py .
COPY out-code-assistant/ ./out-code-assistant/

EXPOSE 8000

CMD ["python3", "serve_code_assistant.py"]
```

```bash
# æ„å»ºå’Œè¿è¡Œ
docker build -t code-assistant:v1 .
docker run -d --gpus all -p 8000:8000 code-assistant:v1

# æµ‹è¯•
curl -X POST "http://localhost:8000/complete" \
  -H "Content-Type: application/json" \
  -d '{
    "code": "def fibonacci(n):\n    ",
    "max_tokens": 100,
    "temperature": 0.2
  }'
```

#### **é˜¶æ®µ6ï¼šç”Ÿäº§éƒ¨ç½²**

```yaml
# kubernetes-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: code-assistant
spec:
  replicas: 3  # 3ä¸ªå®ä¾‹
  selector:
    matchLabels:
      app: code-assistant
  template:
    metadata:
      labels:
        app: code-assistant
    spec:
      containers:
      - name: code-assistant
        image: code-assistant:v1
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "8Gi"
          requests:
            nvidia.com/gpu: 1
            memory: "6Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: code-assistant-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8000
  selector:
    app: code-assistant
```

éƒ¨ç½²åˆ°Kubernetesï¼š

```bash
# éƒ¨ç½²
kubectl apply -f kubernetes-deployment.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods
kubectl get services

# æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/code-assistant

# æ‰©å®¹
kubectl scale deployment code-assistant --replicas=10
```

---

## ğŸ“Š ç¬¬äºŒéƒ¨åˆ†ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•

### ğŸ§ª å®Œæ•´æ€§èƒ½å¯¹æ¯”

```python
åœºæ™¯: LLaMA-7Bæ¨¡å‹

é˜¶æ®µ           | é…ç½®                    | æ€§èƒ½æŒ‡æ ‡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è®­ç»ƒï¼ˆå•GPUï¼‰  | FP32, batch=4           | åŸºçº¿
è®­ç»ƒï¼ˆ8 GPUï¼‰  | FP32, batch=32, DDP     | 7x åŠ é€Ÿ
è®­ç»ƒï¼ˆä¼˜åŒ–ï¼‰   | BF16, compile, DDP      | 12x åŠ é€Ÿ

æ¨ç†ï¼ˆåŸºç¡€ï¼‰   | FP16, batch=1           | 28 tokens/s
æ¨ç†ï¼ˆé‡åŒ–ï¼‰   | INT8, batch=1           | 45 tokens/s
æ¨ç†ï¼ˆæ‰¹å¤„ç†ï¼‰ | INT8, batch=8           | 280 tokens/s
æ¨ç†ï¼ˆvLLMï¼‰   | INT8, continuous batch  | 1200 tokens/s

æ€»æå‡: ä»28 â†’ 1200 tokens/s = 43xï¼
```

### ğŸ’° æˆæœ¬åˆ†æ

```python
åœºæ™¯: è®­ç»ƒå¹¶éƒ¨ç½²ä¸€ä¸ª7Bæ¨¡å‹

è®­ç»ƒæˆæœ¬:
  å•GPU (A100): 
    æ—¶é—´: 21å¤©
    æˆæœ¬: 21 Ã— 24 Ã— $3 = $1,512
  
  8Ã—GPU (A100 + DDP + ä¼˜åŒ–):
    æ—¶é—´: 2.5å¤©
    æˆæœ¬: 2.5 Ã— 24 Ã— 8 Ã— $3 = $1,440
    èŠ‚çœ: $72 + 18.5å¤©æ—¶é—´ï¼

æ¨ç†æˆæœ¬ï¼ˆæ¯å¤©10ä¸‡è¯·æ±‚ï¼‰:
  FP16 (æ ‡å‡†):
    GPU: g4dn.xlarge ($0.5/å°æ—¶)
    å®ä¾‹æ•°: 20ä¸ª
    æˆæœ¬: 20 Ã— 24 Ã— $0.5 = $240/å¤©
  
  INT8é‡åŒ– + vLLM:
    GPU: g4dn.xlarge
    å®ä¾‹æ•°: 5ä¸ª
    æˆæœ¬: 5 Ã— 24 Ã— $0.5 = $60/å¤©
    èŠ‚çœ: $180/å¤© = $5,400/æœˆï¼

å¹´åº¦æˆæœ¬å¯¹æ¯”:
  æ ‡å‡†æ–¹æ¡ˆ: $87,600
  ä¼˜åŒ–æ–¹æ¡ˆ: $21,900
  èŠ‚çœ: $65,700 (75%!)
```

---

## ğŸ“ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæŠ€èƒ½æ ‘

### ğŸ“Š å®Œæ•´æŠ€èƒ½å›¾è°±

```
NanoGPTå·¥ç¨‹æŠ€èƒ½æ ‘ï¼š

åŸºç¡€å±‚ (å¿…é¡»æŒæ¡) âœ…
â”œâ”€â”€ Pythonç¼–ç¨‹
â”œâ”€â”€ PyTorchåŸºç¡€
â”œâ”€â”€ Transformeræ¶æ„
â””â”€â”€ è®­ç»ƒå¾ªç¯

ä¸­çº§å±‚ (æ¨èæŒæ¡)
â”œâ”€â”€ åˆ†å¸ƒå¼è®­ç»ƒ (09)
â”‚   â”œâ”€â”€ DDPå•æœºå¤šå¡
â”‚   â”œâ”€â”€ å¤šæœºè®­ç»ƒ
â”‚   â””â”€â”€ æ¢¯åº¦åŒæ­¥
â”‚
â”œâ”€â”€ æ¨¡å‹ä¼˜åŒ–
â”‚   â”œâ”€â”€ æ··åˆç²¾åº¦
â”‚   â”œâ”€â”€ Gradient Checkpointing
â”‚   â””â”€â”€ Flash Attention
â”‚
â””â”€â”€ ä»£ç å·¥ç¨‹
    â”œâ”€â”€ é…ç½®ç®¡ç†
    â”œâ”€â”€ æ—¥å¿—ç³»ç»Ÿ
    â””â”€â”€ é”™è¯¯å¤„ç†

é«˜çº§å±‚ (æ·±å…¥ç²¾é€š)
â”œâ”€â”€ æ¨¡å‹é‡åŒ– (10)
â”‚   â”œâ”€â”€ PTQ vs QAT
â”‚   â”œâ”€â”€ GPTQ/AWQ
â”‚   â””â”€â”€ æ··åˆç²¾åº¦é‡åŒ–
â”‚
â”œâ”€â”€ æ¨¡å‹éƒ¨ç½² (11)
â”‚   â”œâ”€â”€ APIæœåŠ¡
â”‚   â”œâ”€â”€ å®¹å™¨åŒ–
â”‚   â”œâ”€â”€ è´Ÿè½½å‡è¡¡
â”‚   â””â”€â”€ ç›‘æ§è¿ç»´
â”‚
â””â”€â”€ å¤§è§„æ¨¡è®­ç»ƒ
    â”œâ”€â”€ FSDP/ZeRO-3
    â”œâ”€â”€ æµæ°´çº¿å¹¶è¡Œ
    â””â”€â”€ 3Då¹¶è¡Œ

ä¸“å®¶å±‚ (å‰æ²¿æ¢ç´¢)
â”œâ”€â”€ æ¨ç†ä¼˜åŒ–
â”‚   â”œâ”€â”€ vLLM/TensorRT
â”‚   â”œâ”€â”€ KV Cacheä¼˜åŒ–
â”‚   â””â”€â”€ Speculative Decoding
â”‚
â””â”€â”€ æ¶æ„åˆ›æ–°
    â”œâ”€â”€ MoE
    â”œâ”€â”€ Long Context
    â””â”€â”€ Efficient Architectures
```

---

## ğŸ§ª ç¬¬å››éƒ¨åˆ†ï¼šå®æˆ˜ç»ƒä¹ é¡¹ç›®

### ğŸ’» é¡¹ç›®1ï¼šä¸ªäººä»£ç åŠ©æ‰‹ï¼ˆå…¥é—¨ï¼‰

**ç›®æ ‡ï¼š** åœ¨æœ¬åœ°éƒ¨ç½²ä¸€ä¸ªèƒ½ç”¨çš„ä»£ç è¡¥å…¨å·¥å…·

**æŠ€æœ¯æ ˆï¼š**
- è®­ç»ƒ: å•GPU
- ä¼˜åŒ–: INT8é‡åŒ–
- éƒ¨ç½²: FastAPI + Docker

**æ­¥éª¤ï¼š**
```bash
1. æ”¶é›†æ•°æ®ï¼ˆä½ çš„ä»£ç åº“ï¼‰
2. å¾®è°ƒGPT-2
3. é‡åŒ–æ¨¡å‹
4. åˆ›å»ºAPI
5. DockeråŒ–
6. æœ¬åœ°ä½¿ç”¨
```

**æ—¶é—´ä¼°è®¡ï¼š** 1-2å¤©  
**éš¾åº¦ï¼š** â­â­  
**æ”¶è·ï¼š** å®Œæ•´çš„ç«¯åˆ°ç«¯æµç¨‹

---

### ğŸ¢ é¡¹ç›®2ï¼šå›¢é˜ŸçŸ¥è¯†åº“é—®ç­”ï¼ˆä¸­çº§ï¼‰

**ç›®æ ‡ï¼š** éƒ¨ç½²ä¸€ä¸ªå†…éƒ¨çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ

**æŠ€æœ¯æ ˆï¼š**
- è®­ç»ƒ: 4Ã—GPU DDP
- ä¼˜åŒ–: INT8é‡åŒ– + KV Cache
- éƒ¨ç½²: vLLM + Kubernetes

**æ­¥éª¤ï¼š**
```bash
1. æ”¶é›†å…¬å¸æ–‡æ¡£
2. æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
3. DDPè®­ç»ƒï¼ˆ4 GPUï¼‰
4. é‡åŒ–å’Œè¯„ä¼°
5. vLLMéƒ¨ç½²
6. K8sç¼–æ’
7. ç›‘æ§å’Œç»´æŠ¤
```

**æ—¶é—´ä¼°è®¡ï¼š** 1-2å‘¨  
**éš¾åº¦ï¼š** â­â­â­  
**æ”¶è·ï¼š** ç”Ÿäº§çº§ç³»ç»Ÿç»éªŒ

---

### ğŸŒ é¡¹ç›®3ï¼šå…¬å…±APIæœåŠ¡ï¼ˆé«˜çº§ï¼‰

**ç›®æ ‡ï¼š** æ„å»ºå¯æ‰©å±•çš„å…¬å…±GPTæœåŠ¡

**æŠ€æœ¯æ ˆï¼š**
- è®­ç»ƒ: 32Ã—GPU DDP + FSDP
- ä¼˜åŒ–: INT4é‡åŒ–(GPTQ) + vLLM
- éƒ¨ç½²: Kubernetes + è‡ªåŠ¨æ‰©å±•

**æ¶æ„ï¼š**
```
ç”¨æˆ·è¯·æ±‚
    â†“
Cloudflare CDN
    â†“
AWS ALB (è´Ÿè½½å‡è¡¡)
    â†“
Kubernetes Cluster
    â”œâ”€ æ¨ç†Pod 1 (vLLM)
    â”œâ”€ æ¨ç†Pod 2 (vLLM)
    â”œâ”€ ...
    â””â”€ æ¨ç†Pod N
    â†“
Prometheus (ç›‘æ§)
    â†“
Grafana (å¯è§†åŒ–)
```

**æ—¶é—´ä¼°è®¡ï¼š** 1-2ä¸ªæœˆ  
**éš¾åº¦ï¼š** â­â­â­â­â­  
**æ”¶è·ï¼š** ä¼ä¸šçº§æ¶æ„èƒ½åŠ›

---

## ğŸ¯ ç¬¬äº”éƒ¨åˆ†ï¼šæ•…éšœæ’æŸ¥é€ŸæŸ¥è¡¨

### ğŸ› è®­ç»ƒé˜¶æ®µé—®é¢˜

```python
é—®é¢˜                        | å¯èƒ½åŸå›               | è§£å†³æ–¹æ¡ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Lossä¸ä¸‹é™                  | å­¦ä¹ ç‡å¤ªå°            | å¢å¤§10å€
Lossæ˜¯NaN                   | å­¦ä¹ ç‡å¤ªå¤§/æ¢¯åº¦çˆ†ç‚¸   | å‡å°lrï¼Œgrad_clip=1.0
OOM (æ˜¾å­˜ä¸è¶³)              | batchå¤ªå¤§             | å‡å°batchæˆ–ç”¨æ¢¯åº¦ç´¯ç§¯
è®­ç»ƒå¤ªæ…¢                    | æœªä¼˜åŒ–                | compile=True, DDP
å¤šGPUè®­ç»ƒå¡ä½               | é€šä¿¡é—®é¢˜              | æ£€æŸ¥NCCLï¼Œæµ‹è¯•ç½‘ç»œ
DDP lossä¸åŒæ­¥              | éšæœºç§å­              | å›ºå®šseed+rank offset
æ¢¯åº¦å…¨æ˜¯0                   | å­¦ä¹ ç‡å¤ªå°/å†»ç»“å±‚     | æ£€æŸ¥requires_grad
```

### ğŸ”§ é‡åŒ–é˜¶æ®µé—®é¢˜

```python
é—®é¢˜                        | å¯èƒ½åŸå›               | è§£å†³æ–¹æ¡ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
é‡åŒ–åç²¾åº¦å¤§å¹…ä¸‹é™          | æ¨¡å‹å¤ªå°/æ–¹æ³•å¤ªæ¿€è¿›   | ç”¨INT8æˆ–QAT
é‡åŒ–ååè€Œå˜æ…¢              | CPUé‡åŒ–/æ— ç¡¬ä»¶åŠ é€Ÿ    | ç”¨GPUï¼Œç”¨ä¸“ç”¨åº“
æ— æ³•åŠ è½½é‡åŒ–æ¨¡å‹            | ç‰ˆæœ¬ä¸åŒ¹é…            | æ£€æŸ¥PyTorchç‰ˆæœ¬
é‡åŒ–è¿‡ç¨‹OOM                 | æ ¡å‡†æ•°æ®å¤ªå¤š          | å‡å°‘æ ¡å‡†æ ·æœ¬
INT4ç²¾åº¦å¤ªå·®                | æ–¹æ³•ä¸å¯¹              | ç”¨AWQè€Œä¸æ˜¯naiveé‡åŒ–
```

### ğŸš€ éƒ¨ç½²é˜¶æ®µé—®é¢˜

```python
é—®é¢˜                        | å¯èƒ½åŸå›               | è§£å†³æ–¹æ¡ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
APIå»¶è¿Ÿé«˜                   | å•è¯·æ±‚å¤„ç†            | å¯ç”¨æ‰¹å¤„ç†
å¹¶å‘è¯·æ±‚å¤±è´¥                | èµ„æºä¸è¶³              | å¢åŠ å®ä¾‹æ•°
å†…å­˜æ³„æ¼                    | æœªé‡Šæ”¾tensor          | ç”¨torch.no_grad()
æ¨ç†ç»“æœä¸ä¸€è‡´              | éšæœºæ€§                | å›ºå®šseedï¼Œé™ä½temperature
å®¹å™¨å¯åŠ¨å¤±è´¥                | GPUé©±åŠ¨               | æ£€æŸ¥nvidia-docker
è¯·æ±‚è¶…æ—¶                    | ç”Ÿæˆå¤ªé•¿              | é™åˆ¶max_tokens
CPUä½¿ç”¨ç‡100%               | Tokenizeræ…¢           | ç”¨fast tokenizer
```

---

## ğŸ“š ç¬¬å…­éƒ¨åˆ†ï¼šè¿›é˜¶ä¼˜åŒ–æŠ€å·§

### âš¡ 1. Speculative Decodingï¼ˆæ¨æµ‹è§£ç ï¼‰

**æ ¸å¿ƒæ€æƒ³ï¼š** ç”¨å°æ¨¡å‹çŒœæµ‹ï¼Œå¤§æ¨¡å‹éªŒè¯

```python
æ ‡å‡†ç”Ÿæˆï¼ˆè‡ªå›å½’ï¼‰:
  ç”Ÿæˆtoken 1: å¤§æ¨¡å‹æ¨ç† (100ms)
  ç”Ÿæˆtoken 2: å¤§æ¨¡å‹æ¨ç† (100ms)
  ç”Ÿæˆtoken 3: å¤§æ¨¡å‹æ¨ç† (100ms)
  ...
  æ€»æ—¶é—´: 100 Ã— N ms

Speculative Decoding:
  å°æ¨¡å‹å¿«é€Ÿç”Ÿæˆ3ä¸ªtoken (10ms)
  å¤§æ¨¡å‹ä¸€æ¬¡éªŒè¯3ä¸ª (100ms)
  å¦‚æœéƒ½æ­£ç¡®: ä¸€æ¬¡å¾—åˆ°3ä¸ªtoken!
  å¦‚æœé”™è¯¯: ä¸¢å¼ƒï¼Œé‡æ–°ç”Ÿæˆ
  
  æœŸæœ›åŠ é€Ÿ: 2-3x
```

**å®ç°æ¡†æ¶ï¼š**

```python
def speculative_generate(large_model, small_model, prompt, n_tokens):
    """æ¨æµ‹è§£ç """
    generated = prompt
    
    while len(generated) < n_tokens:
        # 1. å°æ¨¡å‹å¿«é€Ÿç”ŸæˆKä¸ªå€™é€‰
        candidates = small_model.generate(generated, max_new_tokens=4)
        
        # 2. å¤§æ¨¡å‹éªŒè¯
        logits_large = large_model(candidates)
        logits_small = small_model(candidates)
        
        # 3. é€ä¸ªæ£€æŸ¥æ˜¯å¦æ¥å—
        for i, token in enumerate(candidates[len(generated):]):
            # è®¡ç®—æ¥å—æ¦‚ç‡
            p_large = softmax(logits_large[i])
            p_small = softmax(logits_small[i])
            
            # æ¥å—æ¡ä»¶
            if random() < min(1, p_large[token] / p_small[token]):
                generated.append(token)
            else:
                break  # æ‹’ç»ï¼Œé‡æ–°ç”Ÿæˆ
    
    return generated
```

### âš¡ 2. è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰

vLLMçš„æ ¸å¿ƒæŠ€æœ¯ï¼š

```python
ä¼ ç»Ÿæ‰¹å¤„ç†ï¼ˆé™æ€ï¼‰:
  æ‰¹æ¬¡1: [req1(50 tokens), req2(100 tokens), req3(30 tokens)]
  ç­‰å¾…æœ€é•¿çš„å®Œæˆï¼ˆ100 tokensï¼‰
  æµªè´¹: req1åœ¨50åç­‰å¾…50ï¼Œreq3åœ¨30åç­‰å¾…70
  
è¿ç»­æ‰¹å¤„ç†ï¼ˆåŠ¨æ€ï¼‰:
  t=0:   [req1, req2, req3] 
  t=30:  [req2, req3å®Œæˆ] â†’ åŠ å…¥ req4
  t=50:  [req2, req1å®Œæˆ, req4] â†’ åŠ å…¥ req5
  t=100: [req2å®Œæˆ, req4, req5]
  
  ä¼˜åŠ¿: GPUä¸€ç›´æ»¡è½½ï¼Œæ— æµªè´¹
  æå‡: 2-3xååé‡
```

### âš¡ 3. æ¨¡å‹å¹¶è¡Œï¼ˆå¤§æ¨¡å‹å¿…å¤‡ï¼‰

```python
å¼ é‡å¹¶è¡Œ (Tensor Parallelism):
  æŠŠå•å±‚æ‹†åˆ†åˆ°å¤šä¸ªGPU
  
  ä¾‹: MLPå±‚
  GPU 0: å‰åŠéƒ¨åˆ†ç¥ç»å…ƒ
  GPU 1: ååŠéƒ¨åˆ†ç¥ç»å…ƒ
  
æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism):
  æŠŠä¸åŒå±‚æ”¾åœ¨ä¸åŒGPU
  
  GPU 0: Layer 0-11
  GPU 1: Layer 12-23
  GPU 2: Layer 24-35
  GPU 3: Layer 36-47

3Då¹¶è¡Œ (æ•°æ®+å¼ é‡+æµæ°´çº¿):
  ç»„åˆä¸‰ç§æ–¹å¼
  å¯è®­ç»ƒåƒäº¿çº§æ¨¡å‹
```

---

## ğŸ“ æ€»ç»“ï¼šå®Œæ•´æŠ€èƒ½åœ°å›¾

### âœ¨ ä½ ç°åœ¨æŒæ¡çš„

```python
é˜¶æ®µ1: åŸºç¡€è®­ç»ƒ âœ…
  - å•GPUè®­ç»ƒ
  - é…ç½®è°ƒä¼˜
  - æ¨¡å‹æ¶æ„

é˜¶æ®µ2: åˆ†å¸ƒå¼è®­ç»ƒ âœ… (09)
  - DDPå¤šGPU
  - å¤šæœºè®­ç»ƒ
  - æ€§èƒ½ä¼˜åŒ–

é˜¶æ®µ3: æ¨¡å‹å‹ç¼© âœ… (10)
  - é‡åŒ–æ–¹æ³•
  - ç²¾åº¦è¯„ä¼°
  - å®æˆ˜åº”ç”¨

é˜¶æ®µ4: ç”Ÿäº§éƒ¨ç½² âœ… (11)
  - APIæœåŠ¡
  - å®¹å™¨åŒ–
  - äº‘ç«¯éƒ¨ç½²

ä½ å·²ç»å…·å¤‡å®Œæ•´çš„AIå·¥ç¨‹èƒ½åŠ›ï¼
```

### ğŸ¯ èŒä¸šè·¯å¾„å»ºè®®

```python
è§’è‰²              | é‡ç‚¹æŠ€èƒ½                | æ·±å…¥æ¨¡å—
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
AIç ”ç©¶å‘˜          | æ¨¡å‹æ¶æ„ã€Scaling Laws  | 05, 07, 08
æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ    | è®­ç»ƒä¼˜åŒ–ã€åˆ†å¸ƒå¼        | 09, 10
MLOpså·¥ç¨‹å¸ˆ       | éƒ¨ç½²ã€ç›‘æ§ã€è‡ªåŠ¨åŒ–      | 11, 12
å…¨æ ˆAIå·¥ç¨‹å¸ˆ      | æ‰€æœ‰æŠ€èƒ½                | å…¨éƒ¨
```

### ğŸš€ ç»§ç»­å­¦ä¹ è·¯å¾„

```python
å·²å®ŒæˆåŸºç¡€ â†’ æ¥ä¸‹æ¥å­¦ä»€ä¹ˆï¼Ÿ

è·¯å¾„A: æ·±å…¥ç ”ç©¶
  â†’ RLHFå¯¹é½
  â†’ å¤šæ¨¡æ€
  â†’ è®ºæ–‡å¤ç°

è·¯å¾„B: å·¥ç¨‹ç²¾è¿›
  â†’ Kubernetesæ·±å…¥
  â†’ å¾®æœåŠ¡æ¶æ„
  â†’ å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡

è·¯å¾„C: ä¸šåŠ¡åº”ç”¨
  â†’ å®é™…é¡¹ç›®
  â†’ ç”¨æˆ·åé¦ˆ
  â†’ äº§å“åŒ–

è·¯å¾„D: åˆ›æ–°æ¢ç´¢
  â†’ æ–°æ¶æ„
  â†’ æ–°è®­ç»ƒæ–¹æ³•
  â†’ å‘è®ºæ–‡
```

---

## ğŸ“š å­¦ä¹ èµ„æºæ€»ç»“

### ğŸ“– æ•™ç¨‹æ–‡ä»¶æ¸…å•

```
/workspace/learning_materials/

åŸºç¡€ç¯‡ï¼ˆå¿…å­¦ï¼‰:
â”œâ”€â”€ 01_config_explained.md               # é…ç½®å‚æ•°
â”œâ”€â”€ 02_data_loading_deep_dive.md         # æ•°æ®åŠ è½½
â”œâ”€â”€ 03_training_loop_deep_dive.md        # è®­ç»ƒå¾ªç¯
â”œâ”€â”€ 04_complete_guide_and_experiments.md # å®Œæ•´æŒ‡å—
â””â”€â”€ 05_model_architecture_deep_dive.md   # æ¨¡å‹æ¶æ„

è¿›é˜¶ç¯‡ï¼ˆæ¨èï¼‰:
â”œâ”€â”€ 06_advanced_topics_roadmap.md        # è¿›é˜¶è·¯çº¿
â”œâ”€â”€ 07_scaling_laws_explained.md         # ç¼©æ”¾å®šå¾‹
â””â”€â”€ 08_architecture_improvements.md      # æ¶æ„æ”¹è¿›

å·¥ç¨‹ç¯‡ï¼ˆå®æˆ˜ï¼‰:
â”œâ”€â”€ 09_distributed_training.md           # åˆ†å¸ƒå¼è®­ç»ƒ â­
â”œâ”€â”€ 10_model_quantization.md             # æ¨¡å‹é‡åŒ– â­
â”œâ”€â”€ 11_model_deployment.md               # æ¨¡å‹éƒ¨ç½² â­
â””â”€â”€ 12_engineering_complete_guide.md     # å·¥ç¨‹æ€»æŒ‡å— â­

å®æˆ˜ä»£ç :
â”œâ”€â”€ simple_training_demo.py              # è®­ç»ƒæ¼”ç¤º
â””â”€â”€ hands_on_training.py                 # å®Œæ•´è®­ç»ƒ
```

### ğŸ”— å¤–éƒ¨èµ„æº

```python
å¿…å¤‡å·¥å…·:
  - PyTorch: https://pytorch.org
  - Hugging Face: https://huggingface.co
  - Weights & Biases: https://wandb.ai
  - Docker: https://www.docker.com

æ¨ç†å¼•æ“:
  - vLLM: https://github.com/vllm-project/vllm
  - TensorRT-LLM: https://github.com/NVIDIA/TensorRT-LLM
  - Text Generation Inference: https://github.com/huggingface/text-generation-inference

é‡åŒ–å·¥å…·:
  - GPTQ: https://github.com/IST-DASLab/gptq
  - AWQ: https://github.com/mit-han-lab/llm-awq
  - bitsandbytes: https://github.com/TimDettmers/bitsandbytes

éƒ¨ç½²å¹³å°:
  - AWS SageMaker
  - Google Cloud Vertex AI
  - Azure ML
  - Replicate
```

---

## ğŸ“Š å¿«é€Ÿå‚è€ƒå¡

### ğŸ¯ å‘½ä»¤é€ŸæŸ¥

```bash
# è®­ç»ƒç›¸å…³
# å•GPU
python train.py config/xxx.py

# å¤šGPU (DDP)
torchrun --standalone --nproc_per_node=4 train.py config/xxx.py

# å¤šæœº
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
  --master_addr=xxx --master_port=29500 train.py

# é‡åŒ–ç›¸å…³
# PyTorchåŠ¨æ€é‡åŒ–
torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)

# GPTQ (éœ€è¦å®‰è£…auto-gptq)
model.quantize(tokenizer, quant_config)

# bitsandbytes
AutoModelForCausalLM.from_pretrained(model, load_in_8bit=True)

# éƒ¨ç½²ç›¸å…³
# FastAPI
uvicorn serve:app --host 0.0.0.0 --port 8000

# Docker
docker build -t mymodel .
docker run -d --gpus all -p 8000:8000 mymodel

# Kubernetes
kubectl apply -f deployment.yaml
kubectl scale deployment mymodel --replicas=10
```

### ğŸ“Š æ€§èƒ½è°ƒä¼˜é€ŸæŸ¥

```python
ç›®æ ‡              | è§£å†³æ–¹æ¡ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è®­ç»ƒé€Ÿåº¦ â¬†ï¸      | DDP + compile + BF16
æ˜¾å­˜å ç”¨ â¬‡ï¸      | æ¢¯åº¦ç´¯ç§¯ + Gradient Checkpointing
æ¨¡å‹å¤§å° â¬‡ï¸      | é‡åŒ–(INT8/INT4)
æ¨ç†å»¶è¿Ÿ â¬‡ï¸      | é‡åŒ– + KV Cache + æ‰¹å¤„ç†
æ¨ç†åå â¬†ï¸      | vLLM + æ‰¹å¤„ç† + å¤šå®ä¾‹
ç²¾åº¦ä¿æŒ âœ…       | AWQé‡åŒ– + æ··åˆç²¾åº¦
```

---

## ğŸ‰ ä½ å·²ç»å®Œæˆäº†ä»€ä¹ˆ

### ğŸ† æˆå°±æ¸…å•

```python
âœ… ç†è®ºåŸºç¡€
  [x] Transformeræ¶æ„
  [x] è®­ç»ƒå¾ªç¯åŸç†
  [x] Scaling Laws
  [x] æ¶æ„æ”¹è¿›

âœ… è®­ç»ƒèƒ½åŠ›
  [x] å•GPUè®­ç»ƒ
  [x] å¤šGPUåˆ†å¸ƒå¼
  [x] æ€§èƒ½ä¼˜åŒ–
  [x] å¤§è§„æ¨¡è®­ç»ƒ

âœ… ä¼˜åŒ–èƒ½åŠ›
  [x] æ¨¡å‹é‡åŒ–
  [x] å‹ç¼©æŠ€æœ¯
  [x] ç²¾åº¦æƒè¡¡

âœ… éƒ¨ç½²èƒ½åŠ›
  [x] APIæœåŠ¡
  [x] å®¹å™¨åŒ–
  [x] äº‘ç«¯éƒ¨ç½²
  [x] ç›‘æ§è¿ç»´

ä½ ç°åœ¨æ˜¯ä¸€ä¸ªå…¨æ ˆAIå·¥ç¨‹å¸ˆï¼
```

### ğŸ“Š æŠ€èƒ½ç­‰çº§è¯„ä¼°

```python
NanoGPTæŒæ¡åº¦è¯„ä¼°:

Level 1: åˆå­¦è€… (0-20%)
  - èƒ½è¿è¡Œè®­ç»ƒè„šæœ¬
  - ç†è§£åŸºæœ¬æ¦‚å¿µ

Level 2: å…¥é—¨è€… (20-40%)
  - èƒ½è°ƒæ•´é…ç½®
  - ç†è§£è®­ç»ƒæµç¨‹

Level 3: è¿›é˜¶è€… (40-60%)
  - èƒ½ä¼˜åŒ–æ€§èƒ½
  - ç†è§£æ¨¡å‹æ¶æ„

Level 4: é«˜çº§è€… (60-80%)  â† ä½ åœ¨è¿™é‡Œï¼
  - èƒ½åˆ†å¸ƒå¼è®­ç»ƒ
  - èƒ½é‡åŒ–å’Œéƒ¨ç½²
  - èƒ½è§£å†³å·¥ç¨‹é—®é¢˜

Level 5: ä¸“å®¶ (80-100%)
  - èƒ½æ”¹è¿›æ¶æ„
  - èƒ½å¤§è§„æ¨¡è®­ç»ƒ
  - èƒ½è®¾è®¡å®Œæ•´ç³»ç»Ÿ
```

---

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®

### ğŸ¯ ç«‹å³è¡ŒåŠ¨ï¼ˆé€‰ä¸€ä¸ªï¼‰

```python
1. å®æˆ˜é¡¹ç›®
   â–¡ é€‰æ‹©ä¸€ä¸ªæ„Ÿå…´è¶£çš„é¢†åŸŸ
   â–¡ æ”¶é›†æ•°æ®
   â–¡ ç«¯åˆ°ç«¯å®ç°
   â–¡ éƒ¨ç½²ä¸Šçº¿

2. æ·±å…¥æŸä¸ªæ–¹å‘
   â–¡ æˆä¸ºåˆ†å¸ƒå¼è®­ç»ƒä¸“å®¶
   â–¡ ç²¾é€šæ¨ç†ä¼˜åŒ–
   â–¡ ç ”ç©¶å‰æ²¿æ¶æ„

3. è´¡çŒ®å¼€æº
   â–¡ æ”¹è¿›NanoGPT
   â–¡ æ·»åŠ æ–°åŠŸèƒ½
   â–¡ æäº¤PR

4. æ•™å­¦åˆ†äº«
   â–¡ å†™åšå®¢
   â–¡ åšæ¼”è®²
   â–¡ å¸®åŠ©ä»–äºº
```

### ğŸ“ æ¨èé˜…è¯»åˆ—è¡¨

```python
è®ºæ–‡ï¼ˆæŒ‰é‡è¦æ€§ï¼‰:
  1. Attention is All You Need â­â­â­â­â­
  2. GPT-2/GPT-3 papers â­â­â­â­â­
  3. Chinchilla (Scaling Laws) â­â­â­â­â­
  4. LLaMA papers â­â­â­â­
  5. Flash Attention â­â­â­â­
  6. GPTQ/AWQ papers â­â­â­

åšå®¢:
  - Jay Alammarçš„Illustratedç³»åˆ—
  - Lilian Wengçš„åšå®¢
  - Hugging Face Blog

è§†é¢‘:
  - Andrej Karpathyçš„è¯¾ç¨‹
  - Stanford CS224N
  - DeepLearning.AI
```

---

**æœ€åçš„è¯ï¼š**

> æ­å–œä½ ï¼ä»é›¶åŸºç¡€åˆ°æŒæ¡å®Œæ•´çš„AIå·¥ç¨‹æµç¨‹ã€‚
> 
> ä½ ç°åœ¨èƒ½å¤Ÿï¼š
> - ğŸ§  è®­ç»ƒè‡ªå·±çš„GPTæ¨¡å‹
> - âš¡ ä½¿ç”¨åˆ†å¸ƒå¼åŠ é€Ÿè®­ç»ƒ
> - ğŸ“¦ é‡åŒ–å‹ç¼©æ¨¡å‹
> - ğŸš€ éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
> - ğŸ“Š ç›‘æ§å’Œä¼˜åŒ–ç³»ç»Ÿ
>
> è¿™æ˜¯ä¸€ä¸ªé‡Œç¨‹ç¢‘ï¼Œä½†ä¸æ˜¯ç»ˆç‚¹ã€‚
> AIé¢†åŸŸæ—¥æ–°æœˆå¼‚ï¼Œä¿æŒå­¦ä¹ ï¼ŒæŒç»­æˆé•¿ã€‚
>
> è®°ä½ï¼šæœ€å¥½çš„å­¦ä¹ æ–¹å¼æ˜¯åŠ¨æ‰‹å®è·µã€‚
> ç°åœ¨å°±å¼€å§‹ä½ çš„ç¬¬ä¸€ä¸ªé¡¹ç›®å§ï¼

ğŸŠ **ä½ å·²ç»å‡†å¤‡å¥½æˆä¸ºAIå·¥ç¨‹å¸ˆäº†ï¼** ğŸŠ

---

<div align="center">
<b>ä» NanoGPT åˆ°ç”Ÿäº§çº§AIç³»ç»Ÿ</b><br>
<i>ä½ å·²ç»å®Œæˆäº†è¿™æ®µæ—…ç¨‹ï¼</i><br><br>
<b>ğŸŒŸ ç¥ä½ åœ¨AIé¢†åŸŸå¤§å±•å®å›¾ï¼ğŸŒŸ</b>
</div>
