# Transformer æ¶æ„æ”¹è¿›å®Œå…¨æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

æ ‡å‡†çš„GPT-2 Transformerå¾ˆå¥½ï¼Œä½†èƒ½å¦åšå¾—**æ›´å¥½ã€æ›´å¿«ã€æ›´çœèµ„æº**ï¼Ÿ

ç­”æ¡ˆæ˜¯ï¼š**å¯ä»¥ï¼** è¿‡å»å‡ å¹´å‡ºç°äº†å¤§é‡æ¶æ„æ”¹è¿›ã€‚

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸ºä»€ä¹ˆéœ€è¦æ”¹è¿›ï¼Ÿ

### ğŸ” æ ‡å‡†Transformerçš„é—®é¢˜

```python
é—®é¢˜æ¸…å•:

1ï¸âƒ£ ä½ç½®ç¼–ç çš„å±€é™
   æ ‡å‡†: ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå­¦ä¹ å¼ï¼‰
   é—®é¢˜: 
   - è®­ç»ƒé•¿åº¦å›ºå®šï¼ˆå¦‚1024ï¼‰
   - æ¨ç†æ—¶ä¸èƒ½å¤–æ¨åˆ°2048
   - ä½ç½®ä¿¡æ¯ä¸å¤Ÿç²¾ç¡®
   
2ï¸âƒ£ æ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦
   æ ‡å‡†: Self-Attention
   å¤æ‚åº¦: O(nÂ²)
   é—®é¢˜:
   - åºåˆ—é•¿åº¦ç¿»å€ â†’ å†…å­˜/è®¡ç®—é‡4å€ï¼
   - é•¿æ–‡æœ¬å¤„ç†å›°éš¾
   
3ï¸âƒ£ è®­ç»ƒä¸ç¨³å®š
   æ ‡å‡†: Post-LayerNorm
   é—®é¢˜:
   - æ·±å±‚ç½‘ç»œæ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
   - éœ€è¦carefulçš„åˆå§‹åŒ–
   - å­¦ä¹ ç‡è°ƒä¼˜å›°éš¾
   
4ï¸âƒ£ æ•ˆç‡ä¸å¤Ÿé«˜
   æ ‡å‡†: GELUæ¿€æ´»
   é—®é¢˜:
   - è®¡ç®—ä¸æ˜¯æœ€ä¼˜
   - å¯ä»¥æ›´å¿«
```

### ğŸ“Š æ”¹è¿›çš„ç›®æ ‡

```
æ”¹è¿›ç»´åº¦:

æ€§èƒ½ (Performance) â¬†ï¸
  - æ›´ä½çš„loss
  - æ›´å¥½çš„æ³›åŒ–

é€Ÿåº¦ (Speed) â¬†ï¸
  - è®­ç»ƒæ›´å¿«
  - æ¨ç†æ›´å¿«

å†…å­˜ (Memory) â¬‡ï¸
  - æ˜¾å­˜å ç”¨æ›´å°‘
  - å¯ä»¥è®­ç»ƒæ›´å¤§æ¨¡å‹

ç¨³å®šæ€§ (Stability) â¬†ï¸
  - è®­ç»ƒæ›´ç¨³å®š
  - ä¸å®¹æ˜“å´©æºƒ

æ‰©å±•æ€§ (Scalability) â¬†ï¸
  - æ”¯æŒæ›´é•¿åºåˆ—
  - æ›´å®¹æ˜“scale up
```

---

## ğŸ”§ ç¬¬äºŒéƒ¨åˆ†ï¼šä½ç½®ç¼–ç æ”¹è¿›

### ğŸ“ 1. æ ‡å‡†ä½ç½®ç¼–ç å›é¡¾

**NanoGPTä½¿ç”¨çš„æ–¹æ³•ï¼ˆå­¦ä¹ å¼ç»å¯¹ä½ç½®ç¼–ç ï¼‰ï¼š**

```python
# model.py ç¬¬128è¡Œ
self.wpe = nn.Embedding(config.block_size, config.n_embd)

# ä½¿ç”¨æ–¹å¼
pos = torch.arange(0, t, dtype=torch.long, device=device)  # [0, 1, 2, ..., t-1]
pos_emb = self.transformer.wpe(pos)  # æŸ¥è¡¨å¾—åˆ°ä½ç½®å‘é‡

# æœ€ç»ˆè¾“å…¥
x = tok_emb + pos_emb
```

**é—®é¢˜æ¼”ç¤ºï¼š**

```python
# è®­ç»ƒæ—¶
block_size = 1024
pos_emb = Embedding(1024, 768)  # åªå­¦ä¹ äº†1024ä¸ªä½ç½®

# æ¨ç†æ—¶æƒ³ç”¨æ›´é•¿çš„åºåˆ—
test_input = "..." # 2048 tokens
pos = torch.arange(0, 2048)  # âŒ è¶…å‡ºèŒƒå›´ï¼
pos_emb = self.wpe(pos)  # æŠ¥é”™ï¼šIndexError

# å³ä½¿ä¸æŠ¥é”™ï¼Œä½ç½®1025-2048çš„embeddingä¹Ÿæ²¡è§è¿‡
# æ¨¡å‹ä¸çŸ¥é“å¦‚ä½•å¤„ç†
```

---

### ğŸŒ€ 2. RoPE (Rotary Position Embedding)

**æ ¸å¿ƒæ€æƒ³ï¼š** ç”¨æ—‹è½¬çŸ©é˜µç¼–ç ä½ç½®ä¿¡æ¯

**ä¸ºä»€ä¹ˆå«"æ—‹è½¬"ï¼Ÿ**

```python
æƒ³è±¡åœ¨2Dç©ºé—´ï¼š

ä½ç½®0: (1, 0)      â†’  è§’åº¦ 0Â°
ä½ç½®1: (0.7, 0.7)  â†’  è§’åº¦ 45Â°
ä½ç½®2: (0, 1)      â†’  è§’åº¦ 90Â°
ä½ç½®3: (-0.7, 0.7) â†’  è§’åº¦ 135Â°
...

æ¯ä¸ªä½ç½®å¯¹åº”ä¸€ä¸ªæ—‹è½¬è§’åº¦ï¼
ä½ç½®è¶Šè¿œï¼Œæ—‹è½¬è§’åº¦è¶Šå¤§
```

**æ•°å­¦åŸç†ï¼š**

```python
# å¯¹äºä½ç½®mçš„tokenï¼Œå…¶queryå’Œkeyå‘é‡qå’Œk
# åº”ç”¨æ—‹è½¬å˜æ¢

q_m = R(mÎ¸) @ q  # æ—‹è½¬mÎ¸è§’åº¦
k_n = R(nÎ¸) @ k  # æ—‹è½¬nÎ¸è§’åº¦

# æ³¨æ„åŠ›åˆ†æ•°
score = q_m^T @ k_n
      = q^T @ R(mÎ¸)^T @ R(nÎ¸) @ k
      = q^T @ R((n-m)Î¸) @ k
      
å…³é”®å‘ç°ï¼š
  scoreåªä¾èµ–äºç›¸å¯¹ä½ç½® (n-m)ï¼
  è¿™å°±æ˜¯ç›¸å¯¹ä½ç½®ç¼–ç çš„æœ¬è´¨
```

**å®ç°ä»£ç ï¼š**

```python
class RotaryPositionEmbedding(nn.Module):
    """RoPE - LLaMA, GPT-Neo-Xä½¿ç”¨"""
    
    def __init__(self, dim, max_seq_len=2048, base=10000):
        super().__init__()
        # è®¡ç®—æ—‹è½¬é¢‘ç‡
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
        # é¢„è®¡ç®—ä½ç½®ç¼–ç ï¼ˆå¯é€‰ï¼ŒåŠ é€Ÿï¼‰
        t = torch.arange(max_seq_len, dtype=torch.float)
        freqs = torch.outer(t, inv_freq)  # [seq_len, dim//2]
        emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, dim]
        self.register_buffer('cos', emb.cos())
        self.register_buffer('sin', emb.sin())
    
    def rotate_half(self, x):
        """è¾…åŠ©å‡½æ•°ï¼šæ—‹è½¬å‘é‡çš„ä¸€åŠç»´åº¦"""
        x1, x2 = x.chunk(2, dim=-1)
        return torch.cat([-x2, x1], dim=-1)
    
    def forward(self, q, k, seq_len):
        """
        q, k: [batch, heads, seq_len, head_dim]
        """
        # è·å–coså’Œsin
        cos = self.cos[:seq_len, :]
        sin = self.sin[:seq_len, :]
        
        # åº”ç”¨æ—‹è½¬
        # q_rotated = q * cos + rotate_half(q) * sin
        q_embed = (q * cos) + (self.rotate_half(q) * sin)
        k_embed = (k * cos) + (self.rotate_half(k) * sin)
        
        return q_embed, k_embed
```

**ä½¿ç”¨æ–¹å¼ï¼š**

```python
# åœ¨ CausalSelfAttention ä¸­
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ... å…¶ä»–åˆå§‹åŒ– ...
        
        # æ·»åŠ RoPEï¼ˆæ›¿ä»£ä½ç½®embeddingï¼‰
        self.rope = RotaryPositionEmbedding(
            dim=config.n_embd // config.n_head,
            max_seq_len=config.block_size
        )
    
    def forward(self, x):
        B, T, C = x.size()
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
        
        # é‡å¡‘ä¸ºå¤šå¤´
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        
        # åº”ç”¨RoPEï¼ˆå…³é”®æ”¹åŠ¨ï¼ï¼‰
        q, k = self.rope(q, k, T)
        
        # å‰©ä½™çš„æ³¨æ„åŠ›è®¡ç®—ä¸å˜
        # ...
```

**ä¼˜åŠ¿ï¼š**

```python
âœ… ç›¸å¯¹ä½ç½®ä¿¡æ¯
   scoreåªä¾èµ–(n-m)ï¼Œæ›´åˆç†

âœ… æ— é™å¤–æ¨
   è®­ç»ƒ1024ï¼Œæ¨ç†10000 âœ…
   åªéœ€è®¡ç®—æ–°çš„cos/sin

âœ… ä¸å¢åŠ å‚æ•°
   ä¸éœ€è¦å­¦ä¹ embedding table

âœ… æ€§èƒ½æ›´å¥½
   LLaMA, GPT-NeoXéƒ½ç”¨å®ƒ
   
å®æµ‹æ•ˆæœ:
  è®­ç»ƒé•¿åº¦: 2048
  æµ‹è¯•é•¿åº¦: 4096
  
  æ ‡å‡†ä½ç½®ç¼–ç : perplexityçˆ†ç‚¸ âŒ
  RoPE: perplexityç¨³å®š âœ…
```

---

### ğŸ“ 3. ALiBi (Attention with Linear Biases)

**æ ¸å¿ƒæ€æƒ³ï¼š** ç›´æ¥åœ¨attentionåˆ†æ•°ä¸ŠåŠ ä½ç½®åç½®

**ä¸ºä»€ä¹ˆç®€å•æœ‰æ•ˆï¼Ÿ**

```python
ä¼ ç»Ÿæ–¹æ³•: åœ¨è¾“å…¥ä¸ŠåŠ ä½ç½®ä¿¡æ¯
  x = token_emb + pos_emb
  ç„¶åè®¡ç®—attention

ALiBi: åœ¨attentionåˆ†æ•°ä¸Šç›´æ¥å‡å»è·ç¦»
  score = Q @ K^T
  score = score - m * distance
  
å…¶ä¸­ m æ˜¯æ¯ä¸ªheadçš„slopeï¼ˆæ–œç‡ï¼‰
```

**å¯è§†åŒ–ï¼š**

```
Attentionåˆ†æ•°çŸ©é˜µï¼ˆåº”ç”¨ALiBiå‰ï¼‰ï¼š

       k0   k1   k2   k3   k4
   q0  5.2  3.1  2.8  2.1  1.5
   q1  4.1  6.3  3.5  2.9  2.0
   q2  3.2  4.5  7.1  4.2  3.1
   ...

ALiBiåç½®çŸ©é˜µï¼ˆm=0.5ï¼‰ï¼š

       k0   k1   k2   k3   k4
   q0  0   -0.5 -1.0 -1.5 -2.0
   q1  0.5  0   -0.5 -1.0 -1.5
   q2  1.0  0.5  0   -0.5 -1.0
   ...
   
è§„å¾‹: bias = -m Ã— |position_diff|

åº”ç”¨åï¼ˆç›¸åŠ ï¼‰ï¼š

       k0   k1   k2   k3   k4
   q0  5.2  2.6  1.8  0.6 -0.5  â† è·ç¦»è¿œçš„è¢«æƒ©ç½š
   q1  4.6  6.3  3.0  1.9  0.5
   q2  4.2  5.0  7.1  3.7  2.1
```

**å®ç°ä»£ç ï¼š**

```python
class ALiBiPositionalBias(nn.Module):
    """ALiBi - BLOOMä½¿ç”¨"""
    
    def __init__(self, num_heads, max_seq_len=2048):
        super().__init__()
        # ä¸ºæ¯ä¸ªheadè®¾ç½®ä¸åŒçš„slope
        slopes = self._get_slopes(num_heads)
        
        # é¢„è®¡ç®—åç½®çŸ©é˜µ
        position_bias = self._get_bias(max_seq_len, slopes)
        self.register_buffer('position_bias', position_bias)
    
    def _get_slopes(self, n):
        """è®¡ç®—æ¯ä¸ªheadçš„slope"""
        # å‡ ä½•çº§æ•°ï¼š2^(-8/n), 2^(-16/n), ...
        def get_slopes_power_of_2(n):
            start = 2 ** (-2 ** -(math.log2(n) - 3))
            ratio = start
            return [start * (ratio ** i) for i in range(n)]
        
        # å¤„ç†é2çš„å¹‚æ¬¡
        if math.log2(n).is_integer():
            return get_slopes_power_of_2(n)
        else:
            closest_power = 2 ** math.floor(math.log2(n))
            slopes = get_slopes_power_of_2(closest_power)
            # å¡«å……å‰©ä½™çš„slopes
            extra = n - closest_power
            slopes.extend(get_slopes_power_of_2(2 * closest_power)[:extra])
            return slopes
    
    def _get_bias(self, max_len, slopes):
        """æ„å»ºåç½®çŸ©é˜µ"""
        # è·ç¦»çŸ©é˜µ
        # position_ids: [0, 1, 2, ..., max_len-1]
        position_ids = torch.arange(max_len)
        # distance[i, j] = j - i
        distance = position_ids[None, :] - position_ids[:, None]
        
        # åº”ç”¨slopes
        slopes_tensor = torch.tensor(slopes).view(-1, 1, 1)
        bias = -torch.abs(distance)[None, :, :] * slopes_tensor
        
        return bias  # [num_heads, max_len, max_len]
    
    def forward(self, attention_scores, seq_len):
        """
        attention_scores: [batch, heads, seq_len, seq_len]
        """
        # è·å–å½“å‰åºåˆ—é•¿åº¦çš„bias
        bias = self.position_bias[:, :seq_len, :seq_len]
        
        # åŠ åˆ°attention scoresä¸Š
        return attention_scores + bias
```

**ä½¿ç”¨æ–¹å¼ï¼š**

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ... å…¶ä»–åˆå§‹åŒ– ...
        
        # æ·»åŠ ALiBi
        self.alibi = ALiBiPositionalBias(
            num_heads=config.n_head,
            max_seq_len=config.block_size
        )
    
    def forward(self, x):
        # ... è®¡ç®—Q, K, V ...
        
        # è®¡ç®—attention scores
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        
        # åº”ç”¨ALiBiï¼ˆå…³é”®æ”¹åŠ¨ï¼ï¼‰
        att = self.alibi(att, T)
        
        # åº”ç”¨å› æœæ©ç 
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        
        # å‰©ä½™è®¡ç®—
        att = F.softmax(att, dim=-1)
        # ...
```

**ä¼˜åŠ¿ï¼š**

```python
âœ… æç®€å®ç°
   åªéœ€è¦ä¸€ä¸ªåŠ æ³•æ“ä½œ
   
âœ… å¤–æ¨èƒ½åŠ›å¼º
   è®­ç»ƒ512 â†’ æµ‹è¯•2048: perplexityå‡ ä¹ä¸å˜
   
âœ… ä¸å¢åŠ å‚æ•°
   biasæ˜¯é¢„å…ˆè®¡ç®—å¥½çš„
   
âœ… è®­ç»ƒç¨³å®š
   BLOOM (176B)æˆåŠŸä½¿ç”¨
   
å¯¹æ¯”å®éªŒï¼ˆBLOOMè®ºæ–‡ï¼‰:
  è®­ç»ƒé•¿åº¦: 2048
  æµ‹è¯•é•¿åº¦: 8192
  
  æ–¹æ³•          | Perplexityå¢åŠ 
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ ‡å‡†ä½ç½®ç¼–ç    | +150%  âŒ
  RoPE          | +20%   âœ…
  ALiBi         | +3%    âœ…âœ…
```

---

### ğŸ”„ 4. ä½ç½®ç¼–ç å¯¹æ¯”æ€»ç»“

```python
å¯¹æ¯”è¡¨ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹æ³•          â”‚ å‚æ•°é‡  â”‚ å¤–æ¨èƒ½åŠ›â”‚ å®ç°éš¾åº¦â”‚ æ€§èƒ½    â”‚ ä»£è¡¨æ¨¡å‹â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç»å¯¹ä½ç½®ç¼–ç   â”‚ å¢åŠ     â”‚ âŒ å·®   â”‚ â­ç®€å•  â”‚ â­â­    â”‚ GPT-2   â”‚
â”‚ RoPE         â”‚ 0       â”‚ âœ… å¥½   â”‚ â­â­ä¸­  â”‚ â­â­â­  â”‚ LLaMA   â”‚
â”‚ ALiBi        â”‚ 0       â”‚ âœ… å¾ˆå¥½ â”‚ â­ç®€å•  â”‚ â­â­â­  â”‚ BLOOM   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‰æ‹©å»ºè®®:
  æ–°é¡¹ç›®: RoPEï¼ˆç»¼åˆæœ€ä¼˜ï¼‰
  éœ€è¦å¤–æ¨: ALiBiï¼ˆå¤–æ¨æœ€å¼ºï¼‰
  ç®€å•å®ç°: ALiBiï¼ˆæœ€ç®€å•ï¼‰
  å¤ç°è®ºæ–‡: çœ‹è®ºæ–‡ç”¨ä»€ä¹ˆ
```

---

## âš¡ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›

### ğŸš€ 1. Flash Attention

**é—®é¢˜ï¼šæ ‡å‡†Attentionçš„å†…å­˜ç“¶é¢ˆ**

```python
æ ‡å‡†Attention:

# ä¼ªä»£ç 
Q, K, V = split(x)  # [B, H, T, D]

# æ­¥éª¤1: è®¡ç®—scores
S = Q @ K.T  # [B, H, T, T] â† éœ€è¦å­˜å‚¨ï¼
             # å¯¹äºT=2048, H=32: 
             # 2048Ã—2048Ã—32 = 134M å…ƒç´ 

# æ­¥éª¤2: Softmax
P = softmax(S)  # [B, H, T, T] â† åˆè¦å­˜å‚¨ï¼

# æ­¥éª¤3: åŠ æƒæ±‚å’Œ
O = P @ V  # [B, H, T, D]

å†…å­˜å ç”¨: O(TÂ²)
é—®é¢˜: T=2048æ—¶ï¼Œä»…attentionçŸ©é˜µå°±å ç”¨512MBï¼ˆå•æ ·æœ¬ï¼‰
```

**Flash Attentionçš„åˆ›æ–°ï¼š**

```python
æ ¸å¿ƒæ€æƒ³: åˆ†å—è®¡ç®—ï¼Œé¿å…å­˜å‚¨å®Œæ•´çš„attentionçŸ©é˜µ

æ ‡å‡†æ–¹æ³•ï¼ˆHBM â†” SRAMå¤šæ¬¡å¾€è¿”ï¼‰:
  HBM(æ…¢)                SRAM(å¿«)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ Q, K, V â”‚ â”€â”€â”€â”€â†’     â”‚ S=QK â”‚
  â”‚ SçŸ©é˜µ   â”‚ â†â”€â”€â”€â”€     â””â”€â”€â”€â”€â”€â”€â”˜
  â”‚ PçŸ©é˜µ   â”‚ â”€â”€â”€â”€â†’     â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ ç»“æœ    â”‚ â†â”€â”€â”€â”€     â”‚P=softâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”˜
  
  è¯»å†™æ¬¡æ•°: 4æ¬¡HBMè®¿é—®ï¼ˆéå¸¸æ…¢ï¼ï¼‰

Flash Attentionï¼ˆä¸€æ¬¡æ€§åœ¨SRAMå®Œæˆï¼‰:
  HBM(æ…¢)                SRAM(å¿«)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Q, K, V â”‚ â”€â”€â”€â”€â†’     â”‚ åˆ†å—è®¡ç®—  â”‚
  â”‚         â”‚           â”‚ Sâ†’Pâ†’O    â”‚
  â”‚         â”‚           â”‚ å…¨éƒ¨å®Œæˆ  â”‚
  â”‚ ç»“æœ    â”‚ â†â”€â”€â”€â”€     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  è¯»å†™æ¬¡æ•°: 2æ¬¡HBMè®¿é—®ï¼ˆå¿«ï¼ï¼‰

å…³é”®æŠ€æœ¯:
  1. åˆ†å—ï¼ˆTilingï¼‰: æŠŠQ,K,Våˆ†æˆå°å—
  2. é‡æ–°è®¡ç®—ï¼ˆRecomputationï¼‰: ä¸å­˜å‚¨ä¸­é—´ç»“æœ
  3. åœ¨çº¿Softmax: å¢é‡è®¡ç®—ï¼Œä¸å­˜å‚¨å®Œæ•´çŸ©é˜µ
```

**æ€§èƒ½æå‡ï¼š**

```python
å®æµ‹å¯¹æ¯”ï¼ˆT=2048, H=16, D=64ï¼‰:

æŒ‡æ ‡              | æ ‡å‡†Attention | Flash Attention
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å‰å‘æ—¶é—´          | 125ms         | 35ms (3.6x faster)
åå‘æ—¶é—´          | 310ms         | 90ms (3.4x faster)
å†…å­˜å ç”¨          | 8.2GB         | 2.1GB (3.9x less)
æœ€å¤§åºåˆ—é•¿åº¦(A100)| 4096          | 16384 (4x longer)

ç»“è®º: æ›´å¿«ã€æ›´çœå†…å­˜ï¼
```

**åœ¨NanoGPTä¸­å·²ç»é›†æˆï¼š**

```python
# model.py ç¬¬62-64è¡Œ

if self.flash:
    # ä½¿ç”¨PyTorchå†…ç½®çš„Flash Attention
    y = torch.nn.functional.scaled_dot_product_attention(
        q, k, v, 
        attn_mask=None, 
        dropout_p=self.dropout if self.training else 0, 
        is_causal=True
    )
```

---

### ğŸ¯ 2. Multi-Query Attention (MQA)

**æ ¸å¿ƒæ€æƒ³ï¼š** å¤šä¸ªQuery headå…±äº«ä¸€ä¸ªKeyå’ŒValue

**ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿ**

```python
æ ‡å‡†Multi-Head Attention:

æ¯ä¸ªheadç‹¬ç«‹:
  Q1, K1, V1
  Q2, K2, V2
  ...
  Q12, K12, V12

å‚æ•°é‡: 3 Ã— n_embd Ã— n_embd
æ¨ç†KV cache: éœ€è¦å­˜å‚¨æ‰€æœ‰headçš„K, V

Multi-Query Attention:

å¤šä¸ªQuery headå…±äº«K, V:
  Q1, Q2, ..., Q12 (ç‹¬ç«‹)
  K, V (å…±äº«ï¼)

å‚æ•°é‡: å‡å°‘çº¦33%
æ¨ç†KV cache: å‡å°‘å¾ˆå¤šï¼
```

**å®ç°ä»£ç ï¼š**

```python
class MultiQueryAttention(nn.Module):
    """MQA - PaLM, Falconä½¿ç”¨"""
    
    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        
        # Query: æ¯ä¸ªheadç‹¬ç«‹
        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        
        # Key, Value: å…±äº«ï¼ˆå…³é”®ï¼ï¼‰
        head_dim = config.n_embd // config.n_head
        self.k_proj = nn.Linear(config.n_embd, head_dim, bias=config.bias)
        self.v_proj = nn.Linear(config.n_embd, head_dim, bias=config.bias)
        
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
    
    def forward(self, x):
        B, T, C = x.size()
        
        # Query: å¤šå¤´
        q = self.q_proj(x)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        # [B, n_head, T, head_dim]
        
        # Key, Value: å•å¤´ï¼ˆå…±äº«ï¼‰
        k = self.k_proj(x)  # [B, T, head_dim]
        v = self.v_proj(x)  # [B, T, head_dim]
        
        # æ‰©å±•K, Vä»¥åŒ¹é…Qçš„headæ•°
        k = k.unsqueeze(1).expand(-1, self.n_head, -1, -1)
        v = v.unsqueeze(1).expand(-1, self.n_head, -1, -1)
        # [B, n_head, T, head_dim]
        
        # æ ‡å‡†Attentionè®¡ç®—
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = F.softmax(att, dim=-1)
        y = att @ v
        
        # åˆå¹¶heads
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.c_proj(y)
        
        return y
```

**ä¼˜åŠ¿ï¼š**

```python
âœ… æ¨ç†é€Ÿåº¦å¿«
   KV cacheå‡å°‘ â†’ æ¨ç†å»¶è¿Ÿé™ä½30-40%
   
âœ… å‚æ•°é‡å°‘
   å‡å°‘33%çš„å‚æ•°ï¼ˆK, VæŠ•å½±ï¼‰
   
âœ… æ€§èƒ½å‡ ä¹ä¸æŸå¤±
   å®éªŒæ˜¾ç¤º: <2% perplexityå¢åŠ 
   
åº”ç”¨æ¡ˆä¾‹:
  - PaLM (Google)
  - Falcon (TII)
  - StarCoder
```

---

### âš™ï¸ 3. Grouped-Query Attention (GQA)

**æ ¸å¿ƒæ€æƒ³ï¼š** MHAå’ŒMQAçš„æŠ˜ä¸­æ–¹æ¡ˆ

```python
Multi-Head Attention (MHA):
  12ä¸ªQuery heads
  12ä¸ªKey heads     â† æ¯ä¸ªç‹¬ç«‹
  12ä¸ªValue heads
  
Multi-Query Attention (MQA):
  12ä¸ªQuery heads
  1ä¸ªKey head       â† å…¨éƒ¨å…±äº«
  1ä¸ªValue head
  
Grouped-Query Attention (GQA):
  12ä¸ªQuery heads
  3ä¸ªKey heads      â† åˆ†ç»„å…±äº«ï¼ˆå¦‚æ¯4ä¸ªQå…±äº«1ä¸ªK/Vï¼‰
  3ä¸ªValue heads

å¹³è¡¡: MHAçš„æ€§èƒ½ + MQAçš„æ•ˆç‡
```

**å¯è§†åŒ–ï¼š**

```
MHA (n_head=12):
  Q1â†’K1,V1  Q2â†’K2,V2  Q3â†’K3,V3  ...  Q12â†’K12,V12
  
MQA (n_head=12):
  Q1â†’K,V  Q2â†’K,V  Q3â†’K,V  ...  Q12â†’K,V
  
GQA (n_head=12, n_group=3):
  Q1,Q2,Q3,Q4 â†’ K1,V1
  Q5,Q6,Q7,Q8 â†’ K2,V2
  Q9,Q10,Q11,Q12 â†’ K3,V3
```

**å®ç°ä»£ç ï¼š**

```python
class GroupedQueryAttention(nn.Module):
    """GQA - LLaMA-2ä½¿ç”¨"""
    
    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_kv_head = config.n_kv_head  # å¦‚ 3
        self.n_embd = config.n_embd
        
        assert self.n_head % self.n_kv_head == 0
        self.n_rep = self.n_head // self.n_kv_head  # å¦‚ 12/3=4
        
        head_dim = config.n_embd // config.n_head
        
        # Query: æ‰€æœ‰head
        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)
        
        # Key, Value: åªæœ‰n_kv_headä¸ª
        self.k_proj = nn.Linear(config.n_embd, head_dim * self.n_kv_head, bias=False)
        self.v_proj = nn.Linear(config.n_embd, head_dim * self.n_kv_head, bias=False)
        
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)
    
    def forward(self, x):
        B, T, C = x.size()
        
        # Query: [B, n_head, T, head_dim]
        q = self.q_proj(x).view(B, T, self.n_head, -1).transpose(1, 2)
        
        # Key, Value: [B, n_kv_head, T, head_dim]
        k = self.k_proj(x).view(B, T, self.n_kv_head, -1).transpose(1, 2)
        v = self.v_proj(x).view(B, T, self.n_kv_head, -1).transpose(1, 2)
        
        # æ‰©å±•K, Vä»¥åŒ¹é…Q
        # repeat_interleave: [a,b,c] â†’ [a,a,a,a,b,b,b,b,c,c,c,c]
        k = k.repeat_interleave(self.n_rep, dim=1)  # [B, n_head, T, head_dim]
        v = v.repeat_interleave(self.n_rep, dim=1)
        
        # æ ‡å‡†Attention
        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))
        att = F.softmax(att, dim=-1)
        y = att @ v
        
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.c_proj(y)
        
        return y
```

**å¯¹æ¯”ï¼š**

```python
æ€§èƒ½å¯¹æ¯”ï¼ˆT=2048ï¼‰:

æ–¹æ³•    | KV Cache | æ¨ç†é€Ÿåº¦ | è®­ç»ƒé€Ÿåº¦ | æ€§èƒ½
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€
MHA     | 100%     | 1.0x    | 1.0x    | 100%
GQA(4ç»„)| 33%      | 1.3x    | 1.1x    | 99.5%
MQA     | 8%       | 1.5x    | 1.2x    | 98%

æ¨è: GQA (æœ€ä½³å¹³è¡¡)
```

---

## ğŸ“ ç¬¬å››éƒ¨åˆ†ï¼šå½’ä¸€åŒ–æ–¹æ³•æ”¹è¿›

### ğŸ”„ 1. Pre-LN vs Post-LN

**æ ‡å‡†Transformerï¼ˆPost-LNï¼‰ï¼š**

```python
# åŸå§‹Transformerè®ºæ–‡çš„é¡ºåº

def transformer_block(x):
    # Attention
    x = x + Attention(x)         # å…ˆæ®‹å·®
    x = LayerNorm(x)             # åå½’ä¸€åŒ–
    
    # FFN
    x = x + FFN(x)               # å…ˆæ®‹å·®
    x = LayerNorm(x)             # åå½’ä¸€åŒ–
    
    return x

é—®é¢˜:
  - è®­ç»ƒä¸ç¨³å®š
  - éœ€è¦warmup
  - æ·±å±‚ç½‘ç»œå®¹æ˜“å´©æºƒ
```

**Pre-LNï¼ˆç°ä»£æ ‡å‡†ï¼‰ï¼š**

```python
# GPT-2, NanoGPTä½¿ç”¨çš„é¡ºåº

def transformer_block(x):
    # Attention
    x = x + Attention(LayerNorm(x))  # å…ˆå½’ä¸€åŒ–ï¼Œåæ®‹å·®
    
    # FFN
    x = x + FFN(LayerNorm(x))        # å…ˆå½’ä¸€åŒ–ï¼Œåæ®‹å·®
    
    return x

# NanoGPTä»£ç  (model.py ç¬¬103-105è¡Œ)
x = x + self.attn(self.ln_1(x))
x = x + self.mlp(self.ln_2(x))

ä¼˜åŠ¿:
  âœ… è®­ç»ƒç¨³å®š
  âœ… ä¸éœ€è¦warmup
  âœ… å¯ä»¥è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ
  âœ… æ”¶æ•›æ›´å¿«
```

**å¯è§†åŒ–å¯¹æ¯”ï¼š**

```
Post-LN:
  Input â†’ [+]â”€â†’ LN â†’ Output
          â†‘
      Attention
          â”‚
         LN
          
  æ¢¯åº¦è·¯å¾„: ç»è¿‡LayerNormï¼ˆå¯èƒ½æ¶ˆå¤±ï¼‰

Pre-LN:
  Input â†’ LN â†’ Attention â†’ [+] â†’ Output
                           â†‘
                        ç›´æ¥æ®‹å·®
  
  æ¢¯åº¦è·¯å¾„: ç›´æ¥é€šè¿‡æ®‹å·®ï¼ˆç¨³å®šï¼ï¼‰
```

---

### ğŸ“Š 2. RMSNorm

**æ ¸å¿ƒæ€æƒ³ï¼š** ç®€åŒ–çš„LayerNorm

**LayerNormçš„é—®é¢˜ï¼š**

```python
# æ ‡å‡†LayerNorm
def layer_norm(x):
    mean = x.mean(dim=-1, keepdim=True)      # è®¡ç®—å‡å€¼
    var = x.var(dim=-1, keepdim=True)        # è®¡ç®—æ–¹å·®
    x = (x - mean) / sqrt(var + eps)         # æ ‡å‡†åŒ–
    x = x * gamma + beta                      # ç¼©æ”¾å’Œåç§»
    return x

# é—®é¢˜: 
# 1. éœ€è¦è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼ˆä¸¤ä¸ªç»Ÿè®¡é‡ï¼‰
# 2. éœ€è¦ä¸¤æ¬¡éå†æ•°æ®
# 3. å‡å€¼ä¸­å¿ƒåŒ–æ˜¯å¦å¿…è¦ï¼Ÿ
```

**RMSNormçš„ç®€åŒ–ï¼š**

```python
# Root Mean Square Normalization
def rms_norm(x):
    # åªè®¡ç®—RMSï¼Œä¸ä¸­å¿ƒåŒ–
    rms = sqrt(mean(xÂ²) + eps)               # åªéœ€è¦ä¸€ä¸ªç»Ÿè®¡é‡
    x = x / rms                               # ç›´æ¥é™¤ä»¥RMS
    x = x * gamma                             # åªéœ€è¦ç¼©æ”¾ï¼Œä¸éœ€è¦åç§»
    return x

# æ›´ç®€å•:
# 1. åªéœ€è¦ä¸€ä¸ªç»Ÿè®¡é‡ï¼ˆRMSï¼‰
# 2. åªéœ€è¦ä¸€æ¬¡éå†
# 3. å‚æ•°æ›´å°‘ï¼ˆæ²¡æœ‰betaï¼‰
```

**å®ç°ä»£ç ï¼š**

```python
class RMSNorm(nn.Module):
    """RMSNorm - LLaMAä½¿ç”¨"""
    
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x):
        # è®¡ç®—RMS
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        
        # å½’ä¸€åŒ–
        x = x / rms
        
        # ç¼©æ”¾
        return x * self.weight
```

**æ€§èƒ½å¯¹æ¯”ï¼š**

```python
å®æµ‹ï¼ˆn_embd=4096ï¼‰:

æŒ‡æ ‡          | LayerNorm | RMSNorm | æå‡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€
å‰å‘æ—¶é—´      | 0.15ms    | 0.08ms  | 1.9x
åå‘æ—¶é—´      | 0.32ms    | 0.18ms  | 1.8x
å‚æ•°é‡        | 8192      | 4096    | 2x less
è®­ç»ƒç¨³å®šæ€§    | â­â­â­â­ | â­â­â­â­| ç›¸åŒ

ç»“è®º: æ›´å¿«ã€æ›´å°‘å‚æ•°ã€æ•ˆæœç›¸å½“
åº”ç”¨: LLaMA, LLaMA-2å…¨ç³»åˆ—
```

---

## ğŸ¨ ç¬¬äº”éƒ¨åˆ†ï¼šæ¿€æ´»å‡½æ•°æ”¹è¿›

### ğŸ”¥ 1. ä»GELUåˆ°GLUå®¶æ—

**æ ‡å‡†GELUï¼ˆNanoGPTä½¿ç”¨ï¼‰ï¼š**

```python
# model.py ç¬¬83è¡Œ
self.gelu = nn.GELU()

# GELUå®šä¹‰
def gelu(x):
    return 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * xÂ³)))

ç‰¹ç‚¹:
  - å¹³æ»‘
  - æ¥è¿‘ReLUä½†æ›´å¥½
  - GPT-2/3æ ‡å‡†é€‰æ‹©
```

**GLU (Gated Linear Unit)ï¼š**

```python
# GLUçš„æ ¸å¿ƒæ€æƒ³: é—¨æ§æœºåˆ¶

def glu(x):
    # åˆ†æˆä¸¤åŠ
    x, gate = x.chunk(2, dim=-1)
    # ä¸€åŠé€šè¿‡sigmoidä½œä¸ºé—¨
    return x * sigmoid(gate)

ç›´è§‰:
  x: ä¿¡æ¯å†…å®¹
  gate: é—¨æ§ï¼ˆå†³å®šè®©å¤šå°‘ä¿¡æ¯é€šè¿‡ï¼‰
  ç±»ä¼¼LSTMçš„é—¨ï¼
```

---

### âš¡ 2. SwiGLU

**æ ¸å¿ƒæ€æƒ³ï¼š** Swishæ¿€æ´» + GLU

```python
def swish(x):
    """ä¹Ÿå«SiLU"""
    return x * sigmoid(x)

def swi_glu(x, W, V):
    """SwiGLU - LLaMAä½¿ç”¨"""
    # çº¿æ€§æŠ•å½±åˆ°ä¸¤ä»½
    x_gate = W @ x
    x_value = V @ x
    
    # Swishé—¨æ§
    return swish(x_gate) * x_value

# ä¸ºä»€ä¹ˆå¥½ï¼Ÿ
# 1. Swishæ¯”GELUç•¥å¥½
# 2. é—¨æ§æœºåˆ¶å¢åŠ è¡¨è¾¾èƒ½åŠ›
# 3. å®éªŒæ•ˆæœæœ€ä½³
```

**åœ¨MLPä¸­ä½¿ç”¨ï¼š**

```python
class MLP_SwiGLU(nn.Module):
    """ä½¿ç”¨SwiGLUçš„MLP - LLaMAé£æ ¼"""
    
    def __init__(self, config):
        super().__init__()
        hidden_dim = int(config.n_embd * 8/3)  # é€šå¸¸æ˜¯8/3å€ï¼Œå› ä¸ºè¦åˆ†æˆä¸¤ä»½
        
        # ä¸‰ä¸ªæŠ•å½±ï¼ˆé—¨æ§éœ€è¦é¢å¤–çš„æŠ•å½±ï¼‰
        self.w1 = nn.Linear(config.n_embd, hidden_dim, bias=False)  # gate
        self.w2 = nn.Linear(hidden_dim, config.n_embd, bias=False)  # down
        self.w3 = nn.Linear(config.n_embd, hidden_dim, bias=False)  # up
    
    def forward(self, x):
        # SwiGLU
        gate = F.silu(self.w1(x))  # siluå°±æ˜¯swish
        x = gate * self.w3(x)
        x = self.w2(x)
        return x

# å¯¹æ¯”æ ‡å‡†MLP:
# æ ‡å‡†: Linear â†’ GELU â†’ Linear (2ä¸ªçŸ©é˜µ)
# SwiGLU: Linear â†’ SiLU â†’ é—¨æ§ â†’ Linear (3ä¸ªçŸ©é˜µ)
# å‚æ•°é‡ç•¥å¢åŠ ï¼Œä½†æ•ˆæœæ›´å¥½
```

**å®éªŒç»“æœï¼š**

```python
æ¿€æ´»å‡½æ•°å¯¹æ¯”ï¼ˆç›¸åŒå‚æ•°é‡ï¼‰:

æ¿€æ´»å‡½æ•°  | Perplexity | è®­ç»ƒæ—¶é—´ | ä»£è¡¨æ¨¡å‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
ReLU      | 3.45       | 1.0x    | è€æ¨¡å‹
GELU      | 3.21       | 1.02x   | GPT-2/3
SwiGLU    | 3.15       | 1.15x   | LLaMA
GeGLU     | 3.16       | 1.15x   | -

ç»“è®º: SwiGLU/GeGLUç•¥å¥½ï¼Œä½†è®­ç»ƒæ…¢ä¸€ç‚¹
æ¨è: æ–°æ¨¡å‹ç”¨SwiGLUï¼Œè¿½æ±‚é€Ÿåº¦ç”¨GELU
```

---

## ğŸ—ï¸ ç¬¬å…­éƒ¨åˆ†ï¼šå®Œæ•´æ¶æ„å¯¹æ¯”

### ğŸ†š ä¸»æµæ¨¡å‹æ¶æ„å¯¹æ¯”

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç»„ä»¶         â”‚ GPT-2        â”‚ LLaMA        â”‚ BLOOM        â”‚ Falcon       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ä½ç½®ç¼–ç      â”‚ å­¦ä¹ å¼ç»å¯¹   â”‚ RoPE         â”‚ ALiBi        â”‚ RoPE         â”‚
â”‚ å½’ä¸€åŒ–       â”‚ LayerNorm    â”‚ RMSNorm      â”‚ LayerNorm    â”‚ LayerNorm    â”‚
â”‚ å½’ä¸€åŒ–ä½ç½®   â”‚ Post-LN      â”‚ Pre-LN       â”‚ Pre-LN       â”‚ Pre-LN       â”‚
â”‚ æ¿€æ´»å‡½æ•°     â”‚ GELU         â”‚ SwiGLU       â”‚ GELU         â”‚ GELU         â”‚
â”‚ æ³¨æ„åŠ›       â”‚ MHA          â”‚ GQA          â”‚ MHA          â”‚ MQA          â”‚
â”‚ åç½®é¡¹       â”‚ æœ‰           â”‚ æ—            â”‚ æœ‰           â”‚ æ—            â”‚
â”‚ å¹¶è¡ŒåŒ–       â”‚ ä¸²è¡Œ         â”‚ ä¸²è¡Œ         â”‚ å¹¶è¡Œ         â”‚ å¹¶è¡Œ         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ³¨:
  - ä¸²è¡Œ: Attention â†’ MLP é¡ºåºæ‰§è¡Œ
  - å¹¶è¡Œ: Attention å’Œ MLP éƒ¨åˆ†å¹¶è¡Œ
```

### ğŸ¯ æ€§èƒ½å¯¹æ¯”ï¼ˆ7Bå‚æ•°çº§åˆ«ï¼‰

```python
æ¨¡å‹       | Perplexity | æ¨ç†é€Ÿåº¦ | å†…å­˜å ç”¨ | å¤–æ¨èƒ½åŠ›
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPT-2é£æ ¼  | 15.2       | 1.0x    | 100%    | âŒ å·®
LLaMA-2    | 13.8       | 1.3x    | 70%     | âœ… å¥½
BLOOM      | 14.5       | 1.1x    | 95%     | âœ… å¾ˆå¥½
Falcon     | 14.1       | 1.4x    | 65%     | âœ… å¥½

ç»“è®º: ç°ä»£æ¶æ„å…¨é¢ä¼˜äºGPT-2
```

---

## ğŸ”¨ ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå®æˆ˜ï¼šæ”¹é€ NanoGPT

### ğŸ› ï¸ é¡¹ç›®ï¼šå®ç°LLaMAé£æ ¼çš„NanoGPT

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ”¹é€ NanoGPTï¼Œå®ç°LLaMAçš„æ¶æ„ã€‚

#### **æ­¥éª¤1: æ·»åŠ RMSNorm**

```python
# åœ¨model.pyä¸­æ·»åŠ 

class RMSNorm(nn.Module):
    """RMSNorm - æ›¿ä»£LayerNorm"""
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x):
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        return (x / rms) * self.weight
```

#### **æ­¥éª¤2: æ·»åŠ RoPE**

```python
class RotaryEmbedding(nn.Module):
    """RoPEä½ç½®ç¼–ç """
    def __init__(self, dim, max_seq_len=2048, base=10000):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
        t = torch.arange(max_seq_len, dtype=torch.float)
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat([freqs, freqs], dim=-1)
        self.register_buffer('cos', emb.cos())
        self.register_buffer('sin', emb.sin())
    
    def rotate_half(self, x):
        x1, x2 = x.chunk(2, dim=-1)
        return torch.cat([-x2, x1], dim=-1)
    
    def forward(self, q, k):
        seq_len = q.shape[2]
        cos = self.cos[:seq_len, :].unsqueeze(0).unsqueeze(0)
        sin = self.sin[:seq_len, :].unsqueeze(0).unsqueeze(0)
        
        q_embed = (q * cos) + (self.rotate_half(q) * sin)
        k_embed = (k * cos) + (self.rotate_half(k) * sin)
        return q_embed, k_embed
```

#### **æ­¥éª¤3: ä¿®æ”¹Attentionä½¿ç”¨RoPE**

```python
class CausalSelfAttention_LLaMA(nn.Module):
    """LLaMAé£æ ¼çš„Attention"""
    
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = config.n_embd // config.n_head
        
        # QKVæŠ•å½±ï¼ˆæ— biasï¼‰
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)
        
        # RoPE
        self.rope = RotaryEmbedding(self.head_dim, config.block_size)
        
        # Flash Attention
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))
    
    def forward(self, x):
        B, T, C = x.size()
        
        # QKV
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        
        # åº”ç”¨RoPEï¼ˆå…³é”®æ”¹åŠ¨ï¼ï¼‰
        q, k = self.rope(q, k)
        
        # Attention
        if self.flash:
            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, 
                                               dropout_p=0, is_causal=True)
        else:
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
            att = F.softmax(att, dim=-1)
            y = att @ v
        
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.c_proj(y)
        return y
```

#### **æ­¥éª¤4: å®ç°SwiGLU MLP**

```python
class MLP_SwiGLU(nn.Module):
    """LLaMAé£æ ¼çš„MLP"""
    
    def __init__(self, config):
        super().__init__()
        hidden_dim = int(2 * config.n_embd * 4 / 3)  # LLaMAçš„hidden_dimè®¡ç®—æ–¹å¼
        hidden_dim = int(8 * ((hidden_dim + 7) // 8))  # å¯¹é½åˆ°8çš„å€æ•°
        
        self.w1 = nn.Linear(config.n_embd, hidden_dim, bias=False)  # gate
        self.w2 = nn.Linear(hidden_dim, config.n_embd, bias=False)  # down
        self.w3 = nn.Linear(config.n_embd, hidden_dim, bias=False)  # up
    
    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

#### **æ­¥éª¤5: ä¿®æ”¹Block**

```python
class Block_LLaMA(nn.Module):
    """LLaMAé£æ ¼çš„Transformer Block"""
    
    def __init__(self, config):
        super().__init__()
        self.ln_1 = RMSNorm(config.n_embd)  # ä½¿ç”¨RMSNorm
        self.attn = CausalSelfAttention_LLaMA(config)  # ä½¿ç”¨RoPE Attention
        self.ln_2 = RMSNorm(config.n_embd)
        self.mlp = MLP_SwiGLU(config)  # ä½¿ç”¨SwiGLU MLP
    
    def forward(self, x):
        # Pre-LNï¼ˆå·²ç»æ˜¯è¿™æ ·äº†ï¼‰
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```

#### **æ­¥éª¤6: ä¿®æ”¹GPTä¸»ç±»**

```python
class GPT_LLaMA(nn.Module):
    """LLaMAé£æ ¼çš„GPT"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            # æ³¨æ„: ä¸éœ€è¦wpeäº†ï¼RoPEå–ä»£äº†ä½ç½®ç¼–ç 
            h = nn.ModuleList([Block_LLaMA(config) for _ in range(config.n_layer)]),
            ln_f = RMSNorm(config.n_embd),  # ä½¿ç”¨RMSNorm
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        
        # æƒé‡ç»‘å®š
        self.transformer.wte.weight = self.lm_head.weight
        
        self.apply(self._init_weights)
        
        print(f"å‚æ•°é‡: {self.get_num_params()/1e6:.2f}M")
    
    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()
        
        # Token embeddingï¼ˆä¸éœ€è¦ä½ç½®embeddingäº†ï¼ï¼‰
        tok_emb = self.transformer.wte(idx)
        x = tok_emb  # ç›´æ¥ä½¿ç”¨ï¼ŒRoPEä¼šåœ¨Attentionä¸­åŠ å…¥ä½ç½®ä¿¡æ¯
        
        # Transformer
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)
        
        # è¾“å‡º
        if targets is not None:
            logits = self.lm_head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                                   targets.view(-1), ignore_index=-1)
        else:
            logits = self.lm_head(x[:, [-1], :])
            loss = None
        
        return logits, loss
```

#### **æ­¥éª¤7: æµ‹è¯•å¯¹æ¯”**

```python
# test_llama_vs_gpt2.py

from model import GPT, GPTConfig
from model_llama import GPT_LLaMA

# ç›¸åŒé…ç½®
config = GPTConfig(
    n_layer=6,
    n_head=6,
    n_embd=384,
    vocab_size=50257,
    block_size=256,
)

# åˆ›å»ºä¸¤ä¸ªæ¨¡å‹
gpt2_model = GPT(config)
llama_model = GPT_LLaMA(config)

# å¯¹æ¯”å‚æ•°é‡
gpt2_params = sum(p.numel() for p in gpt2_model.parameters())
llama_params = sum(p.numel() for p in llama_model.parameters())

print(f"GPT-2å‚æ•°: {gpt2_params:,}")
print(f"LLaMAå‚æ•°: {llama_params:,}")
print(f"å·®å¼‚: {(llama_params - gpt2_params) / gpt2_params * 100:.1f}%")

# æµ‹è¯•å‰å‘ä¼ æ’­
import torch
x = torch.randint(0, 50257, (2, 128))
y_gpt2, loss_gpt2 = gpt2_model(x, x)
y_llama, loss_llama = llama_model(x, x)

print(f"\nGPT-2 loss: {loss_gpt2.item():.4f}")
print(f"LLaMA loss: {loss_llama.item():.4f}")

# æµ‹è¯•å¤–æ¨èƒ½åŠ›
print("\næµ‹è¯•å¤–æ¨ï¼ˆè®­ç»ƒ256ï¼Œæµ‹è¯•512ï¼‰:")
x_long = torch.randint(0, 50257, (1, 512))
try:
    with torch.no_grad():
        y_gpt2_long, _ = gpt2_model(x_long)
    print("GPT-2: âŒ æ— æ³•å¤„ç†")
except:
    print("GPT-2: âŒ æŠ¥é”™")

try:
    with torch.no_grad():
        y_llama_long, _ = llama_model(x_long)
    print("LLaMA: âœ… å¯ä»¥å¤„ç†ï¼")
except Exception as e:
    print(f"LLaMA: âŒ é”™è¯¯: {e}")
```

---

## ğŸ“Š ç¬¬å…«éƒ¨åˆ†ï¼šæ€§èƒ½è¯„ä¼°

### ğŸ§ª å®éªŒè®¾è®¡

```python
å®éªŒé…ç½®:

åŸºå‡†æ¨¡å‹ï¼ˆGPT-2é£æ ¼ï¼‰:
  - ä½ç½®ç¼–ç : å­¦ä¹ å¼
  - å½’ä¸€åŒ–: LayerNorm (Post-LN)
  - æ¿€æ´»: GELU
  - æ³¨æ„åŠ›: MHA

æ”¹è¿›æ¨¡å‹ï¼ˆLLaMAé£æ ¼ï¼‰:
  - ä½ç½®ç¼–ç : RoPE
  - å½’ä¸€åŒ–: RMSNorm (Pre-LN)
  - æ¿€æ´»: SwiGLU
  - æ³¨æ„åŠ›: GQA

æ•°æ®é›†: Shakespeare
æ¨¡å‹å¤§å°: 10Må‚æ•°
è®­ç»ƒæ­¥æ•°: 5000
```

### ğŸ“ˆ é¢„æœŸç»“æœ

```python
æŒ‡æ ‡å¯¹æ¯”:

æŒ‡æ ‡              | GPT-2é£æ ¼ | LLaMAé£æ ¼ | æ”¹è¿›
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
æœ€ç»ˆLoss          | 1.47      | 1.42      | 3.4% â¬‡ï¸
è®­ç»ƒæ—¶é—´/iter     | 125ms     | 145ms     | 16% â¬†ï¸
æ¨ç†æ—¶é—´/token    | 8ms       | 6ms       | 25% â¬‡ï¸
å‚æ•°é‡            | 10.2M     | 10.8M     | 6% â¬†ï¸
æœ€å¤§åºåˆ—é•¿åº¦      | 256       | 512+      | 2x+ â¬†ï¸
è®­ç»ƒç¨³å®šæ€§        | â­â­â­   | â­â­â­â­ | æ›´ç¨³å®š

ç»“è®º:
  âœ… æ€§èƒ½æ›´å¥½ï¼ˆlossæ›´ä½ï¼‰
  âœ… æ¨ç†æ›´å¿«
  âœ… å¤–æ¨èƒ½åŠ›å¼º
  âŒ è®­ç»ƒç¨æ…¢ï¼ˆSwiGLUå¼€é”€ï¼‰
  
æ¨è: æ–°é¡¹ç›®ä¼˜å…ˆè€ƒè™‘LLaMAé£æ ¼
```

---

## ğŸ“ ç¬¬ä¹éƒ¨åˆ†ï¼šé€‰æ‹©æŒ‡å—

### ğŸ¯ å¦‚ä½•é€‰æ‹©æ¶æ„æ”¹è¿›ï¼Ÿ

```python
å†³ç­–æ ‘:

ä½ çš„é¡¹ç›®éœ€è¦ä»€ä¹ˆï¼Ÿ
â”‚
â”œâ”€ æœ€å¤§åŒ–æ€§èƒ½
â”‚  â””â”€ æ¨è: RoPE + RMSNorm + SwiGLU + GQA
â”‚     ä¾‹å­: LLaMA-2
â”‚
â”œâ”€ æœ€å¤§åŒ–é€Ÿåº¦
â”‚  â””â”€ æ¨è: ALiBi + LayerNorm + GELU + MQA
â”‚     ä¾‹å­: Falcon
â”‚
â”œâ”€ å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
â”‚  â””â”€ æ¨è: RoPE + RMSNorm + GELU + GQA
â”‚     ä¾‹å­: è‡ªå®šä¹‰
â”‚
â”œâ”€ æœ€ç®€å•å®ç°
â”‚  â””â”€ æ¨è: ALiBi + Pre-LN LayerNorm + GELU + MHA
â”‚     ä¾‹å­: æ”¹è¿›çš„GPT-2
â”‚
â””â”€ å¤–æ¨èƒ½åŠ›æœ€é‡è¦
   â””â”€ æ¨è: ALiBi + ä»»æ„å…¶ä»–
      ä¾‹å­: BLOOM
```

### ğŸ“‹ æ”¹è¿›ä¼˜å…ˆçº§

```python
æŒ‰å½±å“å¤§å°æ’åº:

Priority 1 (å¿…åš):
  âœ… Post-LN â†’ Pre-LN
     å½±å“: è®­ç»ƒç¨³å®šæ€§ â¬†â¬†â¬†
     éš¾åº¦: â­ (åªéœ€æ”¹é¡ºåº)
     
Priority 2 (å¼ºçƒˆæ¨è):
  âœ… ç»å¯¹ä½ç½® â†’ RoPE æˆ– ALiBi
     å½±å“: å¤–æ¨èƒ½åŠ› â¬†â¬†â¬†ï¼Œæ€§èƒ½ â¬†â¬†
     éš¾åº¦: â­â­ (éœ€è¦æ–°ä»£ç )
     
Priority 3 (æ¨è):
  âœ… LayerNorm â†’ RMSNorm
     å½±å“: é€Ÿåº¦ â¬†â¬†ï¼Œå†…å­˜ â¬†
     éš¾åº¦: â­ (ä»£ç ç®€å•)
     
Priority 4 (å¯é€‰):
  âœ… GELU â†’ SwiGLU
     å½±å“: æ€§èƒ½ â¬†
     éš¾åº¦: â­â­ (éœ€è¦ä¿®æ”¹MLP)
     
Priority 5 (æ¨ç†ä¼˜åŒ–):
  âœ… MHA â†’ GQA æˆ– MQA
     å½±å“: æ¨ç†é€Ÿåº¦ â¬†â¬†ï¼Œå†…å­˜ â¬†â¬†
     éš¾åº¦: â­â­â­ (æ”¹åŠ¨è¾ƒå¤§)
```

---

## ğŸ”¬ ç¬¬åéƒ¨åˆ†ï¼šå‰æ²¿ç ”ç©¶æ–¹å‘

### ğŸš€ æœ€æ–°æ¶æ„åˆ›æ–°

```python
1ï¸âƒ£ Mixture of Experts (MoE)
   æ€æƒ³: æ¯æ¬¡åªæ¿€æ´»éƒ¨åˆ†å‚æ•°
   
   æ ‡å‡†MLP: å…¨éƒ¨ç¥ç»å…ƒéƒ½å·¥ä½œ
   MoE: é€‰æ‹©æ€§æ¿€æ´»ï¼ˆå¦‚8ä¸ªä¸“å®¶ä¸­çš„2ä¸ªï¼‰
   
   ä¼˜åŠ¿:
   - æ€»å‚æ•°å¤§ï¼Œæ¿€æ´»å‚æ•°å°
   - è®¡ç®—é‡å°‘ï¼Œæ€§èƒ½å¥½
   
   ä¾‹å­: Switch Transformer, GLaM

2ï¸âƒ£ Sliding Window Attention
   æ€æƒ³: åªå…³æ³¨é™„è¿‘çš„token
   
   æ ‡å‡†Attention: å…¨å±€ O(nÂ²)
   Sliding Window: å±€éƒ¨ O(nÃ—w)
   
   ä¼˜åŠ¿:
   - çº¿æ€§å¤æ‚åº¦
   - å¯ä»¥å¤„ç†ç™¾ä¸‡çº§token
   
   ä¾‹å­: Longformer, BigBird

3ï¸âƒ£ Retrieval Augmented
   æ€æƒ³: æŸ¥è¯¢å¤–éƒ¨çŸ¥è¯†åº“
   
   æ ‡å‡†LM: åªä¾èµ–å‚æ•°ä¸­çš„çŸ¥è¯†
   RAG: å‚æ•° + å¤–éƒ¨æ•°æ®åº“
   
   ä¼˜åŠ¿:
   - çŸ¥è¯†æ›´æ–°ä¸éœ€è¦é‡æ–°è®­ç»ƒ
   - æ›´å‡†ç¡®çš„äº‹å®æ€§å›ç­”
   
   ä¾‹å­: RAG, RETRO

4ï¸âƒ£ State Space Models
   æ€æƒ³: ç”¨çŠ¶æ€ç©ºé—´æ›¿ä»£Attention
   
   Transformer: O(nÂ²) Attention
   SSM: O(n) é€’å½’ç»“æ„
   
   ä¼˜åŠ¿:
   - çº¿æ€§å¤æ‚åº¦
   - å¤„ç†è¶…é•¿åºåˆ—
   
   ä¾‹å­: S4, Mamba
```

### ğŸ“– æ¨èé˜…è¯»è®ºæ–‡

```python
å¿…è¯»è®ºæ–‡ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰:

1. Attention Is All You Need (2017)
   - åŸå§‹Transformer
   
2. RoFormer (2021)
   - RoPEä½ç½®ç¼–ç 
   
3. Train Short, Test Long (2021)
   - ALiBi
   
4. Root Mean Square Layer Normalization (2019)
   - RMSNorm
   
5. GLU Variants Improve Transformer (2020)
   - SwiGLUç­‰æ¿€æ´»å‡½æ•°
   
6. GQA: Training Generalized Multi-Query... (2023)
   - Grouped-Query Attention
   
7. FlashAttention (2022)
   - å†…å­˜é«˜æ•ˆçš„Attention

8. LLaMA (2023)
   - ç»¼åˆæœ€ä½³å®è·µ

9. Mistral 7B (2023)
   - Sliding Window + GQA
```

---

## ğŸ¯ æ€»ç»“

### âœ¨ æ ¸å¿ƒè¦ç‚¹

```python
1. ä½ç½®ç¼–ç : ç»å¯¹ â†’ ç›¸å¯¹ï¼ˆRoPE/ALiBiï¼‰
   æ•ˆæœ: å¤–æ¨èƒ½åŠ› â¬†â¬†â¬†

2. å½’ä¸€åŒ–: Post-LN â†’ Pre-LN
   æ•ˆæœ: è®­ç»ƒç¨³å®šæ€§ â¬†â¬†â¬†

3. å½’ä¸€åŒ–æ–¹æ³•: LayerNorm â†’ RMSNorm
   æ•ˆæœ: é€Ÿåº¦ â¬†â¬†

4. æ¿€æ´»å‡½æ•°: GELU â†’ SwiGLU
   æ•ˆæœ: æ€§èƒ½ â¬†

5. æ³¨æ„åŠ›: MHA â†’ GQA
   æ•ˆæœ: æ¨ç†é€Ÿåº¦ â¬†â¬†
```

### ğŸ å®ç”¨å»ºè®®

```python
å¦‚æœä½ æ˜¯:

ğŸ“± åˆå­¦è€…:
  - å…ˆç†è§£æ ‡å‡†Transformer
  - é€æ­¥æ·»åŠ æ”¹è¿›
  - ä»Pre-LNå¼€å§‹
  
ğŸ¢ å·¥ä¸šåº”ç”¨:
  - ç”¨LLaMAæ¶æ„ï¼ˆéªŒè¯è¿‡çš„æœ€ä½³å®è·µï¼‰
  - å…³æ³¨æ¨ç†é€Ÿåº¦ï¼ˆGQA/MQAï¼‰
  - è€ƒè™‘éƒ¨ç½²æˆæœ¬
  
ğŸ“ ç ”ç©¶è€…:
  - å®éªŒæ–°ç»„åˆ
  - æ¶ˆèå®éªŒéªŒè¯æ¯ä¸ªç»„ä»¶
  - å…³æ³¨å‰æ²¿æ–¹å‘ï¼ˆMoE, SSMç­‰ï¼‰
```

### ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

```python
ç«‹å³å¯åš:

1. å®ç°Pre-LN
   - æœ€ç®€å•
   - æ•ˆæœæœ€æ˜¾è‘—
   
2. æ·»åŠ RoPEæˆ–ALiBi
   - æå‡å¤–æ¨èƒ½åŠ›
   - æµ‹è¯•ä¸åŒé•¿åº¦
   
3. å°è¯•RMSNorm
   - åŠ é€Ÿè®­ç»ƒ
   - ä»£ç ç®€å•

è¿›é˜¶å®éªŒ:

4. å®Œæ•´LLaMAæ¶æ„
   - ç»¼åˆæ‰€æœ‰æ”¹è¿›
   - å¯¹æ¯”åŸºå‡†
   
5. æ¶ˆèç ”ç©¶
   - åˆ†åˆ«æµ‹è¯•æ¯ä¸ªæ”¹è¿›
   - é‡åŒ–å½±å“
```

---

## ğŸ“š ä»£ç èµ„æº

```python
å®Œæ•´å®ç°å‚è€ƒ:

1. NanoGPT (åŸºç¡€)
   https://github.com/karpathy/nanoGPT
   
2. LLaMA (Metaå®˜æ–¹)
   https://github.com/facebookresearch/llama
   
3. Mistral (Mistral AI)
   https://github.com/mistralai/mistral-src
   
4. Flash Attention
   https://github.com/Dao-AILab/flash-attention
   
5. xFormers (å„ç§æ”¹è¿›)
   https://github.com/facebookresearch/xformers
```

---

**è®°ä½ï¼š**

> æ¶æ„æ”¹è¿›ä¸æ˜¯å †ç Œæ–°æŠ€æœ¯ï¼Œ
> è€Œæ˜¯ç†è§£æ¯ä¸ªç»„ä»¶çš„ä½œç”¨ï¼Œ
> é€‰æ‹©é€‚åˆè‡ªå·±éœ€æ±‚çš„ç»„åˆã€‚
> 
> æœ€å¥½çš„æ¶æ„ï¼Œæ˜¯ä½ ç†è§£å¹¶èƒ½æŒæ§çš„æ¶æ„ã€‚

ğŸ‰ æ­å–œä½ å®Œæˆæ¶æ„æ”¹è¿›çš„å­¦ä¹ ï¼ç°åœ¨ä½ å…·å¤‡äº†è®¾è®¡ç°ä»£Transformerçš„èƒ½åŠ›ï¼
