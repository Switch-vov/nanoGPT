# ç¬¬11ç« ï¼šå¤šæ¨¡æ€æ¨¡å‹å®Œå…¨æŒ‡å— - ä»é›¶å¼€å§‹

> **å­¦ä¹ ç›®æ ‡**ï¼šç†è§£å¦‚ä½•è®©AIåŒæ—¶ç†è§£å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰å¤šç§ä¿¡æ¯  
> **éš¾åº¦ç­‰çº§**ï¼šğŸŒ¿ğŸŒ¿ğŸŒ¿ è¿›é˜¶ï¼ˆå‰æ²¿æŠ€æœ¯ï¼Œä½†æˆ‘ä»¬ä¼šä»é›¶è®²èµ·ï¼‰  
> **é¢„è®¡æ—¶é—´**ï¼š3-4å°æ—¶  
> **å‰ç½®çŸ¥è¯†**ï¼š05æ¨¡å‹æ¶æ„åŸºç¡€

---

## ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€ï¼Œä¸ºä»€ä¹ˆéœ€è¦å®ƒ
- âœ… æŒæ¡CLIPçš„å·¥ä½œåŸç†ï¼ˆå›¾æ–‡åŒ¹é…çš„é­”æ³•ï¼‰
- âœ… ç†è§£è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ä½•æŠŠå›¾ç‰‡å˜æˆæ•°å­—ï¼‰
- âœ… ç†è§£LLaVAï¼ˆå¦‚ä½•è®©GPTçœ‹æ‡‚å›¾ç‰‡ï¼‰
- âœ… äº†è§£æ–‡ç”Ÿå›¾ã€è§†é¢‘ç†è§£çš„åŸºæœ¬åŸç†
- âœ… èƒ½å¤Ÿä½¿ç”¨å’Œå¾®è°ƒå¤šæ¨¡æ€æ¨¡å‹

---

## ğŸ’­ å¼€å§‹ä¹‹å‰ï¼šä¸ºä»€ä¹ˆè¦å­¦å¤šæ¨¡æ€ï¼Ÿ

### ğŸ¤” å•ä¸€æ¨¡æ€çš„å›°å¢ƒ

æƒ³è±¡ä½ åœ¨å’Œä¸€ä¸ªåªèƒ½è¯»æ–‡å­—ã€çœ‹ä¸è§å›¾ç‰‡çš„äººèŠå¤©ï¼š

```python
ä½ : "å¸®æˆ‘çœ‹çœ‹è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ" 
   [å‘é€ä¸€å¼ çŒ«çš„ç…§ç‰‡]

åªä¼šæ–‡æœ¬çš„AI: "æŠ±æ­‰ï¼Œæˆ‘çœ‹ä¸åˆ°å›¾ç‰‡ï¼Œæˆ‘åªèƒ½ç†è§£æ–‡å­—..."
   âŒ æ— æ³•ç†è§£å›¾åƒå†…å®¹

ä½ : "å›¾ç‰‡é‡Œæœ‰ä¸€åªçŒ«ååœ¨çº¢è‰²å«å­ä¸Š"
   
åªä¼šæ–‡æœ¬çš„AI: "å¥½çš„ï¼Œæˆ‘çŸ¥é“äº†ã€‚"
   âœ… ä½†å®ƒåªæ˜¯'å¬'ä½ æè¿°ï¼Œæ²¡æœ‰çœŸæ­£'çœ‹åˆ°'
```

è¿™å°±æ˜¯**å•æ¨¡æ€æ¨¡å‹çš„å±€é™**ï¼

### ğŸŒŸ å¤šæ¨¡æ€çš„å¨åŠ›

ç°åœ¨æƒ³è±¡ä¸€ä¸ªèƒ½çœ‹ã€èƒ½å¬ã€èƒ½è¯»çš„AIï¼š

```python
ä½ : "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
   [å‘é€ä¸€å¼ çŒ«çš„ç…§ç‰‡]

å¤šæ¨¡æ€AI: ğŸ‘ï¸ çœ‹å›¾ + ğŸ§  ç†è§£
   â†’ "è¿™æ˜¯ä¸€åªæ©˜è‰²çš„çŒ«ï¼Œååœ¨ä¸€ä¸ªçº¢è‰²çš„å«å­ä¸Šã€‚
      å®ƒçœ‹èµ·æ¥å¾ˆæ”¾æ¾ï¼Œé˜³å…‰ä»çª—æˆ·ç…§è¿›æ¥..."
   âœ… çœŸæ­£'çœ‹åˆ°'å¹¶ç†è§£äº†å›¾åƒï¼

ä½ : "ç»™æˆ‘ç”Ÿæˆä¸€å¼ 'çŒ«åœ¨å¤ªç©º'çš„å›¾ç‰‡"

å¤šæ¨¡æ€AI: ğŸ“ ç†è§£æ–‡æœ¬ + ğŸ¨ ç”Ÿæˆå›¾åƒ
   â†’ [ç”Ÿæˆä¸€å¼ çŒ«å®‡èˆªå‘˜çš„å›¾ç‰‡]
   âœ… ä»æ–‡å­—åˆ›é€ å›¾åƒï¼
```

### ğŸ¯ ç”Ÿæ´»ä¸­çš„ç±»æ¯”

**å¤šæ¨¡æ€AI = æ‹¥æœ‰å®Œæ•´æ„Ÿå®˜çš„äººç±»**

```
äººç±»å¦‚ä½•ç†è§£ä¸–ç•Œï¼Ÿ

ğŸ‘ï¸ çœ¼ç›ï¼ˆè§†è§‰ï¼‰:
   çœ‹åˆ°ä¸€åªçŒ« â†’ æ¯›èŒ¸èŒ¸ã€æ©˜è‰²ã€å››æ¡è…¿
   
ğŸ‘‚ è€³æœµï¼ˆå¬è§‰ï¼‰:
   å¬åˆ°"å–µ" â†’ çŸ¥é“æ˜¯çŒ«å«
   
ğŸ“ å¤§è„‘ï¼ˆè¯­è¨€ï¼‰:
   ç»¼åˆä¿¡æ¯ â†’ "è¿™æ˜¯ä¸€åªæ©˜çŒ«åœ¨å«"

AIä¹Ÿéœ€è¦å¤šç§"æ„Ÿå®˜"ï¼
```

### âœ¨ ä¸ºä»€ä¹ˆç°åœ¨è¦å­¦è¿™ä¸ªï¼Ÿ

```python
è¶‹åŠ¿1: GPT-4Vå·²ç»èƒ½çœ‹å›¾
  "è¿™å¼ Xå…‰ç‰‡æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"
  â†’ AIèƒ½ç›´æ¥åˆ†æåŒ»å­¦å½±åƒ

è¶‹åŠ¿2: æ–‡ç”Ÿå›¾çˆ†ç«
  Midjourneyã€Stable Diffusion
  â†’ äººäººéƒ½èƒ½ç”¨æ–‡å­—åˆ›ä½œå›¾åƒ

è¶‹åŠ¿3: è§†é¢‘ç†è§£å´›èµ·
  "è¿™ä¸ªè§†é¢‘è®²äº†ä»€ä¹ˆï¼Ÿ"
  â†’ AIèƒ½æ€»ç»“è§†é¢‘å†…å®¹

ç»“è®º: å¤šæ¨¡æ€æ˜¯AIçš„æœªæ¥ï¼
```

---

## ğŸ“š æœ¬ç« å­¦ä¹ è·¯çº¿

æˆ‘ä»¬å°†æŒ‰ç…§è¿™ä¸ªé¡ºåºå­¦ä¹ ï¼š

```
ç¬¬ä¸€éƒ¨åˆ†: å¤šæ¨¡æ€åŸºç¡€ ğŸŒ±
  â””â”€ ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ
  â””â”€ å¦‚ä½•è¡¨ç¤ºä¸åŒçš„æ¨¡æ€ï¼ˆå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ï¼‰

ç¬¬äºŒéƒ¨åˆ†: è§†è§‰-è¯­è¨€æ¨¡å‹ ğŸŒ¿
  â””â”€ CLIP: å¦‚ä½•è®©AIç†è§£å›¾æ–‡å¯¹åº”å…³ç³»
  â””â”€ LLaVA: å¦‚ä½•è®©GPTçœ‹æ‡‚å›¾ç‰‡

ç¬¬ä¸‰éƒ¨åˆ†: è§†é¢‘å’ŒéŸ³é¢‘ ğŸŒ¿ğŸŒ¿
  â””â”€ å¦‚ä½•ç†è§£è§†é¢‘ï¼ˆè¿ç»­çš„å›¾åƒï¼‰
  â””â”€ å¦‚ä½•å¤„ç†éŸ³é¢‘ï¼ˆå£°éŸ³â†’æ–‡å­—ï¼‰

ç¬¬å››éƒ¨åˆ†: æ„å»ºå¤šæ¨¡æ€GPT ğŸŒ¿ğŸŒ¿ğŸŒ¿
  â””â”€ å¦‚ä½•æŠŠæ‰€æœ‰æ¨¡æ€ç»Ÿä¸€èµ·æ¥
  â””â”€ å®æˆ˜ï¼šè®­ç»ƒä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹

ç¬¬äº”éƒ¨åˆ†: å®æˆ˜ä¸è¯„ä¼° ğŸ¯
  â””â”€ å¦‚ä½•ä½¿ç”¨ç°æˆçš„å¤šæ¨¡æ€æ¨¡å‹
  â””â”€ å¦‚ä½•è¯„ä¼°æ•ˆæœ
```

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬ä»æœ€åŸºç¡€çš„å¼€å§‹ï¼** ğŸš€

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šå¤šæ¨¡æ€åŸºç¡€ï¼ˆä»é›¶å¼€å§‹ï¼‰

### ğŸŒ± 1.1 ä»€ä¹ˆæ˜¯"æ¨¡æ€"ï¼Ÿ

#### ğŸ’¡ ç›´è§‚ç†è§£

**æ˜¯ä»€ä¹ˆï¼Ÿ**  
"æ¨¡æ€"ï¼ˆModalityï¼‰å°±æ˜¯ä¿¡æ¯çš„ä¸åŒå½¢å¼ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼šå­¦ä¹ ä¸€é“èœ**

```
è€å¸ˆæ•™ä½ åšç•ªèŒ„ç‚’è›‹ï¼Œå¯ä»¥ç”¨ä¸åŒæ–¹å¼ï¼š

æ¨¡æ€1ï¼šæ–‡å­—é£Ÿè°± ğŸ“
  "1. æ‰“æ•£3ä¸ªé¸¡è›‹
   2. åˆ‡2ä¸ªç•ªèŒ„æˆå—
   3. çƒ­é”…å€’æ²¹..."
  â†’ ä¿¡æ¯å®Œæ•´ï¼Œä½†æ¯”è¾ƒæŠ½è±¡

æ¨¡æ€2ï¼šå›¾ç‰‡æ­¥éª¤ ğŸ“·
  [å›¾1: æ‰“è›‹]
  [å›¾2: åˆ‡ç•ªèŒ„]
  [å›¾3: ç‚’åˆ¶]
  â†’ ç›´è§‚ï¼Œä½†ç¼ºå°‘ç»†èŠ‚

æ¨¡æ€3ï¼šè§†é¢‘æ•™å­¦ ğŸ¥
  [å®Œæ•´çš„çƒ¹é¥ªè¿‡ç¨‹è§†é¢‘]
  â†’ æœ€ç›´è§‚ï¼Œä½†ä¸æ–¹ä¾¿æŸ¥æ‰¾

æ¨¡æ€4ï¼šå£å¤´æŒ‡å¯¼ ğŸ”Š
  "å¥½çš„ï¼Œç°åœ¨æ‰“æ•£é¸¡è›‹..."
  â†’ è¾¹å¬è¾¹åšï¼Œä½†è®°ä¸ä½

æœ€å¥½çš„å­¦ä¹ æ–¹å¼ï¼Ÿ
  æ–‡å­— + å›¾ç‰‡ + è§†é¢‘ + è®²è§£ = å¤šæ¨¡æ€å­¦ä¹ ï¼
```

#### ğŸ“Š å¸¸è§çš„æ¨¡æ€ç±»å‹

| æ¨¡æ€ | å½¢å¼ | ä¼˜åŠ¿ | å±€é™ | ä¾‹å­ |
|-----|------|------|------|------|
| **æ–‡æœ¬** | å•è¯ã€å¥å­ | è¡¨è¾¾ç²¾ç¡®ã€æ˜“å­˜å‚¨ | ä¸å¤Ÿç›´è§‚ | "ä¸€åªçŒ«" |
| **å›¾åƒ** | åƒç´ çŸ©é˜µ | ç›´è§‚ã€ä¿¡æ¯ä¸°å¯Œ | éš¾ä»¥æœç´¢ | ğŸ±ç…§ç‰‡ |
| **éŸ³é¢‘** | å£°æ³¢ä¿¡å· | ä¼ é€’æƒ…æ„Ÿã€å®æ—¶ | éœ€è¦æ—¶é—´ | çŒ«å«å£° |
| **è§†é¢‘** | è¿ç»­å›¾åƒ | å®Œæ•´å±•ç¤ºè¿‡ç¨‹ | æ–‡ä»¶å¤§ | çŒ«ç©è€è§†é¢‘ |

---

### ğŸŒ¿ 1.2 ä¸ºä»€ä¹ˆéœ€è¦"å¤š"æ¨¡æ€ï¼Ÿ

#### ğŸ’¡ é—®é¢˜åœºæ™¯

**åœºæ™¯1ï¼šåªæœ‰æ–‡æœ¬çš„AI**

```python
ä½ : "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
   [ä¸Šä¼ ä¸€å¼ çŒ«çš„ç…§ç‰‡]

çº¯æ–‡æœ¬AI: ğŸ˜µ "æˆ‘çœ‹ä¸åˆ°å›¾ç‰‡..."
   âŒ å®Œå…¨æ— æ³•å¤„ç†

ä½ åªå¥½æè¿°: "å›¾ç‰‡é‡Œæœ‰ä¸€åªæ©˜è‰²çš„çŒ«"

çº¯æ–‡æœ¬AI: ğŸ˜Š "å¥½çš„ï¼Œæ©˜è‰²çš„çŒ«"
   âœ… èƒ½ç†è§£æ–‡å­—
   âŒ ä½†æ²¡æœ‰çœŸæ­£"çœ‹åˆ°"
```

**åœºæ™¯2ï¼šåªæœ‰è§†è§‰çš„AI**

```python
[ç»™AIçœ‹ä¸€å¼ çŒ«çš„ç…§ç‰‡]

çº¯è§†è§‰AI: ğŸ¤– æ£€æµ‹åˆ°ï¼š
   - å½¢çŠ¶ï¼šå››è¶³åŠ¨ç‰©
   - é¢œè‰²ï¼šæ©˜è‰²
   - ä½ç½®ï¼šåœ¨å«å­ä¸Š
   âœ… èƒ½è¯†åˆ«è§†è§‰ç‰¹å¾

ä½ : "è¿™æ˜¯ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿ"

çº¯è§†è§‰AI: ğŸ˜¶ æ— æ³•å›ç­”
   âŒ ä¸ä¼šç”¨è¯­è¨€è¡¨è¾¾
```

**åœºæ™¯3ï¼šå¤šæ¨¡æ€AI**

```python
ä½ : "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
   [ä¸Šä¼ ä¸€å¼ çŒ«çš„ç…§ç‰‡]

å¤šæ¨¡æ€AI: 
   ğŸ‘ï¸ çœ‹å›¾ï¼šæ©˜è‰²ã€æ¯›èŒ¸èŒ¸ã€å››æ¡è…¿...
   ğŸ§  ç†è§£ï¼šè¿™æ˜¯ä¸€åªçŒ«
   ğŸ“ è¡¨è¾¾ï¼š"è¿™æ˜¯ä¸€åªæ©˜è‰²çš„çŒ«ï¼Œååœ¨çº¢è‰²å«å­ä¸Š"
   âœ… å®Œæ•´çš„ç†è§£å’Œè¡¨è¾¾ï¼

ä½ : "å®ƒçœ‹èµ·æ¥å¼€å¿ƒå—ï¼Ÿ"

å¤šæ¨¡æ€AI:
   ğŸ‘ï¸ å†æ¬¡çœ‹å›¾ï¼šçœ¼ç›åŠé—­ã€å§¿åŠ¿æ”¾æ¾
   ğŸ§  åˆ†ææƒ…ç»ª
   ğŸ“ å›ç­”ï¼š"å®ƒçœ‹èµ·æ¥å¾ˆæ”¾æ¾å’Œæ»¡è¶³"
   âœ… æ·±å±‚æ¬¡çš„ç†è§£ï¼
```

#### ğŸ¯ å¤šæ¨¡æ€çš„å››å¤§èƒ½åŠ›

```python
èƒ½åŠ›1: è·¨æ¨¡æ€ç†è§£
  è¾“å…¥å›¾åƒ â†’ ç†è§£å†…å®¹ â†’ ç”¨æ–‡å­—æè¿°
  
  ä¾‹: å›¾åƒæè¿° (Image Captioning)
  [çŒ«çš„ç…§ç‰‡] â†’ "ä¸€åªæ©˜çŒ«åœ¨æ™’å¤ªé˜³"

èƒ½åŠ›2: è·¨æ¨¡æ€æ£€ç´¢
  ç”¨æ–‡å­—æœç´¢å›¾ç‰‡ï¼Œæˆ–ç”¨å›¾ç‰‡æœç´¢æ–‡å­—
  
  ä¾‹: å›¾æ–‡æ£€ç´¢
  æœç´¢"å¯çˆ±çš„çŒ«" â†’ æ‰¾åˆ°åŒ¹é…çš„çŒ«å›¾ç‰‡

èƒ½åŠ›3: è·¨æ¨¡æ€ç”Ÿæˆ
  ä»ä¸€ç§æ¨¡æ€ç”Ÿæˆå¦ä¸€ç§æ¨¡æ€
  
  ä¾‹: æ–‡ç”Ÿå›¾ (Text-to-Image)
  "çŒ«å®‡èˆªå‘˜åœ¨å¤ªç©º" â†’ ç”Ÿæˆå¯¹åº”å›¾ç‰‡

èƒ½åŠ›4: å¤šæ¨¡æ€æ¨ç†
  ç»¼åˆå¤šç§ä¿¡æ¯è¿›è¡Œå¤æ‚æ¨ç†
  
  ä¾‹: è§†è§‰é—®ç­” (VQA)
  [å›¾ç‰‡] + "å›¾ä¸­æœ‰å‡ åªçŒ«ï¼Ÿ" â†’ "2åª"
```

---

### ğŸŒ¿ 1.3 å¤šæ¨¡æ€çš„æ ¸å¿ƒä»»åŠ¡

#### ğŸ“Š å››å¤§ä»»åŠ¡è¯¦è§£

**ä»»åŠ¡1ï¼šè¡¨ç¤ºå­¦ä¹  (Representation Learning)**

```python
ç›®æ ‡: å°†ä¸åŒæ¨¡æ€è½¬æ¢æˆç»Ÿä¸€çš„æ•°å­—è¡¨ç¤º

é—®é¢˜:
  å›¾åƒ = 1920Ã—1080Ã—3 = 6,220,800ä¸ªæ•°å­— ğŸ˜±
  æ–‡æœ¬ = "ä¸€åªçŒ«" = 3ä¸ªè¯
  
  å¦‚ä½•æ¯”è¾ƒå®ƒä»¬ï¼Ÿ

è§£å†³æ–¹æ¡ˆ: æ˜ å°„åˆ°ç»Ÿä¸€ç©ºé—´
  å›¾åƒ â†’ è§†è§‰ç¼–ç å™¨ â†’ [0.2, 0.5, 0.1, ...] (512ç»´å‘é‡)
  æ–‡æœ¬ â†’ æ–‡æœ¬ç¼–ç å™¨ â†’ [0.3, 0.4, 0.2, ...] (512ç»´å‘é‡)
  
  ç°åœ¨å¯ä»¥æ¯”è¾ƒäº†ï¼ âœ…

ä¾‹å­: CLIPæ¨¡å‹
  å›¾åƒembedding: [0.8, 0.2, 0.5, ...]
  æ–‡æœ¬embedding: [0.7, 0.3, 0.4, ...]
  
  è®¡ç®—ç›¸ä¼¼åº¦:
  cosine_similarity(å›¾åƒ, æ–‡æœ¬) = 0.92
  â†’ éå¸¸åŒ¹é…ï¼
```

**ä»»åŠ¡2ï¼šè·¨æ¨¡æ€è½¬æ¢ (Translation)**

```python
ç›®æ ‡: ä»ä¸€ç§æ¨¡æ€ç”Ÿæˆå¦ä¸€ç§æ¨¡æ€

æ–¹å‘1: å›¾åƒ â†’ æ–‡æœ¬ (Image Captioning)
  [çŒ«çš„ç…§ç‰‡]
    â†“
  "ä¸€åªæ©˜è‰²çš„çŒ«ååœ¨çª—è¾¹"

æ–¹å‘2: æ–‡æœ¬ â†’ å›¾åƒ (Text-to-Image)
  "ä¸€åªçŒ«ç©¿ç€å®‡èˆªæœåœ¨æœˆçƒä¸Š"
    â†“
  [ç”Ÿæˆçš„å›¾ç‰‡]

æ–¹å‘3: éŸ³é¢‘ â†’ æ–‡æœ¬ (è¯­éŸ³è¯†åˆ«)
  [å£°éŸ³: "Hello world"]
    â†“
  "Hello world"

æ–¹å‘4: æ–‡æœ¬ â†’ éŸ³é¢‘ (è¯­éŸ³åˆæˆ)
  "ä½ å¥½ä¸–ç•Œ"
    â†“
  [åˆæˆçš„å£°éŸ³]
```

**ä»»åŠ¡3ï¼šæ¨¡æ€å¯¹é½ (Alignment)**

```python
ç›®æ ‡: æ‰¾åˆ°ä¸åŒæ¨¡æ€ä¸­å¯¹åº”çš„éƒ¨åˆ†

ä¾‹å­1: å›¾æ–‡å¯¹é½
  å›¾åƒ: [ä¸€åªçŒ«åœ¨æ¡Œå­ä¸Š]
  æ–‡æœ¬: "The cat on the table"
  
  å¯¹é½å…³ç³»:
  å›¾åƒä¸­çš„çŒ« â†” æ–‡æœ¬ä¸­çš„"cat"
  å›¾åƒä¸­çš„æ¡Œå­ â†” æ–‡æœ¬ä¸­çš„"table"
  ä½ç½®å…³ç³» â†” "on"

ä¾‹å­2: è§†é¢‘-æ–‡æœ¬å¯¹é½
  è§†é¢‘: [ä¸€ä¸ªäººåœ¨è·‘æ­¥]
  æ–‡æœ¬: "A person is running"
  
  å¯¹é½å…³ç³»:
  æ—¶åˆ»1-5ç§’: äººç‰© â†” "person"
  æ•´ä¸ªè¿‡ç¨‹: è·‘æ­¥åŠ¨ä½œ â†” "running"
```

**ä»»åŠ¡4ï¼šå¤šæ¨¡æ€èåˆ (Fusion)**

```python
ç›®æ ‡: ç»¼åˆå¤šä¸ªæ¨¡æ€çš„ä¿¡æ¯åšå†³ç­–

ä¾‹å­: è§†è§‰é—®ç­” (VQA)
  è¾“å…¥:
    å›¾åƒ: [ä¸¤åªçŒ«çš„ç…§ç‰‡]
    é—®é¢˜: "å›¾ä¸­æœ‰å‡ åªçŒ«ï¼Ÿ"
  
  å¤„ç†æµç¨‹:
    å›¾åƒç‰¹å¾: [0.8, 0.2, ...]
    æ–‡æœ¬ç‰¹å¾: [0.3, 0.7, ...]
    â†“
    èåˆ: [0.8, 0.2, 0.3, 0.7, ...]
    â†“
    å›ç­”: "2åª"

èåˆç­–ç•¥:
  æ—©æœŸèåˆ: åœ¨è¾“å…¥å±‚å°±æ··åˆ
  æ™šæœŸèåˆ: å„è‡ªå¤„ç†åå†æ··åˆ
  ä¸­æœŸèåˆ: åœ¨ä¸­é—´å±‚æ··åˆï¼ˆæœ€å¸¸ç”¨ï¼‰
```

---

### ğŸŒ¿ 1.4 ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºæ–¹å¼

#### ğŸ“ æ–‡æœ¬æ¨¡æ€

```python
åŸå§‹å½¢å¼: "ä¸€åªçŒ«"

æ­¥éª¤1: åˆ†è¯
  ["ä¸€", "åª", "çŒ«"]

æ­¥éª¤2: TokenåŒ–
  [101, 203, 456]  # æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªID

æ­¥éª¤3: Embedding
  [0.2, 0.5, 0.1, ..., 0.3]  # è½¬æˆ768ç»´å‘é‡

ç‰¹ç‚¹:
  âœ… ç¦»æ•£ï¼ˆæœ‰é™çš„è¯æ±‡è¡¨ï¼‰
  âœ… æœ‰æ˜ç¡®è¯­ä¹‰
  âœ… æ˜“äºå¤„ç†
  âŒ æœ‰æ­§ä¹‰ï¼ˆ"é“¶è¡Œ"å¯èƒ½æ˜¯æ²³å²¸æˆ–é‡‘èæœºæ„ï¼‰
```

#### ğŸ“ å›¾åƒæ¨¡æ€

```python
åŸå§‹å½¢å¼: 224Ã—224Ã—3çš„åƒç´ çŸ©é˜µ

æ­¥éª¤1: åˆ†å— (Patch)
  å°†å›¾åƒåˆ‡æˆ16Ã—16çš„å°å—
  224Ã—224 â†’ 14Ã—14 = 196ä¸ªpatch

æ­¥éª¤2: å±•å¹³æ¯ä¸ªpatch
  æ¯ä¸ª16Ã—16Ã—3 â†’ 768ç»´å‘é‡

æ­¥éª¤3: é€šè¿‡Vision Transformer
  196ä¸ªpatch â†’ 196ä¸ªembedding
  
æ­¥éª¤4: æ±‡æ€»
  å–[CLS] token â†’ 1ä¸ª768ç»´å‘é‡ä»£è¡¨æ•´å¼ å›¾

ç‰¹ç‚¹:
  âœ… è¿ç»­ï¼ˆåƒç´ å€¼å¯ä»¥æ˜¯ä»»æ„æ•°ï¼‰
  âœ… ä¿¡æ¯ä¸°å¯Œ
  âŒ é«˜ç»´åº¦ï¼ˆéš¾ä»¥ç›´æ¥å¤„ç†ï¼‰
  âŒ è®¡ç®—é‡å¤§
```

#### ğŸ“ éŸ³é¢‘æ¨¡æ€

```python
åŸå§‹å½¢å¼: æ³¢å½¢ä¿¡å·ï¼ˆæ—¶é—´ Ã— å¹…åº¦ï¼‰

æ–¹æ³•1: Melé¢‘è°±
  éŸ³é¢‘æ³¢å½¢ â†’ Mel-Spectrogram â†’ 2Då›¾åƒ
  [æ—¶é—´ç‚¹, æŒ¯å¹…] â†’ [æ—¶é—´, é¢‘ç‡] â†’ [80, T]

æ–¹æ³•2: æ³¢å½¢ç›´æ¥ç¼–ç 
  ä½¿ç”¨Wav2Vec 2.0ç­‰æ¨¡å‹
  ç›´æ¥ä»æ³¢å½¢å­¦ä¹ è¡¨ç¤º

ç‰¹ç‚¹:
  âœ… æ—¶åºä¿¡æ¯é‡è¦
  âœ… èƒ½ä¼ è¾¾æƒ…æ„Ÿ
  âŒ å™ªå£°æ•æ„Ÿ
  âŒ éœ€è¦æ—¶é—´ç»´åº¦
```

#### ğŸ“ è§†é¢‘æ¨¡æ€

```python
åŸå§‹å½¢å¼: Tå¸§ Ã— H Ã— W Ã— 3

é—®é¢˜: è®¡ç®—é‡çˆ†ç‚¸ï¼
  30fps Ã— 10ç§’ Ã— 1920Ã—1080Ã—3 
  = 1,866,240,000ä¸ªæ•°å­— ğŸ˜±

è§£å†³æ–¹æ¡ˆ1: ç¨€ç–é‡‡æ ·
  æ¯1ç§’é‡‡æ ·1å¸§
  30fps â†’ 1fps
  è®¡ç®—é‡å‡å°‘30å€

è§£å†³æ–¹æ¡ˆ2: 3Då·ç§¯
  åŒæ—¶å¤„ç†æ—¶é—´å’Œç©ºé—´ç»´åº¦

è§£å†³æ–¹æ¡ˆ3: åˆ†ç¦»å¤„ç†
  ç©ºé—´æµ: å¤„ç†å•å¸§å†…å®¹
  æ—¶é—´æµ: å¤„ç†å¸§é—´è¿åŠ¨

ç‰¹ç‚¹:
  âœ… åŒ…å«è¿åŠ¨ä¿¡æ¯
  âœ… æœ€å®Œæ•´çš„è§†è§‰ä¿¡æ¯
  âŒ è®¡ç®—æˆæœ¬æé«˜
  âŒ å­˜å‚¨éœ€æ±‚å¤§
```

---

### ğŸ¯ 1.5 å¤šæ¨¡æ€çš„æŒ‘æˆ˜

```python
æŒ‘æˆ˜1: æ¨¡æ€å¼‚è´¨æ€§
  é—®é¢˜: å›¾åƒæ˜¯è¿ç»­çš„ï¼Œæ–‡æœ¬æ˜¯ç¦»æ•£çš„
  è§£å†³: æ˜ å°„åˆ°ç»Ÿä¸€çš„å‘é‡ç©ºé—´

æŒ‘æˆ˜2: æ¨¡æ€ä¸å¯¹é½
  é—®é¢˜: å›¾åƒå’Œæè¿°å¯èƒ½ä¸å®Œå…¨åŒ¹é…
  è§£å†³: å¼±ç›‘ç£å­¦ä¹ ï¼Œå®¹å¿å™ªå£°

æŒ‘æˆ˜3: è®¡ç®—å¤æ‚åº¦
  é—®é¢˜: å¤„ç†å¤šæ¨¡æ€è®¡ç®—é‡æˆå€å¢åŠ 
  è§£å†³: ç¨€ç–æ¿€æ´»ã€é«˜æ•ˆæ¶æ„

æŒ‘æˆ˜4: æ•°æ®æ”¶é›†
  é—®é¢˜: éœ€è¦å¤§é‡é…å¯¹æ•°æ®
  è§£å†³: ä»äº’è”ç½‘è‡ªåŠ¨çˆ¬å–ï¼ˆå¦‚CLIPçš„4äº¿å›¾æ–‡å¯¹ï¼‰

æŒ‘æˆ˜5: è¯„ä¼°å›°éš¾
  é—®é¢˜: å¦‚ä½•è¯„ä»·"å›¾ç‰‡å¥½ä¸å¥½"ï¼Ÿ
  è§£å†³: ç»“åˆè‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»è¯„ä¼°
```

---

### âœ… ç¬¬ä¸€éƒ¨åˆ†å°ç»“

ç°åœ¨ä½ åº”è¯¥ç†è§£äº†ï¼š

**åŸºç¡€æ¦‚å¿µ**
- [ ] ä»€ä¹ˆæ˜¯æ¨¡æ€ï¼Ÿï¼ˆä¿¡æ¯çš„ä¸åŒå½¢å¼ï¼‰
- [ ] ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€ï¼Ÿï¼ˆå®Œæ•´ç†è§£ä¸–ç•Œï¼‰
- [ ] å¤šæ¨¡æ€çš„å››å¤§ä»»åŠ¡ï¼ˆè¡¨ç¤ºã€è½¬æ¢ã€å¯¹é½ã€èåˆï¼‰

**ä¸åŒæ¨¡æ€**
- [ ] æ–‡æœ¬ï¼šç¦»æ•£ã€è¯­ä¹‰æ˜ç¡®
- [ ] å›¾åƒï¼šè¿ç»­ã€é«˜ç»´åº¦
- [ ] éŸ³é¢‘ï¼šæ—¶åºã€æƒ…æ„Ÿä¸°å¯Œ
- [ ] è§†é¢‘ï¼šæ—¶ç©ºç»“åˆã€è®¡ç®—é‡å¤§

**æ ¸å¿ƒæŒ‘æˆ˜**
- [ ] å¦‚ä½•ç»Ÿä¸€ä¸åŒå½¢å¼çš„ä¿¡æ¯
- [ ] å¦‚ä½•é«˜æ•ˆå¤„ç†å¤§é‡æ•°æ®
- [ ] å¦‚ä½•è¯„ä¼°æ•ˆæœ

**ä¸‹ä¸€æ­¥ï¼š** æˆ‘ä»¬å°†å­¦ä¹ CLIPâ€”â€”ç¬¬ä¸€ä¸ªçœŸæ­£æˆåŠŸçš„å¤šæ¨¡æ€æ¨¡å‹ï¼

---

## ğŸ–¼ï¸ ç¬¬äºŒéƒ¨åˆ†ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆæ ¸å¿ƒæŠ€æœ¯ï¼‰

### ğŸŒ³ 2.1 CLIP - å›¾æ–‡åŒ¹é…çš„é­”æ³•

#### ğŸ’¡ ç›´è§‚ç†è§£

**CLIPæ˜¯ä»€ä¹ˆï¼Ÿ**  
CLIP = èƒ½åŒæ—¶ç†è§£å›¾ç‰‡å’Œæ–‡å­—çš„AI

**ç”Ÿæ´»æ¯”å–»ï¼šé…å¯¹æ¸¸æˆ**

```
æƒ³è±¡ä½ åœ¨ç©é…å¯¹æ¸¸æˆï¼š

å·¦è¾¹æ˜¯å›¾ç‰‡ï¼š
  ğŸ± çŒ«çš„ç…§ç‰‡
  ğŸ• ç‹—çš„ç…§ç‰‡
  ğŸš— è½¦çš„ç…§ç‰‡

å³è¾¹æ˜¯æ–‡å­—ï¼š
  A. "a photo of a cat"
  B. "a photo of a dog"  
  C. "a photo of a car"

ä»»åŠ¡ï¼šå°†å›¾ç‰‡å’Œæ–‡å­—æ­£ç¡®é…å¯¹

æ™®é€šäººï¼šğŸ‘ï¸ çœ‹å›¾ â†’ ğŸ§  ç†è§£ â†’ âœ… è½»æ¾é…å¯¹

CLIPæ¨¡å‹ï¼šåšåŒæ ·çš„äº‹ï¼
  ä½†å®ƒå¤„ç†çš„æ˜¯4äº¿å¯¹å›¾æ–‡ ğŸ˜±
```

#### ğŸ¯ CLIPçš„æ ¸å¿ƒæ€æƒ³

**é—®é¢˜ï¼šå¦‚ä½•è®©AIç†è§£å›¾æ–‡å¯¹åº”å…³ç³»ï¼Ÿ**

```python
ä¼ ç»Ÿæ–¹æ³•ï¼ˆç›‘ç£å­¦ä¹ ï¼‰:
  éœ€è¦ï¼š
    å›¾ç‰‡1 + æ ‡ç­¾"çŒ«"
    å›¾ç‰‡2 + æ ‡ç­¾"ç‹—"
    ...
  
  é—®é¢˜ï¼š
    âŒ éœ€è¦äººå·¥æ ‡æ³¨ï¼ˆæ˜‚è´µï¼‰
    âŒ æ ‡ç­¾å›ºå®šï¼ˆåªèƒ½è¯†åˆ«è®­ç»ƒè¿‡çš„ç±»åˆ«ï¼‰
    âŒ æ— æ³•æ³›åŒ–

CLIPçš„æ–¹æ³•ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰:
  éœ€è¦ï¼š
    å›¾ç‰‡1 + æè¿°"a cat sitting on a mat"
    å›¾ç‰‡2 + æè¿°"a dog playing in the park"
    ...
  
  ä¼˜åŠ¿ï¼š
    âœ… ä»äº’è”ç½‘è‡ªåŠ¨æ”¶é›†ï¼ˆä¾¿å®œï¼‰
    âœ… æè¿°çµæ´»ï¼ˆä»»æ„æ–‡å­—ï¼‰
    âœ… å¼ºå¤§æ³›åŒ–ï¼ˆé›¶æ ·æœ¬èƒ½åŠ›ï¼‰
```

#### ğŸ“Š CLIPçš„è®­ç»ƒè¿‡ç¨‹

**æ­¥éª¤è¯¦è§£ï¼ˆç”¨é…å¯¹æ¸¸æˆç†è§£ï¼‰**

```python
è®­ç»ƒæ•°æ®ï¼šä¸€æ‰¹å›¾æ–‡å¯¹ï¼ˆæ¯”å¦‚32å¯¹ï¼‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch of 32 image-text pairs           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. [çŒ«å›¾ç‰‡]  "a cat"         âœ… åŒ¹é…    â”‚
â”‚ 2. [ç‹—å›¾ç‰‡]  "a dog"         âœ… åŒ¹é…    â”‚
â”‚ 3. [è½¦å›¾ç‰‡]  "a car"         âœ… åŒ¹é…    â”‚
â”‚ ...                                     â”‚
â”‚ 32. [æ ‘å›¾ç‰‡] "a tree"        âœ… åŒ¹é…    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ­¥éª¤1: ç¼–ç 
  å›¾åƒ â†’ Vision Encoder â†’ 32ä¸ªå›¾åƒå‘é‡
  æ–‡æœ¬ â†’ Text Encoder â†’ 32ä¸ªæ–‡æœ¬å‘é‡
  
  æ¯ä¸ªå‘é‡éƒ½æ˜¯512ç»´

æ­¥éª¤2: è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
  è®¡ç®—æ‰€æœ‰å›¾åƒå’Œæ‰€æœ‰æ–‡æœ¬çš„ç›¸ä¼¼åº¦
  
  ç»“æœï¼š32Ã—32çš„çŸ©é˜µ
  
    æ–‡æœ¬1  æ–‡æœ¬2  æ–‡æœ¬3  ...  æ–‡æœ¬32
  å›¾1 [0.9]  0.1   0.05  ...  0.03  â† å¯¹è§’çº¿åº”è¯¥é«˜ï¼
  å›¾2  0.1  [0.85] 0.08  ...  0.04
  å›¾3  0.05  0.08 [0.92] ...  0.02
  ...
  å›¾32 0.03  0.04  0.02  ... [0.88]

æ­¥éª¤3: å¯¹æ¯”å­¦ä¹ æŸå¤±
  ç›®æ ‡ï¼šè®©å¯¹è§’çº¿çš„å€¼é«˜ï¼Œå…¶ä»–å€¼ä½
  
  å¯¹äºå›¾1ï¼š
    âœ… å›¾1-æ–‡æœ¬1åº”è¯¥ç›¸ä¼¼åº¦é«˜ï¼ˆå®ƒä»¬åŒ¹é…ï¼‰
    âŒ å›¾1-æ–‡æœ¬2åº”è¯¥ç›¸ä¼¼åº¦ä½ï¼ˆå®ƒä»¬ä¸åŒ¹é…ï¼‰
    âŒ å›¾1-æ–‡æœ¬3åº”è¯¥ç›¸ä¼¼åº¦ä½
    ...
  
  æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µ
    é¼“åŠ±æ¨¡å‹ç»™æ­£ç¡®é…å¯¹æ‰“é«˜åˆ†
```

#### ğŸ—ï¸ CLIPæ¶æ„è¯¦è§£

```
å®Œæ•´æ¶æ„å›¾ï¼š

è¾“å…¥å±‚
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                 â”‚                 â”‚
å›¾åƒåˆ†æ”¯           æ–‡æœ¬åˆ†æ”¯          â”‚
  â”‚                 â”‚                 â”‚
  â†“                 â†“                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚ Image   â”‚    â”‚  Text   â”‚           â”‚
â”‚224Ã—224Ã—3â”‚    â”‚ "a cat" â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
  â”‚                 â”‚                 â”‚
  â†“                 â†“                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚ Vision  â”‚    â”‚  Text   â”‚           â”‚
â”‚Transformâ”‚    â”‚Transformâ”‚           â”‚
â”‚  (ViT)  â”‚    â”‚ (GPT-2) â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
  â”‚                 â”‚                 â”‚
  â†“                 â†“                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚ Image   â”‚    â”‚  Text   â”‚           â”‚
â”‚Embeddingâ”‚    â”‚Embeddingâ”‚           â”‚
â”‚ (512ç»´) â”‚    â”‚ (512ç»´) â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
  â”‚                 â”‚                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
           â†“                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
    â”‚ Cosine       â”‚                  â”‚
    â”‚ Similarity   â”‚                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
           â†“                          â”‚
      ç›¸ä¼¼åº¦åˆ†æ•°                       â”‚
                                      â”‚
è®­ç»ƒæ—¶ï¼šå¯¹æ¯”å­¦ä¹ æŸå¤± â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ¨ç†æ—¶ï¼šç”¨äºå›¾æ–‡åŒ¹é…ã€é›¶æ ·æœ¬åˆ†ç±»ç­‰
```

#### ğŸ”§ CLIPå®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥å®ç°CLIPçš„æ ¸å¿ƒï¼š

**æ­¥éª¤1ï¼šå›¾åƒç¼–ç å™¨ï¼ˆVision Encoderï¼‰**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ImageEncoder(nn.Module):
    """æŠŠå›¾åƒè½¬æ¢æˆå‘é‡"""
    def __init__(self, image_size=224, patch_size=16, embed_dim=512):
        super().__init__()
        
        # æŠŠå›¾åƒåˆ‡æˆpatchï¼ˆç±»ä¼¼æ‹¼å›¾ï¼‰
        self.num_patches = (image_size // patch_size) ** 2  # 196ä¸ªpatch
        
        # Patch Embeddingï¼šæŠŠæ¯ä¸ªpatchå˜æˆå‘é‡
        self.patch_embed = nn.Conv2d(
            in_channels=3,       # RGB 3é€šé“
            out_channels=embed_dim,  # è¾“å‡º512ç»´
            kernel_size=patch_size,  # 16Ã—16çš„patch
            stride=patch_size
        )
        
        # ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡å‹æ¯ä¸ªpatchçš„ä½ç½®
        self.pos_embed = nn.Parameter(
            torch.randn(1, self.num_patches + 1, embed_dim)
        )
        
        # [CLS] tokenï¼šç”¨æ¥æ±‡æ€»æ•´å¼ å›¾çš„ä¿¡æ¯
        self.cls_token = nn.Parameter(
            torch.randn(1, 1, embed_dim)
        )
        
        # Transformerï¼šå¤„ç†patchä¹‹é—´çš„å…³ç³»
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=8,
                dim_feedforward=2048
            ),
            num_layers=12
        )
        
        # æŠ•å½±å±‚ï¼šæœ€ç»ˆè¾“å‡º
        self.proj = nn.Linear(embed_dim, embed_dim)
    
    def forward(self, x):
        """
        x: [batch, 3, 224, 224] - å›¾åƒ
        è¿”å›: [batch, 512] - å›¾åƒå‘é‡
        """
        batch_size = x.shape[0]
        
        # 1. åˆ‡æˆpatch
        # [batch, 3, 224, 224] â†’ [batch, 512, 14, 14]
        x = self.patch_embed(x)
        
        # 2. å±•å¹³
        # [batch, 512, 14, 14] â†’ [batch, 196, 512]
        x = x.flatten(2).transpose(1, 2)
        
        # 3. æ·»åŠ [CLS] token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # [batch, 197, 512]
        
        # 4. æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # 5. Transformerå¤„ç†
        x = self.transformer(x)
        
        # 6. å–[CLS] tokenï¼ˆä»£è¡¨æ•´å¼ å›¾ï¼‰
        x = x[:, 0]  # [batch, 512]
        
        # 7. æŠ•å½±
        x = self.proj(x)
        
        # 8. L2å½’ä¸€åŒ–ï¼ˆé‡è¦ï¼è®©å‘é‡é•¿åº¦ä¸º1ï¼‰
        x = F.normalize(x, dim=-1)
        
        return x
```

**ä¸ºä»€ä¹ˆè¦L2å½’ä¸€åŒ–ï¼Ÿ**

```python
ä¾‹å­ï¼šä¸å½’ä¸€åŒ–çš„é—®é¢˜

å‘é‡A = [1.0, 2.0, 3.0]  é•¿åº¦ = âˆš14 â‰ˆ 3.74
å‘é‡B = [100, 200, 300]  é•¿åº¦ = âˆš140000 â‰ˆ 374.17

é—®é¢˜ï¼šBçš„é•¿åº¦æ˜¯Açš„100å€ï¼
      å³ä½¿æ–¹å‘ç›¸åŒï¼Œç›¸ä¼¼åº¦è®¡ç®—ä¼šè¢«é•¿åº¦å½±å“

å½’ä¸€åŒ–åï¼š
å‘é‡A' = [0.27, 0.53, 0.80]  é•¿åº¦ = 1.0
å‘é‡B' = [0.27, 0.53, 0.80]  é•¿åº¦ = 1.0

âœ… åªæ¯”è¾ƒæ–¹å‘ï¼Œä¸å—é•¿åº¦å½±å“
```

**æ­¥éª¤2ï¼šæ–‡æœ¬ç¼–ç å™¨ï¼ˆText Encoderï¼‰**

```python
class TextEncoder(nn.Module):
    """æŠŠæ–‡æœ¬è½¬æ¢æˆå‘é‡"""
    def __init__(self, vocab_size=50000, embed_dim=512, max_len=77):
        super().__init__()
        
        # Token Embeddingï¼šæ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªå‘é‡
        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        
        # ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡å‹æ¯ä¸ªè¯çš„ä½ç½®
        self.pos_embed = nn.Parameter(
            torch.randn(1, max_len, embed_dim)
        )
        
        # Transformerï¼šç†è§£å¥å­è¯­ä¹‰
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=8,
                dim_feedforward=2048
            ),
            num_layers=12
        )
        
        # æŠ•å½±å±‚
        self.proj = nn.Linear(embed_dim, embed_dim)
    
    def forward(self, x):
        """
        x: [batch, seq_len] - token IDs
        è¿”å›: [batch, 512] - æ–‡æœ¬å‘é‡
        """
        # 1. Token embedding
        x = self.token_embed(x)  # [batch, seq_len, 512]
        
        # 2. æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed[:, :x.size(1), :]
        
        # 3. Transformerå¤„ç†
        x = self.transformer(x)
        
        # 4. å–æœ€åä¸€ä¸ªtokenï¼ˆæˆ–ç”¨[EOS] tokenï¼‰
        x = x[:, -1, :]  # [batch, 512]
        
        # 5. æŠ•å½±å’Œå½’ä¸€åŒ–
        x = self.proj(x)
        x = F.normalize(x, dim=-1)
        
        return x
```

**æ­¥éª¤3ï¼šCLIPä¸»æ¨¡å‹**

```python
class CLIP(nn.Module):
    """å®Œæ•´çš„CLIPæ¨¡å‹"""
    def __init__(self, embed_dim=512):
        super().__init__()
        
        # ä¸¤ä¸ªç¼–ç å™¨
        self.image_encoder = ImageEncoder(embed_dim=embed_dim)
        self.text_encoder = TextEncoder(embed_dim=embed_dim)
        
        # æ¸©åº¦å‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
        # ç”¨äºæ§åˆ¶ç›¸ä¼¼åº¦çš„å°ºåº¦
        self.logit_scale = nn.Parameter(
            torch.ones([]) * torch.log(torch.tensor(1.0 / 0.07))
        )
    
    def forward(self, images, texts):
        """
        images: [batch, 3, 224, 224]
        texts: [batch, seq_len]
        """
        # ç¼–ç 
        image_features = self.image_encoder(images)  # [batch, 512]
        text_features = self.text_encoder(texts)     # [batch, 512]
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # [batch, 512] @ [512, batch] = [batch, batch]
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.T
        logits_per_text = logits_per_image.T
        
        return logits_per_image, logits_per_text
```

**æ­¥éª¤4ï¼šå¯¹æ¯”å­¦ä¹ æŸå¤±**

```python
def contrastive_loss(logits_per_image, logits_per_text):
    """
    CLIPçš„å¯¹æ¯”å­¦ä¹ æŸå¤±
    
    logits_per_image: [batch, batch] - å›¾åƒâ†’æ–‡æœ¬çš„ç›¸ä¼¼åº¦
    logits_per_text: [batch, batch] - æ–‡æœ¬â†’å›¾åƒçš„ç›¸ä¼¼åº¦
    """
        batch_size = logits_per_image.shape[0]
    
    # æ ‡ç­¾ï¼šå¯¹è§’çº¿ä½ç½®æ˜¯æ­£ç¡®é…å¯¹
    # [0, 1, 2, 3, ..., batch_size-1]
        labels = torch.arange(batch_size, device=logits_per_image.device)
        
    # å›¾åƒâ†’æ–‡æœ¬çš„äº¤å‰ç†µæŸå¤±
    loss_i2t = F.cross_entropy(logits_per_image, labels)
    
    # æ–‡æœ¬â†’å›¾åƒçš„äº¤å‰ç†µæŸå¤±
    loss_t2i = F.cross_entropy(logits_per_text, labels)
    
    # æ€»æŸå¤±ï¼šä¸¤ä¸ªæ–¹å‘çš„å¹³å‡
    loss = (loss_i2t + loss_t2i) / 2
    
        return loss

# ä½¿ç”¨ç¤ºä¾‹
model = CLIP()
images = torch.randn(32, 3, 224, 224)  # 32å¼ å›¾ç‰‡
texts = torch.randint(0, 50000, (32, 77))  # 32å¥è¯

# å‰å‘ä¼ æ’­
            logits_img, logits_txt = model(images, texts)
            
# è®¡ç®—æŸå¤±
loss = contrastive_loss(logits_img, logits_txt)
```

---

#### ğŸ¯ CLIPçš„åº”ç”¨ï¼šé›¶æ ·æœ¬åˆ†ç±»

**ä»€ä¹ˆæ˜¯é›¶æ ·æœ¬åˆ†ç±»ï¼Ÿ**

```python
é—®é¢˜ï¼šæˆ‘æƒ³åˆ†ç±»ä¸€å¼ å›¾ç‰‡ï¼Œä½†æ²¡æœ‰è®­ç»ƒè¿‡è¿™ä¸ªç±»åˆ«

ä¼ ç»Ÿæ–¹æ³•ï¼š
  è®­ç»ƒé›†ï¼šçŒ«ã€ç‹—ã€é¸Ÿ
  æµ‹è¯•é›†ï¼šçŒ«ã€ç‹—ã€é¸Ÿ âœ…
  æµ‹è¯•é›†ï¼šè½¦ï¼Ÿï¼Ÿï¼Ÿ âŒ ä»æ²¡è§è¿‡ï¼Œæ— æ³•åˆ†ç±»

CLIPçš„æ–¹æ³•ï¼š
  ä¸éœ€è¦è®­ç»ƒï¼ç›´æ¥ç”¨æ–‡æœ¬æè¿°ç±»åˆ«
  
  å€™é€‰ç±»åˆ«ï¼š["cat", "dog", "car", "tree"]
  â†’ è®¡ç®—å›¾åƒå’Œæ¯ä¸ªæ–‡å­—çš„ç›¸ä¼¼åº¦
  â†’ é€‰æœ€ç›¸ä¼¼çš„
  
  âœ… å³ä½¿ä»æ²¡è§è¿‡"car"ï¼Œä¹Ÿèƒ½åˆ†ç±»ï¼
```

**å®ç°é›¶æ ·æœ¬åˆ†ç±»**

```python
def zero_shot_classification(clip_model, image, candidate_texts):
    """
    é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
    
    clip_model: è®­ç»ƒå¥½çš„CLIPæ¨¡å‹
    image: [3, 224, 224] - å¾…åˆ†ç±»çš„å›¾åƒ
    candidate_texts: ["a cat", "a dog", "a car"] - å€™é€‰ç±»åˆ«
    """
    with torch.no_grad():  # ä¸éœ€è¦æ¢¯åº¦
        # 1. ç¼–ç å›¾åƒ
        image_features = clip_model.image_encoder(
            image.unsqueeze(0)  # [1, 3, 224, 224]
        )
        
        # 2. ç¼–ç æ‰€æœ‰å€™é€‰æ–‡æœ¬
        text_features_list = []
        for text in candidate_texts:
            # tokenizeæ–‡æœ¬ï¼ˆçœç•¥å®ç°ç»†èŠ‚ï¼‰
            text_tokens = tokenize(text)  
            # ç¼–ç 
            text_feat = clip_model.text_encoder(text_tokens.unsqueeze(0))
            text_features_list.append(text_feat)
        
        # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬ç‰¹å¾
        text_features = torch.cat(text_features_list, dim=0)
        # [num_texts, 512]
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = clip_model.logit_scale.exp()
        logits = logit_scale * image_features @ text_features.T
        # [1, num_texts]
        
        # 4. Softmaxå¾—åˆ°æ¦‚ç‡
        probs = F.softmax(logits, dim=-1)
        
        return probs[0].cpu().numpy()

# ä½¿ç”¨ç¤ºä¾‹
clip = load_pretrained_clip()  # åŠ è½½é¢„è®­ç»ƒçš„CLIP
image = load_image("cat.jpg")

# å®šä¹‰å€™é€‰ç±»åˆ«
candidates = [
    "a photo of a cat",
    "a photo of a dog", 
    "a photo of a car",
    "a photo of a tree"
]

# åˆ†ç±»
probs = zero_shot_classification(clip, image, candidates)

# è¾“å‡ºç»“æœ
for text, prob in zip(candidates, probs):
    print(f"{text}: {prob:.2%}")

# è¾“å‡ºï¼š
# a photo of a cat: 87.5%  â† æœ€é«˜ï¼
# a photo of a dog: 8.2%
# a photo of a car: 2.1%
# a photo of a tree: 2.2%
```

**ä¸ºä»€ä¹ˆé›¶æ ·æœ¬åˆ†ç±»è¿™ä¹ˆå¼ºå¤§ï¼Ÿ**

```python
ä¼˜åŠ¿1: ä¸éœ€è¦è®­ç»ƒ
  ä¼ ç»Ÿï¼šéœ€è¦æ”¶é›†æ•°æ®ã€æ ‡æ³¨ã€è®­ç»ƒ
  CLIPï¼šç›´æ¥ç”¨ï¼
  
ä¼˜åŠ¿2: çµæ´»
  ä¼ ç»Ÿï¼šç±»åˆ«å›ºå®šï¼ˆåªèƒ½è¯†åˆ«è®­ç»ƒè¿‡çš„ï¼‰
  CLIPï¼šç±»åˆ«éšæ„ï¼ˆç”¨æ–‡å­—æè¿°å°±è¡Œï¼‰
  
  ä¾‹å­ï¼š
  "a photo of a cat"
  "a cute orange cat"
  "ä¸€åªå¯çˆ±çš„æ©˜çŒ«"
  éƒ½å¯ä»¥ï¼
  
ä¼˜åŠ¿3: æ³›åŒ–èƒ½åŠ›å¼º
  å³ä½¿è®­ç»ƒæ—¶æ²¡è§è¿‡æŸä¸ªç±»åˆ«
  åªè¦èƒ½ç”¨æ–‡å­—æè¿°ï¼Œå°±èƒ½è¯†åˆ«
```

---

#### âœ… CLIPå°ç»“

ç°åœ¨ä½ åº”è¯¥ç†è§£äº†ï¼š

**æ ¸å¿ƒæ¦‚å¿µ**
- [ ] CLIPé€šè¿‡å¯¹æ¯”å­¦ä¹ å­¦ä¹ å›¾æ–‡å¯¹åº”å…³ç³»
- [ ] ä½¿ç”¨4äº¿å›¾æ–‡å¯¹è¿›è¡Œè®­ç»ƒ
- [ ] å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ª512ç»´ç©ºé—´

**å…³é”®æŠ€æœ¯**
- [ ] å›¾åƒç¼–ç å™¨ï¼šVision Transformerï¼ˆViTï¼‰
- [ ] æ–‡æœ¬ç¼–ç å™¨ï¼šText Transformer
- [ ] å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼šè®©åŒ¹é…çš„å›¾æ–‡ç›¸ä¼¼åº¦é«˜

**é‡è¦åº”ç”¨**
- [ ] é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆæ— éœ€è®­ç»ƒï¼‰
- [ ] å›¾æ–‡æ£€ç´¢ï¼ˆæœå›¾æ‰¾æ–‡ã€æœæ–‡æ‰¾å›¾ï¼‰
- [ ] ä½œä¸ºå…¶ä»–æ¨¡å‹çš„backbone

**ä¸‹ä¸€æ­¥ï¼š** æˆ‘ä»¬å°†å­¦ä¹ LLaVAâ€”â€”å¦‚ä½•è®©GPTçœ‹æ‡‚å›¾ç‰‡ï¼

---

### ğŸŒ³ 2.2 LLaVA - è®©GPTçœ‹æ‡‚å›¾ç‰‡

#### ğŸ’¡ ç›´è§‚ç†è§£

**LLaVAæ˜¯ä»€ä¹ˆï¼Ÿ**  
LLaVA = è§†è§‰ç‰ˆçš„ChatGPT

**ç”Ÿæ´»æ¯”å–»ï¼šç»™ç›²äººè£…ä¸Šçœ¼ç›**

```
æƒ³è±¡GPTæ˜¯ä¸€ä¸ªèªæ˜ä½†çœ‹ä¸è§çš„äººï¼š

æ²¡æœ‰LLaVA:
  ä½ : "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ" [å‘é€å›¾ç‰‡]
  GPT: "æŠ±æ­‰ï¼Œæˆ‘çœ‹ä¸åˆ°å›¾ç‰‡..."
  âŒ åªèƒ½èŠå¤©ï¼Œä¸èƒ½çœ‹å›¾

æœ‰äº†LLaVA:
  ä½ : "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ" [å‘é€å›¾ç‰‡]
  LLaVA: ğŸ‘ï¸ "æˆ‘çœ‹åˆ°ä¸€åªæ©˜çŒ«ååœ¨çº¢è‰²å«å­ä¸Šï¼Œ
         é˜³å…‰ä»çª—æˆ·ç…§è¿›æ¥..."
  âœ… æ—¢èƒ½çœ‹å›¾ï¼Œåˆèƒ½èŠå¤©ï¼

ä½ : "å®ƒçœ‹èµ·æ¥å¼€å¿ƒå—ï¼Ÿ"
LLaVA: "æ˜¯çš„ï¼Œå®ƒçœ‹èµ·æ¥å¾ˆæ”¾æ¾å’Œæ»¡è¶³ï¼Œ
       çœ¼ç›åŠé—­ç€ï¼Œå§¿åŠ¿å¾ˆèˆ’é€‚ã€‚"
  âœ… èƒ½å¤Ÿç†è§£å’Œæ¨ç†ï¼
```

#### ğŸ¯ LLaVAçš„æ ¸å¿ƒæ€æƒ³

**é—®é¢˜ï¼šå¦‚ä½•è®©GPTç†è§£å›¾åƒï¼Ÿ**

```python
æ–¹æ¡ˆ1: é‡æ–°è®­ç»ƒGPTï¼ˆä»å¤´æ¥è¿‡ï¼‰
  âŒ å¤ªè´µäº†ï¼GPTè®­ç»ƒè¦èŠ±æ•°ç™¾ä¸‡ç¾å…ƒ
  âŒ æµªè´¹å·²æœ‰çš„è¯­è¨€èƒ½åŠ›

æ–¹æ¡ˆ2: LLaVAçš„æ–¹æ¡ˆï¼ˆå·§å¦™ï¼ï¼‰
  âœ… ä¿ç•™GPTçš„è¯­è¨€èƒ½åŠ›ï¼ˆå†»ç»“ï¼‰
  âœ… åªè®­ç»ƒä¸€ä¸ª"ç¿»è¯‘å™¨"
  âœ… æŠŠå›¾åƒ"ç¿»è¯‘"æˆGPTèƒ½æ‡‚çš„è¯­è¨€

æ¯”å–»ï¼š
  GPT = åªæ‡‚ä¸­æ–‡çš„æ•™æˆ
  å›¾åƒ = è‹±æ–‡ä¹¦
  LLaVAçš„é€‚é…å™¨ = ç¿»è¯‘å™¨
  
  è‹±æ–‡ä¹¦ â†’ ç¿»è¯‘æˆä¸­æ–‡ â†’ æ•™æˆç†è§£ âœ…
```

#### ğŸ—ï¸ LLaVAæ¶æ„è¯¦è§£

```
å®Œæ•´æµç¨‹ï¼š

è¾“å…¥
  â”œâ”€ å›¾åƒ (224Ã—224Ã—3)
  â””â”€ æ–‡æœ¬ "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"

æ­¥éª¤1: å›¾åƒç¼–ç 
  å›¾åƒ â†’ CLIPè§†è§‰ç¼–ç å™¨ â†’ å›¾åƒç‰¹å¾
  [224Ã—224Ã—3] â†’ [196, 1024]
  ï¼ˆ196ä¸ªpatchï¼Œæ¯ä¸ª1024ç»´ï¼‰

æ­¥éª¤2: ç‰¹å¾æŠ•å½±ï¼ˆå…³é”®ï¼ï¼‰
  å›¾åƒç‰¹å¾ â†’ æŠ•å½±å±‚ â†’ GPTç©ºé—´çš„ç‰¹å¾
  [196, 1024] â†’ [196, 4096]
  
  ä¸ºä»€ä¹ˆè¦æŠ•å½±ï¼Ÿ
  CLIPçš„ç‰¹å¾ç»´åº¦: 1024
  LLaMAçš„ç‰¹å¾ç»´åº¦: 4096
  éœ€è¦"ç¿»è¯‘"åˆ°åŒä¸€ä¸ªè¯­è¨€ç©ºé—´ï¼

æ­¥éª¤3: æ‹¼æ¥ç‰¹å¾
  [å›¾åƒtokens] + [æ–‡æœ¬tokens]
  [196ä¸ªè§†è§‰token] + [50ä¸ªæ–‡æœ¬token]
  = [246ä¸ªtoken]

æ­¥éª¤4: é€å…¥LLaMA
  LLaMAçœ‹åˆ°çš„è¾“å…¥ï¼š
    å‰196ä¸ªtoken: "è¿™æ˜¯ä¸€å¼ å›¾ç‰‡"ï¼ˆè§†è§‰ä¿¡æ¯ï¼‰
    å50ä¸ªtoken: "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"ï¼ˆæ–‡æœ¬é—®é¢˜ï¼‰
  
  LLaMAè¾“å‡ºï¼š
    "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä¸€åªæ©˜çŒ«..."

å…³é”®æ´å¯Ÿï¼š
  å¯¹LLaMAæ¥è¯´ï¼Œå›¾åƒåªæ˜¯"ç‰¹æ®Šçš„è¯"ï¼
  å°±åƒ"cat"ã€"dog"ä¸€æ ·ï¼Œåªä¸è¿‡æ˜¯è§†è§‰token
```

#### ğŸ“Š æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LLaVA Architecture               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å›¾åƒåˆ†æ”¯ï¼ˆå†»ç»“CLIPï¼‰
  å›¾åƒ [224Ã—224Ã—3]
    â†“
  CLIP Vision Encoder (å†»ç»“ ğŸ”’)
    â†“
  å›¾åƒç‰¹å¾ [196, 1024]
    â†“
  æŠ•å½±å±‚ (å¯è®­ç»ƒ ğŸ”“)
    â†“
  è§†è§‰tokens [196, 4096]
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
æ–‡æœ¬åˆ†æ”¯          â”‚
  "What's in     â”‚
   this image?"  â”‚
    â†“             â”‚
  Tokenize       â”‚
    â†“             â”‚
  æ–‡æœ¬tokens      â”‚
  [50, 4096]      â”‚
    â”‚             â”‚
    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   æ‹¼æ¥ Concat
       â†“
  æ··åˆtokens [246, 4096]
       â†“
  LLaMA (å†»ç»“éƒ¨åˆ† ğŸ”’)
       â†“
  ç”Ÿæˆå›ç­”
  "This is an orange cat..."
```

#### ğŸ”§ LLaVAå®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥å®ç°LLaVAï¼š

**æ­¥éª¤1ï¼šæŠ•å½±å±‚ï¼ˆæ ¸å¿ƒç»„ä»¶ï¼‰**

```python
import torch
import torch.nn as nn

class ProjectionLayer(nn.Module):
    """
    æŠŠCLIPçš„ç‰¹å¾æŠ•å½±åˆ°LLaMAçš„ç©ºé—´
    è¿™æ˜¯LLaVAå”¯ä¸€éœ€è¦è®­ç»ƒçš„éƒ¨åˆ†ï¼
    """
    def __init__(self, vision_hidden_size=1024, llm_hidden_size=4096):
        super().__init__()
        
        # ä¸¤å±‚MLP
        self.proj = nn.Sequential(
            nn.Linear(vision_hidden_size, llm_hidden_size),
            nn.GELU(),
            nn.Linear(llm_hidden_size, llm_hidden_size)
        )
    
    def forward(self, vision_features):
        """
        vision_features: [batch, num_patches, 1024]
        è¿”å›: [batch, num_patches, 4096]
        """
        # æŠ•å½±åˆ°LLMç©ºé—´
        projected = self.proj(vision_features)
        return projected

# ä¸ºä»€ä¹ˆç”¨ä¸¤å±‚MLPï¼Ÿ
"""
å•å±‚ï¼švision_space â†’ llm_space
      ç›´æ¥æ˜ å°„ï¼Œå¯èƒ½ä¸¢å¤±ä¿¡æ¯

ä¸¤å±‚ï¼švision_space â†’ ä¸­é—´ç©ºé—´ â†’ llm_space
      éçº¿æ€§å˜æ¢ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯
      
ç±»æ¯”ï¼š
  å•å±‚ = ç›´è¯‘ï¼ˆword by wordï¼‰
  ä¸¤å±‚ = æ„è¯‘ï¼ˆç†è§£å«ä¹‰åå†è¡¨è¾¾ï¼‰
"""
```

**æ­¥éª¤2ï¼šå®Œæ•´çš„LLaVAæ¨¡å‹**

```python
from transformers import CLIPVisionModel, LlamaForCausalLM

class LLaVA(nn.Module):
    """ç®€åŒ–çš„LLaVAæ¨¡å‹"""
    def __init__(
        self,
        vision_model_name='openai/clip-vit-large-patch14',
        llm_model_name='meta-llama/Llama-2-7b-hf'
    ):
        super().__init__()
        
        # 1. è§†è§‰ç¼–ç å™¨ï¼ˆCLIPï¼Œå†»ç»“ï¼‰
        self.vision_tower = CLIPVisionModel.from_pretrained(vision_model_name)
        # å†»ç»“å‚æ•°ï¼Œä¸è®­ç»ƒ
        for param in self.vision_tower.parameters():
            param.requires_grad = False
        
        # 2. è¯­è¨€æ¨¡å‹ï¼ˆLLaMAï¼‰
        self.llm = LlamaForCausalLM.from_pretrained(llm_model_name)
        
        # 3. æŠ•å½±å±‚ï¼ˆå”¯ä¸€éœ€è¦è®­ç»ƒçš„ï¼ï¼‰
        vision_hidden_size = self.vision_tower.config.hidden_size  # 1024
        llm_hidden_size = self.llm.config.hidden_size  # 4096
        
        self.mm_projector = ProjectionLayer(
            vision_hidden_size, 
            llm_hidden_size
        )
    
    def encode_images(self, images):
        """
        ç¼–ç å›¾åƒ
        images: [batch, 3, 224, 224]
        è¿”å›: [batch, num_patches, llm_hidden]
        """
        # 1. CLIPç¼–ç ï¼ˆå†»ç»“ï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            vision_outputs = self.vision_tower(images)
            # è·å–æ‰€æœ‰patchçš„ç‰¹å¾
            image_features = vision_outputs.last_hidden_state
            # [batch, 197, 1024] (196ä¸ªpatch + 1ä¸ªCLS)
        
        # 2. æŠ•å½±åˆ°LLMç©ºé—´
        image_features = self.mm_projector(image_features)
        # [batch, 197, 4096]
        
        return image_features
    
    def forward(self, images, input_ids, attention_mask=None, labels=None):
        """
        å‰å‘ä¼ æ’­
        
        images: [batch, 3, 224, 224] - å›¾åƒ
        input_ids: [batch, seq_len] - æ–‡æœ¬tokens
        labels: [batch, seq_len] - ç”¨äºè®¡ç®—lossï¼ˆè®­ç»ƒæ—¶ï¼‰
        """
        batch_size = images.shape[0]
        
        # 1. ç¼–ç å›¾åƒ
        image_features = self.encode_images(images)
        # [batch, 197, 4096]
        
        # 2. è·å–æ–‡æœ¬embeddings
        text_embeds = self.llm.model.embed_tokens(input_ids)
        # [batch, seq_len, 4096]
        
        # 3. æ‹¼æ¥å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
        # ç®€åŒ–ç‰ˆï¼šå›¾åƒåœ¨å‰ï¼Œæ–‡æœ¬åœ¨å
        combined_embeds = torch.cat([image_features, text_embeds], dim=1)
        # [batch, 197+seq_len, 4096]
        
        # 4. è°ƒæ•´attention_mask
        image_attention_mask = torch.ones(
            batch_size, image_features.shape[1],
            dtype=torch.long, 
            device=images.device
        )
        if attention_mask is not None:
            combined_attention_mask = torch.cat(
                [image_attention_mask, attention_mask], 
                dim=1
            )
        else:
            combined_attention_mask = None
        
        # 5. è°ƒæ•´labelsï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        if labels is not None:
            # å›¾åƒéƒ¨åˆ†ä¸è®¡ç®—loss
            image_labels = torch.full(
                (batch_size, image_features.shape[1]),
                -100,  # -100ä¼šè¢«å¿½ç•¥
                dtype=torch.long,
                device=images.device
            )
            combined_labels = torch.cat([image_labels, labels], dim=1)
        else:
            combined_labels = None
        
        # 6. é€šè¿‡LLaMA
        outputs = self.llm(
            inputs_embeds=combined_embeds,  # æ³¨æ„ï¼šç”¨embedsè€Œä¸æ˜¯input_ids
            attention_mask=combined_attention_mask,
            labels=combined_labels,
            return_dict=True
        )
        
        return outputs
    
    def generate(self, images, input_ids, max_new_tokens=100, temperature=0.7):
        """
        ç”Ÿæˆå›ç­”
        
        images: [batch, 3, 224, 224]
        input_ids: [batch, seq_len] - é—®é¢˜
        """
        # ç¼–ç å›¾åƒ
        image_features = self.encode_images(images)
        
        # è·å–æ–‡æœ¬embeddings
        text_embeds = self.llm.model.embed_tokens(input_ids)
        
        # æ‹¼æ¥
        combined_embeds = torch.cat([image_features, text_embeds], dim=1)
        
        # ç”Ÿæˆï¼ˆæ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…å®ç°æ›´å¤æ‚ï¼‰
        # å› ä¸ºéœ€è¦ç‰¹æ®Šå¤„ç†inputs_embeds
        outputs = self.llm.generate(
            inputs_embeds=combined_embeds,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True
        )
        
        return outputs

# åˆ›å»ºæ¨¡å‹
model = LLaVA()

# æŸ¥çœ‹å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"æ€»å‚æ•°: {total_params / 1e9:.2f}B")
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e6:.2f}M")

# è¾“å‡ºç±»ä¼¼ï¼š
# æ€»å‚æ•°: 7.5B
# å¯è®­ç»ƒå‚æ•°: 8.4M  â† åªè®­ç»ƒæŠ•å½±å±‚ï¼
```

---

#### ğŸ¯ LLaVAçš„è®­ç»ƒè¿‡ç¨‹

**ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥**

```python
é˜¶æ®µ1: ç‰¹å¾å¯¹é½é¢„è®­ç»ƒ
  ç›®æ ‡: è®©æŠ•å½±å±‚å­¦ä¼š"ç¿»è¯‘"
  
  æ•°æ®: å¤§é‡å›¾æ–‡æè¿°å¯¹
    ä¾‹: [å›¾ç‰‡] â†’ "è¿™æ˜¯ä¸€åªçŒ«ååœ¨å«å­ä¸Š"
  
  å†»ç»“:
    âœ… CLIPè§†è§‰ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
    âœ… LLaMAè¯­è¨€æ¨¡å‹ï¼ˆå†»ç»“ï¼‰
  
  è®­ç»ƒ:
    âŒ åªè®­ç»ƒæŠ•å½±å±‚ï¼
  
  æ—¶é—´: 1-2å¤©ï¼ˆA100Ã—8ï¼‰
  æ•°æ®: 558Kå›¾æ–‡å¯¹
  æˆæœ¬: ç›¸å¯¹ä¾¿å®œ

é˜¶æ®µ2: è§†è§‰æŒ‡ä»¤å¾®è°ƒ
  ç›®æ ‡: è®©æ¨¡å‹å­¦ä¼šå¯¹è¯å’Œæ¨ç†
  
  æ•°æ®: æŒ‡ä»¤-å›ç­”å¯¹
    ä¾‹: 
      å›¾ç‰‡ + "è¿™å›¾é‡Œæœ‰å‡ åªçŒ«ï¼Ÿ" â†’ "æœ‰ä¸¤åªçŒ«"
      å›¾ç‰‡ + "å®ƒä»¬åœ¨åšä»€ä¹ˆï¼Ÿ" â†’ "å®ƒä»¬åœ¨ç©è€"
  
  å†»ç»“:
    âœ… CLIPè§†è§‰ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
    âŒ LLaMAè¯­è¨€æ¨¡å‹ï¼ˆéƒ¨åˆ†è§£å†»ï¼ï¼‰
    âŒ æŠ•å½±å±‚ï¼ˆç»§ç»­è®­ç»ƒï¼‰
  
  æ—¶é—´: 1-2å¤©
  æ•°æ®: 150KæŒ‡ä»¤å¯¹
  æˆæœ¬: é€‚ä¸­
```

**è®­ç»ƒä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰**

```python
def train_llava_stage1(model, dataloader, epochs=1):
    """
    é˜¶æ®µ1ï¼šç‰¹å¾å¯¹é½é¢„è®­ç»ƒ
    åªè®­ç»ƒæŠ•å½±å±‚
    """
    # å†»ç»“CLIPå’ŒLLaMA
    for param in model.vision_tower.parameters():
        param.requires_grad = False
    for param in model.llm.parameters():
        param.requires_grad = False
    
    # åªä¼˜åŒ–æŠ•å½±å±‚
    optimizer = torch.optim.AdamW(
        model.mm_projector.parameters(),
        lr=2e-3,  # ç›¸å¯¹è¾ƒå¤§çš„å­¦ä¹ ç‡
        weight_decay=0.0
    )
    
    model.train()
    for epoch in range(epochs):
        for batch in dataloader:
            images = batch['images']  # [B, 3, 224, 224]
            input_ids = batch['input_ids']  # [B, seq_len]
            labels = batch['labels']  # [B, seq_len]
            
            # å‰å‘ä¼ æ’­
            outputs = model(images, input_ids, labels=labels)
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if step % 100 == 0:
                print(f"Step {step}, Loss: {loss.item():.4f}")

def train_llava_stage2(model, dataloader, epochs=1):
    """
    é˜¶æ®µ2ï¼šè§†è§‰æŒ‡ä»¤å¾®è°ƒ
    è®­ç»ƒæŠ•å½±å±‚ + LLaMAï¼ˆéƒ¨åˆ†ï¼‰
    """
    # è§£å†»LLaMAï¼ˆå¯é€‰ï¼šåªè§£å†»æœ€åå‡ å±‚ï¼‰
    for param in model.llm.parameters():
        param.requires_grad = True
    
    # ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
    optimizer = torch.optim.AdamW([
        {'params': model.mm_projector.parameters(), 'lr': 2e-5},
        {'params': model.llm.parameters(), 'lr': 2e-6}  # æ›´å°ï¼
    ])
    
    model.train()
    for epoch in range(epochs):
        for batch in dataloader:
            images = batch['images']
            input_ids = batch['input_ids']
            labels = batch['labels']
            
            outputs = model(images, input_ids, labels=labels)
            loss = outputs.loss
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            if step % 100 == 0:
                print(f"Step {step}, Loss: {loss.item():.4f}")
```

---

#### ğŸ® ä½¿ç”¨LLaVA

**å®é™…ä½¿ç”¨ç¤ºä¾‹**

```python
from transformers import AutoTokenizer
from PIL import Image

# 1. åŠ è½½æ¨¡å‹
model = LLaVA.from_pretrained("llava-hf/llava-1.5-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
model.eval()

# 2. å‡†å¤‡å›¾åƒ
image = Image.open("cat.jpg")
image = transform(image)  # é¢„å¤„ç†ï¼šresizeåˆ°224Ã—224ï¼Œå½’ä¸€åŒ–ç­‰

# 3. å‡†å¤‡é—®é¢˜
prompt = "### Human: What is in this image?\n### Assistant:"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
# 4. ç”Ÿæˆå›ç­”
with torch.no_grad():
    output_ids = model.generate(
        images=image.unsqueeze(0),
        input_ids=input_ids,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9
    )

# 5. è§£ç è¾“å‡º
response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(response)

# è¾“å‡ºç¤ºä¾‹ï¼š
# "This image shows an orange cat sitting on a red mat. 
#  The cat appears relaxed, with its eyes half-closed. 
#  Sunlight is streaming in from the window..."
```

**å¤šè½®å¯¹è¯**

```python
def chat_with_image(model, tokenizer, image, conversation_history=[]):
    """
    ä¸å›¾åƒè¿›è¡Œå¤šè½®å¯¹è¯
    """
    while True:
        # ç”¨æˆ·è¾“å…¥
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        # æ„å»ºå¯¹è¯å†å²
        conversation_history.append(f"### Human: {user_input}")
        prompt = "\n".join(conversation_history) + "\n### Assistant:"
        
        # ç”Ÿæˆå›ç­”
        input_ids = tokenizer.encode(prompt, return_tensors='pt')
    output_ids = model.generate(
        images=image.unsqueeze(0),
        input_ids=input_ids,
        max_new_tokens=200
    )
    
    # è§£ç 
        response = tokenizer.decode(
            output_ids[0][input_ids.shape[1]:],  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            skip_special_tokens=True
        )
        
        print(f"Assistant: {response}")
        conversation_history.append(f"### Assistant: {response}")

# ä½¿ç”¨
image = load_image("cat.jpg")
chat_with_image(model, tokenizer, image)

# å¯¹è¯ç¤ºä¾‹ï¼š
# You: What's in this image?
# Assistant: I see an orange cat sitting on a red mat.
#
# You: What color are its eyes?
# Assistant: The cat's eyes appear to be green or amber colored.
#
# You: Does it look happy?
# Assistant: Yes, the cat looks relaxed and content...
```

---

#### âœ… LLaVAå°ç»“

ç°åœ¨ä½ åº”è¯¥ç†è§£äº†ï¼š

**æ ¸å¿ƒæ¦‚å¿µ**
- [ ] LLaVA = CLIPè§†è§‰ç¼–ç å™¨ + æŠ•å½±å±‚ + LLaMA
- [ ] åªéœ€è®­ç»ƒæŠ•å½±å±‚ï¼ˆå‚æ•°é‡å¾ˆå°ï¼ï¼‰
- [ ] å›¾åƒè¢«å½“ä½œ"ç‰¹æ®Šçš„token"è¾“å…¥LLM

**å…³é”®æŠ€æœ¯**
- [ ] ç‰¹å¾æŠ•å½±ï¼šæŠŠè§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç©ºé—´
- [ ] ä¸¤é˜¶æ®µè®­ç»ƒï¼šå…ˆå¯¹é½ï¼Œå†å¾®è°ƒ
- [ ] å†»ç»“é¢„è®­ç»ƒæ¨¡å‹ï¼šèŠ‚çœè®¡ç®—èµ„æº

**é‡è¦åº”ç”¨**
- [ ] å›¾åƒé—®ç­”ï¼ˆVQAï¼‰
- [ ] å›¾åƒæè¿°ï¼ˆè¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ï¼‰
- [ ] è§†è§‰å¯¹è¯ï¼ˆå¤šè½®äº¤äº’ï¼‰
- [ ] è§†è§‰æ¨ç†ï¼ˆå›ç­”éœ€è¦æ¨ç†çš„é—®é¢˜ï¼‰

**ä¼˜åŠ¿**
- [ ] è®­ç»ƒæˆæœ¬ä½ï¼ˆåªè®­ç»ƒæŠ•å½±å±‚ï¼‰
- [ ] æ•ˆæœå¥½ï¼ˆåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„èƒ½åŠ›ï¼‰
- [ ] æ˜“äºæ‰©å±•ï¼ˆå¯ä»¥æ¢ç”¨æ›´å¥½çš„è§†è§‰æˆ–è¯­è¨€æ¨¡å‹ï¼‰

---

### ğŸ“Š CLIP vs LLaVA å¯¹æ¯”

| ç‰¹æ€§ | CLIP | LLaVA |
|------|------|-------|
| **ä»»åŠ¡** | å›¾æ–‡åŒ¹é… | è§†è§‰å¯¹è¯ |
| **è¾“å‡º** | ç›¸ä¼¼åº¦åˆ†æ•° | è‡ªç„¶è¯­è¨€æ–‡æœ¬ |
| **èƒ½åŠ›** | åˆ†ç±»ã€æ£€ç´¢ | é—®ç­”ã€æè¿°ã€æ¨ç† |
| **è®­ç»ƒ** | å¯¹æ¯”å­¦ä¹  | æŒ‡ä»¤å¾®è°ƒ |
| **å‚æ•°** | 400M | 7B+ |
| **åº”ç”¨** | å›¾åƒåˆ†ç±»ã€æœç´¢ | è§†è§‰åŠ©æ‰‹ã€å¯¹è¯ |

```python
ä½•æ—¶ç”¨CLIPï¼Ÿ
  âœ… éœ€è¦å¿«é€Ÿåˆ¤æ–­å›¾æ–‡åŒ¹é…
  âœ… é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡
  âœ… å›¾åƒæ£€ç´¢
  
ä½•æ—¶ç”¨LLaVAï¼Ÿ
  âœ… éœ€è¦è¯¦ç»†æè¿°å›¾ç‰‡
  âœ… éœ€è¦å›ç­”å¤æ‚é—®é¢˜
  âœ… éœ€è¦å¤šè½®å¯¹è¯
  âœ… éœ€è¦è§†è§‰æ¨ç†
```

**ä¸‹ä¸€æ­¥ï¼š** æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘ï¼

---

## ğŸ¥ ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†é¢‘å’ŒéŸ³é¢‘ï¼ˆæ‰©å±•æ¨¡æ€ï¼‰

### ğŸŒ³ 3.1 è§†é¢‘ç†è§£ - æ—¶é—´ç»´åº¦çš„æŒ‘æˆ˜

#### ğŸ’¡ ç›´è§‚ç†è§£

**è§†é¢‘ = è¿ç»­çš„å›¾åƒ + æ—¶é—´ä¿¡æ¯**

**ç”Ÿæ´»æ¯”å–»ï¼šçœ‹ç”µå½± vs çœ‹ç…§ç‰‡**

```
çœ‹ç…§ç‰‡ï¼ˆå›¾åƒï¼‰:
  ä½ : "è¿™å¼ ç…§ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
  AI: "ä¸€åªçŒ«åç€"
  âœ… çŸ¥é“é™æ€å†…å®¹

çœ‹è§†é¢‘ï¼ˆè¿ç»­å›¾åƒï¼‰:
  ä½ : "è¿™ä¸ªè§†é¢‘é‡ŒçŒ«åœ¨åšä»€ä¹ˆï¼Ÿ"
  AI: "çŒ«å…ˆåç€ï¼Œç„¶åç«™èµ·æ¥ï¼Œè·³åˆ°çª—å°ä¸Š"
  âœ… ç†è§£åŠ¨ä½œå’Œå˜åŒ–

å…³é”®åŒºåˆ«ï¼š
  å›¾åƒ = ä¸€ä¸ªç¬é—´
  è§†é¢‘ = ä¸€æ®µæ•…äº‹
```

#### ğŸ¯ è§†é¢‘çš„æŒ‘æˆ˜

```python
é—®é¢˜1: æ•°æ®é‡çˆ†ç‚¸
  å›¾åƒ: 224Ã—224Ã—3 = 150Kä¸ªæ•°å­—
  è§†é¢‘: 30fps Ã— 10ç§’ Ã— 224Ã—224Ã—3 = 45Mä¸ªæ•°å­— ğŸ˜±
  
  å¦‚æœç›´æ¥å¤„ç†ï¼šæ˜¾å­˜çˆ†ç‚¸ï¼

é—®é¢˜2: æ—¶é—´ä¾èµ–
  ç¬¬1å¸§: çŒ«åç€
  ç¬¬50å¸§: çŒ«ç«™èµ·æ¥
  ç¬¬100å¸§: çŒ«è·³è·ƒ
  
  å¿…é¡»è®°ä½å‰é¢çš„å¸§æ‰èƒ½ç†è§£åé¢çš„ï¼

é—®é¢˜3: è®¡ç®—æ•ˆç‡
  å¤„ç†ä¸€å¼ å›¾ç‰‡: 0.1ç§’
  å¤„ç†300å¸§è§†é¢‘: 30ç§’ âŒ å¤ªæ…¢äº†ï¼
```

#### ğŸ“Š è§£å†³æ–¹æ¡ˆå¯¹æ¯”

```python
æ–¹æ¡ˆ1: ç¨€ç–é‡‡æ ·ï¼ˆæœ€ç®€å•ï¼‰
  æ€è·¯: ä¸æ˜¯æ¯å¸§éƒ½çœ‹ï¼Œåªçœ‹å…³é”®å¸§
  
  30fpsè§†é¢‘ â†’ æ¯ç§’é‡‡æ ·2å¸§ â†’ 2fps
  è®¡ç®—é‡å‡å°‘: 15å€ï¼
  
  ä¼˜åŠ¿: âœ… ç®€å•ã€å¿«é€Ÿ
  åŠ£åŠ¿: âŒ å¯èƒ½é”™è¿‡é‡è¦åŠ¨ä½œ

æ–¹æ¡ˆ2: 3Då·ç§¯ï¼ˆåŒæ—¶å¤„ç†æ—¶ç©ºï¼‰
  æ€è·¯: æŠŠæ—¶é—´ä¹Ÿå½“ä½œä¸€ä¸ªç»´åº¦
  
  2Då·ç§¯: åœ¨ç©ºé—´ä¸Šæ»‘åŠ¨ï¼ˆé«˜Ã—å®½ï¼‰
  3Då·ç§¯: åœ¨æ—¶ç©ºä¸Šæ»‘åŠ¨ï¼ˆæ—¶é—´Ã—é«˜Ã—å®½ï¼‰
  
  ä¼˜åŠ¿: âœ… èƒ½æ•è·è¿åŠ¨
  åŠ£åŠ¿: âŒ è®¡ç®—é‡å¤§

æ–¹æ¡ˆ3: åˆ†ç¦»å¤„ç†ï¼ˆTwo-Streamï¼‰
  æ€è·¯: ç©ºé—´å’Œæ—¶é—´åˆ†å¼€å¤„ç†
  
  ç©ºé—´æµ: ç†è§£"æ˜¯ä»€ä¹ˆ"ï¼ˆç‰©ä½“è¯†åˆ«ï¼‰
  æ—¶é—´æµ: ç†è§£"åœ¨åšä»€ä¹ˆ"ï¼ˆåŠ¨ä½œè¯†åˆ«ï¼‰
  
  æœ€åèåˆä¸¤ä¸ªæµçš„ç»“æœ
  
  ä¼˜åŠ¿: âœ… å¹³è¡¡æ•ˆæœå’Œæ•ˆç‡
  åŠ£åŠ¿: âš–ï¸ éœ€è¦ä¸¤ä¸ªç½‘ç»œ

æ–¹æ¡ˆ4: Transformerï¼ˆæœ€ç°ä»£ï¼‰
  æ€è·¯: æŠŠæ¯å¸§å½“ä½œä¸€ä¸ªtoken
  
  16å¸§è§†é¢‘ â†’ 16ä¸ª"å¸§token"
  ç”¨Transformerå¤„ç†æ—¶é—´ä¾èµ–
  
  ä¼˜åŠ¿: âœ… çµæ´»ã€æ•ˆæœå¥½
  åŠ£åŠ¿: âŒ æ˜¾å­˜éœ€æ±‚é«˜
```

#### ğŸ”§ è§†é¢‘Transformerå®ç°

```python
import torch
import torch.nn as nn

class VideoTransformer(nn.Module):
    """
    ç®€åŒ–çš„è§†é¢‘ç†è§£æ¨¡å‹
    æŠŠè§†é¢‘å½“ä½œåºåˆ—å¤„ç†
    """
    def __init__(
        self, 
        num_frames=16,      # é‡‡æ ·16å¸§
        image_size=224, 
        patch_size=16,
        embed_dim=768,
        num_classes=400     # æ¯”å¦‚Kinetics-400æ•°æ®é›†
    ):
        super().__init__()
        
        self.num_frames = num_frames
        self.num_patches_per_frame = (image_size // patch_size) ** 2  # 196
        
        # 1. Patch Embeddingï¼ˆå¤„ç†ç©ºé—´ç»´åº¦ï¼‰
        # å¯¹æ¯ä¸€å¸§éƒ½åˆ‡æˆpatch
        self.patch_embed = nn.Conv2d(
            3, embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
        
        # 2. Position Embeddingï¼ˆç©ºé—´ä½ç½®ï¼‰
        # å‘Šè¯‰æ¨¡å‹æ¯ä¸ªpatchåœ¨å¸§å†…çš„ä½ç½®
        self.pos_embed_spatial = nn.Parameter(
            torch.randn(1, self.num_patches_per_frame, embed_dim)
        )
        
        # 3. Temporal Embeddingï¼ˆæ—¶é—´ä½ç½®ï¼‰
        # å‘Šè¯‰æ¨¡å‹æ¯ä¸€å¸§çš„æ—¶é—´é¡ºåº
        self.pos_embed_temporal = nn.Parameter(
            torch.randn(1, num_frames, embed_dim)
        )
        
        # 4. CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        
        # 5. Transformerï¼ˆå¤„ç†æ—¶ç©ºå…³ç³»ï¼‰
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=12,
                dim_feedforward=3072
            ),
            num_layers=12
        )
        
        # 6. åˆ†ç±»å¤´
        self.head = nn.Linear(embed_dim, num_classes)
    
    def forward(self, x):
        """
        x: [batch, num_frames, channels, height, width]
           [B, T, C, H, W]
        """
        B, T, C, H, W = x.shape
        
        # æ­¥éª¤1: å¤„ç†æ¯ä¸€å¸§ï¼ˆPatch Embeddingï¼‰
        # æŠŠæ‰€æœ‰å¸§flattenæˆä¸€ä¸ªbatchå¤„ç†
        x = x.view(B * T, C, H, W)  # [B*T, C, H, W]
        x = self.patch_embed(x)      # [B*T, embed_dim, 14, 14]
        x = x.flatten(2).transpose(1, 2)  # [B*T, 196, embed_dim]
        
        # æ­¥éª¤2: æ·»åŠ ç©ºé—´ä½ç½®ç¼–ç 
        x = x + self.pos_embed_spatial  # æ¯ä¸ªpatchçŸ¥é“è‡ªå·±åœ¨å¸§å†…çš„ä½ç½®
        
        # æ­¥éª¤3: é‡ç»„å›æ—¶é—´ç»´åº¦
        x = x.view(B, T, self.num_patches_per_frame, -1)
        # [B, T, 196, embed_dim]
        
        # æ­¥éª¤4: æ·»åŠ æ—¶é—´ä½ç½®ç¼–ç 
        # å¹³å‡æ¯ä¸€å¸§çš„æ‰€æœ‰patch
        x_temporal = x.mean(dim=2)  # [B, T, embed_dim]
        x_temporal = x_temporal + self.pos_embed_temporal
        
        # æ­¥éª¤5: Flattenæ‰€æœ‰token
        x = x.view(B, T * self.num_patches_per_frame, -1)
        # [B, 16*196, embed_dim] = [B, 3136, embed_dim]
        
        # æ­¥éª¤6: æ·»åŠ CLS token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        # [B, 3137, embed_dim]
        
        # æ­¥éª¤7: Transformerå¤„ç†
        x = self.transformer(x)
        
        # æ­¥éª¤8: åˆ†ç±»
        cls_output = x[:, 0]  # å–CLS token
        logits = self.head(cls_output)
        
        return logits

# ä½¿ç”¨ç¤ºä¾‹
model = VideoTransformer(num_frames=16, num_classes=400)

# è¾“å…¥ï¼š16å¸§çš„è§†é¢‘
video = torch.randn(2, 16, 3, 224, 224)  # 2ä¸ªè§†é¢‘ï¼Œæ¯ä¸ª16å¸§

# å‰å‘ä¼ æ’­
logits = model(video)  # [2, 400]

print(f"è¾“å…¥shape: {video.shape}")
print(f"è¾“å‡ºshape: {logits.shape}")

# è¾“å‡ºï¼š
# è¾“å…¥shape: torch.Size([2, 16, 3, 224, 224])
# è¾“å‡ºshape: torch.Size([2, 400])
```

#### ğŸ¯ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

```python
é—®é¢˜ï¼šä¸ºä»€ä¹ˆåˆ†å¼€å¤„ç†ç©ºé—´å’Œæ—¶é—´ï¼Ÿ

ç­”æ¡ˆï¼šé™ä½å¤æ‚åº¦ï¼

ç›´æ¥å¤„ç†ï¼š
  è¾“å…¥ï¼š[B, 16, 3, 224, 224]
  å…¨éƒ¨flattenï¼š16Ã—196 = 3136ä¸ªtoken
  Transformerå¤æ‚åº¦ï¼šO(3136Â²) = 9,834,496 ğŸ˜±

åˆ†ç¦»å¤„ç†ï¼š
  å…ˆå¤„ç†ç©ºé—´ï¼š196ä¸ªpatch â†’ O(196Â²) = 38,416
  å†å¤„ç†æ—¶é—´ï¼š16å¸§ â†’ O(16Â²) = 256
  
  æ€»å¤æ‚åº¦æ›´ä½ï¼

ç±»æ¯”ï¼š
  ä¸åˆ†ç¦» = åŒæ—¶è®°ä½æ‰€æœ‰ç»†èŠ‚
  åˆ†ç¦» = å…ˆç†è§£æ¯å¸§å†…å®¹ï¼Œå†ç†è§£æ—¶é—´å…³ç³»
```

---

### ğŸŒ³ 3.2 éŸ³é¢‘å¤„ç† - å£°æ³¢çš„è¡¨ç¤º

#### ğŸ’¡ ç›´è§‚ç†è§£

**éŸ³é¢‘ = éšæ—¶é—´å˜åŒ–çš„å£°æ³¢**

**ç”Ÿæ´»æ¯”å–»ï¼šå¬éŸ³ä¹ vs çœ‹ä¹è°±**

```
åŸå§‹å£°æ³¢ï¼ˆWaveformï¼‰:
  [æŒ¯å¹…, æŒ¯å¹…, æŒ¯å¹…, ...]
  å°±åƒå¿ƒç”µå›¾çš„æ³¢æµªçº¿
  
  é—®é¢˜ï¼šå¤ªåŸå§‹äº†ï¼Œæ¨¡å¼ä¸æ˜æ˜¾

é¢‘è°±ï¼ˆSpectrogramï¼‰:
  æŠŠå£°æ³¢åˆ†è§£æˆä¸åŒé¢‘ç‡
  æ¨ªè½´=æ—¶é—´ï¼Œçºµè½´=é¢‘ç‡ï¼Œé¢œè‰²=å¼ºåº¦
  
  å°±åƒéŸ³ä¹çš„"å¯è§†åŒ–"
  âœ… æ›´å®¹æ˜“çœ‹å‡ºæ¨¡å¼

Melé¢‘è°±ï¼ˆMel-Spectrogramï¼‰:
  æ¨¡æ‹Ÿäººè€³çš„æ„ŸçŸ¥
  ä½é¢‘éƒ¨åˆ†æ›´ç»†è‡´ï¼Œé«˜é¢‘éƒ¨åˆ†æ›´ç²—ç³™
  
  âœ… æœ€é€‚åˆè¯­éŸ³è¯†åˆ«
```

#### ğŸ“Š éŸ³é¢‘å¤„ç†æµç¨‹

```python
æ­¥éª¤1: é‡‡æ ·éŸ³é¢‘
  åŸå§‹å£°éŸ³ â†’ æ•°å­—åŒ–
  é‡‡æ ·ç‡: 16000 Hzï¼ˆæ¯ç§’é‡‡æ ·16000æ¬¡ï¼‰
  
  1ç§’éŸ³é¢‘ = 16000ä¸ªæ•°å­—

æ­¥éª¤2: è½¬æ¢ä¸ºMelé¢‘è°±
  æ³¢å½¢ â†’ çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ â†’ é¢‘è°± â†’ Melé¢‘è°±
  
  è¾“å…¥: [16000] (1ç§’éŸ³é¢‘)
  è¾“å‡º: [80, 100] (80ä¸ªé¢‘ç‡å¸¦ Ã— 100ä¸ªæ—¶é—´æ­¥)
  
  ç°åœ¨éŸ³é¢‘å˜æˆäº†"å›¾åƒ"ï¼

æ­¥éª¤3: ç¼–ç 
  æŠŠMelé¢‘è°±å½“ä½œå›¾åƒå¤„ç†
  ä½¿ç”¨CNNæˆ–Transformer
  
  è¾“å‡º: éŸ³é¢‘embedding
```

#### ğŸ”§ éŸ³é¢‘ç¼–ç å™¨å®ç°

```python
import torch
import torch.nn as nn
import torchaudio

class AudioEncoder(nn.Module):
    """éŸ³é¢‘ç¼–ç å™¨ï¼ˆWav2Vecé£æ ¼ï¼‰"""
    def __init__(
        self,
        sample_rate=16000,  # é‡‡æ ·ç‡
        n_mels=80,          # Melé¢‘æ®µæ•°
        embed_dim=512
    ):
        super().__init__()
        
        # 1. Melé¢‘è°±è½¬æ¢å™¨
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=400,      # FFTçª—å£å¤§å°
            hop_length=160, # æ­¥é•¿
            n_mels=n_mels   # Melé¢‘æ®µ
        )
        
        # 2. CNNç¼–ç å™¨ï¼ˆå¤„ç†é¢‘è°±ï¼‰
        self.encoder = nn.Sequential(
            # ç¬¬1å±‚
            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            
            # ç¬¬2å±‚
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            
            # ç¬¬3å±‚
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            
            # å…¨å±€æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # 3. æŠ•å½±å±‚
        self.proj = nn.Linear(256, embed_dim)
    
    def forward(self, waveform):
        """
        waveform: [batch, time] - åŸå§‹éŸ³é¢‘æ³¢å½¢
        è¿”å›: [batch, embed_dim] - éŸ³é¢‘å‘é‡
        """
        # æ­¥éª¤1: è½¬æ¢ä¸ºMelé¢‘è°±
        mel = self.mel_transform(waveform)  
        # [batch, n_mels, time] = [batch, 80, T]
        
        # æ­¥éª¤2: æ·»åŠ é€šé“ç»´åº¦ï¼ˆå½“ä½œç°åº¦å›¾ï¼‰
        mel = mel.unsqueeze(1)  # [batch, 1, 80, T]
        
        # æ­¥éª¤3: CNNç¼–ç 
        features = self.encoder(mel)  # [batch, 256, 1, 1]
        features = features.squeeze(-1).squeeze(-1)  # [batch, 256]
        
        # æ­¥éª¤4: æŠ•å½±
        embedding = self.proj(features)  # [batch, embed_dim]
        
        # æ­¥éª¤5: å½’ä¸€åŒ–
        embedding = F.normalize(embedding, dim=-1)
        
        return embedding

# ä½¿ç”¨ç¤ºä¾‹
encoder = AudioEncoder()

# 1ç§’çš„éŸ³é¢‘ï¼ˆ16000ä¸ªé‡‡æ ·ç‚¹ï¼‰
audio = torch.randn(2, 16000)  # 2ä¸ªéŸ³é¢‘æ ·æœ¬

# ç¼–ç 
audio_embedding = encoder(audio)  # [2, 512]

print(f"è¾“å…¥shape: {audio.shape}")
print(f"è¾“å‡ºshape: {audio_embedding.shape}")

# è¾“å‡ºï¼š
# è¾“å…¥shape: torch.Size([2, 16000])
# è¾“å‡ºshape: torch.Size([2, 512])
```

#### ğŸ¯ éŸ³é¢‘-æ–‡æœ¬æ¨¡å‹ï¼ˆAudioCLIPï¼‰

ç±»ä¼¼CLIPï¼Œæˆ‘ä»¬å¯ä»¥åšéŸ³é¢‘-æ–‡æœ¬çš„å¯¹æ¯”å­¦ä¹ ï¼š

```python
class AudioCLIP(nn.Module):
    """éŸ³é¢‘-æ–‡æœ¬å¯¹é½æ¨¡å‹"""
    def __init__(self, embed_dim=512):
        super().__init__()
        
        # éŸ³é¢‘ç¼–ç å™¨
        self.audio_encoder = AudioEncoder(embed_dim=embed_dim)
        
        # æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¤ç”¨CLIPçš„ï¼‰
        self.text_encoder = TextEncoder(embed_dim=embed_dim)
        
        # æ¸©åº¦å‚æ•°
        self.logit_scale = nn.Parameter(
            torch.ones([]) * torch.log(torch.tensor(1.0 / 0.07))
        )
    
    def forward(self, audio, text):
        """
        audio: [batch, time] - éŸ³é¢‘æ³¢å½¢
        text: [batch, seq_len] - æ–‡æœ¬tokens
        """
        # ç¼–ç 
        audio_features = self.audio_encoder(audio)
        text_features = self.text_encoder(text)
        
        # å½’ä¸€åŒ–ï¼ˆå·²åœ¨ç¼–ç å™¨ä¸­å®Œæˆï¼‰
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = self.logit_scale.exp()
        logits = logit_scale * audio_features @ text_features.T
        
        return logits

# åº”ç”¨ï¼šéŸ³é¢‘åˆ†ç±»
def audio_zero_shot_classification(model, audio, candidate_texts):
    """
    é›¶æ ·æœ¬éŸ³é¢‘åˆ†ç±»
    
    ä¾‹å­ï¼š
      audio: [éŸ³é¢‘: çŒ«å«å£°]
      candidates: ["a cat meowing", "a dog barking", "a bird chirping"]
      â†’ é¢„æµ‹ï¼ša cat meowing
    """
    with torch.no_grad():
        # ç¼–ç éŸ³é¢‘
        audio_feat = model.audio_encoder(audio.unsqueeze(0))
        
        # ç¼–ç æ‰€æœ‰å€™é€‰æ–‡æœ¬
        text_feats = []
        for text in candidate_texts:
            text_ids = tokenize(text)
            text_feat = model.text_encoder(text_ids.unsqueeze(0))
            text_feats.append(text_feat)
        
        text_feats = torch.cat(text_feats, dim=0)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = model.logit_scale.exp()
        logits = logit_scale * audio_feat @ text_feats.T
        probs = F.softmax(logits, dim=-1)
        
        return probs[0].cpu().numpy()
```

---

### âœ… ç¬¬ä¸‰éƒ¨åˆ†å°ç»“

ç°åœ¨ä½ åº”è¯¥ç†è§£äº†ï¼š

**è§†é¢‘ç†è§£**
- [ ] è§†é¢‘ = å›¾åƒåºåˆ— + æ—¶é—´ä¾èµ–
- [ ] ä¸»è¦æŒ‘æˆ˜ï¼šè®¡ç®—é‡å¤§ã€æ˜¾å­˜éœ€æ±‚é«˜
- [ ] è§£å†³æ–¹æ¡ˆï¼šç¨€ç–é‡‡æ ·ã€3Då·ç§¯ã€Transformer
- [ ] å…³é”®ï¼šåˆ†ç¦»å¤„ç†ç©ºé—´å’Œæ—¶é—´ç»´åº¦

**éŸ³é¢‘å¤„ç†**
- [ ] éŸ³é¢‘è¡¨ç¤ºï¼šæ³¢å½¢ â†’ Melé¢‘è°± â†’ embedding
- [ ] Melé¢‘è°±å¯ä»¥å½“ä½œ"å›¾åƒ"å¤„ç†
- [ ] å¯ä»¥ç”¨CLIPçš„æ€è·¯åšéŸ³é¢‘-æ–‡æœ¬å¯¹é½
- [ ] åº”ç”¨ï¼šè¯­éŸ³è¯†åˆ«ã€éŸ³é¢‘åˆ†ç±»ã€éŸ³ä¹ç”Ÿæˆ

**å…±åŒç‚¹**
- [ ] éƒ½æ¶‰åŠæ—¶åºä¿¡æ¯
- [ ] éƒ½å¯ä»¥è½¬æ¢æˆ2Dè¡¨ç¤ºï¼ˆé¢‘è°±å›¾ã€å¸§åºåˆ—ï¼‰
- [ ] éƒ½å¯ä»¥ç”¨Transformerå¤„ç†
- [ ] éƒ½å¯ä»¥ä¸æ–‡æœ¬å¯¹é½

**ä¸‹ä¸€æ­¥ï¼š** æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•æ„å»ºç»Ÿä¸€çš„å¤šæ¨¡æ€GPTï¼

---

## ğŸ—ï¸ ç¬¬å››éƒ¨åˆ†ï¼šæ„å»ºå¤šæ¨¡æ€GPTï¼ˆç»Ÿä¸€æ¶æ„ï¼‰

### ğŸŒ³ 4.1 ç»Ÿä¸€å¤šæ¨¡æ€æ¶æ„è®¾è®¡

#### ğŸ’¡ ç›´è§‚ç†è§£ï¼šä¸‡ç‰©çš†Token

**æ ¸å¿ƒæ€æƒ³ï¼šæŠŠæ‰€æœ‰æ¨¡æ€éƒ½è½¬æ¢æˆToken**

**ç”Ÿæ´»æ¯”å–»ï¼šè”åˆå›½ç¿»è¯‘ç³»ç»Ÿ**

```
è”åˆå›½å¤§ä¼šï¼š
  ä¸­æ–‡ä»£è¡¨: "å¤§å®¶å¥½" (ä¸­æ–‡)
  è‹±æ–‡ä»£è¡¨: "Hello everyone" (è‹±æ–‡)
  æ³•æ–‡ä»£è¡¨: "Bonjour Ã  tous" (æ³•è¯­)
  
  âŒ é—®é¢˜ï¼šå¤§å®¶è¯´ä¸åŒè¯­è¨€ï¼Œæ— æ³•ç›´æ¥äº¤æµ
  
  âœ… è§£å†³æ–¹æ¡ˆï¼šç»Ÿä¸€ç¿»è¯‘æˆè‹±è¯­
    ä¸­æ–‡ â†’ è‹±è¯­ â†’ "Hello everyone"
    æ³•è¯­ â†’ è‹±è¯­ â†’ "Hello everyone"
    
  ç°åœ¨å¤§å®¶éƒ½èƒ½ç†è§£äº†ï¼

å¤šæ¨¡æ€GPTä¹Ÿæ˜¯ä¸€æ ·ï¼š
  å›¾åƒ â†’ Token â†’ [101, 234, 567, ...]
  æ–‡æœ¬ â†’ Token â†’ [45, 789, 123, ...]
  éŸ³é¢‘ â†’ Token â†’ [890, 456, 234, ...]
  
  ç»Ÿä¸€åˆ°Tokenç©ºé—´ â†’ Transformerå¤„ç†ï¼
```

---

#### ğŸ¯ æ¶æ„è®¾è®¡åŸåˆ™

```python
ç›®æ ‡: ä¸€ä¸ªæ¨¡å‹å¤„ç†æ‰€æœ‰æ¨¡æ€

è®¾è®¡æ€è·¯:
  1ï¸âƒ£ å°†æ‰€æœ‰æ¨¡æ€æ˜ å°„åˆ°ç»Ÿä¸€tokenç©ºé—´
  2ï¸âƒ£ ç”¨Transformerç»Ÿä¸€å¤„ç†
  3ï¸âƒ£ æ ¹æ®ä»»åŠ¡ç”Ÿæˆç›¸åº”æ¨¡æ€çš„è¾“å‡º

æ¶æ„æµç¨‹:
  [å›¾åƒ] â†’ å›¾åƒç¼–ç å™¨ â†’ [å›¾åƒtokens]
                              â†“
  [æ–‡æœ¬] â†’ æ–‡æœ¬ç¼–ç å™¨ â†’ [æ–‡æœ¬tokens] â†’ æ‹¼æ¥ â†’ Transformer â†’ è§£ç å™¨ â†’ [è¾“å‡º]
                              â†“
  [éŸ³é¢‘] â†’ éŸ³é¢‘ç¼–ç å™¨ â†’ [éŸ³é¢‘tokens]

å…³é”®ç‰¹ç‚¹:
  âœ… ç»Ÿä¸€çš„tokenè¡¨ç¤º
  âœ… æ¨¡æ€ä¹‹é—´å¯ä»¥äº’ç›¸äº¤äº’
  âœ… çµæ´»çš„è¾“å…¥è¾“å‡º
  âœ… ç«¯åˆ°ç«¯è®­ç»ƒ
```

---

#### ğŸ“Š æ¶æ„ç»„ä»¶è¯¦è§£

```python
ç»„ä»¶1: å¤šæ¨¡æ€ç¼–ç å™¨
  ä½œç”¨: æŠŠä¸åŒæ¨¡æ€è½¬æ¢æˆtoken

  å›¾åƒç¼–ç å™¨:
    è¾“å…¥: [B, 3, H, W] (RGBå›¾åƒ)
    è¾“å‡º: [B, N_patches, D] (å›¾åƒtokens)
    ä¾‹å­: 224Ã—224å›¾åƒ â†’ 196ä¸ªtokens
    
  æ–‡æœ¬ç¼–ç å™¨:
    è¾“å…¥: [B, seq_len] (æ–‡æœ¬IDs)
    è¾“å‡º: [B, seq_len, D] (æ–‡æœ¬tokens)
    ä¾‹å­: "hello world" â†’ 2ä¸ªtokens
    
  éŸ³é¢‘ç¼–ç å™¨:
    è¾“å…¥: [B, time] (éŸ³é¢‘æ³¢å½¢)
    è¾“å‡º: [B, N_frames, D] (éŸ³é¢‘tokens)
    ä¾‹å­: 1ç§’éŸ³é¢‘ â†’ 50ä¸ªtokens

ç»„ä»¶2: ç»Ÿä¸€Transformer
  ä½œç”¨: å¤„ç†æ··åˆçš„å¤šæ¨¡æ€tokens
  
  è¾“å…¥: [å›¾åƒtokens] + [æ–‡æœ¬tokens] + [éŸ³é¢‘tokens]
  å¤„ç†: Multi-head Self-Attention
        â†’ tokensä¹‹é—´äº’ç›¸å…³æ³¨
  è¾“å‡º: èåˆåçš„å¤šæ¨¡æ€ç‰¹å¾

ç»„ä»¶3: å¤šæ¨¡æ€è§£ç å™¨
  ä½œç”¨: æ ¹æ®ä»»åŠ¡ç”Ÿæˆè¾“å‡º
  
  æ–‡æœ¬ç”Ÿæˆ: è¾“å‡ºæ–‡æœ¬tokens â†’ è§£ç æˆæ–‡å­—
  å›¾åƒç”Ÿæˆ: è¾“å‡ºå›¾åƒtokens â†’ è§£ç æˆå›¾ç‰‡
  éŸ³é¢‘ç”Ÿæˆ: è¾“å‡ºéŸ³é¢‘tokens â†’ è§£ç æˆå£°éŸ³
```

---

### ğŸ”§ 4.2 å¤šæ¨¡æ€Tokenizerå®ç°

#### ğŸ“ æ¦‚å¿µï¼šå¦‚ä½•TokenåŒ–ä¸åŒæ¨¡æ€

**æ–‡æœ¬TokenåŒ–ï¼ˆå·²çŸ¥ï¼‰**
```python
æ–‡æœ¬: "Hello world"
  â†“ åˆ†è¯
Token IDs: [15496, 995]
```

**å›¾åƒTokenåŒ–ï¼ˆVQ-VAEï¼‰**
```python
å›¾åƒ: [3, 224, 224]
  â†“ VQ-VAEç¼–ç 
é‡åŒ–Codes: [512, 1024, 256, ...]  # ç¦»æ•£çš„æ•´æ•°
  â†“ æ¯ä¸ªcodeå¯¹åº”ä¸€ä¸ª"è§†è§‰è¯"
å›¾åƒTokens: [8192, 8193, 8194, ...]  # ä»ç‰¹æ®ŠIDå¼€å§‹
```

**éŸ³é¢‘TokenåŒ–ï¼ˆç±»ä¼¼ï¼‰**
```python
éŸ³é¢‘: [16000] (1ç§’æ³¢å½¢)
  â†“ éŸ³é¢‘VQ-VAE
é‡åŒ–Codes: [42, 89, 156, ...]
  â†“
éŸ³é¢‘Tokens: [10000, 10001, 10002, ...]  # ä»å¦ä¸€ä¸ªIDå¼€å§‹
```

---

#### ğŸ”§ å®ç°ï¼šå¤šæ¨¡æ€Tokenizer

```python
# multimodal_tokenizer.py
import torch
import torch.nn as nn
from transformers import AutoTokenizer
from PIL import Image

class MultiModalTokenizer:
    """
    å¤šæ¨¡æ€tokenizer
    ç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘
    """
    def __init__(
        self,
        text_tokenizer_name='gpt2',
        image_vocab_size=8192,
        audio_vocab_size=1024
    ):
        # 1. æ–‡æœ¬tokenizerï¼ˆå¤ç”¨GPT-2ï¼‰
        self.text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer_name)
        self.text_vocab_size = len(self.text_tokenizer)  # 50257
        
        # 2. ç‰¹æ®Štokenså®šä¹‰
        self.SPECIAL_TOKENS = {
            'IMG_START': self.text_vocab_size,      # 50257
            'IMG_END': self.text_vocab_size + 1,    # 50258
            'AUD_START': self.text_vocab_size + 2,  # 50259
            'AUD_END': self.text_vocab_size + 3,    # 50260
        }
        
        # 3. å„æ¨¡æ€çš„token IDèŒƒå›´
        self.image_token_start = self.text_vocab_size + 4      # 50261
        self.audio_token_start = self.image_token_start + image_vocab_size  # 58453
        
        # 4. æ€»è¯æ±‡è¡¨å¤§å°
        self.total_vocab_size = self.audio_token_start + audio_vocab_size
        
        print(f"è¯æ±‡è¡¨å¤§å°åˆ†å¸ƒ:")
        print(f"  æ–‡æœ¬: 0 - {self.text_vocab_size-1}")
        print(f"  å›¾åƒ: {self.image_token_start} - {self.image_token_start + image_vocab_size - 1}")
        print(f"  éŸ³é¢‘: {self.audio_token_start} - {self.audio_token_start + audio_vocab_size - 1}")
        print(f"  æ€»å¤§å°: {self.total_vocab_size}")
    
    def encode_text(self, text):
        """
        æ–‡æœ¬ç¼–ç 
        
        Args:
            text: å­—ç¬¦ä¸²
        Returns:
            token IDsåˆ—è¡¨
        """
        return self.text_tokenizer.encode(text)
    
    def decode_text(self, token_ids):
        """æ–‡æœ¬è§£ç """
        return self.text_tokenizer.decode(token_ids)
    
    def encode_image(self, image, vqvae_model):
        """
        å›¾åƒç¼–ç 
        
        Args:
            image: PIL Imageæˆ–torch.Tensor
            vqvae_model: é¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹
        Returns:
            [IMG_START] + å›¾åƒtokens + [IMG_END]
        """
        # 1. ç”¨VQ-VAEç¼–ç å›¾åƒ
        with torch.no_grad():
            if isinstance(image, Image.Image):
                # é¢„å¤„ç†
                image = transforms.ToTensor()(image)
                image = image.unsqueeze(0)  # [1, 3, H, W]
            
            # VQ-VAEç¼–ç  â†’ é‡åŒ–codes
            _, _, _, indices = vqvae_model.encode(image)
            # indices: [1, h, w] å…¶ä¸­h=H/8, w=W/8
            
            # Flatten
            image_codes = indices.flatten().tolist()  # [0, 3, 5, ...]
        
        # 2. è½¬æ¢ä¸ºå›¾åƒtoken IDs
        image_tokens = [
            self.SPECIAL_TOKENS['IMG_START']
        ] + [
            code + self.image_token_start  # æ˜ å°„åˆ°å›¾åƒtokenèŒƒå›´
            for code in image_codes
        ] + [
            self.SPECIAL_TOKENS['IMG_END']
        ]
        
        return image_tokens
    
    def decode_image(self, token_ids, vqvae_model):
        """
        å›¾åƒè§£ç 
        
        Args:
            token_ids: åŒ…å«å›¾åƒtokensçš„åˆ—è¡¨
            vqvae_model: VQ-VAEæ¨¡å‹
        Returns:
            PIL Image
        """
        # 1. æå–å›¾åƒtokens
        try:
            start_idx = token_ids.index(self.SPECIAL_TOKENS['IMG_START'])
            end_idx = token_ids.index(self.SPECIAL_TOKENS['IMG_END'])
            image_tokens = token_ids[start_idx + 1:end_idx]
        except ValueError:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å›¾åƒtokens")
        
        # 2. è½¬æ¢å›VQ-VAE codes
        image_codes = [
            token - self.image_token_start
            for token in image_tokens
        ]
        
        # 3. Reshapeå¹¶è§£ç 
        h = w = int(len(image_codes) ** 0.5)  # å‡è®¾æ˜¯æ­£æ–¹å½¢
        indices = torch.tensor(image_codes).reshape(1, h, w)
        
        with torch.no_grad():
            decoded_image = vqvae_model.decode_indices(indices)
        
        # è½¬æ¢ä¸ºPIL Image
        image = transforms.ToPILImage()(decoded_image[0])
        return image
    
    def encode_audio(self, audio, audio_vqvae_model):
        """
        éŸ³é¢‘ç¼–ç ï¼ˆç±»ä¼¼å›¾åƒï¼‰
        
        Args:
            audio: éŸ³é¢‘æ³¢å½¢ [time]
            audio_vqvae_model: éŸ³é¢‘VQ-VAE
        Returns:
            [AUD_START] + éŸ³é¢‘tokens + [AUD_END]
        """
        with torch.no_grad():
            _, _, _, indices = audio_vqvae_model.encode(audio)
            audio_codes = indices.flatten().tolist()
        
        audio_tokens = [
            self.SPECIAL_TOKENS['AUD_START']
        ] + [
            code + self.audio_token_start
            for code in audio_codes
        ] + [
            self.SPECIAL_TOKENS['AUD_END']
        ]
        
        return audio_tokens
    
    def encode_multimodal(self, inputs):
        """
        ç¼–ç å¤šæ¨¡æ€è¾“å…¥
        
        Args:
            inputs: å­—å…¸ï¼Œä¾‹å¦‚
                {
                    'image': PIL Image,
                    'text': "æè¿°è¿™å¼ å›¾",
                    'audio': torch.Tensor
                }
        Returns:
            æ··åˆçš„tokenåºåˆ—
        """
        tokens = []
        
        # æŒ‰é¡ºåºæ·»åŠ å„æ¨¡æ€
        if 'image' in inputs:
            image_tokens = self.encode_image(inputs['image'], inputs['vqvae'])
            tokens.extend(image_tokens)
        
        if 'text' in inputs:
            text_tokens = self.encode_text(inputs['text'])
            tokens.extend(text_tokens)
        
        if 'audio' in inputs:
            audio_tokens = self.encode_audio(inputs['audio'], inputs['audio_vqvae'])
            tokens.extend(audio_tokens)
        
        return tokens

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    tokenizer = MultiModalTokenizer()
    
    # ç¤ºä¾‹1ï¼šæ–‡æœ¬
    text = "Hello, multimodal world!"
    text_tokens = tokenizer.encode_text(text)
    print(f"æ–‡æœ¬tokens: {text_tokens[:10]}...")
    # è¾“å‡º: [15496, 11, 1963, 320, 38672, 995, 0]
    
    # ç¤ºä¾‹2ï¼šå›¾åƒï¼ˆéœ€è¦VQ-VAEï¼‰
    # image = Image.open("cat.jpg")
    # image_tokens = tokenizer.encode_image(image, vqvae_model)
    # print(f"å›¾åƒtokens: {image_tokens[:10]}...")
    # è¾“å‡º: [50257, 50261, 50342, 51023, ...]
    #       â†‘       â†‘ å›¾åƒå†…å®¹tokens
    #       IMG_START
    
    # ç¤ºä¾‹3ï¼šæ··åˆè¾“å…¥
    # tokens = tokenizer.encode_multimodal({
    #     'image': image,
    #     'text': "è¿™æ˜¯ä¸€åªçŒ«",
    #     'vqvae': vqvae_model
    # })
    # è¾“å‡º: [50257, ..., 50258, è¿™æ˜¯, ä¸€åª, çŒ«]
    #       â†‘ å›¾åƒéƒ¨åˆ†      â†‘ æ–‡æœ¬éƒ¨åˆ†
```

---

### ğŸ”§ 4.3 ç»Ÿä¸€å¤šæ¨¡æ€GPTå®ç°

```python
# multimodal_gpt.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiModalGPT(nn.Module):
    """
    ç»Ÿä¸€çš„å¤šæ¨¡æ€GPT
    èƒ½å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„æ··åˆè¾“å…¥
    """
    def __init__(
        self,
        vocab_size=60000,      # æ€»è¯æ±‡è¡¨å¤§å°
        embed_dim=768,
        num_layers=12,
        num_heads=12,
        max_seq_len=2048
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        
        # 1. Token Embeddingï¼ˆç»Ÿä¸€çš„embeddingå±‚ï¼‰
        # æ‰€æœ‰æ¨¡æ€å…±äº«åŒä¸€ä¸ªembeddingè¡¨
        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        
        # 2. Position Embedding
        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)
        
        # 3. Modality Embeddingï¼ˆå¯é€‰ï¼‰
        # è®©æ¨¡å‹çŸ¥é“æ¯ä¸ªtokenæ¥è‡ªå“ªä¸ªæ¨¡æ€
        self.modality_embed = nn.Embedding(4, embed_dim)  # 4ç§æ¨¡æ€
        # 0: æ–‡æœ¬, 1: å›¾åƒ, 2: éŸ³é¢‘, 3: ç‰¹æ®Štoken
        
        # 4. Transformer Layers
        self.transformer = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads)
            for _ in range(num_layers)
        ])
        
        # 5. Layer Norm
        self.ln_f = nn.LayerNorm(embed_dim)
        
        # 6. Output Headsï¼ˆä¸ºæ¯ä¸ªæ¨¡æ€å‡†å¤‡ä¸€ä¸ªè¾“å‡ºå¤´ï¼‰
        self.text_head = nn.Linear(embed_dim, 50257)      # GPT-2è¯æ±‡è¡¨
        self.image_head = nn.Linear(embed_dim, 8192)      # VQ-VAE codebook
        self.audio_head = nn.Linear(embed_dim, 1024)      # éŸ³é¢‘codebook
        
        # åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, input_ids, modality_ids=None, targets=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: [B, T] æ··åˆçš„å¤šæ¨¡æ€token IDs
            modality_ids: [B, T] æ¯ä¸ªtokençš„æ¨¡æ€ç±»å‹
                0: æ–‡æœ¬, 1: å›¾åƒ, 2: éŸ³é¢‘, 3: ç‰¹æ®Š
            targets: [B, T] è®­ç»ƒæ—¶çš„ç›®æ ‡token
        
        Returns:
            logits æˆ– loss
        """
        B, T = input_ids.shape
        device = input_ids.device
        
        # 1. Token Embedding
        token_emb = self.token_embed(input_ids)  # [B, T, D]
        
        # 2. Position Embedding
        positions = torch.arange(T, device=device)
        pos_emb = self.pos_embed(positions)  # [T, D]
        
        # 3. Modality Embeddingï¼ˆå¦‚æœæä¾›ï¼‰
        if modality_ids is not None:
            modality_emb = self.modality_embed(modality_ids)  # [B, T, D]
            x = token_emb + pos_emb + modality_emb
        else:
        x = token_emb + pos_emb
        
        # 4. Transformer
        for block in self.transformer:
            x = block(x)
        
        # 5. Final Layer Norm
        x = self.ln_f(x)  # [B, T, D]
        
        # 6. è®¡ç®—logitsæˆ–loss
        if targets is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—loss
            loss = self.compute_loss(x, input_ids, targets, modality_ids)
            return loss
        else:
            # æ¨ç†æ¨¡å¼ï¼šè¿”å›logits
            logits = self.route_to_heads(x, modality_ids)
        return logits
    
    def route_to_heads(self, x, modality_ids):
        """
        æ ¹æ®æ¨¡æ€ç±»å‹é€‰æ‹©è¾“å‡ºå¤´
        
        Args:
            x: [B, T, D] éšè—çŠ¶æ€
            modality_ids: [B, T] æ¨¡æ€ç±»å‹
        
        Returns:
            logits: [B, T, vocab_size]
        """
        if modality_ids is None:
            # é»˜è®¤ä½¿ç”¨æ–‡æœ¬å¤´
            return self.text_head(x)
        
        B, T, D = x.shape
        
        # ä¸ºæ¯ä¸ªä½ç½®é€‰æ‹©åˆé€‚çš„è¾“å‡ºå¤´
        # è¿™é‡Œç®€åŒ–ï¼šæ ¹æ®ä¸‹ä¸€ä¸ªtokençš„æ¨¡æ€é€‰æ‹©
        # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        
        # æ–¹æ³•1ï¼šç»Ÿä¸€è¾“å‡ºï¼ˆç®€å•ä½†ä¸ä¼˜ï¼‰
        # return self.text_head(x)  # è¾“å‡ºæ‰€æœ‰tokençš„logits
        
        # æ–¹æ³•2ï¼šåˆ†åˆ«å¤„ç†å„æ¨¡æ€ï¼ˆæ›´å‡†ç¡®ï¼‰
        text_mask = (modality_ids == 0)
        image_mask = (modality_ids == 1)
        audio_mask = (modality_ids == 2)
        
        # åˆå§‹åŒ–è¾“å‡º
        max_vocab = max(50257, 8192, 1024)
        logits = torch.zeros(B, T, max_vocab, device=x.device)
        
        # æ–‡æœ¬ä½ç½®
        if text_mask.any():
            text_logits = self.text_head(x)
            logits[:, :, :50257] = text_logits
        
        # å›¾åƒä½ç½®
        if image_mask.any():
            image_logits = self.image_head(x)
            logits[:, :, :8192] = image_logits
        
        # éŸ³é¢‘ä½ç½®
        if audio_mask.any():
            audio_logits = self.audio_head(x)
            logits[:, :, :1024] = audio_logits
        
        return logits
    
    def compute_loss(self, x, input_ids, targets, modality_ids):
        """
        è®¡ç®—å¤šæ¨¡æ€loss
        
        ç­–ç•¥ï¼šæ ¹æ®æ¨¡æ€ç±»å‹é€‰æ‹©å¯¹åº”çš„è¾“å‡ºå¤´è®¡ç®—loss
        """
        B, T, D = x.shape
        total_loss = 0
        count = 0
        
        # å¯¹æ¯ç§æ¨¡æ€åˆ†åˆ«è®¡ç®—loss
        for modality in [0, 1, 2]:  # æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘
            mask = (modality_ids == modality)
            if not mask.any():
                continue
            
            # é€‰æ‹©è¾“å‡ºå¤´
            if modality == 0:
                logits = self.text_head(x)
            elif modality == 1:
                logits = self.image_head(x)
            else:
                logits = self.audio_head(x)
            
            # è®¡ç®—è¯¥æ¨¡æ€çš„loss
            logits_masked = logits[mask]
            targets_masked = targets[mask]
            
            loss = F.cross_entropy(
                logits_masked,
                targets_masked,
                ignore_index=-100
            )
            
            total_loss += loss
            count += 1
        
        return total_loss / max(count, 1)
    
    @torch.no_grad()
    def generate(
        self,
        input_ids,
        modality_ids=None,
        max_new_tokens=100,
        temperature=1.0,
        top_k=None
    ):
        """
        è‡ªå›å½’ç”Ÿæˆ
        
        Args:
            input_ids: [B, T] è¾“å…¥åºåˆ—
            modality_ids: [B, T] æ¨¡æ€ç±»å‹
            max_new_tokens: ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
        
        Returns:
            generated_ids: [B, T+max_new_tokens]
        """
        for _ in range(max_new_tokens):
            # å‰å‘ä¼ æ’­
            logits = self.forward(input_ids, modality_ids)  # [B, T, vocab]
            
            # åªå–æœ€åä¸€ä¸ªtokençš„logits
            logits = logits[:, -1, :] / temperature  # [B, vocab]
            
            # Top-ké‡‡æ ·ï¼ˆå¯é€‰ï¼‰
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            
            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)  # [B, 1]
            
            # æ‹¼æ¥
            input_ids = torch.cat([input_ids, next_token], dim=1)
            
            # æ›´æ–°modality_idsï¼ˆç®€åŒ–ï¼šå‡è®¾ç»§ç»­å½“å‰æ¨¡æ€ï¼‰
            if modality_ids is not None:
                next_modality = modality_ids[:, -1:]
                modality_ids = torch.cat([modality_ids, next_modality], dim=1)
            
            # åœæ­¢æ¡ä»¶ï¼šé‡åˆ°ç»“æŸtoken
            # if next_token.item() in [IMG_END, AUD_END, EOS]:
            #     break
        
        return input_ids


class TransformerBlock(nn.Module):
    """æ ‡å‡†Transformer Block"""
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.ln1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.ln2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.GELU(),
            nn.Linear(4 * embed_dim, embed_dim)
        )
    
    def forward(self, x):
        # Self-attention
        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]
        # MLP
        x = x + self.mlp(self.ln2(x))
        return x


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹
    model = MultiModalGPT(
        vocab_size=60000,
        embed_dim=768,
        num_layers=12,
        num_heads=12
    )
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    
    # ç¤ºä¾‹è¾“å…¥
    batch_size = 2
    seq_len = 100
    
    # æ··åˆåºåˆ—ï¼šå›¾åƒtokens + æ–‡æœ¬tokens
    input_ids = torch.randint(0, 60000, (batch_size, seq_len))
    
    # æ¨¡æ€æ ‡è®°ï¼šå‰50ä¸ªæ˜¯å›¾åƒï¼Œå50ä¸ªæ˜¯æ–‡æœ¬
    modality_ids = torch.cat([
        torch.ones(batch_size, 50, dtype=torch.long),   # å›¾åƒ
        torch.zeros(batch_size, 50, dtype=torch.long)   # æ–‡æœ¬
    ], dim=1)
    
    # å‰å‘ä¼ æ’­
    logits = model(input_ids, modality_ids)
    print(f"è¾“å‡ºlogits shape: {logits.shape}")
    # è¾“å‡º: [2, 100, vocab_size]
    
    # ç”Ÿæˆ
    prompt = input_ids[:, :10]  # å‰10ä¸ªtokenä½œä¸ºprompt
    prompt_modality = modality_ids[:, :10]
    
    generated = model.generate(
        prompt,
        prompt_modality,
        max_new_tokens=20,
        temperature=0.8
    )
    print(f"ç”Ÿæˆåºåˆ—shape: {generated.shape}")
    # è¾“å‡º: [2, 30]
```

---

### ğŸ”§ 4.4 è®­ç»ƒç­–ç•¥

#### ğŸ“Š åˆ†é˜¶æ®µè®­ç»ƒ

```python
é˜¶æ®µ1: å•æ¨¡æ€é¢„è®­ç»ƒï¼ˆå„è‡ªç‹¬ç«‹ï¼‰
  ç›®æ ‡: è®©å„ç¼–ç å™¨å­¦ä¼šè‡ªå·±çš„æ¨¡æ€
  
  æ–‡æœ¬: GPTé¢„è®­ç»ƒï¼ˆè¯­è¨€å»ºæ¨¡ï¼‰
    æ•°æ®: å¤§é‡æ–‡æœ¬ï¼ˆBookCorpus, Wikipediaï¼‰
    ç›®æ ‡: é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
    æ—¶é—´: æ•°å‘¨
  
  å›¾åƒ: VQ-VAEè®­ç»ƒ
    æ•°æ®: ImageNet, LAION
    ç›®æ ‡: é‡å»ºå›¾åƒ
    æ—¶é—´: æ•°å¤©
  
  éŸ³é¢‘: Wav2Vec 2.0
    æ•°æ®: LibriSpeech
    ç›®æ ‡: å¯¹æ¯”å­¦ä¹ 
    æ—¶é—´: æ•°å¤©

é˜¶æ®µ2: æ¨¡æ€å¯¹é½ï¼ˆé…å¯¹æ•°æ®ï¼‰
  ç›®æ ‡: è®©ä¸åŒæ¨¡æ€å­¦ä¼šå¯¹åº”å…³ç³»
  
  å›¾æ–‡å¯¹é½:
    æ•°æ®: COCO, Conceptual Captions
    æ–¹æ³•: å¯¹æ¯”å­¦ä¹ ï¼ˆCLIPé£æ ¼ï¼‰
    ç›®æ ‡: max similarity(image, matching_text)
    æ—¶é—´: 1-2å‘¨
  
  éŸ³é¢‘-æ–‡æœ¬å¯¹é½:
    æ•°æ®: AudioCaps
    æ–¹æ³•: å¯¹æ¯”å­¦ä¹ 
    æ—¶é—´: æ•°å¤©

é˜¶æ®µ3: è”åˆå¾®è°ƒï¼ˆç«¯åˆ°ç«¯ï¼‰
  ç›®æ ‡: è®©æ¨¡å‹å­¦ä¼šå¤šæ¨¡æ€æ¨ç†
  
  æ•°æ®: å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®
    ä¾‹: {
      'image': cat.jpg,
      'question': "What's in this image?",
      'answer': "A cat sitting on a mat."
    }
  
  æ–¹æ³•: è¯­è¨€å»ºæ¨¡loss
  æ—¶é—´: æ•°å¤©
  
  æŠ€å·§:
    âœ… å†»ç»“éƒ¨åˆ†å±‚ï¼ˆåªè®­ç»ƒé¡¶å±‚ï¼‰
    âœ… ä½¿ç”¨LoRAå‡å°‘å‚æ•°
    âœ… æ¢¯åº¦ç´¯ç§¯ï¼ˆå°batchï¼‰
```

---

#### ğŸ”§ è®­ç»ƒä»£ç 

```python
# train_multimodal_gpt.py
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

def train_multimodal_gpt(
    model,
    train_loader,
    val_loader,
    epochs=3,
    lr=1e-4,
    device='cuda'
):
    """
    è®­ç»ƒå¤šæ¨¡æ€GPT
    
    Args:
        model: MultiModalGPTæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®
        val_loader: éªŒè¯æ•°æ®
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
    """
    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=len(train_loader) * epochs
    )
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch in pbar:
            # è·å–æ•°æ®
            input_ids = batch['input_ids'].to(device)      # [B, T]
            modality_ids = batch['modality_ids'].to(device)  # [B, T]
            targets = batch['targets'].to(device)          # [B, T]
            
            # å‰å‘ä¼ æ’­
            loss = model(input_ids, modality_ids, targets)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            scheduler.step()
            
            # è®°å½•
            train_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_train_loss = train_loss / len(train_loader)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                modality_ids = batch['modality_ids'].to(device)
                targets = batch['targets'].to(device)
                
                loss = model(input_ids, modality_ids, targets)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        print(f"Epoch {epoch+1}:")
        print(f"  Train Loss: {avg_train_loss:.4f}")
        print(f"  Val Loss: {avg_val_loss:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_multimodal_gpt.pt')
            print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆval_loss={avg_val_loss:.4f}ï¼‰")
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    return model


# æ•°æ®å‡†å¤‡ç¤ºä¾‹
class MultiModalDataset(torch.utils.data.Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†"""
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ç¼–ç å¤šæ¨¡æ€è¾“å…¥
        input_tokens = self.tokenizer.encode_multimodal(item['input'])
        
        # ç›®æ ‡ï¼ˆä¸‹ä¸€ä¸ªtokenï¼‰
        target_tokens = input_tokens[1:] + [-100]  # å‘å·¦ç§»åŠ¨ä¸€ä½
        
        # æ¨¡æ€æ ‡è®°
        modality_ids = self.create_modality_ids(input_tokens)
        
        return {
            'input_ids': torch.tensor(input_tokens),
            'targets': torch.tensor(target_tokens),
            'modality_ids': torch.tensor(modality_ids)
        }
    
    def create_modality_ids(self, tokens):
        """æ ¹æ®tokenèŒƒå›´åˆ¤æ–­æ¨¡æ€ç±»å‹"""
        modality_ids = []
        for token in tokens:
            if token < 50257:
                modality_ids.append(0)  # æ–‡æœ¬
            elif 50261 <= token < 58453:
                modality_ids.append(1)  # å›¾åƒ
            elif token >= 58453:
                modality_ids.append(2)  # éŸ³é¢‘
            else:
                modality_ids.append(3)  # ç‰¹æ®Štoken
        return modality_ids


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. å‡†å¤‡æ•°æ®
    train_data = [...]  # ä½ çš„è®­ç»ƒæ•°æ®
    val_data = [...]    # ä½ çš„éªŒè¯æ•°æ®
    
    tokenizer = MultiModalTokenizer()
    train_dataset = MultiModalDataset(train_data, tokenizer)
    val_dataset = MultiModalDataset(val_data, tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8)
    
    # 2. åˆ›å»ºæ¨¡å‹
    model = MultiModalGPT(
        vocab_size=tokenizer.total_vocab_size,
        embed_dim=768,
        num_layers=12
    )
    
    # 3. è®­ç»ƒ
    model = train_multimodal_gpt(
        model,
        train_loader,
        val_loader,
        epochs=3,
        lr=1e-4
    )
    
    # 4. æµ‹è¯•ç”Ÿæˆ
    test_input = torch.tensor([[...]])  # ä½ çš„æµ‹è¯•è¾“å…¥
    test_modality = torch.tensor([[...]])
    
    generated = model.generate(
        test_input,
        test_modality,
        max_new_tokens=50
    )
    
    print("ç”Ÿæˆç»“æœ:")
    print(tokenizer.decode_multimodal(generated[0]))
```

---

### âœ… ç¬¬å››éƒ¨åˆ†å°ç»“

ç°åœ¨ä½ åº”è¯¥æŒæ¡äº†ï¼š

**ç»Ÿä¸€æ¶æ„è®¾è®¡**
- [ ] ç†è§£"ä¸‡ç‰©çš†Token"çš„æ€æƒ³
- [ ] çŸ¥é“å¦‚ä½•è®¾è®¡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¶æ„
- [ ] äº†è§£æ¨¡æ€å¯¹é½çš„é‡è¦æ€§

**å¤šæ¨¡æ€Tokenizer**
- [ ] çŸ¥é“å¦‚ä½•TokenåŒ–ä¸åŒæ¨¡æ€
- [ ] ç†è§£ç‰¹æ®Štokençš„ä½œç”¨
- [ ] èƒ½å®ç°å¤šæ¨¡æ€ç¼–è§£ç 

**æ¨¡å‹å®ç°**
- [ ] èƒ½å®ç°ç»Ÿä¸€çš„Transformer backbone
- [ ] çŸ¥é“å¦‚ä½•é€‰æ‹©è¾“å‡ºå¤´
- [ ] ç†è§£æ¨¡æ€è·¯ç”±æœºåˆ¶

**è®­ç»ƒç­–ç•¥**
- [ ] æŒæ¡åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥
- [ ] äº†è§£å„é˜¶æ®µçš„æ•°æ®å’Œç›®æ ‡
- [ ] èƒ½ç¼–å†™è®­ç»ƒä»£ç 

**å…³é”®è¦ç‚¹**
- [ ] ç»Ÿä¸€è¡¨ç¤ºæ˜¯æ ¸å¿ƒï¼ˆTokenåŒ–ï¼‰
- [ ] åˆ†é˜¶æ®µè®­ç»ƒæ›´é«˜æ•ˆ
- [ ] æ¨¡æ€å¯¹é½æ˜¯å…³é”®
- [ ] ç«¯åˆ°ç«¯å¾®è°ƒæå‡æ€§èƒ½

---

## ğŸ”§ ç¬¬äº”éƒ¨åˆ†ï¼šå®æˆ˜ä¸è¯„ä¼°

### ğŸŒ³ 5.1 ä½¿ç”¨é¢„è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹

#### ğŸ’¡ ç›´è§‚ç†è§£ï¼šç«™åœ¨å·¨äººçš„è‚©è†€ä¸Š

**ä¸ºä»€ä¹ˆè¦ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿ**

```
è‡ªå·±ä»å¤´è®­ç»ƒï¼š
  å°±åƒè‡ªå·±å‘æ˜æ±½è½¦ ğŸš—
  âŒ éœ€è¦å¤§é‡æ•°æ®ï¼ˆæ•°äº¿æ ·æœ¬ï¼‰
  âŒ éœ€è¦å¼ºå¤§ç®—åŠ›ï¼ˆæ•°ç™¾GPUï¼‰
  âŒ éœ€è¦æ•°å‘¨ç”šè‡³æ•°æœˆ
  âŒ æˆæœ¬é«˜è¾¾æ•°ç™¾ä¸‡ç¾å…ƒ

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼š
  å°±åƒä¹°ä¸€è¾†ç°æˆçš„è½¦ ğŸï¸
  âœ… ç«‹å³å¯ç”¨
  âœ… æ•ˆæœå·²éªŒè¯
  âœ… å…è´¹æˆ–ä½æˆæœ¬
  âœ… å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šå®šåˆ¶

ç±»æ¯”ï¼š
  ä»å¤´è®­ç»ƒ = è‡ªå·±ç§å°éº¦ â†’ ç£¨é¢ç²‰ â†’ çƒ¤é¢åŒ…
  ä½¿ç”¨é¢„è®­ç»ƒ = å»é¢åŒ…åº—ä¹°ç°æˆçš„é¢åŒ…
  
  ï¼ˆå¦‚æœåªæ˜¯æƒ³åƒé¢åŒ…ï¼Œä½ ä¼šé€‰å“ªä¸ªï¼ŸğŸ˜Šï¼‰
```

---

#### ğŸ¯ ä¸»æµé¢„è®­ç»ƒæ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | ä»»åŠ¡ | å‚æ•°é‡ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ | ä¸Šæ‰‹éš¾åº¦ |
|------|------|--------|------|----------|---------|
| **CLIP** | å›¾æ–‡åŒ¹é… | 400M | é›¶æ ·æœ¬èƒ½åŠ›å¼ºã€é€Ÿåº¦å¿« | å›¾åƒåˆ†ç±»ã€æ£€ç´¢ | â­ ç®€å• |
| **BLIP-2** | å›¾æ–‡ç†è§£ | 3.9B | æ€§èƒ½å¼ºã€å¤šä»»åŠ¡ | å›¾åƒæè¿°ã€VQA | â­â­ ä¸­ç­‰ |
| **LLaVA** | è§†è§‰å¯¹è¯ | 7B-13B | å¯¹è¯èƒ½åŠ›å¼º | æ™ºèƒ½åŠ©æ‰‹ã€å¤æ‚æ¨ç† | â­â­â­ è¾ƒéš¾ |
| **Stable Diffusion** | æ–‡ç”Ÿå›¾ | 1B | å¼€æºã€å¯æ§æ€§å¼º | å›¾åƒç”Ÿæˆã€ç¼–è¾‘ | â­â­ ä¸­ç­‰ |
| **Whisper** | è¯­éŸ³è¯†åˆ« | 1.5B | å¤šè¯­è¨€ã€é²æ£’æ€§å¼º | ASRã€ç¿»è¯‘ | â­ ç®€å• |

---

#### ğŸ”§ å®æˆ˜1ï¼šä½¿ç”¨CLIPè¿›è¡Œå›¾åƒåˆ†ç±»

**åœºæ™¯ï¼š** ä½ æœ‰ä¸€å †å›¾ç‰‡ï¼Œæƒ³è‡ªåŠ¨åˆ†ç±»ï¼ˆçŒ«ã€ç‹—ã€é¸Ÿ...ï¼‰ï¼Œä½†æ²¡æœ‰è®­ç»ƒæ•°æ®ã€‚

```python
# clip_classification.py
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch

class CLIPClassifier:
    """
    ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»
    æ— éœ€è®­ç»ƒï¼åªéœ€è¦ç±»åˆ«åç§°
    """
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        print(f"åŠ è½½CLIPæ¨¡å‹: {model_name}")
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()  # æ¨ç†æ¨¡å¼
    
    def classify(self, image_path, candidate_labels, top_k=3):
        """
        å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            candidate_labels: å€™é€‰ç±»åˆ«åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªé¢„æµ‹
        
        Returns:
            é¢„æµ‹ç»“æœå’Œæ¦‚ç‡
        """
        # 1. åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert("RGB")
        
        # 2. æ„å»ºæ–‡æœ¬æè¿°
        # æŠ€å·§ï¼šæ·»åŠ "a photo of"èƒ½æå‡å‡†ç¡®ç‡ï¼
        texts = [f"a photo of a {label}" for label in candidate_labels]
        
        # 3. é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        # 4. æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image  # [1, num_labels]
            probs = logits_per_image.softmax(dim=1)[0]  # [num_labels]
        
        # 5. è·å–top-kç»“æœ
        top_probs, top_indices = torch.topk(probs, k=min(top_k, len(candidate_labels)))
        
        results = []
        for prob, idx in zip(top_probs, top_indices):
            results.append({
                'label': candidate_labels[idx],
                'probability': prob.item()
            })
        
        return results
    
    def batch_classify(self, image_paths, candidate_labels):
        """æ‰¹é‡åˆ†ç±»ï¼ˆæ›´é«˜æ•ˆï¼‰"""
        images = [Image.open(p).convert("RGB") for p in image_paths]
        texts = [f"a photo of a {label}" for label in candidate_labels]
        
        # æ‰¹é‡å¤„ç†
        inputs = self.processor(
            text=texts,
            images=images,
            return_tensors="pt",
            padding=True
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits_per_image  # [batch, num_labels]
            probs = logits.softmax(dim=1)  # [batch, num_labels]
        
        # æ¯å¼ å›¾åƒçš„é¢„æµ‹
        batch_results = []
        for i, image_probs in enumerate(probs):
            pred_idx = image_probs.argmax().item()
            batch_results.append({
                'image': image_paths[i],
                'label': candidate_labels[pred_idx],
                'probability': image_probs[pred_idx].item()
            })
        
        return batch_results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    classifier = CLIPClassifier()
    
    # åœºæ™¯1ï¼šåŠ¨ç‰©åˆ†ç±»
    labels = ["cat", "dog", "bird", "fish", "horse"]
    results = classifier.classify("pet.jpg", labels, top_k=3)
    
    print("é¢„æµ‹ç»“æœ:")
    for r in results:
        print(f"  {r['label']}: {r['probability']:.2%}")
    
    # è¾“å‡ºç¤ºä¾‹:
    # é¢„æµ‹ç»“æœ:
    #   cat: 92.5%
    #   dog: 5.2%
    #   bird: 1.8%
    
    # åœºæ™¯2ï¼šæƒ…æ„Ÿåˆ†ç±»
    emotion_labels = ["happy", "sad", "angry", "surprised", "neutral"]
    results = classifier.classify("face.jpg", emotion_labels, top_k=2)
    
    # åœºæ™¯3ï¼šåœºæ™¯åˆ†ç±»
    scene_labels = ["indoor", "outdoor", "beach", "mountain", "city", "forest"]
    results = classifier.classify("landscape.jpg", scene_labels)
    
    # åœºæ™¯4ï¼šæ‰¹é‡å¤„ç†
    image_paths = ["img1.jpg", "img2.jpg", "img3.jpg"]
    batch_results = classifier.batch_classify(image_paths, labels)
    
    for r in batch_results:
        print(f"{r['image']}: {r['label']} ({r['probability']:.2%})")
```

**ğŸ’¡ CLIPåˆ†ç±»æŠ€å·§**

```python
æŠ€å·§1: ä¼˜åŒ–æ–‡æœ¬æè¿°
  âŒ å·®: ["cat", "dog"]
  âœ… å¥½: ["a photo of a cat", "a photo of a dog"]
  
  ä¸ºä»€ä¹ˆï¼ŸCLIPåœ¨"è‡ªç„¶å¥å­"ä¸Šè®­ç»ƒçš„ï¼

æŠ€å·§2: ä½¿ç”¨å¤šä¸ªæè¿°
  # å¦‚æœä¸ç¡®å®šå›¾ç‰‡é£æ ¼
  texts = [
      "a photo of a cat",
      "a drawing of a cat", 
      "a painting of a cat"
  ]
  # å–å¹³å‡

æŠ€å·§3: æ·»åŠ é¢†åŸŸçŸ¥è¯†
  # åŒ»å­¦å›¾åƒ
  texts = [
      "a chest X-ray showing pneumonia",
      "a normal chest X-ray"
  ]
  # ä¸“ä¸šæœ¯è¯­ï¼

æŠ€å·§4: å¤„ç†ç»†ç²’åº¦åˆ†ç±»
  # ç‹—çš„å“ç§
  texts = [
      "a photo of a golden retriever",
      "a photo of a labrador",
      "a photo of a poodle"
  ]
  # å…·ä½“å“ç§å
```

**ğŸ“Š CLIPæ€§èƒ½åŸºå‡†**

```python
æ•°æ®é›†                  é›¶æ ·æœ¬å‡†ç¡®ç‡    ç›‘ç£å­¦ä¹ åŸºçº¿    å·®è·
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ImageNet               76.2%          85.0%         8.8%
CIFAR-10               94.9%          97.0%         2.1%
CIFAR-100              68.3%          80.0%        11.7%
Food-101               90.2%          92.5%         2.3%

ç»“è®ºï¼š
  âœ… CLIPé›¶æ ·æœ¬ â‰ˆ æœ‰ç›‘ç£è®­ç»ƒçš„85-95%
  âœ… å¯¹äºå¿«é€ŸåŸå‹ï¼ŒCLIPè¶³å¤Ÿå¥½ï¼
  âœ… å¦‚æœéœ€è¦æè‡´æ€§èƒ½ï¼Œå†å¾®è°ƒ
```

---

#### ğŸ”§ å®æˆ˜2ï¼šä½¿ç”¨LLaVAè¿›è¡Œè§†è§‰é—®ç­”

**åœºæ™¯ï¼š** æ„å»ºä¸€ä¸ªèƒ½"çœ‹å›¾è¯´è¯"çš„AIåŠ©æ‰‹ã€‚

```python
# llava_vqa.py
from transformers import LlavaForConditionalGeneration, AutoProcessor
from PIL import Image
import torch

class VisualAssistant:
    """
    åŸºäºLLaVAçš„è§†è§‰åŠ©æ‰‹
    å¯ä»¥å›ç­”å…³äºå›¾åƒçš„é—®é¢˜
    """
    def __init__(self, model_name="llava-hf/llava-1.5-7b-hf", device="cuda"):
        print(f"åŠ è½½LLaVAæ¨¡å‹: {model_name}")
        print(f"è­¦å‘Šï¼šè¿™æ˜¯ä¸€ä¸ª7Bæ¨¡å‹ï¼Œéœ€è¦çº¦14GBæ˜¾å­˜")
        
        self.device = device
        self.model = LlavaForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
            device_map="auto"  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
        )
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.model.eval()
    
    def ask(self, image_path, question, max_new_tokens=200):
        """
        å‘æ¨¡å‹æé—®
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        
        Returns:
            æ¨¡å‹çš„å›ç­”
        """
        # 1. åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert("RGB")
        
        # 2. æ„å»ºpromptï¼ˆLLaVAçš„ç‰¹å®šæ ¼å¼ï¼‰
        prompt = f"USER: <image>\n{question}\nASSISTANT:"
        
        # 3. é¢„å¤„ç†
        inputs = self.processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        ).to(self.device)
        
        # 4. ç”Ÿæˆå›ç­”
        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,  # æ§åˆ¶éšæœºæ€§
                top_p=0.9  # nucleus sampling
            )
        
        # 5. è§£ç 
        # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰promptï¼‰
        input_len = inputs['input_ids'].shape[1]
        generated_ids = output_ids[0][input_len:]
        answer = self.processor.decode(generated_ids, skip_special_tokens=True)
        
        return answer.strip()
    
    def chat(self, image_path):
        """
        å¤šè½®å¯¹è¯æ¨¡å¼
        """
        image = Image.open(image_path).convert("RGB")
        conversation_history = []
        
        print("è§†è§‰åŠ©æ‰‹å·²å¯åŠ¨ï¼è¾“å…¥'quit'é€€å‡º\n")
        
        while True:
            question = input("You: ")
            if question.lower() == 'quit':
                break
        
            # æ„å»ºåŒ…å«å†å²çš„prompt
            conversation_history.append(f"USER: {question}")
            full_prompt = "USER: <image>\n" + "\n".join(conversation_history) + "\nASSISTANT:"
            
            # ç”Ÿæˆå›ç­”
            inputs = self.processor(
                text=full_prompt,
                images=image,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                output_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.7
                )
            
            input_len = inputs['input_ids'].shape[1]
            generated_ids = output_ids[0][input_len:]
            answer = self.processor.decode(generated_ids, skip_special_tokens=True).strip()
            
            print(f"Assistant: {answer}\n")
            conversation_history.append(f"ASSISTANT: {answer}")
    
    def describe(self, image_path, detail_level="normal"):
        """
        æè¿°å›¾åƒå†…å®¹
        
        detail_level: 'brief', 'normal', 'detailed'
        """
        prompts = {
            'brief': "Describe this image in one sentence.",
            'normal': "Describe this image.",
            'detailed': "Describe this image in detail, including objects, colors, actions, and the overall scene."
        }
        
        question = prompts.get(detail_level, prompts['normal'])
        return self.ask(image_path, question)
    
    def count(self, image_path, object_name):
        """è®¡æ•°ï¼šå›¾åƒä¸­æœ‰å¤šå°‘ä¸ªæŸç‰©ä½“"""
        question = f"How many {object_name}s are in this image?"
        return self.ask(image_path, question)
    
    def compare(self, image_path, obj1, obj2):
        """æ¯”è¾ƒï¼šä¸¤ä¸ªç‰©ä½“çš„å·®å¼‚"""
        question = f"What are the differences between the {obj1} and the {obj2} in this image?"
        return self.ask(image_path, question)
    
    def reason(self, image_path, question):
        """æ¨ç†ï¼šéœ€è¦æ¨ç†çš„å¤æ‚é—®é¢˜"""
        question_with_reasoning = f"{question} Please explain your reasoning."
        return self.ask(image_path, question_with_reasoning)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    assistant = VisualAssistant()
    
    # 1. ç®€å•é—®ç­”
    print("=== åœºæ™¯1ï¼šæè¿°å›¾åƒ ===")
    answer = assistant.describe("room.jpg", detail_level="detailed")
    print(f"æè¿°: {answer}\n")
    
    # è¾“å‡ºç¤ºä¾‹:
    # "This image shows a cozy living room with a large brown sofa,
    #  a wooden coffee table, and a flat-screen TV mounted on the wall.
    #  The room is well-lit with natural light from a window on the right..."
    
    # 2. è®¡æ•°
    print("=== åœºæ™¯2ï¼šç‰©ä½“è®¡æ•° ===")
    answer = assistant.count("park.jpg", "person")
    print(f"äººæ•°: {answer}\n")
    
    # è¾“å‡º: "There are 5 people in this image."
    
    # 3. æ¨ç†
    print("=== åœºæ™¯3ï¼šæ¨ç†é—®é¢˜ ===")
    answer = assistant.reason(
        "street.jpg",
        "Is it safe to cross the street now?"
    )
    print(f"æ¨ç†: {answer}\n")
    
    # è¾“å‡º: "No, it's not safe to cross. The traffic light is red
    #        and there are several cars approaching..."
    
    # 4. å¤šè½®å¯¹è¯
    print("=== åœºæ™¯4ï¼šå¤šè½®å¯¹è¯ ===")
    assistant.chat("photo.jpg")
    
    # å¯¹è¯ç¤ºä¾‹:
    # You: What's in this image?
    # Assistant: I see a cat sitting on a window sill.
    #
    # You: What color is the cat?
    # Assistant: The cat is orange and white.
    #
    # You: What is it looking at?
    # Assistant: The cat appears to be looking outside the window, 
    #            possibly watching birds or other activity outside.
```

**ğŸ’¡ LLaVAä½¿ç”¨æŠ€å·§**

```python
æŠ€å·§1: æé—®æ–¹å¼å½±å“ç­”æ¡ˆè´¨é‡
  âŒ æ¨¡ç³Š: "Tell me about this."
  âœ… å…·ä½“: "Describe the colors, objects, and overall mood of this image."

æŠ€å·§2: ä½¿ç”¨æ€ç»´é“¾æç¤º
  âŒ ç›´æ¥: "Is this safe?"
  âœ… æ€ç»´é“¾: "Is this safe? Let's think step by step."
  
  æ•ˆæœï¼šå‡†ç¡®ç‡æå‡10-20%ï¼

æŠ€å·§3: æ§åˆ¶ç”Ÿæˆå‚æ•°
  temperature=0.1: ä¿å®ˆã€ä¸€è‡´ï¼ˆé€‚åˆäº‹å®æ€§é—®é¢˜ï¼‰
  temperature=0.7: å¹³è¡¡ï¼ˆæ¨èï¼‰
  temperature=1.0: åˆ›é€ æ€§ï¼ˆé€‚åˆå¼€æ”¾å¼æè¿°ï¼‰

æŠ€å·§4: å¤„ç†æ˜¾å­˜ä¸è¶³
  # æ–¹æ³•1: ä½¿ç”¨4-bité‡åŒ–
  from transformers import BitsAndBytesConfig
  quantization_config = BitsAndBytesConfig(load_in_4bit=True)
  model = LlavaForConditionalGeneration.from_pretrained(
      model_name,
      quantization_config=quantization_config
  )
  # æ˜¾å­˜éœ€æ±‚: 14GB â†’ 5GBï¼
  
  # æ–¹æ³•2: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
  model_name = "llava-hf/llava-1.5-7b-hf"  # 7B
  # æˆ–
  model_name = "llava-hf/bakLlava-v1-hf"  # æ›´å°ä½†å¿«
```

---

#### ğŸ”§ å®æˆ˜3ï¼šä½¿ç”¨Stable Diffusionç”Ÿæˆå›¾åƒ

**åœºæ™¯ï¼š** æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒã€‚

```python
# stable_diffusion_generator.py
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from PIL import Image
import torch

class ImageGenerator:
    """
    ä½¿ç”¨Stable Diffusionç”Ÿæˆå›¾åƒ
    """
    def __init__(
        self,
        model_name="stabilityai/stable-diffusion-2-1",
        device="cuda"
    ):
        print(f"åŠ è½½Stable Diffusion: {model_name}")
        
        # åŠ è½½ç®¡é“
        self.pipe = StableDiffusionPipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16  # åŠç²¾åº¦
        )
        
        # ä½¿ç”¨æ›´å¿«çš„è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            self.pipe.scheduler.config
        )
        
        self.pipe = self.pipe.to(device)
        
        # å¯ç”¨å†…å­˜ä¼˜åŒ–ï¼ˆå¦‚æœæ˜¾å­˜ä¸è¶³ï¼‰
        # self.pipe.enable_attention_slicing()
        # self.pipe.enable_xformers_memory_efficient_attention()
    
    def generate(
        self,
        prompt,
        negative_prompt="ugly, blurry, low quality, distorted",
        num_images=1,
        height=512,
        width=512,
        num_inference_steps=50,
        guidance_scale=7.5,
        seed=None
    ):
        """
        ç”Ÿæˆå›¾åƒ
        
        Args:
            prompt: æ­£å‘æç¤ºè¯ï¼ˆä½ æƒ³è¦ä»€ä¹ˆï¼‰
            negative_prompt: è´Ÿå‘æç¤ºè¯ï¼ˆä½ ä¸æƒ³è¦ä»€ä¹ˆï¼‰
            num_images: ç”Ÿæˆæ•°é‡
            height, width: å›¾åƒå°ºå¯¸ï¼ˆå»ºè®®512çš„å€æ•°ï¼‰
            num_inference_steps: å»å™ªæ­¥æ•°ï¼ˆ20-50ï¼Œè¶Šé«˜è´¨é‡è¶Šå¥½ä½†è¶Šæ…¢ï¼‰
            guidance_scale: æ–‡æœ¬å¼•å¯¼å¼ºåº¦ï¼ˆ7-15ï¼Œè¶Šé«˜è¶Šç¬¦åˆæç¤ºè¯ï¼‰
            seed: éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
        
        Returns:
            ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨
        """
        # è®¾ç½®éšæœºç§å­
        if seed is not None:
            generator = torch.Generator(device=self.pipe.device).manual_seed(seed)
    else:
            generator = None
        
        # ç”Ÿæˆ
        print(f"ç”Ÿæˆä¸­... (æ­¥æ•°: {num_inference_steps})")
        output = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_images_per_prompt=num_images,
            height=height,
            width=width,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            generator=generator
        )
        
        return output.images
    
    def generate_batch(self, prompts, **kwargs):
        """æ‰¹é‡ç”Ÿæˆï¼ˆæ¯ä¸ªpromptä¸€å¼ å›¾ï¼‰"""
        images = []
        for prompt in prompts:
            image = self.generate(prompt, num_images=1, **kwargs)[0]
            images.append(image)
        return images
    
    def optimize_prompt(self, base_prompt, style=None, quality_boosters=True):
        """
        ä¼˜åŒ–æç¤ºè¯
        
        æŠ€å·§ï¼šæ·»åŠ é£æ ¼å’Œè´¨é‡è¯èƒ½æ˜¾è‘—æå‡æ•ˆæœï¼
        """
        # é£æ ¼è¯å…¸
        styles = {
            'photorealistic': 'photorealistic, 8k uhd, high quality, detailed',
            'anime': 'anime style, studio ghibli, vibrant colors',
            'oil_painting': 'oil painting, impressionist, artistic',
            'digital_art': 'digital art, artstation, concept art',
            '3d': '3d render, octane render, highly detailed'
        }
        
        # åŸºç¡€prompt
        optimized = base_prompt
        
        # æ·»åŠ é£æ ¼
        if style and style in styles:
            optimized = f"{optimized}, {styles[style]}"
        
        # æ·»åŠ è´¨é‡å¢å¼ºè¯
        if quality_boosters:
            optimized = f"{optimized}, high quality, detailed, professional"
        
        return optimized

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    generator = ImageGenerator()
    
    # åœºæ™¯1ï¼šåŸºç¡€ç”Ÿæˆ
    print("=== åœºæ™¯1ï¼šåŸºç¡€ç”Ÿæˆ ===")
    prompt = "a cat astronaut in space"
    images = generator.generate(prompt, num_images=1)
    images[0].save("cat_astronaut.png")
    
    # åœºæ™¯2ï¼šä¼˜åŒ–åçš„prompt
    print("=== åœºæ™¯2ï¼šä¼˜åŒ–prompt ===")
    base_prompt = "a serene mountain landscape"
    optimized_prompt = generator.optimize_prompt(
        base_prompt,
        style='photorealistic'
    )
    print(f"ä¼˜åŒ–å: {optimized_prompt}")
    images = generator.generate(optimized_prompt)
    images[0].save("mountain.png")
    
    # åœºæ™¯3ï¼šå‚æ•°å¯¹æ¯”
    print("=== åœºæ™¯3ï¼šå‚æ•°å®éªŒ ===")
    prompt = "a futuristic city at sunset"
    
    # ä¸åŒguidance_scale
    for guidance in [5.0, 7.5, 10.0, 15.0]:
        images = generator.generate(
            prompt,
            guidance_scale=guidance,
            num_inference_steps=30,
            seed=42  # å›ºå®šç§å­ä»¥å¯¹æ¯”
        )
        images[0].save(f"city_guidance_{guidance}.png")
    
    # åœºæ™¯4ï¼šæ‰¹é‡ç”Ÿæˆ
    print("=== åœºæ™¯4ï¼šæ‰¹é‡ç”Ÿæˆ ===")
    prompts = [
        "a red apple on a table",
        "a blue butterfly on a flower",
        "a golden sunset over the ocean"
    ]
    images = generator.generate_batch(
        prompts,
        num_inference_steps=30
    )
    
    for i, img in enumerate(images):
        img.save(f"batch_{i}.png")
```

**ğŸ“Š Stable Diffusionå‚æ•°å½±å“**

| å‚æ•° | èŒƒå›´ | æ•ˆæœ | å»ºè®® |
|------|------|------|------|
| **num_inference_steps** | 20-50 | å»å™ªæ­¥æ•° | 30-50ï¼ˆè´¨é‡å¥½ï¼‰<br>20ï¼ˆå¿«é€ŸåŸå‹ï¼‰ |
| **guidance_scale** | 1-20 | æ–‡æœ¬å¼•å¯¼å¼ºåº¦ | 7-8ï¼ˆå¹³è¡¡ï¼‰<br>10-15ï¼ˆä¸¥æ ¼éµå¾ªpromptï¼‰<br>1-5ï¼ˆæ›´è‡ªç”±ï¼‰ |
| **heightÃ—width** | 512-1024 | å›¾åƒå°ºå¯¸ | 512Ã—512ï¼ˆå¿«é€Ÿï¼‰<br>768Ã—768ï¼ˆé«˜è´¨é‡ï¼‰<br>å¿…é¡»æ˜¯64çš„å€æ•°ï¼ |
| **negative_prompt** | - | é¿å…çš„å†…å®¹ | å§‹ç»ˆä½¿ç”¨ï¼åŠ å…¥"low quality, blurry"ç­‰ |

```python
# å‚æ•°ç»„åˆæ¨è

# å¿«é€ŸåŸå‹ï¼ˆ~10ç§’/å¼ ï¼‰
generate(
    prompt=prompt,
    num_inference_steps=20,
    guidance_scale=7.5,
    height=512,
    width=512
)

# é«˜è´¨é‡ï¼ˆ~30ç§’/å¼ ï¼‰
generate(
    prompt=prompt,
    num_inference_steps=50,
    guidance_scale=10.0,
    height=768,
    width=768
)

# è‰ºæœ¯åˆ›ä½œï¼ˆæ›´è‡ªç”±ï¼‰
generate(
    prompt=prompt,
    num_inference_steps=40,
    guidance_scale=5.0,  # æ›´ä½ï¼Œç»™æ¨¡å‹æ›´å¤šåˆ›é€ ç©ºé—´
    height=512,
    width=512
)
```

**ğŸ’¡ Promptå·¥ç¨‹æŠ€å·§**

```python
æŠ€å·§1: å…·ä½“>æ¨¡ç³Š
  âŒ "a cat"
  âœ… "an orange tabby cat with green eyes, sitting on a red cushion"

æŠ€å·§2: æ·»åŠ è´¨é‡è¯
  åŸºç¡€: "a landscape"
  å¢å¼º: "a landscape, 8k uhd, highly detailed, professional photograph"
  
  å¸¸ç”¨è´¨é‡è¯:
    - æ‘„å½±: "8k uhd, sharp focus, professional photograph"
    - è‰ºæœ¯: "trending on artstation, award winning, masterpiece"
    - æ¸²æŸ“: "octane render, unreal engine, highly detailed"

æŠ€å·§3: ä½¿ç”¨è‰ºæœ¯å®¶/é£æ ¼åç§°
  "a cat in the style of Van Gogh"
  "a landscape by Albert Bierstadt"
  "anime style by Makoto Shinkai"

æŠ€å·§4: åˆ†ç¦»ä¸»é¢˜å’Œé£æ ¼
  ä¸»é¢˜: "a magical forest"
  é£æ ¼: "watercolor painting, soft colors, dreamy"
  ç»„åˆ: "a magical forest, watercolor painting, soft colors, dreamy"

æŠ€å·§5: ä½¿ç”¨æƒé‡ï¼ˆé«˜çº§ï¼‰
  "(cat:1.5), (dog:0.5)"  # å¼ºè°ƒçŒ«ï¼Œå¼±åŒ–ç‹—
  "a cat, (blurry:-1.0)"  # é¿å…æ¨¡ç³Š

æŠ€å·§6: Negative Promptçš„é‡è¦æ€§
  å§‹ç»ˆæ·»åŠ :
    "ugly, blurry, low quality, distorted, deformed, bad anatomy"
  
  é’ˆå¯¹æ€§æ·»åŠ :
    äººç‰©: "extra limbs, bad hands, bad eyes"
    é£æ™¯: "oversaturated, unnatural colors"
```

---

### ğŸŒ³ 5.2 è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹

#### ğŸ’¡ ç›´è§‚ç†è§£ï¼šå¦‚ä½•åˆ¤æ–­æ¨¡å‹å¥½åï¼Ÿ

**ç”Ÿæ´»æ¯”å–»ï¼šè¯„ä¼°å­¦ç”Ÿçš„è€ƒè¯•æˆç»©**

```
å•æ¨¡æ€ï¼ˆè¯­æ–‡è€ƒè¯•ï¼‰:
  åªæœ‰ä¸€ä¸ªç»´åº¦ï¼šåˆ†æ•°
  ç®€å•æ˜äº†ï¼š90åˆ† > 80åˆ†

å¤šæ¨¡æ€ï¼ˆç»¼åˆè¯„ä¼°ï¼‰:
  å¤šä¸ªç»´åº¦ï¼š
    - ç†è§£èƒ½åŠ›ï¼ˆå›¾åƒè¯†åˆ«å‡†ç¡®å—ï¼Ÿï¼‰
    - ç”Ÿæˆè´¨é‡ï¼ˆç”Ÿæˆçš„å›¾åƒå¥½çœ‹å—ï¼Ÿï¼‰
    - è¯­ä¹‰å¯¹é½ï¼ˆå›¾æ–‡åŒ¹é…å—ï¼Ÿï¼‰
    - æ¨ç†èƒ½åŠ›ï¼ˆèƒ½å›ç­”å¤æ‚é—®é¢˜å—ï¼Ÿï¼‰
  
  éœ€è¦å¤šä¸ªæŒ‡æ ‡ç»¼åˆåˆ¤æ–­ï¼
```

---

#### ğŸ“Š å¸¸ç”¨è¯„ä¼°æŒ‡æ ‡è¯¦è§£

##### 1ï¸âƒ£ å›¾æ–‡æ£€ç´¢ä»»åŠ¡

**åœºæ™¯ï¼š** ç»™å®šä¸€å¼ å›¾ï¼Œä»1000æ®µæ–‡æœ¬ä¸­æ‰¾åˆ°åŒ¹é…çš„ï¼ˆæˆ–åè¿‡æ¥ï¼‰

```python
# æŒ‡æ ‡1: Recall@K
å®šä¹‰: å‰Kä¸ªç»“æœä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹

ä¾‹å­:
  ç»™å®šå›¾åƒ: [ä¸€åªçŒ«çš„ç…§ç‰‡]
  å€™é€‰æ–‡æœ¬: 1000æ®µæè¿°
  æ­£ç¡®ç­”æ¡ˆ: "a cat sitting on a mat"
  
  æ¨¡å‹è¿”å›å‰5ä¸ªæœ€åŒ¹é…çš„æ–‡æœ¬:
    1. "a cat sitting on a mat" âœ…
    2. "a dog playing"
    3. "a bird flying"
    4. "a fish swimming"
    5. "a cat sleeping"
  
  Recall@1 = 1.0  (ç¬¬1ä¸ªå°±æ˜¯æ­£ç¡®ç­”æ¡ˆ)
  Recall@5 = 1.0  (å‰5ä¸ªä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆ)
  
  å¦‚æœæ­£ç¡®ç­”æ¡ˆåœ¨ç¬¬6ä¸ª:
    Recall@1 = 0.0
    Recall@5 = 0.0
    Recall@10 = 1.0

# æŒ‡æ ‡2: Mean Rank
å®šä¹‰: æ­£ç¡®ç­”æ¡ˆçš„å¹³å‡æ’å

ä¾‹å­:
  æµ‹è¯•100å¼ å›¾åƒ:
    å›¾1: æ­£ç¡®ç­”æ¡ˆæ’ç¬¬1  â†’ Rank = 1
    å›¾2: æ­£ç¡®ç­”æ¡ˆæ’ç¬¬3  â†’ Rank = 3
    å›¾3: æ­£ç¡®ç­”æ¡ˆæ’ç¬¬2  â†’ Rank = 2
    ...
  
  Mean Rank = (1 + 3 + 2 + ...) / 100
  
  è¶Šå°è¶Šå¥½ï¼ç†æƒ³æƒ…å†µ = 1.0

# æŒ‡æ ‡3: Median Rank
å®šä¹‰: æ’åçš„ä¸­ä½æ•°
  æ¯”Mean Rankæ›´é²æ£’ï¼ˆä¸å—æç«¯å€¼å½±å“ï¼‰

# å®ç°ç¤ºä¾‹
def evaluate_retrieval(model, test_data):
    """
    è¯„ä¼°å›¾æ–‡æ£€ç´¢æ€§èƒ½
    """
    ranks = []
    recall_at_1 = 0
    recall_at_5 = 0
    recall_at_10 = 0
    
    for image, text, all_texts in test_data:
        # è®¡ç®—ç›¸ä¼¼åº¦
        image_feat = model.encode_image(image)
        text_feats = model.encode_texts(all_texts)
        
        similarities = image_feat @ text_feats.T  # [1, num_texts]
        
        # æ’åº
        sorted_indices = similarities.argsort(descending=True)[0]
        
        # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
        correct_idx = all_texts.index(text)
        rank = (sorted_indices == correct_idx).nonzero()[0].item() + 1
        ranks.append(rank)
        
        # è®¡ç®—Recall@K
        if rank <= 1:
            recall_at_1 += 1
        if rank <= 5:
            recall_at_5 += 1
        if rank <= 10:
            recall_at_10 += 1
    
    num_samples = len(test_data)
    
    return {
        'Recall@1': recall_at_1 / num_samples,
        'Recall@5': recall_at_5 / num_samples,
        'Recall@10': recall_at_10 / num_samples,
        'Mean_Rank': sum(ranks) / num_samples,
        'Median_Rank': sorted(ranks)[num_samples // 2]
    }

# å®é™…æ•°æ®ç¤ºä¾‹ï¼ˆCOCOæ•°æ®é›†ï¼‰
CLIP ViT-B/32:
  Recall@1: 58.4%
  Recall@5: 81.5%
  Recall@10: 88.1%
  Mean Rank: 2.3

è§£è¯»:
  âœ… 58.4%çš„æƒ…å†µä¸‹ï¼Œç¬¬1ä¸ªç»“æœå°±æ˜¯æ­£ç¡®çš„
  âœ… 88.1%çš„æƒ…å†µä¸‹ï¼Œå‰10ä¸ªç»“æœåŒ…å«æ­£ç¡®ç­”æ¡ˆ
  âœ… å¹³å‡æ’åç¬¬2.3ï¼ˆå¾ˆå¥½ï¼ï¼‰
```

---

##### 2ï¸âƒ£ å›¾åƒæè¿°ä»»åŠ¡

**åœºæ™¯ï¼š** ç»™å®šå›¾åƒï¼Œç”Ÿæˆä¸€æ®µæè¿°æ–‡å­—

```python
# æŒ‡æ ‡1: BLEU (Bilingual Evaluation Understudy)
åŸç†: è¡¡é‡ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬çš„n-gramé‡å 

ä¾‹å­:
  å‚è€ƒç­”æ¡ˆ: "a cat is sitting on a red mat"
  æ¨¡å‹ç”Ÿæˆ: "a cat sitting on a mat"
  
  1-gramåŒ¹é…: "a" "cat" "sitting" "on" "a" "mat" â†’ 6/6
  2-gramåŒ¹é…: "a cat" "cat sitting" "sitting on" ... â†’ 4/5
  3-gramåŒ¹é…: "a cat sitting" "cat sitting on" ... â†’ 3/4
  
  BLEU-4 = å‡ ä½•å¹³å‡(1-gram, 2-gram, 3-gram, 4-gram precision)
  
  èŒƒå›´: 0-100ï¼Œè¶Šé«˜è¶Šå¥½
  >40 ç®—å¥½ï¼Œ>60 éå¸¸å¥½

# æŒ‡æ ‡2: CIDEr (Consensus-based Image Description Evaluation)
åŸç†: è¡¡é‡ç”Ÿæˆæ–‡æœ¬ä¸å¤šä¸ªå‚è€ƒç­”æ¡ˆçš„å…±è¯†

ç‰¹ç‚¹:
  - ä¸“é—¨ä¸ºå›¾åƒæè¿°è®¾è®¡
  - è€ƒè™‘å¤šä¸ªå‚è€ƒç­”æ¡ˆ
  - å¯¹å›¾åƒç‰¹å®šè¯æ±‡ç»™äºˆæ›´é«˜æƒé‡

ä¾‹å­:
  å‚è€ƒ1: "a cat on a mat"
  å‚è€ƒ2: "an orange cat sitting"
  å‚è€ƒ3: "a feline resting on a rug"
  
  ç”Ÿæˆ: "an orange cat on a mat"
  
  CIDErä¼šè¡¡é‡ç”Ÿæˆæ–‡æœ¬ä¸æ‰€æœ‰å‚è€ƒçš„ç»¼åˆç›¸ä¼¼åº¦
  
  èŒƒå›´: 0-10+ï¼Œ>1.0 ç®—å¥½ï¼Œ>1.5 éå¸¸å¥½

# æŒ‡æ ‡3: SPICE (Semantic Propositional Image Caption Evaluation)
åŸç†: è¡¡é‡è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œè€Œéè¡¨é¢æ–‡å­—

ä¾‹å­:
  å‚è€ƒ: "a cat is sitting on a red mat"
  
  ç”Ÿæˆ1: "a cat sits on a mat"  
  â†’ è¯­ä¹‰å‡ ä¹ç›¸åŒï¼ŒSPICEé«˜ âœ…
  
  ç”Ÿæˆ2: "a feline is resting on a crimson rug"
  â†’ è¯ä¸åŒä½†è¯­ä¹‰ç›¸åŒï¼ŒSPICEä»é«˜ âœ…
  
  ç”Ÿæˆ3: "a cat and a mat"
  â†’ åŒ…å«å…³é”®è¯ä½†ç¼ºå¤±å…³ç³»ï¼ŒSPICEä½ âŒ

# å®ç°ç¤ºä¾‹
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.bleu.bleu import Bleu

def evaluate_captioning(predictions, references):
    """
    è¯„ä¼°å›¾åƒæè¿°è´¨é‡
    
    Args:
        predictions: {image_id: [caption]}
        references: {image_id: [ref1, ref2, ref3, ...]}
    """
    scorers = {
        'BLEU': Bleu(4),  # BLEU-1 åˆ° BLEU-4
        'CIDEr': Cider(),
    }
    
    scores = {}
    for name, scorer in scorers.items():
        score, _ = scorer.compute_score(references, predictions)
        scores[name] = score
    
    return scores

# å®é™…ä¾‹å­
predictions = {
    'img1': ['a cat sitting on a red mat']
}

references = {
    'img1': [
        'a cat is sitting on a mat',
        'an orange cat on a red mat',
        'a feline resting on a rug'
    ]
}

scores = evaluate_captioning(predictions, references)
# Output:
# {
#   'BLEU': [0.75, 0.62, 0.51, 0.42],  # BLEU-1åˆ°BLEU-4
#   'CIDEr': 1.23
# }
```

**ğŸ“Š ä¸»æµæ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ˆCOCOæ•°æ®é›†ï¼‰**

| æ¨¡å‹ | BLEU-4 | CIDEr | SPICE | é€Ÿåº¦ |
|------|--------|-------|-------|------|
| **CLIP+GPT-2** | 32.1 | 0.95 | 0.18 | å¿« âš¡ |
| **BLIP** | 38.6 | 1.30 | 0.23 | ä¸­ âš–ï¸ |
| **BLIP-2** | 42.5 | 1.44 | 0.25 | æ…¢ ğŸ¢ |
| **LLaVA** | 43.8 | 1.51 | 0.26 | æ…¢ ğŸ¢ |
| **GPT-4V** | 47.2 | 1.68 | 0.29 | å¾ˆæ…¢ ğŸŒ |

---

##### 3ï¸âƒ£ è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡

**åœºæ™¯ï¼š** ç»™å®šå›¾åƒå’Œé—®é¢˜ï¼Œå›ç­”é—®é¢˜

```python
# æŒ‡æ ‡1: Accuracy
å®šä¹‰: ç®€å•å‡†ç¡®ç‡

ä½†VQAæœ‰ç‰¹æ®Šè§„åˆ™ï¼šè€ƒè™‘å¤šä¸ªäººç±»æ ‡æ³¨è€…çš„ç­”æ¡ˆ

VQA Scoreå…¬å¼:
  score = min(matching_answers / 3, 1.0)

ä¾‹å­:
  é—®é¢˜: "What color is the cat?"
  å›¾åƒ: [ä¸€åªæ©™è‰²çš„çŒ«]
  
  10ä¸ªäººç±»æ ‡æ³¨ç­”æ¡ˆ:
    "orange" Ã— 7äºº
    "ginger" Ã— 2äºº  
    "brown" Ã— 1äºº
  
  æ¨¡å‹å›ç­”: "orange"
  â†’ æœ‰7äººå›ç­”"orange"
  â†’ score = min(7/3, 1.0) = 1.0 âœ…
  
  æ¨¡å‹å›ç­”: "ginger"
  â†’ æœ‰2äººå›ç­”"ginger"
  â†’ score = min(2/3, 1.0) = 0.67 âš–ï¸
  
  æ¨¡å‹å›ç­”: "blue"
  â†’ æ²¡äººå›ç­”"blue"
  â†’ score = 0.0 âŒ

# å®ç°
def vqa_score(predicted_answer, ground_truth_answers):
    """
    è®¡ç®—VQAåˆ†æ•°
    
    Args:
        predicted_answer: æ¨¡å‹é¢„æµ‹
        ground_truth_answers: äººç±»æ ‡æ³¨ç­”æ¡ˆåˆ—è¡¨ï¼ˆå¯èƒ½æœ‰é‡å¤ï¼‰
    """
    # æ ‡å‡†åŒ–ç­”æ¡ˆï¼ˆå°å†™ã€å»æ ‡ç‚¹ç­‰ï¼‰
    predicted = normalize_answer(predicted_answer)
    gt_answers = [normalize_answer(ans) for ans in ground_truth_answers]
    
    # è®¡ç®—åŒ¹é…æ•°
    matching_count = gt_answers.count(predicted)
    
    # VQAè§„åˆ™
    score = min(matching_count / 3.0, 1.0)
    
    return score

# è¯„ä¼°æ•´ä¸ªæ•°æ®é›†
def evaluate_vqa(model, test_data):
    total_score = 0
    
    for image, question, answers in test_data:
        # æ¨¡å‹é¢„æµ‹
        prediction = model.answer(image, question)
        
        # è®¡ç®—åˆ†æ•°
        score = vqa_score(prediction, answers)
        total_score += score
    
    accuracy = total_score / len(test_data)
    return accuracy

# å®é™…æ€§èƒ½ï¼ˆVQAv2æ•°æ®é›†ï¼‰
CLIP baseline: 45.2%
BLIP: 65.3%
LLaVA-1.5-7B: 78.5%
LLaVA-1.5-13B: 80.0%
GPT-4V: 87.2%
```

---

##### 4ï¸âƒ£ æ–‡ç”Ÿå›¾ä»»åŠ¡

**åœºæ™¯ï¼š** è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡

```python
# æŒ‡æ ‡1: FID (FrÃ©chet Inception Distance)
åŸç†: è¡¡é‡ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒçš„åˆ†å¸ƒå·®å¼‚

è®¡ç®—æ­¥éª¤:
  1. ç”¨Inceptionç½‘ç»œæå–ç‰¹å¾
  2. è®¡ç®—çœŸå®å›¾åƒç‰¹å¾çš„åˆ†å¸ƒï¼ˆå‡å€¼Î¼1, åæ–¹å·®Î£1ï¼‰
  3. è®¡ç®—ç”Ÿæˆå›¾åƒç‰¹å¾çš„åˆ†å¸ƒï¼ˆå‡å€¼Î¼2, åæ–¹å·®Î£2ï¼‰
  4. FID = ||Î¼1 - Î¼2||Â² + Tr(Î£1 + Î£2 - 2âˆš(Î£1Î£2))

ç›´è§‚ç†è§£:
  FID = çœŸå®å’Œç”Ÿæˆå›¾åƒåœ¨ç‰¹å¾ç©ºé—´çš„è·ç¦»
  è¶Šå°è¶Šå¥½ï¼
  
  FID < 10: éå¸¸å¥½ âœ…
  FID < 50: å¯ä»¥æ¥å— âš–ï¸
  FID > 100: è´¨é‡å·® âŒ

# ä½¿ç”¨pytorch-fidè®¡ç®—
from pytorch_fid import fid_score

# è®¡ç®—FID
fid_value = fid_score.calculate_fid_given_paths(
    ['/path/to/real/images', '/path/to/generated/images'],
    batch_size=50,
    device='cuda',
    dims=2048  # Inceptionç‰¹å¾ç»´åº¦
)

print(f"FID: {fid_value:.2f}")

# æŒ‡æ ‡2: CLIP Score
åŸç†: è¡¡é‡ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„åŒ¹é…åº¦

è®¡ç®—:
  CLIP_Score = CLIP(image, text) / 100
  
  å³ç”¨CLIPè®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦

ä¾‹å­:
  prompt: "a cat sitting on a mat"
  generated_image: [ç”Ÿæˆçš„å›¾åƒ]
  
  CLIP_Score = CLIPç›¸ä¼¼åº¦
  
  >0.3: éå¸¸åŒ¹é… âœ…
  0.2-0.3: åŒ¹é… âš–ï¸
  <0.2: ä¸åŒ¹é… âŒ

# å®ç°
from transformers import CLIPProcessor, CLIPModel

def calculate_clip_score(images, prompts):
    """
    è®¡ç®—CLIP Score
    
    Args:
        images: ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨
        prompts: å¯¹åº”çš„æ–‡æœ¬æç¤º
    """
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    clip_scores = []
    
    for image, prompt in zip(images, prompts):
        inputs = processor(
            text=[prompt],
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        outputs = model(**inputs)
        logits = outputs.logits_per_image[0, 0]
        score = logits / 100.0
        
        clip_scores.append(score.item())
    
    return sum(clip_scores) / len(clip_scores)

# æŒ‡æ ‡3: Inception Score (IS)
åŸç†: è¡¡é‡ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§

è®¡ç®—:
  1. ç”¨Inceptionç½‘ç»œé¢„æµ‹æ¯å¼ å›¾çš„ç±»åˆ«åˆ†å¸ƒ p(y|x)
  2. è®¡ç®—è¾¹ç¼˜åˆ†å¸ƒ p(y)
  3. IS = exp(E[KL(p(y|x) || p(y))])

ç›´è§‚ç†è§£:
  å¥½çš„ç”Ÿæˆå™¨åº”è¯¥:
    - æ¯å¼ å›¾æ¸…æ™°æ˜ç¡®ï¼ˆp(y|x)ç†µä½ï¼‰
    - ç”Ÿæˆå¤šæ ·ï¼ˆp(y)ç†µé«˜ï¼‰
  
  IS > 10: å¾ˆå¥½ âœ…
  IS > 5: å¯ä»¥ âš–ï¸
  IS < 3: å·® âŒ

# å®é™…æ€§èƒ½å¯¹æ¯”
æ¨¡å‹                 FIDâ†“    CLIP Scoreâ†‘   ISâ†‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
SD 1.5              12.6    0.31          10.2
SD 2.1              9.8     0.32          11.5
SDXL                7.2     0.34          13.8
DALL-E 3            5.4     0.36          15.2

```

---

#### ğŸ¯ è¯„ä¼°å®æˆ˜ï¼šå®Œæ•´è¯„ä¼°æµç¨‹

```python
# complete_evaluation.py
"""
å¤šæ¨¡æ€æ¨¡å‹çš„å®Œæ•´è¯„ä¼°æµç¨‹
"""

import torch
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
from datasets import load_dataset
from tqdm import tqdm

class MultimodalEvaluator:
    """å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()
    
    def evaluate_zero_shot_classification(self, dataset, labels):
        """
        è¯„ä¼°é›¶æ ·æœ¬åˆ†ç±»
        
        Returns:
            accuracy, top5_accuracy, per_class_accuracy
        """
        correct = 0
        top5_correct = 0
        total = 0
        
        class_correct = {label: 0 for label in labels}
        class_total = {label: 0 for label in labels}
        
        for item in tqdm(dataset, desc="Evaluating"):
            image = item['image']
            true_label = item['label']
            
            # é¢„æµ‹
            texts = [f"a photo of a {label}" for label in labels]
            inputs = self.processor(
                text=texts,
                images=image,
                return_tensors="pt",
                padding=True
            )
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits_per_image[0]
                probs = logits.softmax(dim=0)
            
            # Top-1
            pred_idx = probs.argmax().item()
            if labels[pred_idx] == true_label:
                correct += 1
                class_correct[true_label] += 1
            
            # Top-5
            top5_indices = probs.topk(5).indices
            if true_label in [labels[i] for i in top5_indices]:
                top5_correct += 1
            
            class_total[true_label] += 1
            total += 1
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = correct / total
        top5_accuracy = top5_correct / total
        
        per_class_acc = {
            label: class_correct[label] / class_total[label]
            for label in labels
            if class_total[label] > 0
        }
        
        return {
            'accuracy': accuracy,
            'top5_accuracy': top5_accuracy,
            'per_class_accuracy': per_class_acc
        }
    
    def evaluate_retrieval(self, dataset, k_values=[1, 5, 10]):
        """
        è¯„ä¼°å›¾æ–‡æ£€ç´¢
        
        Returns:
            recall@k for each k
        """
        recalls = {f'recall@{k}': 0 for k in k_values}
        ranks = []
        
        for item in tqdm(dataset, desc="Evaluating Retrieval"):
            image = item['image']
            correct_text = item['text']
            all_texts = item['all_texts']  # åŒ…æ‹¬æ­£ç¡®ç­”æ¡ˆçš„å€™é€‰é›†
            
            # ç¼–ç 
            image_inputs = self.processor(
                images=image,
                return_tensors="pt"
            )
            text_inputs = self.processor(
                text=all_texts,
                return_tensors="pt",
                padding=True
            )
            
            with torch.no_grad():
                image_features = self.model.get_image_features(**image_inputs)
                text_features = self.model.get_text_features(**text_inputs)
                
                # ç›¸ä¼¼åº¦
                similarities = (image_features @ text_features.T)[0]
                sorted_indices = similarities.argsort(descending=True)
            
            # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
            correct_idx = all_texts.index(correct_text)
            rank = (sorted_indices == correct_idx).nonzero()[0].item() + 1
            ranks.append(rank)
            
            # è®¡ç®—Recall@K
            for k in k_values:
                if rank <= k:
                    recalls[f'recall@{k}'] += 1
        
        # å½’ä¸€åŒ–
        num_samples = len(dataset)
        for k in k_values:
            recalls[f'recall@{k}'] /= num_samples
        
        recalls['mean_rank'] = sum(ranks) / num_samples
        recalls['median_rank'] = sorted(ranks)[num_samples // 2]
        
        return recalls
    
    def generate_report(self, results, output_file='evaluation_report.txt'):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("=" * 50)
        report.append("å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 50)
        report.append("")
        
        for task, metrics in results.items():
            report.append(f"ã€{task}ã€‘")
            for metric_name, value in metrics.items():
                if isinstance(value, float):
                    report.append(f"  {metric_name}: {value:.4f}")
                elif isinstance(value, dict):
                    report.append(f"  {metric_name}:")
                    for k, v in value.items():
                        report.append(f"    {k}: {v:.4f}")
            report.append("")
        
        report_text = "\n".join(report)
        
        # æ‰“å°
        print(report_text)
        
        # ä¿å­˜
        with open(output_file, 'w') as f:
            f.write(report_text)
        
        return report_text

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    evaluator = MultimodalEvaluator()
    
    # 1. è¯„ä¼°é›¶æ ·æœ¬åˆ†ç±»
    # ä½¿ç”¨CIFAR-10æ•°æ®é›†
    cifar10 = load_dataset("cifar10", split="test")
    labels = ["airplane", "automobile", "bird", "cat", "deer",
              "dog", "frog", "horse", "ship", "truck"]
    
    print("è¯„ä¼°é›¶æ ·æœ¬åˆ†ç±»...")
    classification_results = evaluator.evaluate_zero_shot_classification(
        cifar10,
        labels
    )
    
    # 2. è¯„ä¼°å›¾æ–‡æ£€ç´¢
    # ä½¿ç”¨COCOæ•°æ®é›†ï¼ˆå‡è®¾å·²å‡†å¤‡å¥½ï¼‰
    # coco_test = load_coco_retrieval_test()
    # print("è¯„ä¼°å›¾æ–‡æ£€ç´¢...")
    # retrieval_results = evaluator.evaluate_retrieval(coco_test)
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    all_results = {
        "é›¶æ ·æœ¬åˆ†ç±» (CIFAR-10)": classification_results,
        # "å›¾æ–‡æ£€ç´¢ (COCO)": retrieval_results,
    }
    
    evaluator.generate_report(all_results)
    
    # è¾“å‡ºç¤ºä¾‹:
    # ==================================================
    # å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
    # ==================================================
    #
    # ã€é›¶æ ·æœ¬åˆ†ç±» (CIFAR-10)ã€‘
    #   accuracy: 0.8542
    #   top5_accuracy: 0.9823
    #   per_class_accuracy:
    #     airplane: 0.8900
    #     automobile: 0.9100
    #     ...
    #
    # ã€å›¾æ–‡æ£€ç´¢ (COCO)ã€‘
    #   recall@1: 0.5840
    #   recall@5: 0.8150
    #   recall@10: 0.8810
    #   mean_rank: 2.34
    #   median_rank: 1.00
```

---

#### ğŸ“Š è¯„ä¼°æŒ‡æ ‡é€ŸæŸ¥è¡¨

| ä»»åŠ¡ | ä¸»è¦æŒ‡æ ‡ | ä¼˜ç§€æ ‡å‡† | è®¡ç®—å·¥å…· |
|------|---------|---------|---------|
| **å›¾æ–‡æ£€ç´¢** | Recall@1<br>Recall@5<br>Mean Rank | >50%<br>>80%<br><3.0 | è‡ªå®ç° |
| **å›¾åƒæè¿°** | BLEU-4<br>CIDEr<br>SPICE | >40<br>>1.3<br>>0.22 | `pycocoevalcap` |
| **VQA** | VQA Score | >75% | è‡ªå®ç° |
| **æ–‡ç”Ÿå›¾** | FID<br>CLIP Score<br>IS | <10<br>>0.30<br>>10 | `pytorch-fid`<br>`clip-score` |
| **è¯­éŸ³è¯†åˆ«** | WER<br>CER | <5%<br><3% | `jiwer` |

---

### âœ… ç¬¬äº”éƒ¨åˆ†å°ç»“

ç°åœ¨ä½ åº”è¯¥æŒæ¡äº†ï¼š

**ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹**
- [ ] çŸ¥é“ä¸»æµæ¨¡å‹åŠå…¶é€‚ç”¨åœºæ™¯
- [ ] èƒ½ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»
- [ ] èƒ½ä½¿ç”¨LLaVAè¿›è¡Œè§†è§‰é—®ç­”
- [ ] èƒ½ä½¿ç”¨Stable Diffusionç”Ÿæˆå›¾åƒ
- [ ] æŒæ¡promptå·¥ç¨‹æŠ€å·§

**è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹**
- [ ] ç†è§£ä¸åŒä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡
- [ ] çŸ¥é“å¦‚ä½•è®¡ç®—Recall@Kã€BLEUã€FIDç­‰
- [ ] èƒ½ç¼–å†™å®Œæ•´çš„è¯„ä¼°è„šæœ¬
- [ ] èƒ½ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

**å®æˆ˜ç»éªŒ**
- [ ] ä¼˜å…ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç«™åœ¨å·¨äººè‚©è†€ä¸Šï¼‰
- [ ] è¯„ä¼°æ—¶ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ï¼ˆç»¼åˆåˆ¤æ–­ï¼‰
- [ ] æ³¨æ„promptå·¥ç¨‹çš„é‡è¦æ€§
- [ ] ç†è§£ä¸åŒæŒ‡æ ‡çš„å«ä¹‰å’Œé€‚ç”¨åœºæ™¯

---

#### ğŸ”¬ å®æˆ˜æŠ€å·§æ€»ç»“

**1. é€‰æ‹©æ¨¡å‹çš„ä¸‰ä¸ªå…³é”®é—®é¢˜**

```python
é—®é¢˜1: æˆ‘çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ
  å›¾æ–‡æ£€ç´¢ â†’ CLIP
  è§†è§‰é—®ç­” â†’ LLaVA
  æ–‡ç”Ÿå›¾ â†’ Stable Diffusion
  
é—®é¢˜2: æˆ‘æœ‰å¤šå°‘èµ„æºï¼Ÿ
  æ˜¾å­˜ < 8GB â†’ ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬æˆ–æ›´å°çš„æ¨¡å‹
  æ˜¾å­˜ 8-16GB â†’ å¯ä»¥ä½¿ç”¨7Bæ¨¡å‹
  æ˜¾å­˜ > 24GB â†’ å¯ä»¥ä½¿ç”¨13B+æ¨¡å‹
  
é—®é¢˜3: æˆ‘éœ€è¦å¤šå¿«çš„æ¨ç†é€Ÿåº¦ï¼Ÿ
  å®æ—¶åº”ç”¨ â†’ CLIP (å¿«)
  ç¦»çº¿å¤„ç† â†’ LLaVA (æ…¢ä½†æ•ˆæœå¥½)
  æ‰¹é‡ç”Ÿæˆ â†’ Stable Diffusion (å¯å¹¶è¡Œ)
```

**2. æ˜¾å­˜ä¼˜åŒ–æŠ€å·§**

```python
# æŠ€å·§1: ä½¿ç”¨åŠç²¾åº¦
model = model.half()  # FP16
# æ˜¾å­˜å‡åŠï¼

# æŠ€å·§2: é‡åŒ–
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
# æ˜¾å­˜å‡å°‘75%ï¼

# æŠ€å·§3: æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆè®­ç»ƒæ—¶ï¼‰
model.gradient_checkpointing_enable()
# æ˜¾å­˜å‡å°‘ï¼Œä½†è®­ç»ƒå˜æ…¢

# æŠ€å·§4: æ‰¹é‡å¤§å°è°ƒæ•´
# æ¨ç†æ—¶ä½¿ç”¨batch_size=1
# è®­ç»ƒæ—¶é€æ­¥å¢åŠ batch_sizeç›´åˆ°OOMï¼Œç„¶åå‡å°

# æŠ€å·§5: æ³¨æ„åŠ›åˆ‡ç‰‡ï¼ˆStable Diffusionï¼‰
pipe.enable_attention_slicing()
# æ˜¾å­˜éœ€æ±‚é™ä½
```

**3. æå‡æ€§èƒ½çš„æŠ€å·§**

```python
# CLIPæå‡æŠ€å·§
âœ… ä½¿ç”¨å®Œæ•´å¥å­è€Œéå•è¯
âœ… æ·»åŠ é¢†åŸŸç›¸å…³çš„ä¸Šä¸‹æ–‡
âœ… å°è¯•å¤šä¸ªæ¨¡æ¿å¹¶å¹³å‡
âœ… ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆViT-L/14ï¼‰

# LLaVAæå‡æŠ€å·§
âœ… ä½¿ç”¨æ€ç»´é“¾æç¤º
âœ… æä¾›æ¸…æ™°å…·ä½“çš„é—®é¢˜
âœ… å¤šè½®å¯¹è¯ä¿æŒä¸Šä¸‹æ–‡
âœ… è°ƒæ•´temperatureæ§åˆ¶åˆ›é€ æ€§

# Stable Diffusionæå‡æŠ€å·§
âœ… è¯¦ç»†çš„promptæè¿°
âœ… ä½¿ç”¨negative prompt
âœ… è°ƒæ•´guidance_scaleï¼ˆ7-15ï¼‰
âœ… å¢åŠ æ¨ç†æ­¥æ•°ï¼ˆ30-50ï¼‰
âœ… å›ºå®šseedä»¥å¤ç°ç»“æœ
```

**4. è¯„ä¼°æœ€ä½³å®è·µ**

```python
# å®æˆ˜è¯„ä¼°ä»£ç 
from torchmetrics import Accuracy
from torchmetrics.image.fid import FrechetInceptionDistance

# VQAè¯„ä¼°
accuracy = Accuracy()
for batch in test_loader:
    preds = model(batch['images'], batch['questions'])
    acc = accuracy(preds, batch['answers'])
print(f"VQA Accuracy: {acc:.2%}")

# æ–‡ç”Ÿå›¾è¯„ä¼°
fid = FrechetInceptionDistance(feature=2048)
fid.update(real_images, real=True)
fid.update(generated_images, real=False)
fid_score = fid.compute()
print(f"FID: {fid_score:.2f}")  # è¶Šä½è¶Šå¥½ï¼Œ<50ç®—å¥½

# æ£€ç´¢è¯„ä¼°
recall_at_5 = 0
for image, text, candidates in test_data:
    similarities = compute_similarities(image, candidates)
    top5 = similarities.topk(5).indices
    if text in [candidates[i] for i in top5]:
        recall_at_5 += 1
recall_at_5 /= len(test_data)
print(f"Recall@5: {recall_at_5:.2%}")
```

**5. å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ**

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| **CLIPåˆ†ç±»æ•ˆæœå·®** | Promptä¸å¤Ÿå¥½ | ä½¿ç”¨"a photo of a {class}"æ ¼å¼<br>æ·»åŠ é¢†åŸŸçŸ¥è¯† |
| **LLaVAå›ç­”ä¸å‡†** | é—®é¢˜å¤ªæ¨¡ç³Š | æä¾›æ¸…æ™°å…·ä½“çš„é—®é¢˜<br>ä½¿ç”¨æ€ç»´é“¾æç¤º |
| **SDç”Ÿæˆå›¾åƒä¸ç¬¦** | Guidance scaleå¤ªä½ | å¢åŠ åˆ°10-15<br>ä¼˜åŒ–prompt |
| **æ˜¾å­˜ä¸è¶³OOM** | æ¨¡å‹å¤ªå¤§/batchå¤ªå¤§ | ä½¿ç”¨é‡åŒ–<br>å‡å°batch_size<br>ä½¿ç”¨æ›´å°çš„æ¨¡å‹ |
| **æ¨ç†é€Ÿåº¦æ…¢** | æ¨¡å‹å¤ªå¤§/å‚æ•°ä¸ä¼˜ | ä½¿ç”¨æ›´å°çš„æ¨¡å‹<br>å‡å°‘æ¨ç†æ­¥æ•°<br>ä½¿ç”¨æ‰¹å¤„ç† |

**6. å®é™…é¡¹ç›®æµç¨‹**

```python
# ç¬¬1æ­¥ï¼šå¿«é€ŸåŸå‹ï¼ˆ1å¤©ï¼‰
ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
  â†“
é›¶æ ·æœ¬æµ‹è¯•
  â†“
å¦‚æœæ•ˆæœ > 70%ï¼Œç›´æ¥ä½¿ç”¨ âœ…
å¦‚æœæ•ˆæœ < 70%ï¼Œç»§ç»­

# ç¬¬2æ­¥ï¼šä¼˜åŒ–promptï¼ˆ1-2å¤©ï¼‰
æ”¹è¿›æç¤ºè¯
  â†“
A/Bæµ‹è¯•ä¸åŒprompt
  â†“
å¦‚æœæ•ˆæœ > 80%ï¼Œä½¿ç”¨ä¼˜åŒ–ç‰ˆ âœ…
å¦‚æœæ•ˆæœ < 80%ï¼Œç»§ç»­

# ç¬¬3æ­¥ï¼šæ•°æ®å‡†å¤‡ï¼ˆ3-5å¤©ï¼‰
æ”¶é›†1K-10Kæ ‡æ³¨æ•°æ®
  â†“
æ•°æ®æ¸…æ´—å’Œå¢å¼º
  â†“
å‡†å¤‡è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†

# ç¬¬4æ­¥ï¼šå¾®è°ƒï¼ˆ2-3å¤©ï¼‰
ä½¿ç”¨LoRAæˆ–å…¨å‚æ•°å¾®è°ƒ
  â†“
ç›‘æ§éªŒè¯é›†æŒ‡æ ‡
  â†“
é€‰æ‹©æœ€ä½³checkpoint

# ç¬¬5æ­¥ï¼šè¯„ä¼°å’Œéƒ¨ç½²ï¼ˆ2-3å¤©ï¼‰
å…¨é¢è¯„ä¼°ï¼ˆå¤šä¸ªæŒ‡æ ‡ï¼‰
  â†“
ä¼˜åŒ–æ¨ç†é€Ÿåº¦
  â†“
éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

æ€»æ—¶é—´ï¼š~2å‘¨
```



---

## ğŸ“ æ€»ç»“ä¸èµ„æº

### âœ… æœ¬ç« çŸ¥è¯†æ£€æŸ¥æ¸…å•

å®Œæˆå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**ğŸŒ± ç¬¬ä¸€éƒ¨åˆ†ï¼šå¤šæ¨¡æ€åŸºç¡€ï¼ˆå¿…é¡»æŒæ¡ï¼‰**
- [ ] èƒ½è§£é‡Šä»€ä¹ˆæ˜¯"æ¨¡æ€"ï¼Œå¹¶ä¸¾å‡º3ä¸ªä¾‹å­
- [ ] ç†è§£ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€AIï¼ˆç”Ÿæ´»åœºæ™¯ä¸¾ä¾‹ï¼‰
- [ ] çŸ¥é“å¤šæ¨¡æ€çš„å››å¤§æ ¸å¿ƒä»»åŠ¡ï¼ˆè¡¨ç¤ºã€ç¿»è¯‘ã€å¯¹é½ã€èåˆï¼‰
- [ ] èƒ½è¯´å‡º3ä¸ªå¤šæ¨¡æ€çš„æŒ‘æˆ˜å’Œå¯¹åº”è§£å†³æ–¹æ¡ˆ
- [ ] ç†è§£ä¸åŒæ¨¡æ€çš„æ•°æ®è¡¨ç¤ºæ–¹å¼ï¼ˆå‘é‡åŒ–ï¼‰

**ğŸŒ¿ ç¬¬äºŒéƒ¨åˆ†ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆæ ¸å¿ƒæŠ€æœ¯ï¼‰**
- [ ] ç†è§£CLIPçš„æ ¸å¿ƒæ€æƒ³ï¼šå¯¹æ¯”å­¦ä¹ 
- [ ] èƒ½ç”»å‡ºCLIPçš„æ¶æ„å›¾ï¼ˆåŒç¼–ç å™¨ç»“æ„ï¼‰
- [ ] çŸ¥é“Vision Transformerå¦‚ä½•å¤„ç†å›¾åƒï¼ˆpatch embeddingï¼‰
- [ ] ç†è§£å¯¹æ¯”æŸå¤±å‡½æ•°çš„è®¡ç®—æ–¹å¼
- [ ] èƒ½å®ç°ç®€åŒ–ç‰ˆçš„CLIPåˆ†ç±»å™¨
- [ ] ç†è§£LLaVAçš„æ ¸å¿ƒåˆ›æ–°ï¼šæŠ•å½±å±‚
- [ ] çŸ¥é“LLaVAçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥
- [ ] èƒ½è§£é‡Šä¸ºä»€ä¹ˆåªè®­ç»ƒæŠ•å½±å±‚ï¼ˆä¿ç•™é¢„è®­ç»ƒèƒ½åŠ›ï¼‰
- [ ] ç†è§£å›¾åƒå¦‚ä½•ä½œä¸º"ç‰¹æ®Štoken"è¾“å…¥LLM
- [ ] èƒ½ä½¿ç”¨LLaVAè¿›è¡Œè§†è§‰é—®ç­”

**ğŸŒ¿ğŸŒ¿ ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†é¢‘å’ŒéŸ³é¢‘ï¼ˆæ‰©å±•æ¨¡æ€ï¼‰**
- [ ] ç†è§£è§†é¢‘ç†è§£çš„ä¸‰å¤§æŒ‘æˆ˜ï¼ˆæ•°æ®é‡ã€æ—¶é—´ä¾èµ–ã€æ•ˆç‡ï¼‰
- [ ] çŸ¥é“4ç§è§†é¢‘å¤„ç†æ–¹æ¡ˆçš„ä¼˜åŠ£ï¼ˆç¨€ç–é‡‡æ ·ã€3Då·ç§¯ã€Two-Streamã€Transformerï¼‰
- [ ] èƒ½å®ç°ç®€åŒ–ç‰ˆçš„VideoTransformer
- [ ] ç†è§£éŸ³é¢‘çš„è¡¨ç¤ºæµç¨‹ï¼ˆæ³¢å½¢â†’é¢‘è°±â†’embeddingï¼‰
- [ ] çŸ¥é“Melé¢‘è°±çš„ä½œç”¨å’Œä¼˜åŠ¿
- [ ] èƒ½å®ç°éŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ æ¨¡å‹

**ğŸŒ¿ğŸŒ¿ğŸŒ¿ ç¬¬å››éƒ¨åˆ†ï¼šæ„å»ºå¤šæ¨¡æ€GPTï¼ˆé«˜çº§ï¼‰**
- [ ] ç†è§£"ä¸‡ç‰©çš†Token"çš„æ ¸å¿ƒæ€æƒ³
- [ ] çŸ¥é“å¦‚ä½•è®¾è®¡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¶æ„
- [ ] èƒ½å®ç°å¤šæ¨¡æ€Tokenizerï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰
- [ ] ç†è§£VQ-VAEåœ¨å›¾åƒTokenåŒ–ä¸­çš„ä½œç”¨
- [ ] èƒ½å®ç°ç»Ÿä¸€çš„MultiModalGPTæ¨¡å‹
- [ ] ç†è§£æ¨¡æ€è·¯ç”±å’Œå¤šè¾“å‡ºå¤´æœºåˆ¶
- [ ] çŸ¥é“åˆ†é˜¶æ®µè®­ç»ƒçš„ä¸‰ä¸ªé˜¶æ®µå’Œç›®æ ‡
- [ ] èƒ½ç¼–å†™å®Œæ•´çš„å¤šæ¨¡æ€GPTè®­ç»ƒä»£ç 

**ğŸ¯ ç¬¬äº”éƒ¨åˆ†ï¼šå®æˆ˜ä¸è¯„ä¼°ï¼ˆå®è·µèƒ½åŠ›ï¼‰**
- [ ] èƒ½ä½¿ç”¨Hugging FaceåŠ è½½CLIPæ¨¡å‹
- [ ] ä¼šç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»
- [ ] æŒæ¡CLIPçš„promptå·¥ç¨‹æŠ€å·§
- [ ] èƒ½ä½¿ç”¨LLaVAè¿›è¡Œè§†è§‰é—®ç­”å’Œå¤šè½®å¯¹è¯
- [ ] ä¼šä½¿ç”¨Stable Diffusionç”Ÿæˆå›¾åƒ
- [ ] æŒæ¡æ–‡ç”Ÿå›¾çš„promptä¼˜åŒ–æŠ€å·§
- [ ] ç†è§£ä¸åŒä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆRecall@Kã€BLEUã€CIDErã€FIDç­‰ï¼‰
- [ ] èƒ½è®¡ç®—å¹¶è§£è¯»è¯„ä¼°æŒ‡æ ‡
- [ ] ä¼šç¼–å†™å®Œæ•´çš„è¯„ä¼°è„šæœ¬
- [ ] æŒæ¡æ˜¾å­˜ä¼˜åŒ–æŠ€å·§ï¼ˆé‡åŒ–ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ç­‰ï¼‰
- [ ] çŸ¥é“å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¤šæ¨¡æ€æ¨¡å‹
- [ ] èƒ½åˆ¶å®šå®Œæ•´çš„é¡¹ç›®å®æ–½æµç¨‹

**ğŸ† ç»¼åˆèƒ½åŠ›ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰**
- [ ] èƒ½ç‹¬ç«‹é€‰æ‹©å’Œä½¿ç”¨é¢„è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹
- [ ] ä¼šé’ˆå¯¹å…·ä½“ä»»åŠ¡å¾®è°ƒå¤šæ¨¡æ€æ¨¡å‹
- [ ] èƒ½è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½
- [ ] ä¼šè®¾è®¡å¹¶å®ç°ç®€å•çš„å¤šæ¨¡æ€åº”ç”¨
- [ ] ç†è§£å¤šæ¨¡æ€æ¨¡å‹çš„å±€é™æ€§å’Œæ”¹è¿›æ–¹å‘
- [ ] èƒ½é˜…è¯»å’Œç†è§£æœ€æ–°çš„å¤šæ¨¡æ€è®ºæ–‡

---

### ğŸ“Š å¤šæ¨¡æ€æ¨¡å‹é€ŸæŸ¥è¡¨

#### ğŸ¯ ä¸»æµæ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | ä»»åŠ¡ | æ¨¡æ€ | å‚æ•°é‡ | ç‰¹ç‚¹ | æ¨èåœºæ™¯ | éš¾åº¦ | æˆæœ¬ |
|------|------|------|--------|------|---------|------|------|
| **CLIP** | å›¾æ–‡åŒ¹é… | å›¾åƒ+æ–‡æœ¬ | 400M | é›¶æ ·æœ¬å¼ºã€é€Ÿåº¦å¿« | å›¾æ–‡æ£€ç´¢ã€åˆ†ç±» â­â­â­â­â­ | â­ ç®€å• | å…è´¹ |
| **ViT** | å›¾åƒåˆ†ç±» | å›¾åƒ | 86M-632M | Transformeræ¶æ„ | å›¾åƒç†è§£ã€ç‰¹å¾æå– â­â­â­â­ | â­â­ ä¸­ç­‰ | å…è´¹ |
| **BLIP** | å›¾æ–‡ç†è§£ | å›¾åƒ+æ–‡æœ¬ | 200M | ç»Ÿä¸€æ¡†æ¶ã€å¤šä»»åŠ¡ | å›¾åƒæè¿°ã€æ£€ç´¢ â­â­â­â­ | â­â­ ä¸­ç­‰ | å…è´¹ |
| **BLIP-2** | å›¾æ–‡ç†è§£ | å›¾åƒ+æ–‡æœ¬ | 3.9B | æ€§èƒ½æ›´å¼º | å¤æ‚å›¾æ–‡ä»»åŠ¡ â­â­â­â­â­ | â­â­â­ è¾ƒéš¾ | å…è´¹ |
| **LLaVA** | è§†è§‰å¯¹è¯ | å›¾åƒ+æ–‡æœ¬ | 7B-13B | åŸºäºLLMã€å¯¹è¯å¼º | æ™ºèƒ½åŠ©æ‰‹ã€VQA â­â­â­â­â­ | â­â­â­ è¾ƒéš¾ | å…è´¹ |
| **GPT-4V** | é€šç”¨ç†è§£ | å›¾åƒ+æ–‡æœ¬ | æœªçŸ¥ | æœ€å¼ºæ€§èƒ½ | å•†ä¸šåº”ç”¨ â­â­â­â­â­ | â­ ç®€å• | ä»˜è´¹API |
| **Stable Diffusion** | æ–‡ç”Ÿå›¾ | æ–‡æœ¬â†’å›¾åƒ | 1B | å¼€æºã€å¯æ§ | å›¾åƒç”Ÿæˆã€ç¼–è¾‘ â­â­â­â­â­ | â­â­ ä¸­ç­‰ | å…è´¹ |
| **DALL-E 3** | æ–‡ç”Ÿå›¾ | æ–‡æœ¬â†’å›¾åƒ | æœªçŸ¥ | è´¨é‡æœ€é«˜ | åˆ›æ„è®¾è®¡ â­â­â­â­â­ | â­ ç®€å• | ä»˜è´¹API |
| **Whisper** | è¯­éŸ³è¯†åˆ« | éŸ³é¢‘â†’æ–‡æœ¬ | 1.5B | å¤šè¯­è¨€ã€é²æ£’ | ASRã€å­—å¹• â­â­â­â­â­ | â­ ç®€å• | å…è´¹ |
| **VideoLLaMA** | è§†é¢‘ç†è§£ | è§†é¢‘+æ–‡æœ¬ | 7B | è§†é¢‘å¯¹è¯ | è§†é¢‘åˆ†æ â­â­â­â­ | â­â­â­ è¾ƒéš¾ | å…è´¹ |

#### ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ï¼ˆå¸¸è§åŸºå‡†ï¼‰

| ä»»åŠ¡ | æ•°æ®é›† | CLIP | BLIP | BLIP-2 | LLaVA | GPT-4V |
|------|--------|------|------|--------|-------|--------|
| **é›¶æ ·æœ¬åˆ†ç±»** | ImageNet | 76.2% | - | - | - | - |
| **å›¾æ–‡æ£€ç´¢** | COCO (R@5) | 81.5% | 87.3% | 89.2% | - | - |
| **å›¾åƒæè¿°** | COCO (CIDEr) | - | 133.0 | 144.5 | 151.2 | 168.0 |
| **VQA** | VQAv2 | 45.2% | 65.3% | 75.8% | 80.0% | 87.2% |
| **æ–‡ç”Ÿå›¾** | FID | - | - | - | - | 5.4 (DALL-E 3) |

#### ğŸ”§ æŠ€æœ¯ç‰¹å¾å¯¹æ¯”

| ç‰¹å¾ | CLIP | LLaVA | Stable Diffusion |
|------|------|-------|------------------|
| **æ ¸å¿ƒæŠ€æœ¯** | å¯¹æ¯”å­¦ä¹  | æŠ•å½±å±‚+LLM | æ‰©æ•£æ¨¡å‹ |
| **è®­ç»ƒæ•°æ®** | 4äº¿å›¾æ–‡å¯¹ | 15ä¸‡æŒ‡ä»¤å¯¹ | æ•°äº¿å›¾åƒ |
| **è®­ç»ƒæˆæœ¬** | æ•°ç™¾ä¸‡ç¾å…ƒ | æ•°åƒç¾å…ƒ | æ•°åä¸‡ç¾å…ƒ |
| **æ¨ç†é€Ÿåº¦** | å¿«ï¼ˆ<100msï¼‰ | ä¸­ï¼ˆ1-2sï¼‰ | æ…¢ï¼ˆ10-30sï¼‰ |
| **æ˜¾å­˜éœ€æ±‚** | ä½ï¼ˆ2GBï¼‰ | é«˜ï¼ˆ14GB+ï¼‰ | ä¸­ï¼ˆ8GBï¼‰ |
| **å¾®è°ƒéš¾åº¦** | ä½ | ä¸­ | ä½ |
| **ä¸»è¦åº”ç”¨** | åˆ†ç±»ã€æ£€ç´¢ | é—®ç­”ã€å¯¹è¯ | å›¾åƒç”Ÿæˆ |

---

### ğŸ¯ å¦‚ä½•é€‰æ‹©å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ

#### ğŸ“‹ å®Œæ•´å†³ç­–æµç¨‹

```python
# ç¬¬1æ­¥ï¼šæ˜ç¡®ä½ çš„ä»»åŠ¡
ä»»åŠ¡ç±»å‹ = input("ä½ è¦åšä»€ä¹ˆï¼Ÿ")

if ä»»åŠ¡ç±»å‹ == "å›¾æ–‡æ£€ç´¢/åˆ†ç±»":
    # åœºæ™¯ï¼šå•†å“æœç´¢ã€å›¾ç‰‡åˆ†ç±»ã€ç›¸ä¼¼å›¾ç‰‡æŸ¥æ‰¾
    æ¨èæ¨¡å‹ = "CLIP"
    ç†ç”± = "é›¶æ ·æœ¬èƒ½åŠ›å¼ºï¼Œé€Ÿåº¦å¿«ï¼Œç®€å•æ˜“ç”¨"
    
    # å…·ä½“é…ç½®
    if æ•°æ®é‡ < 1000:
        æ–¹æ¡ˆ = "ç›´æ¥ä½¿ç”¨CLIPé›¶æ ·æœ¬"
    elif æ•°æ®é‡ < 10000:
        æ–¹æ¡ˆ = "CLIP + å°‘é‡å¾®è°ƒ"
    else:
        æ–¹æ¡ˆ = "CLIP + LoRAå¾®è°ƒ"
    
    å®é™…åº”ç”¨ = [
        "ç”µå•†å•†å“æœç´¢",
        "å›¾åº“ç®¡ç†ç³»ç»Ÿ",
        "å†…å®¹å®¡æ ¸ï¼ˆå›¾ç‰‡åˆ†ç±»ï¼‰",
        "ç›¸ä¼¼å›¾ç‰‡æ¨è"
    ]

elif ä»»åŠ¡ç±»å‹ == "å›¾åƒæè¿°/ç†è§£":
    # åœºæ™¯ï¼šè‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡è¯´æ˜ã€å›¾ç‰‡ç†è§£
    if éœ€è¦ç®€å•æè¿°:
        æ¨èæ¨¡å‹ = "BLIP"
        ç†ç”± = "è½»é‡ã€å¿«é€Ÿã€æ•ˆæœå¥½"
    else:
        æ¨èæ¨¡å‹ = "BLIP-2"
        ç†ç”± = "æè¿°æ›´è¯¦ç»†ã€ç†è§£æ›´æ·±å…¥"
    
    å®é™…åº”ç”¨ = [
        "ç¤¾äº¤åª’ä½“è‡ªåŠ¨é…æ–‡",
        "æ— éšœç¢è¾…åŠ©ï¼ˆä¸ºç›²äººæè¿°å›¾ç‰‡ï¼‰",
        "å›¾ç‰‡æ ‡æ³¨ç³»ç»Ÿ",
        "å›¾åƒå†…å®¹å®¡æ ¸"
    ]

elif ä»»åŠ¡ç±»å‹ == "è§†è§‰é—®ç­”/å¯¹è¯":
    # åœºæ™¯ï¼šå›¾ç‰‡é—®ç­”ã€æ™ºèƒ½åŠ©æ‰‹
    if é¢„ç®—å……è¶³ and éœ€è¦æœ€é«˜è´¨é‡:
        æ¨èæ¨¡å‹ = "GPT-4V"
        ç†ç”± = "æ€§èƒ½æœ€å¼ºï¼Œç†è§£æœ€å‡†ç¡®"
        æˆæœ¬ = "$0.01-0.03/å›¾"
    elif éœ€è¦æœ¬åœ°éƒ¨ç½² or é¢„ç®—æœ‰é™:
        æ¨èæ¨¡å‹ = "LLaVA-13B"
        ç†ç”± = "å¼€æºã€å¯å®šåˆ¶ã€æ•ˆæœå¥½"
        æˆæœ¬ = "å…è´¹ï¼ˆéœ€è¦GPUï¼‰"
    else:
        æ¨èæ¨¡å‹ = "LLaVA-7B"
        ç†ç”± = "å¹³è¡¡æ€§èƒ½å’Œèµ„æº"
    
    å®é™…åº”ç”¨ = [
        "æ™ºèƒ½å®¢æœï¼ˆå›¾ç‰‡å’¨è¯¢ï¼‰",
        "æ•™è‚²è¾…å¯¼ï¼ˆçœ‹å›¾ç­”é¢˜ï¼‰",
        "åŒ»ç–—è¾…åŠ©ï¼ˆå½±åƒåˆ†æï¼‰",
        "äº§å“å’¨è¯¢ï¼ˆæ‹ç…§è¯¢é—®ï¼‰"
    ]

elif ä»»åŠ¡ç±»å‹ == "æ–‡ç”Ÿå›¾/å›¾åƒç”Ÿæˆ":
    # åœºæ™¯ï¼šåˆ›æ„è®¾è®¡ã€å›¾åƒç¼–è¾‘
    if éœ€è¦å®Œå…¨å¼€æº and å¯æ§æ€§:
        æ¨èæ¨¡å‹ = "Stable Diffusion 2.1"
        ç†ç”± = "å¼€æºã€å¯å®šåˆ¶ã€ç¤¾åŒºèµ„æºä¸°å¯Œ"
    elif è¿½æ±‚æè‡´è´¨é‡:
        æ¨èæ¨¡å‹ = "DALL-E 3"
        ç†ç”± = "è´¨é‡æœ€é«˜ã€ç†è§£æœ€å‡†"
        æˆæœ¬ = "$0.04/å›¾ (1024Ã—1024)"
    elif éœ€è¦é«˜è´¨é‡ + é€Ÿåº¦:
        æ¨èæ¨¡å‹ = "SDXL"
        ç†ç”± = "æ–°ä¸€ä»£SDï¼Œè´¨é‡å¤§å¹…æå‡"
    
    å®é™…åº”ç”¨ = [
        "UI/UXè®¾è®¡",
        "å¹¿å‘Šåˆ›æ„",
        "æ¸¸æˆç¾æœ¯èµ„æº",
        "ä¸ªæ€§åŒ–å¤´åƒç”Ÿæˆ"
    ]

elif ä»»åŠ¡ç±»å‹ == "è§†é¢‘ç†è§£":
    # åœºæ™¯ï¼šè§†é¢‘åˆ†æã€å†…å®¹ç†è§£
    æ¨èæ¨¡å‹ = "VideoLLaMA or Video-ChatGPT"
    ç†ç”± = "åŸºäºLLMï¼Œèƒ½ç†è§£è§†é¢‘å†…å®¹å’Œæ—¶åº"
    
    å®é™…åº”ç”¨ = [
        "è§†é¢‘å†…å®¹å®¡æ ¸",
        "è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ‘˜è¦",
        "è§†é¢‘é—®ç­”ç³»ç»Ÿ",
        "æ•™å­¦è§†é¢‘åˆ†æ"
    ]

elif ä»»åŠ¡ç±»å‹ == "è¯­éŸ³è¯†åˆ«/è½¬å†™":
    # åœºæ™¯ï¼šè¯­éŸ³è½¬æ–‡å­—
    æ¨èæ¨¡å‹ = "Whisper"
    ç†ç”± = "å¤šè¯­è¨€ã€å‡†ç¡®ç‡é«˜ã€å¼€æº"
    
    å®é™…åº”ç”¨ = [
        "ä¼šè®®è®°å½•",
        "è§†é¢‘å­—å¹•ç”Ÿæˆ",
        "è¯­éŸ³åŠ©æ‰‹",
        "å®æ—¶ç¿»è¯‘"
    ]

# ç¬¬2æ­¥ï¼šè¯„ä¼°èµ„æºé™åˆ¶
if GPUæ˜¾å­˜ < 8GB:
    å»ºè®® = [
        "ä½¿ç”¨CLIPï¼ˆæ˜¾å­˜éœ€æ±‚ä½ï¼‰",
        "ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹",
        "ä½¿ç”¨äº‘ç«¯APIï¼ˆGPT-4Vã€DALL-Eï¼‰",
        "æ‰¹é‡å¤„ç†æ—¶å‡å°batch_size"
    ]
elif GPUæ˜¾å­˜ 8GB-16GB:
    å»ºè®® = [
        "å¯ä»¥è¿è¡ŒLLaVA-7B",
        "å¯ä»¥è¿è¡ŒStable Diffusion",
        "ä½¿ç”¨4-bité‡åŒ–è¿è¡Œæ›´å¤§æ¨¡å‹"
    ]
else:  # >16GB
    å»ºè®® = [
        "å¯ä»¥è¿è¡ŒLLaVA-13B",
        "å¯ä»¥å¾®è°ƒä¸­ç­‰è§„æ¨¡æ¨¡å‹",
        "å¯ä»¥è¿è¡Œå¤šä¸ªæ¨¡å‹"
    ]

# ç¬¬3æ­¥ï¼šè€ƒè™‘å»¶è¿Ÿè¦æ±‚
if éœ€è¦å®æ—¶å“åº” (<100ms):
    æ¨è = ["CLIP"]
elif å¯æ¥å—1-2ç§’:
    æ¨è = ["BLIP", "LLaVA"]
else:  # ç¦»çº¿å¤„ç†
    æ¨è = ["ä»»ä½•æ¨¡å‹éƒ½å¯ä»¥"]
```

#### ğŸ¢ è¡Œä¸šåº”ç”¨åœºæ™¯æ¨è

| è¡Œä¸š/åœºæ™¯ | æ¨èæ¨¡å‹ | å…·ä½“åº”ç”¨ | ä¸ºä»€ä¹ˆ |
|----------|---------|----------|--------|
| **ç”µå•†é›¶å”®** | CLIP | å•†å“æœç´¢ã€æ¨è | é›¶æ ·æœ¬ã€é€Ÿåº¦å¿«ã€å‡†ç¡® |
| **å†…å®¹å¹³å°** | CLIP + LLaVA | å†…å®¹å®¡æ ¸ã€è‡ªåŠ¨æ ‡ç­¾ | CLIPåˆ†ç±» + LLaVAç†è§£ç»†èŠ‚ |
| **æ•™è‚²** | LLaVA | ä½œä¸šæ‰¹æ”¹ã€ç­”ç–‘ | éœ€è¦ç†è§£å’Œæ¨ç† |
| **åŒ»ç–—** | å®šåˆ¶æ¨¡å‹ | å½±åƒåˆ†æã€è¾…åŠ©è¯Šæ–­ | éœ€è¦ä¸“ä¸šæ•°æ®å¾®è°ƒ |
| **è®¾è®¡åˆ›æ„** | Stable Diffusion | å¿«é€ŸåŸå‹ã€ç´ æç”Ÿæˆ | å¯æ§æ€§å¼ºã€è¿­ä»£å¿« |
| **å®¢æœ** | GPT-4V | å›¾ç‰‡å’¨è¯¢ã€é—®é¢˜è¯†åˆ« | ç†è§£å‡†ç¡®ã€å›ç­”ä¸“ä¸š |
| **åª’ä½“** | Whisper + Stable Diffusion | å­—å¹•ç”Ÿæˆã€å°é¢è®¾è®¡ | å¤šæ¨¡æ€åä½œ |
| **ç¤¾äº¤** | BLIP | è‡ªåŠ¨é…æ–‡ã€å†…å®¹æ¨è | å¹³è¡¡æ•ˆæœå’Œæˆæœ¬ |

---

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ è·¯å¾„

#### ğŸ“š æ¨èå­¦ä¹ é¡ºåº

```
1. ç»§ç»­å­¦ä¹ æœ¬ç³»åˆ—
   â”œâ”€ 12_mixture_of_experts.md
   â”‚  å­¦ä¹ MoEç¨€ç–æ¨¡å‹ï¼ˆæå‡æ•ˆç‡ï¼‰
   â”‚  
   â”œâ”€ 13_rlhf_and_alignment.md
   â”‚  å­¦ä¹ å¦‚ä½•å¯¹é½æ¨¡å‹ä¸äººç±»åå¥½
   â”‚  
   â””â”€ å›é¡¾å‰é¢ç« èŠ‚
      å·©å›ºåŸºç¡€çŸ¥è¯†

2. æ·±å…¥å¤šæ¨¡æ€é¢†åŸŸ
   â”œâ”€ é˜…è¯»ç»å…¸è®ºæ–‡ï¼ˆè§ä¸‹æ–¹æ¨èï¼‰
   â”œâ”€ å¤ç°å…³é”®æ¨¡å‹
   â””â”€ å‚åŠ Kaggleç«èµ›

3. å®è·µé¡¹ç›®
   â”œâ”€ æ„å»ºå›¾æ–‡æœç´¢å¼•æ“
   â”œâ”€ å¼€å‘è§†è§‰é—®ç­”ç³»ç»Ÿ
   â””â”€ åˆ›å»ºAIç»˜ç”»åº”ç”¨

4. å‰æ²¿æ¢ç´¢
   â”œâ”€ å…³æ³¨æœ€æ–°è®ºæ–‡ï¼ˆarXivï¼‰
   â”œâ”€ å‚ä¸å¼€æºé¡¹ç›®
   â””â”€ å°è¯•æ–°çš„æ¨¡æ€ç»„åˆ
```

---

### ğŸ’¡ å®è·µå»ºè®®

#### ğŸ¯ ç«‹å³å¯åšï¼ˆä»Šå¤©å°±èƒ½å¼€å§‹ï¼‰

**ç»ƒä¹ 1ï¼šCLIPå›¾åƒåˆ†ç±»ï¼ˆ30åˆ†é’Ÿï¼‰**
```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ä½ çš„å›¾ç‰‡
image = Image.open("your_image.jpg")

# å®šä¹‰ç±»åˆ«ï¼ˆå¯ä»¥æ˜¯ä»»ä½•æ–‡å­—ï¼ï¼‰
labels = ["çŒ«", "ç‹—", "é¸Ÿ", "æ±½è½¦", "å»ºç­‘"]

# é¢„æµ‹
inputs = processor(text=labels, images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)
probs = outputs.logits_per_image.softmax(dim=1)

# ç»“æœ
for label, prob in zip(labels, probs[0]):
    print(f"{label}: {prob:.2%}")

# ğŸ¯ ä»»åŠ¡ï¼š
# 1. æ‰¾5å¼ ä¸åŒç±»å‹çš„å›¾ç‰‡æµ‹è¯•
# 2. å°è¯•ä¸åŒçš„labelæè¿°æ–¹å¼
# 3. è®°å½•å“ªç§æè¿°æ•ˆæœæœ€å¥½
```

**ç»ƒä¹ 2ï¼šStable Diffusionå›¾åƒç”Ÿæˆï¼ˆ30åˆ†é’Ÿï¼‰**
```python
from diffusers import StableDiffusionPipeline

# åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡éœ€è¦ä¸‹è½½ï¼‰
pipe = StableDiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    torch_dtype=torch.float16
).to("cuda")

# ç”Ÿæˆå›¾åƒ
prompt = "a beautiful sunset over the ocean, vibrant colors, 8k"
negative_prompt = "ugly, blurry, low quality"

image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_inference_steps=30,
    guidance_scale=7.5
).images[0]

image.save("sunset.png")

# ğŸ¯ ä»»åŠ¡ï¼š
# 1. ç”Ÿæˆ5å¼ ä¸åŒä¸»é¢˜çš„å›¾ç‰‡
# 2. å®éªŒä¸åŒçš„guidance_scale (5, 7.5, 10, 15)
# 3. è§‚å¯Ÿå‚æ•°å¦‚ä½•å½±å“ç»“æœ
```

**ç»ƒä¹ 3ï¼šLLaVAè§†è§‰é—®ç­”ï¼ˆ1å°æ—¶ï¼‰**
```python
# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œä½¿ç”¨é‡åŒ–ç‰ˆæœ¬
from transformers import LlavaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-7b-hf",
    quantization_config=quantization_config
)
processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

# æé—®
image = Image.open("photo.jpg")
prompt = "USER: <image>\nWhat's happening in this image?\nASSISTANT:"

inputs = processor(text=prompt, images=image, return_tensors="pt")
output = model.generate(**inputs, max_new_tokens=200)
answer = processor.decode(output[0], skip_special_tokens=True)

print(answer)

# ğŸ¯ ä»»åŠ¡ï¼š
# 1. å‡†å¤‡3-5å¼ å¤æ‚åœºæ™¯çš„å›¾ç‰‡
# 2. æå‡ºä¸åŒç±»å‹çš„é—®é¢˜ï¼ˆæè¿°ã€è®¡æ•°ã€æ¨ç†ï¼‰
# 3. è¯„ä¼°æ¨¡å‹çš„å›ç­”è´¨é‡
```

#### ğŸ”¬ ç³»ç»Ÿå®éªŒï¼ˆä¸€å‘¨é¡¹ç›®ï¼‰

**å®éªŒ1ï¼šæ„å»ºå›¾æ–‡æœç´¢å¼•æ“**
```bash
# ç›®æ ‡ï¼šç”¨CLIPæ„å»ºä¸€ä¸ªå›¾ç‰‡æœç´¢ç³»ç»Ÿ
# æ•°æ®ï¼šæ”¶é›†100-1000å¼ å›¾ç‰‡

# æ­¥éª¤ï¼š
# 1. æå–æ‰€æœ‰å›¾ç‰‡çš„CLIPç‰¹å¾
python extract_features.py --image_dir ./images/ --output features.pkl

# 2. æ„å»ºç´¢å¼•
python build_index.py --features features.pkl

# 3. æœç´¢æµ‹è¯•
python search.py --query "red car in the street" --top_k 10

# 4. è¯„ä¼°
python evaluate.py --test_queries queries.txt

# ğŸ“Š è¯„ä¼°æŒ‡æ ‡ï¼š
# - Recall@5: å‰5ä¸ªç»“æœä¸­æœ‰æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹
# - å¹³å‡æŸ¥è¯¢æ—¶é—´
# - ç”¨æˆ·æ»¡æ„åº¦

# ğŸ¯ æ”¹è¿›æ–¹å‘ï¼š
# - å°è¯•ä¸åŒçš„CLIPæ¨¡å‹ï¼ˆViT-B vs ViT-Lï¼‰
# - æ·»åŠ æ–‡æœ¬æè¿°æå‡æœç´¢
# - ä½¿ç”¨FAISSåŠ é€Ÿæœç´¢
```

**å®éªŒ2ï¼šå¾®è°ƒLLaVAï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰**
```bash
# ç›®æ ‡ï¼šåœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒLLaVA
# æ•°æ®ï¼šå‡†å¤‡100-1000ä¸ªå›¾æ–‡QAå¯¹

# æ•°æ®æ ¼å¼ï¼š
# {
#   "image": "path/to/image.jpg",
#   "conversations": [
#     {"from": "human", "value": "é—®é¢˜"},
#     {"from": "gpt", "value": "ç­”æ¡ˆ"}
#   ]
# }

# å¾®è°ƒè„šæœ¬
python finetune_llava.py \
  --model_name_or_path llava-hf/llava-1.5-7b-hf \
  --data_path custom_data.json \
  --output_dir ./outputs \
  --num_train_epochs 3 \
  --per_device_train_batch_size 4 \
  --gradient_accumulation_steps 4 \
  --learning_rate 2e-5 \
  --use_lora True

# ğŸ“Š è¯„ä¼°ï¼š
# - åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•å‡†ç¡®ç‡
# - å¯¹æ¯”å¾®è°ƒå‰åçš„æ•ˆæœ
# - äººå·¥è¯„ä¼°å›ç­”è´¨é‡

# ğŸ’¡ æŠ€å·§ï¼š
# - ä½¿ç”¨LoRAå‡å°‘æ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´
# - æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡
# - å®šæœŸä¿å­˜checkpoint
```

**å®éªŒ3ï¼šå¤šæ¨¡æ€åº”ç”¨å¼€å‘**
```bash
# é¡¹ç›®ï¼šæ™ºèƒ½å›¾ç‰‡åŠ©æ‰‹Webåº”ç”¨
# åŠŸèƒ½ï¼šä¸Šä¼ å›¾ç‰‡ â†’ è‡ªåŠ¨æè¿° + å›ç­”é—®é¢˜

# æŠ€æœ¯æ ˆï¼š
# - åç«¯ï¼šFastAPI + PyTorch
# - å‰ç«¯ï¼šReact/Vue
# - æ¨¡å‹ï¼šCLIP + LLaVA

# æ¶æ„ï¼š
# 1. ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡
# 2. CLIPå¿«é€Ÿåˆ†ç±»å’Œæ‰“æ ‡ç­¾
# 3. LLaVAç”Ÿæˆè¯¦ç»†æè¿°
# 4. ç”¨æˆ·å¯ä»¥ç»§ç»­æé—®
# 5. ä¿å­˜å¯¹è¯å†å²

# æŒ‘æˆ˜ï¼š
# - æ¨¡å‹åŠ è½½å’Œæ¨ç†é€Ÿåº¦ä¼˜åŒ–
# - å¹¶å‘è¯·æ±‚å¤„ç†
# - æ˜¾å­˜ç®¡ç†

# ğŸ’¡ ä¼˜åŒ–æŠ€å·§ï¼š
# - ä½¿ç”¨æ¨¡å‹ç¼“å­˜
# - æ‰¹é‡å¤„ç†è¯·æ±‚
# - ä½¿ç”¨é‡åŒ–æ¨¡å‹
# - è€ƒè™‘ä½¿ç”¨æ¨¡å‹APIï¼ˆGPT-4Vï¼‰
```

#### ğŸ“– è¿›é˜¶ç ”ç©¶ï¼ˆæ·±å…¥å­¦ä¹ ï¼‰

**é˜…è¯»æ¸…å•ï¼ˆæŒ‰é¡ºåºï¼‰**

1. **åŸºç¡€è®ºæ–‡**ï¼ˆå¿…è¯»ï¼‰
   - [ ] CLIP (2021) - ç†è§£å¯¹æ¯”å­¦ä¹ 
   - [ ] ViT (2020) - è§†è§‰Transformer
   - [ ] LLaVA (2023) - è§†è§‰æŒ‡ä»¤å¾®è°ƒ

2. **è¿›é˜¶è®ºæ–‡**ï¼ˆæ¨èï¼‰
   - [ ] BLIP-2 (2023) - Q-Formeræ¶æ„
   - [ ] Flamingo (2022) - å°‘æ ·æœ¬å­¦ä¹ 
   - [ ] Stable Diffusion (2022) - æ‰©æ•£æ¨¡å‹

3. **å‰æ²¿è®ºæ–‡**ï¼ˆé€‰è¯»ï¼‰
   - [ ] GPT-4V System Card - æœ€å¼ºå¤šæ¨¡æ€
   - [ ] Gemini Technical Report - Googleçš„å¤šæ¨¡æ€
   - [ ] Video-ChatGPT - è§†é¢‘ç†è§£

**ç ”ç©¶æ–¹å‘**
- æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€èåˆæ–¹æ³•
- æ›´å°‘æ•°æ®çš„å¤šæ¨¡æ€å­¦ä¹ 
- å¤šæ¨¡æ€å¹»è§‰é—®é¢˜ç ”ç©¶
- 3Då’Œç©ºé—´ç†è§£
- å…·èº«æ™ºèƒ½ï¼ˆæœºå™¨äººè§†è§‰ï¼‰

---

### ğŸ“š æ¨èèµ„æº

#### ğŸ“„ å¿…è¯»è®ºæ–‡ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰

**ğŸŒŸ åŸºç¡€å¿…è¯»ï¼ˆ3ç¯‡ï¼‰**

1. **CLIP: Learning Transferable Visual Models From Natural Language Supervision**
   - ä½œè€…ï¼šRadford et al. (OpenAI)
   - å¹´ä»½ï¼š2021
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2103.00020
   - ä¸ºä»€ä¹ˆè¯»ï¼šå¤šæ¨¡æ€å­¦ä¹ çš„é‡Œç¨‹ç¢‘ï¼Œç†è§£å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒ
   - é˜…è¯»æ—¶é—´ï¼š2-3å°æ—¶
   - å…³é”®ç‚¹ï¼šå¯¹æ¯”å­¦ä¹ ã€é›¶æ ·æœ¬èƒ½åŠ›ã€å¤§è§„æ¨¡æ•°æ®

2. **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)**
   - ä½œè€…ï¼šDosovitskiy et al. (Google)
   - å¹´ä»½ï¼š2020
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2010.11929
   - ä¸ºä»€ä¹ˆè¯»ï¼šç†è§£å¦‚ä½•ç”¨Transformerå¤„ç†å›¾åƒ
   - é˜…è¯»æ—¶é—´ï¼š1-2å°æ—¶
   - å…³é”®ç‚¹ï¼šPatch Embeddingã€Position Embeddingã€Transformeræ¶æ„

3. **Visual Instruction Tuning (LLaVA)**
   - ä½œè€…ï¼šLiu et al.
   - å¹´ä»½ï¼š2023
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2304.08485
   - ä¸ºä»€ä¹ˆè¯»ï¼šç†è§£å¦‚ä½•è®©LLMçœ‹æ‡‚å›¾ç‰‡
   - é˜…è¯»æ—¶é—´ï¼š1-2å°æ—¶
   - å…³é”®ç‚¹ï¼šæŠ•å½±å±‚ã€ä¸¤é˜¶æ®µè®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒ

**ğŸ”¥ è¿›é˜¶æ¨èï¼ˆ6ç¯‡ï¼‰**

4. **BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2201.12086
   - å…³é”®åˆ›æ–°ï¼šç»Ÿä¸€æ¡†æ¶ã€Caption and Filteræ–¹æ³•
   - é€‚åˆï¼šæƒ³æ·±å…¥ç†è§£å›¾æ–‡ç†è§£çš„åŒå­¦

5. **BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2301.12597
   - å…³é”®åˆ›æ–°ï¼šQ-Formerã€å†»ç»“é¢„è®­ç»ƒæ¨¡å‹
   - é€‚åˆï¼šæƒ³äº†è§£é«˜æ•ˆå¤šæ¨¡æ€æ¶æ„çš„åŒå­¦

6. **Flamingo: a Visual Language Model for Few-Shot Learning**
   - ä½œè€…ï¼šAlayrac et al. (DeepMind)
   - å¹´ä»½ï¼š2022
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2204.14198
   - å…³é”®åˆ›æ–°ï¼šFew-shotå­¦ä¹ ã€äº¤å‰æ³¨æ„åŠ›
   - é€‚åˆï¼šç ”ç©¶è€…

7. **High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion)**
   - ä½œè€…ï¼šRombach et al.
   - å¹´ä»½ï¼š2022
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2112.10752
   - å…³é”®åˆ›æ–°ï¼šæ½œåœ¨ç©ºé—´æ‰©æ•£ã€æ•ˆç‡æå‡
   - é€‚åˆï¼šå¯¹å›¾åƒç”Ÿæˆæ„Ÿå…´è¶£çš„åŒå­¦

8. **Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2)**
   - ä½œè€…ï¼šRamesh et al. (OpenAI)
   - å¹´ä»½ï¼š2022
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2204.06125
   - å…³é”®åˆ›æ–°ï¼šCLIPå¼•å¯¼ã€Unifyæ‰©æ•£
   - é€‚åˆï¼šæƒ³äº†è§£DALL-Eçš„åŒå­¦

9. **Attention Is All You Need (TransformeråŸè®ºæ–‡)**
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/1706.03762
   - ä¸ºä»€ä¹ˆè¯»ï¼šç†è§£å¤šæ¨¡æ€æ¨¡å‹çš„åŸºç¡€æ¶æ„
   - é€‚åˆï¼šæƒ³æ·±å…¥ç†è§£Transformerçš„åŒå­¦

**ğŸš€ å‰æ²¿è®ºæ–‡ï¼ˆé€‰è¯»ï¼‰**

10. **GPT-4 Technical Report**
    - åŒ…å«GPT-4Vï¼ˆVisionï¼‰éƒ¨åˆ†
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2303.08774
    - å…³é”®ï¼šæœ€å¼ºå¤šæ¨¡æ€èƒ½åŠ›å±•ç¤º

11. **Gemini: A Family of Highly Capable Multimodal Models**
    - ä½œè€…ï¼šGoogle Team
    - å¹´ä»½ï¼š2023
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2312.11805
    - å…³é”®ï¼šåŸç”Ÿå¤šæ¨¡æ€è®¾è®¡

12. **Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2306.05424
    - å…³é”®ï¼šè§†é¢‘ç†è§£çš„LLMæ–¹æ³•

13. **ImageBind: One Embedding Space To Bind Them All**
    - ä½œè€…ï¼šMeta AI
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2305.05665
    - å…³é”®ï¼š6ç§æ¨¡æ€ç»Ÿä¸€åˆ°ä¸€ä¸ªç©ºé—´

---

#### ğŸ”§ å®ç”¨å·¥å…·å’Œåº“

**æ ¸å¿ƒåº“**
```bash
# 1. Transformersï¼ˆå¿…è£…ï¼‰
pip install transformers
# åŒ…å«ï¼šCLIP, BLIP, LLaVAç­‰æ¨¡å‹

# 2. Diffusersï¼ˆæ–‡ç”Ÿå›¾ï¼‰
pip install diffusers
# åŒ…å«ï¼šStable Diffusion, DALL-Eç­‰

# 3. PyTorchï¼ˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼‰
pip install torch torchvision

# 4. PIL/Pillowï¼ˆå›¾åƒå¤„ç†ï¼‰
pip install Pillow

# 5. OpenAI CLIPï¼ˆå®˜æ–¹å®ç°ï¼‰
pip install git+https://github.com/openai/CLIP.git

# 6. LLaVAï¼ˆå®˜æ–¹ä»“åº“ï¼‰
git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA
pip install -e .
```

**è¾…åŠ©å·¥å…·**
```bash
# åŠ é€Ÿæ¨ç†
pip install accelerate bitsandbytes  # é‡åŒ–å’ŒåŠ é€Ÿ
pip install xformers  # æ›´å¿«çš„attention

# è¯„ä¼°å·¥å…·
pip install pycocoevalcap  # å›¾åƒæè¿°è¯„ä¼°
pip install pytorch-fid  # FIDè®¡ç®—
pip install clip-score  # CLIP Scoreè®¡ç®—

# æ•°æ®å¤„ç†
pip install datasets  # Hugging Faceæ•°æ®é›†
pip install opencv-python  # è§†é¢‘å¤„ç†
pip install librosa  # éŸ³é¢‘å¤„ç†
```

**å¼€æºé¡¹ç›®å’Œèµ„æº**

| é¡¹ç›® | æè¿° | é“¾æ¥ | æ¨èåº¦ |
|------|------|------|--------|
| **CLIP** | OpenAIå®˜æ–¹å®ç° | https://github.com/openai/CLIP | â­â­â­â­â­ |
| **LLaVA** | è§†è§‰æŒ‡ä»¤å¾®è°ƒ | https://github.com/haotian-liu/LLaVA | â­â­â­â­â­ |
| **Stable Diffusion WebUI** | SDå¯è§†åŒ–ç•Œé¢ | https://github.com/AUTOMATIC1111/stable-diffusion-webui | â­â­â­â­â­ |
| **Hugging Face Hub** | é¢„è®­ç»ƒæ¨¡å‹åº“ | https://huggingface.co/models | â­â­â­â­â­ |
| **LAVIS** | BLIPå®˜æ–¹åº“ | https://github.com/salesforce/LAVIS | â­â­â­â­ |
| **Whisper** | OpenAIè¯­éŸ³è¯†åˆ« | https://github.com/openai/whisper | â­â­â­â­â­ |
| **Video-ChatGPT** | è§†é¢‘å¯¹è¯ | https://github.com/mbzuai-oryx/Video-ChatGPT | â­â­â­â­ |

---

#### ğŸ“Š é‡è¦æ•°æ®é›†

**å›¾æ–‡æ•°æ®é›†**

| æ•°æ®é›† | è§„æ¨¡ | ä»»åŠ¡ | é“¾æ¥ | æ¨èåº¦ |
|--------|------|------|------|--------|
| **COCO** | 330Kå›¾ + 5æè¿°/å›¾ | å›¾åƒæè¿°ã€æ£€æµ‹ | https://cocodataset.org | â­â­â­â­â­ |
| **Conceptual Captions** | 3.3M/12Må›¾æ–‡å¯¹ | å›¾æ–‡é¢„è®­ç»ƒ | https://ai.google.com/research/ConceptualCaptions | â­â­â­â­â­ |
| **LAION-5B** | 58äº¿å›¾æ–‡å¯¹ | å¤§è§„æ¨¡é¢„è®­ç»ƒ | https://laion.ai/blog/laion-5b | â­â­â­â­â­ |
| **VQA v2** | 200Kå›¾ + 110ä¸‡é—®ç­” | è§†è§‰é—®ç­” | https://visualqa.org | â­â­â­â­â­ |
| **Flickr30k** | 31Kå›¾ + 5æè¿°/å›¾ | å›¾åƒæè¿°ã€æ£€ç´¢ | http://shannon.cs.illinois.edu/DenotationGraph | â­â­â­â­ |
| **TextCaps** | 145Kå›¾æ–‡å¯¹ | OCR+æè¿° | https://textvqa.org/textcaps | â­â­â­ |

**è§†é¢‘æ•°æ®é›†**

| æ•°æ®é›† | è§„æ¨¡ | ä»»åŠ¡ | æ¨èåº¦ |
|--------|------|------|--------|
| **Kinetics-400/700** | 240K+/650K+è§†é¢‘ | åŠ¨ä½œè¯†åˆ« | â­â­â­â­â­ |
| **ActivityNet** | 20Kè§†é¢‘ | æ—¶åºåŠ¨ä½œæ£€æµ‹ | â­â­â­â­ |
| **MSR-VTT** | 10Kè§†é¢‘ + 200Kæè¿° | è§†é¢‘æè¿° | â­â­â­â­ |

**éŸ³é¢‘æ•°æ®é›†**

| æ•°æ®é›† | è§„æ¨¡ | ä»»åŠ¡ | æ¨èåº¦ |
|--------|------|------|--------|
| **LibriSpeech** | 1000å°æ—¶è¯­éŸ³ | è¯­éŸ³è¯†åˆ« | â­â­â­â­â­ |
| **AudioCaps** | 50KéŸ³é¢‘+æè¿° | éŸ³é¢‘æè¿° | â­â­â­â­ |
| **VGGSound** | 200Kè§†é¢‘+éŸ³é¢‘ | éŸ³è§†é¢‘å¯¹é½ | â­â­â­â­ |

---

#### ğŸ“ å­¦ä¹ èµ„æº

**åœ¨çº¿è¯¾ç¨‹**
- Stanford CS231nï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰- ç†è§£è§†è§‰åŸºç¡€
- Deep Learning Specialization (Coursera) - æ·±åº¦å­¦ä¹ åŸºç¡€
- Hugging Face Course - å®è·µå¯¼å‘çš„Transformerè¯¾ç¨‹

**åšå®¢å’Œæ•™ç¨‹**
- Hugging Face Blog - æœ€æ–°æ¨¡å‹å’ŒæŠ€æœ¯
- Jay Alammar's Blog (https://jalammar.github.io) - å¯è§†åŒ–è§£é‡Š
- Lil'Log (https://lilianweng.github.io) - æ·±åº¦æŠ€æœ¯åšå®¢
- Distill.pub - äº¤äº’å¼è®ºæ–‡

**è§†é¢‘æ•™ç¨‹**
- Andrej Karpathy's YouTube - Zero to Heroç³»åˆ—
- Two Minute Papers - è®ºæ–‡å¿«é€Ÿè§£è¯»
- Yannic Kilcher - æ·±åº¦è®ºæ–‡è§£æ

**ç¤¾åŒºå’Œè®ºå›**
- Hugging Face Forums - æŠ€æœ¯è®¨è®º
- r/MachineLearning (Reddit) - æœ€æ–°åŠ¨æ€
- Papers with Code - è®ºæ–‡+ä»£ç 
- arXiv - æœ€æ–°è®ºæ–‡

---

### ğŸ› å¸¸è§é—®é¢˜ FAQ

#### Q1: å¤šæ¨¡æ€æ¨¡å‹å’Œå•æ¨¡æ€æ¨¡å‹çš„æ ¹æœ¬åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: æ ¸å¿ƒåŒºåˆ«åœ¨äºä¿¡æ¯èåˆèƒ½åŠ›å’Œåº”ç”¨åœºæ™¯ã€‚

```python
å•æ¨¡æ€ï¼ˆå¦‚GPT-3ï¼‰:
  è¾“å…¥ï¼šçº¯æ–‡æœ¬
  å¤„ç†ï¼šæ–‡æœ¬ç¼–ç å™¨
  è¾“å‡ºï¼šæ–‡æœ¬
  
  ä¼˜åŠ¿ï¼š
    âœ… ä¸“æ³¨ä¸€ä¸ªæ¨¡æ€ï¼ŒæŠ€æœ¯æˆç†Ÿ
    âœ… è®­ç»ƒå’Œéƒ¨ç½²ç›¸å¯¹ç®€å•
    âœ… åœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€
  
  å±€é™ï¼š
    âŒ åªèƒ½ç†è§£æ–‡å­—æè¿°
    âŒ æ— æ³•ç›´æ¥å¤„ç†å›¾åƒã€éŸ³é¢‘
    âŒ ç¼ºä¹å¤šæ„Ÿå®˜ç†è§£èƒ½åŠ›

å¤šæ¨¡æ€ï¼ˆå¦‚CLIPã€LLaVAï¼‰:
  è¾“å…¥ï¼šå›¾åƒ + æ–‡æœ¬ / éŸ³é¢‘ + æ–‡æœ¬ ç­‰
  å¤„ç†ï¼šå¤šä¸ªç¼–ç å™¨ + èåˆå±‚
  è¾“å‡ºï¼šè·¨æ¨¡æ€è¡¨ç¤ºæˆ–ç”Ÿæˆ
  
  ä¼˜åŠ¿ï¼š
    âœ… ç†è§£å¤šç§ä¿¡æ¯ç±»å‹
    âœ… æ›´æ¥è¿‘äººç±»æ„ŸçŸ¥æ–¹å¼
    âœ… èƒ½å¤„ç†æ›´å¤æ‚çš„ç°å®ä»»åŠ¡
    âœ… é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›å¼º
  
  å±€é™ï¼š
    âŒ æ¶æ„æ›´å¤æ‚
    âŒ è®­ç»ƒæˆæœ¬æ›´é«˜
    âŒ éœ€è¦é…å¯¹æ•°æ®

å®é™…å¯¹æ¯”ä¾‹å­:
  ä»»åŠ¡: "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
  
  å•æ¨¡æ€GPT:
    è¾“å…¥: "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
    è¾“å‡º: "æŠ±æ­‰ï¼Œæˆ‘çœ‹ä¸åˆ°å›¾ç‰‡ï¼Œæˆ‘åªèƒ½ç†è§£æ–‡å­—..."
    âŒ æ— æ³•å®Œæˆä»»åŠ¡
  
  å¤šæ¨¡æ€LLaVA:
    è¾“å…¥: [å›¾ç‰‡] + "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
    è¾“å‡º: "è¿™æ˜¯ä¸€åªæ©˜è‰²çš„çŒ«ï¼Œååœ¨çº¢è‰²çš„å«å­ä¸Š..."
    âœ… çœŸæ­£"çœ‹åˆ°"å¹¶ç†è§£äº†å›¾åƒ

ç»“è®ºï¼š
  å•æ¨¡æ€ = å•ä¸€æ„Ÿå®˜çš„ä¸“å®¶
  å¤šæ¨¡æ€ = ç»¼åˆæ„Ÿå®˜çš„å…¨æ‰
```

---

#### Q2: CLIPä¸ºä»€ä¹ˆè¿™ä¹ˆå¼ºå¤§ï¼Ÿé›¶æ ·æœ¬åˆ†ç±»çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: å¯¹æ¯”å­¦ä¹  + å¤§è§„æ¨¡æ•°æ® + è‡ªç„¶è¯­è¨€ç›‘ç£

```python
CLIPå¼ºå¤§çš„ä¸‰ä¸ªæ ¸å¿ƒåŸå› :

1ï¸âƒ£ æµ·é‡æ•°æ®ï¼ˆè§„æ¨¡ä¼˜åŠ¿ï¼‰
è®­ç»ƒæ•°æ®: 4äº¿å›¾æ–‡å¯¹ï¼ˆä»äº’è”ç½‘æ”¶é›†ï¼‰
   æ•°æ®æ¥æº: ç½‘é¡µä¸Šçš„å›¾ç‰‡+altæ–‡æœ¬
   
   å¯¹æ¯”:
   ImageNet: 140ä¸‡å¼ æœ‰æ ‡æ³¨å›¾ç‰‡ï¼ˆäººå·¥æ ‡æ³¨ï¼‰
   CLIP: 4äº¿å›¾æ–‡å¯¹ï¼ˆè‡ªç„¶äº§ç”Ÿï¼‰
   
   æ•°æ®é‡ç›¸å·®è¿‘300å€ï¼

2ï¸âƒ£ å¯¹æ¯”å­¦ä¹ ï¼ˆæ–¹æ³•åˆ›æ–°ï¼‰
   åŸç†: è®©åŒ¹é…çš„å›¾æ–‡å¯¹ç›¸ä¼¼åº¦é«˜ï¼Œä¸åŒ¹é…çš„ä½
   
   è®­ç»ƒè¿‡ç¨‹:
   Batch = 32Kå›¾æ–‡å¯¹
   
   æ­£æ ·æœ¬: 32Kä¸ªï¼ˆå¯¹è§’çº¿ï¼ŒåŒ¹é…çš„å›¾æ–‡å¯¹ï¼‰
   è´Ÿæ ·æœ¬: 32KÃ—32K - 32K â‰ˆ 10äº¿ä¸ªï¼ˆå…¶ä»–ç»„åˆï¼‰
   
   loss = -log( exp(åŒ¹é…ç›¸ä¼¼åº¦) / Î£ exp(æ‰€æœ‰ç›¸ä¼¼åº¦) )
   
   ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
   - å¤§é‡è´Ÿæ ·æœ¬è®©æ¨¡å‹å­¦ä¼šåŒºåˆ†
   - ä¸éœ€è¦äººå·¥æ ‡æ³¨ç±»åˆ«
   - è‡ªç„¶å­¦ä¼šå›¾æ–‡å¯¹é½

3ï¸âƒ£ è‡ªç„¶è¯­è¨€ç›‘ç£ï¼ˆçµæ´»æ€§ï¼‰
   ä¼ ç»Ÿæ–¹æ³•: å›¾åƒ â†’ å›ºå®šç±»åˆ«ï¼ˆçŒ«ã€ç‹—ã€é¸Ÿ...ï¼‰
   CLIP: å›¾åƒ â†’ ä»»æ„æ–‡æœ¬æè¿°
   
   çµæ´»æ€§ä½“ç°:
   ä¼ ç»Ÿ: åªèƒ½åˆ†ç±»è®­ç»ƒè¿‡çš„1000ä¸ªç±»åˆ«
   CLIP: å¯ä»¥åˆ†ç±»ä»»æ„ç”¨æ–‡å­—æè¿°çš„æ¦‚å¿µï¼
   
   ä¾‹å­:
   "a photo of a cat"        âœ…
   "a photo of a dog"        âœ…
   "a cat in the rain"       âœ… ç»„åˆæ¦‚å¿µï¼
   "a cyberpunk style cat"   âœ… é£æ ¼æ¦‚å¿µï¼

é›¶æ ·æœ¬åˆ†ç±»åŸç†:
   æ­¥éª¤1: æŠŠå›¾åƒç¼–ç æˆå‘é‡
   image_vec = image_encoder(image)
   
   æ­¥éª¤2: æŠŠæ¯ä¸ªç±»åˆ«æè¿°ç¼–ç æˆå‘é‡
   text_vecs = [
     text_encoder("a photo of a cat"),
     text_encoder("a photo of a dog"),
     ...
   ]
   
   æ­¥éª¤3: è®¡ç®—ç›¸ä¼¼åº¦ï¼Œé€‰æœ€é«˜çš„
   similarities = image_vec @ text_vecs.T
   predicted_class = argmax(similarities)
   
   å…³é”®: ä¸éœ€è¦åœ¨ç›®æ ‡ç±»åˆ«ä¸Šè®­ç»ƒï¼

å®é™…æ€§èƒ½:
   ImageNeté›¶æ ·æœ¬: 76.2%
   ï¼ˆå¾ˆå¤šæœ‰ç›‘ç£æ¨¡å‹åªæœ‰80%ï¼‰
   
   å·®è·ä»…4%ï¼Œä½†CLIPæ ¹æœ¬æ²¡è§è¿‡ImageNetï¼

ä¸ºä»€ä¹ˆæ³›åŒ–è¿™ä¹ˆå¥½ï¼Ÿ
   å› ä¸ºå­¦åˆ°äº†å›¾åƒå’Œæ–‡æœ¬çš„"é€šç”¨"å¯¹åº”å…³ç³»
   è€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒç‰¹å®šç±»åˆ«
```

---

#### Q3: è§†è§‰ç¼–ç å™¨åº”è¯¥é€‰CNNè¿˜æ˜¯Transformerï¼Ÿ

**A**: ç°åœ¨ä¸»æµæ˜¯Vision Transformer (ViT)ï¼Œä½†è¦æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©

```python
ğŸ“Š è¯¦ç»†å¯¹æ¯”è¡¨:

                CNN (ResNet)              ViT (Vision Transformer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ¶æ„        å±€éƒ¨å·ç§¯ + æ± åŒ–           å…¨å±€è‡ªæ³¨æ„åŠ›
æ„Ÿå—é‡      å±€éƒ¨ â†’ å…¨å±€ï¼ˆé€å±‚æ‰©å¤§ï¼‰    ä¸€å¼€å§‹å°±æ˜¯å…¨å±€
å½’çº³åç½®    å¼ºï¼ˆå±€éƒ¨æ€§ã€å¹³ç§»ä¸å˜æ€§ï¼‰    å¼±ï¼ˆéœ€è¦æ•°æ®å­¦ä¹ ï¼‰
æ•°æ®éœ€æ±‚    å°ï¼ˆ10ä¸‡-100ä¸‡ï¼‰           å¤§ï¼ˆ100ä¸‡-10äº¿ï¼‰
è®­ç»ƒæ—¶é—´    å¿«ï¼ˆæ•°å°æ—¶-æ•°å¤©ï¼‰          æ…¢ï¼ˆæ•°å¤©-æ•°å‘¨ï¼‰
æ¨ç†é€Ÿåº¦    å¿«ï¼ˆ50msï¼‰                 æ…¢ï¼ˆ100msï¼‰
æ˜¾å­˜éœ€æ±‚    ä½ï¼ˆ4GBï¼‰                  é«˜ï¼ˆ8GB+ï¼‰
å‚æ•°é‡      å°ï¼ˆ25M-60Mï¼‰              å¤§ï¼ˆ86M-632Mï¼‰
å¯è§£é‡Šæ€§    é«˜ï¼ˆå¯è§†åŒ–å·ç§¯æ ¸ï¼‰         ä½ï¼ˆæ³¨æ„åŠ›å›¾ï¼‰
ä¸LLMç»Ÿä¸€   âŒ æ¶æ„ä¸åŒ                âœ… éƒ½æ˜¯Transformer

ğŸ¯ å®é™…é€‰æ‹©æŒ‡å—:

if æ•°æ®é‡ < 10ä¸‡:
    é€‰æ‹© = "CNN (ResNet)"
    ç†ç”± = "ViTéœ€è¦å¤§é‡æ•°æ®ï¼Œå°æ•°æ®ä¸ŠCNNæ›´å¥½"
    
elif æ•°æ®é‡ < 100ä¸‡:
    é€‰æ‹© = "CNN (ResNet) or å°ViT"
    ç†ç”± = "ä¸¤è€…éƒ½å¯ä»¥ï¼ŒCNNå¯èƒ½ç¨å¥½"
    
elif æ•°æ®é‡ > 100ä¸‡:
    é€‰æ‹© = "ViT"
    ç†ç”± = "å¤§æ•°æ®ä¸ŠViTæ€§èƒ½æ›´ä¼˜"

if ä»»åŠ¡ == "å¤šæ¨¡æ€" (ä¸Transformer LLMç»“åˆ):
    å¼ºçƒˆæ¨è = "ViT"
    ç†ç”± = "æ¶æ„ç»Ÿä¸€ï¼Œæ›´å®¹æ˜“èåˆ"
    
elif ä»»åŠ¡ == "è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²":
    æ¨è = "MobileNet (CNNå˜ä½“)"
    ç†ç”± = "æ›´å¿«ã€æ›´å°"
    
elif ä»»åŠ¡ == "å®æ—¶åº”ç”¨" (å¦‚è§†é¢‘):
    æ¨è = "CNN"
    ç†ç”± = "æ¨ç†é€Ÿåº¦å¿«"

ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ï¼ˆImageNetï¼‰:

æ¨¡å‹                å‚æ•°é‡    Top-1å‡†ç¡®ç‡    æ¨ç†é€Ÿåº¦
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ResNet-50          25M       76.2%         50ms
ResNet-152         60M       78.3%         100ms
ViT-B/16          86M       79.7%         100ms  â­ å‚æ•°å°‘ï¼Œæ€§èƒ½å¥½
ViT-L/16         307M       82.6%         300ms  â­â­ æ€§èƒ½æœ€å¥½
EfficientNet-B7   66M       84.3%         150ms  â­â­â­ ç»¼åˆæœ€ä¼˜ï¼ˆCNNï¼‰

ğŸ”¥ è¶‹åŠ¿åˆ¤æ–­:

è¿‡å»ï¼ˆ2010-2020ï¼‰: CNNç»Ÿæ²»
  ResNet, VGG, Inception...
  
ç°åœ¨ï¼ˆ2020-2025ï¼‰: ViTå´›èµ·
  ViT, CLIP, LLaVAéƒ½ç”¨ViT
  
æœªæ¥ï¼ˆ2025+ï¼‰: æ··åˆæ¶æ„ï¼Ÿ
  ConvNext (ç°ä»£åŒ–çš„CNN)
  Mobile ViT (è½»é‡ViT)
  å¯èƒ½å‡ºç°CNN+Transformeræ··åˆ

ğŸ’¡ å®ç”¨å»ºè®®:

æ–°æ‰‹å­¦ä¹ :
  å…ˆå­¦CNNï¼ˆç›´è§‚ã€æ˜“ç†è§£ï¼‰
  å†å­¦ViTï¼ˆç†è§£æ³¨æ„åŠ›æœºåˆ¶ï¼‰

å®é™…é¡¹ç›®:
  âœ… é¢„è®­ç»ƒæ¨¡å‹: ä¼˜å…ˆViTï¼ˆCLIPã€LLaVAéƒ½ç”¨ï¼‰
  âœ… ä»å¤´è®­ç»ƒ: çœ‹æ•°æ®é‡å†³å®š
  âœ… ç§»åŠ¨ç«¯: CNNï¼ˆMobileNetï¼‰
  âœ… å¤šæ¨¡æ€: ViTï¼ˆæ¶æ„ç»Ÿä¸€ï¼‰
```

---

#### Q4: LLaVAä¸ºä»€ä¹ˆåªè®­ç»ƒæŠ•å½±å±‚ï¼Ÿæ•ˆæœä¼šä¸ä¼šæ‰“æŠ˜æ‰£ï¼Ÿ

**A**: è¿™æ˜¯é«˜æ•ˆå¤šæ¨¡æ€å­¦ä¹ çš„å…³é”®è®¾è®¡ï¼Œæ•ˆæœä¸ä»…ä¸æ‰“æŠ˜æ‰£ï¼Œåè€Œæ›´å¥½ï¼

```python
ğŸ¯ æ ¸å¿ƒè®¾è®¡ç†å¿µ: "ç«™åœ¨å·¨äººè‚©è†€ä¸Š"

å®Œæ•´åˆ†æ:

1ï¸âƒ£ ä¸ºä»€ä¹ˆè¦å†»ç»“CLIPè§†è§‰ç¼–ç å™¨ï¼Ÿ

CLIPå·²ç»å­¦ä¼šçš„èƒ½åŠ›:
  âœ… è¯†åˆ«æ•°åƒç§ç‰©ä½“
  âœ… ç†è§£åœºæ™¯å’Œä¸Šä¸‹æ–‡
  âœ… æå–å›¾åƒçš„è¯­ä¹‰ç‰¹å¾
  
è®­ç»ƒæˆæœ¬:
  æ•°æ®: 4äº¿å›¾æ–‡å¯¹
  æ—¶é—´: æ•°å‘¨
  è´¹ç”¨: æ•°ç™¾ä¸‡ç¾å…ƒ
  
ç»“è®º: é‡æ–°è®­ç»ƒCLIP = é‡å¤é€ è½®å­ + æµªè´¹èµ„æº

2ï¸âƒ£ ä¸ºä»€ä¹ˆè¦å†»ç»“LLaMAè¯­è¨€æ¨¡å‹ï¼Ÿ

LLaMAå·²ç»å­¦ä¼šçš„èƒ½åŠ›:
  âœ… ç”Ÿæˆæµç•…çš„æ–‡æœ¬
  âœ… ç†è§£å¤æ‚çš„è¯­è¨€é€»è¾‘
  âœ… è¿›è¡Œæ¨ç†å’Œå¯¹è¯
  
è®­ç»ƒæˆæœ¬:
  æ•°æ®: 1-2ä¸‡äº¿tokens
  æ—¶é—´: æ•°æœˆ
  è´¹ç”¨: æ•°åƒä¸‡ç¾å…ƒ
  
ç»“è®º: é‡æ–°è®­ç»ƒLLaMA = å¾—ä¸å¿å¤±

3ï¸âƒ£ æŠ•å½±å±‚éœ€è¦å­¦ä»€ä¹ˆï¼Ÿ

æ ¸å¿ƒä»»åŠ¡: æŠŠè§†è§‰ç‰¹å¾"ç¿»è¯‘"æˆè¯­è¨€æ¨¡å‹èƒ½ç†è§£çš„å½¢å¼

ç±»æ¯”:
  CLIP: ä¼šè¯´"å›¾åƒè¯­"
  LLaMA: ä¼šè¯´"æ–‡æœ¬è¯­"
  æŠ•å½±å±‚: ç¿»è¯‘å®˜ï¼ˆå›¾åƒè¯­ â†’ æ–‡æœ¬è¯­ï¼‰

æŠ•å½±å±‚æ¶æ„:
  input: CLIPç‰¹å¾ [1024ç»´]
    â†“
  Linear(1024 â†’ 4096) + GELU
    â†“
  Linear(4096 â†’ 4096)
    â†“
  output: LLaMAç‰¹å¾ [4096ç»´]

å‚æ•°é‡å¯¹æ¯”:
  CLIP: 400Må‚æ•°ï¼ˆå†»ç»“ï¼‰
  æŠ•å½±å±‚: 8Må‚æ•°ï¼ˆè®­ç»ƒï¼‰â­ ä»…å 0.1%ï¼
  LLaMA: 7Bå‚æ•°ï¼ˆå†»ç»“ï¼‰
  
è®­ç»ƒæˆæœ¬å¯¹æ¯”:
  å®Œå…¨å¾®è°ƒæ‰€æœ‰å‚æ•°:
    æ•°æ®: æ•°ç™¾ä¸‡å›¾æ–‡å¯¹
    æ—¶é—´: æ•°å‘¨
    GPU: 256Ã—A100
    æˆæœ¬: >$100ä¸‡
  
  åªè®­ç»ƒæŠ•å½±å±‚:
    æ•°æ®: 15ä¸‡æŒ‡ä»¤å¯¹
    æ—¶é—´: æ•°å°æ—¶-1å¤©
    GPU: 8Ã—A100
    æˆæœ¬: $3000-5000 â­â­â­
  
  æˆæœ¬é™ä½: 200å€ï¼

4ï¸âƒ£ æ•ˆæœä¼šæ‰“æŠ˜æ‰£å—ï¼Ÿ

å®éªŒå¯¹æ¯”ï¼ˆVQA v2æ•°æ®é›†ï¼‰:

æ–¹æ¡ˆA: å®Œå…¨å¾®è°ƒï¼ˆCLIP + æŠ•å½±å±‚ + LLaMAï¼‰
  å‡†ç¡®ç‡: 80.5%
  è®­ç»ƒæ—¶é—´: 7å¤©ï¼ˆ8Ã—A100ï¼‰
  æˆæœ¬: $20,000
  
æ–¹æ¡ˆB: åªè®­ç»ƒæŠ•å½±å±‚ï¼ˆLLaVAï¼‰
  å‡†ç¡®ç‡: 80.0%
  è®­ç»ƒæ—¶é—´: 20å°æ—¶ï¼ˆ8Ã—A100ï¼‰
  æˆæœ¬: $3,000
  
ç»“è®º: å‡†ç¡®ç‡å·®è·<1%ï¼Œä½†æˆæœ¬é™ä½7å€ï¼

ä¸ºä»€ä¹ˆæ•ˆæœä¸æ‰“æŠ˜æ‰£ï¼Ÿ
  1. é¢„è®­ç»ƒæ¨¡å‹å·²ç»å¾ˆå¼º
  2. åªéœ€è¦å­¦ä¹ æ¨¡æ€å¯¹é½
  3. é¿å…ç¾éš¾æ€§é—å¿˜
     ï¼ˆå®Œå…¨å¾®è°ƒå¯èƒ½ç ´ååŸæœ‰èƒ½åŠ›ï¼‰

5ï¸âƒ£ ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥:

é˜¶æ®µ1: ç‰¹å¾å¯¹é½é¢„è®­ç»ƒ
  æ•°æ®: 60ä¸‡å›¾æ–‡å¯¹ï¼ˆLAION-CC-SBUï¼‰
  ç›®æ ‡: è®©æŠ•å½±å±‚å­¦ä¼šåŸºæœ¬çš„å›¾åƒ-æ–‡æœ¬æ˜ å°„
  å†»ç»“: CLIP + LLaMA
  è®­ç»ƒ: æŠ•å½±å±‚
  æ—¶é—´: æ•°å°æ—¶
  
é˜¶æ®µ2: è§†è§‰æŒ‡ä»¤å¾®è°ƒ
  æ•°æ®: 15ä¸‡æŒ‡ä»¤å¯¹ï¼ˆLLaVA-Instruct-150Kï¼‰
  ç›®æ ‡: å­¦ä¼šå¯¹è¯å’Œæ¨ç†
  å†»ç»“: CLIP + LLaMAï¼ˆæˆ–éƒ¨åˆ†è§£å†»LLaMAï¼‰
  è®­ç»ƒ: æŠ•å½±å±‚ + (å¯é€‰)LLaMAæœ€åå‡ å±‚
  æ—¶é—´: 1å¤©
  
æ€»è®­ç»ƒæ—¶é—´: ~1.5å¤© vs ä»å¤´è®­ç»ƒéœ€è¦æ•°æœˆ

6ï¸âƒ£ è¿›ä¸€æ­¥ä¼˜åŒ–: LoRA

å¦‚æœæƒ³å¾®è°ƒLLaMAçš„å¯¹è¯èƒ½åŠ›ï¼Œä½¿ç”¨LoRA:

LoRAåŸç†:
  ä¸ä¿®æ”¹åŸå‚æ•° W
  æ·»åŠ ä½ç§©çŸ©é˜µ Î”W = AB
  æ–°å‚æ•° = W + Î”W
  
å‚æ•°é‡:
  LLaMAå…¨å‚æ•°: 7B
  LoRAå‚æ•°: 4M (ä»…0.06%ï¼)
  
æ•ˆæœ:
  å‡ ä¹ç­‰åŒäºå…¨é‡å¾®è°ƒ
  ä½†å‚æ•°é‡å°‘1000å€

LLaVA + LoRA:
  è®­ç»ƒå‚æ•° = æŠ•å½±å±‚(8M) + LoRA(4M) = 12M
  æ€»å‚æ•°ä¸­å æ¯” = 12M / 7400M = 0.16%
  
  æ€§èƒ½æŸå¤±: <0.5%
  è®­ç»ƒåŠ é€Ÿ: 5-10å€
  æ˜¾å­˜éœ€æ±‚: å‡å°‘50%

ğŸ’¡ æ ¸å¿ƒå¯ç¤º:

ä¸æ˜¯æ‰€æœ‰å‚æ•°éƒ½éœ€è¦è®­ç»ƒï¼
  é¢„è®­ç»ƒæ¨¡å‹ = å¼ºå¤§çš„åŸºç¡€
  åªéœ€è®­ç»ƒ = ä»»åŠ¡ç›¸å…³çš„éƒ¨åˆ†
  
å¤šæ¨¡æ€å­¦ä¹  â‰  ä»å¤´è®­ç»ƒæ‰€æœ‰ä¸œè¥¿
å¤šæ¨¡æ€å­¦ä¹  = å·§å¦™åœ°è¿æ¥å·²æœ‰èƒ½åŠ›

è¿™å°±æ˜¯LLaVAçš„æ™ºæ…§ï¼
```

---

#### Q5: å¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿä¸åŒæŒ‡æ ‡ä»£è¡¨ä»€ä¹ˆï¼Ÿ

**A**: æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç»¼åˆè¯„åˆ¤æ¨¡å‹æ€§èƒ½

```python
ğŸ“Š å¤šæ¨¡æ€è¯„ä¼°æŒ‡æ ‡å®Œå…¨æŒ‡å—

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1ï¸âƒ£ å›¾æ–‡æ£€ç´¢ä»»åŠ¡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å¸¸ç”¨æŒ‡æ ‡:
  - Recall@K: å‰Kä¸ªç»“æœä¸­æœ‰æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹
  - Mean Rank: æ­£ç¡®ç­”æ¡ˆçš„å¹³å‡æ’å
  - Median Rank: æ­£ç¡®ç­”æ¡ˆçš„ä¸­ä½æ’å

è¯¦ç»†è§£é‡Š:

Recall@K:
  å®šä¹‰: åœ¨å‰Kä¸ªæ£€ç´¢ç»“æœä¸­æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æŸ¥è¯¢å æ¯”
  
  ä¾‹å­:
  æŸ¥è¯¢: "çº¢è‰²æ±½è½¦"
  å€™é€‰: 1000å¼ å›¾ç‰‡
  æ­£ç¡®ç­”æ¡ˆ: å›¾ç‰‡#453
  
  æ¨¡å‹æ’åº: [#123, #453, #789, ...]
             â†‘ç¬¬1  â†‘ç¬¬2
  
  Recall@1 = 0 (ç¬¬1ä¸ªä¸æ˜¯æ­£ç¡®ç­”æ¡ˆ)
  Recall@5 = 1 (å‰5ä¸ªä¸­æœ‰æ­£ç¡®ç­”æ¡ˆ)
  
  å¥½ååˆ¤æ–­:
  Recall@5 > 80%: ä¼˜ç§€ âœ…
  Recall@5 60-80%: è‰¯å¥½ âš–ï¸
  Recall@5 < 60%: è¾ƒå·® âŒ

Mean Rank vs Median Rank:
  Mean Rank = æ‰€æœ‰æŸ¥è¯¢çš„æ’åå¹³å‡å€¼
  Median Rank = æ’åçš„ä¸­ä½æ•°
  
  ä¾‹å­:
  100ä¸ªæŸ¥è¯¢ï¼Œæ­£ç¡®ç­”æ¡ˆæ’å:
  [1, 1, 2, 1, 3, 2, 1, ..., 150]
        â†‘ å¤§éƒ¨åˆ†å¾ˆé å‰   â†‘æœ‰ä¸ªåˆ«ç‰¹åˆ«é å
  
  Mean Rank = 5.8 (å—æç«¯å€¼å½±å“)
  Median Rank = 2.0 (æ›´èƒ½åæ˜ å…¸å‹æƒ…å†µ)
  
  å»ºè®®: ä¸¤ä¸ªéƒ½çœ‹ï¼ŒMedianæ›´é²æ£’

å®é™…æ¡ˆä¾‹ï¼ˆCOCO 5K testï¼‰:
  CLIP ViT-B/32:
    R@1 = 58.4%
    R@5 = 81.5%
    R@10 = 88.1%
    Mean Rank = 2.3
  
  è§£è¯»:
  âœ… 58%çš„æŸ¥è¯¢ç¬¬1ä¸ªå°±å¯¹
  âœ… 88%çš„æŸ¥è¯¢å‰10ä¸ªèƒ½æ‰¾åˆ°
  âœ… å¹³å‡ç¬¬2-3ä¸ªå°±æ˜¯æ­£ç¡®ç­”æ¡ˆ
  
  ç»“è®º: æ€§èƒ½ä¼˜ç§€ï¼

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2ï¸âƒ£ å›¾åƒæè¿°ä»»åŠ¡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å¸¸ç”¨æŒ‡æ ‡:
  - BLEU: n-gramé‡å 
  - METEOR: è€ƒè™‘åŒä¹‰è¯
  - CIDEr: ä¸“ä¸ºå›¾åƒæè¿°è®¾è®¡
  - SPICE: è¯­ä¹‰ç›¸ä¼¼åº¦

BLEU (Bilingual Evaluation Understudy):
  åŸç†: è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬çš„n-gramé‡å æ¯”ä¾‹
  
  ä¾‹å­:
  å‚è€ƒ: "a cat is sitting on a red mat"
  ç”Ÿæˆ: "a cat sitting on a mat"
  
  1-gramåŒ¹é…: 6/6 = 100%
  2-gramåŒ¹é…: 4/5 = 80%
  3-gramåŒ¹é…: 3/4 = 75%
  4-gramåŒ¹é…: 2/3 = 67%
  
  BLEU-4 = å‡ ä½•å¹³å‡ â‰ˆ 80.5
  
  å¥½ååˆ¤æ–­:
  BLEU-4 > 40: ä¼˜ç§€ âœ…
  BLEU-4 30-40: è‰¯å¥½ âš–ï¸
  BLEU-4 < 30: è¾ƒå·® âŒ
  
  å±€é™: åªçœ‹è¯æ±‡é‡å ï¼Œä¸è€ƒè™‘è¯­ä¹‰

CIDEr (Consensus-based Description Evaluation):
  åŸç†: è¡¡é‡ç”Ÿæˆæè¿°ä¸å¤šä¸ªå‚è€ƒæè¿°çš„å…±è¯†
  
  ç‰¹ç‚¹:
  âœ… ä¸“é—¨ä¸ºå›¾åƒæè¿°è®¾è®¡
  âœ… è€ƒè™‘å¤šä¸ªå‚è€ƒç­”æ¡ˆ
  âœ… å›¾åƒç‰¹å®šè¯æ±‡æƒé‡æ›´é«˜
  âœ… ä¸äººç±»è¯„åˆ†ç›¸å…³æ€§æœ€é«˜
  
  ä¾‹å­:
  å‚è€ƒ1: "a cat on a mat"
  å‚è€ƒ2: "an orange cat sitting"
  å‚è€ƒ3: "a feline resting on a rug"
  
  ç”Ÿæˆ: "an orange cat on a mat"
  
  CIDErä¼š:
  1. æå–TF-IDFç‰¹å¾
  2. è®¡ç®—ä¸æ¯ä¸ªå‚è€ƒçš„ç›¸ä¼¼åº¦
  3. ç»¼åˆå¾—åˆ†
  
  CIDErèŒƒå›´: 0-10+
  >1.5: ä¼˜ç§€ âœ…
  1.0-1.5: è‰¯å¥½ âš–ï¸
  <1.0: è¾ƒå·® âŒ

SPICE (Semantic Propositional Evaluation):
  åŸç†: æ¯”è¾ƒè¯­ä¹‰å›¾ï¼ˆsemantic graphsï¼‰
  
  ä¾‹å­:
  å‚è€ƒ: "a black dog playing with a red ball"
  
  è¯­ä¹‰å›¾:
  Objects: {dog(black), ball(red)}
  Relations: {playing(dog, ball)}
  Attributes: {black(dog), red(ball)}
  
  ç”Ÿæˆ1: "a dog plays with a ball"
  â†’ Objects: âœ…  Relations: âœ…  Attributes: âŒ
  â†’ SPICE: 0.67
  
  ç”Ÿæˆ2: "a black canine and a red sphere"
  â†’ Objects: âœ…(åŒä¹‰)  Attributes: âœ…  Relations: âŒ
  â†’ SPICE: 0.67
  
  ä¼˜åŠ¿: å…³æ³¨è¯­ä¹‰è€Œéè¡¨é¢æ–‡å­—

å®é™…æ¡ˆä¾‹ï¼ˆCOCO testï¼‰:
  LLaVA-1.5-13B:
    BLEU-4: 43.8
    METEOR: 30.5
    CIDEr: 151.2
    SPICE: 26.1
  
  GPT-4V:
    BLEU-4: 47.2
    CIDEr: 168.0
    SPICE: 29.3
  
  è§£è¯»: GPT-4Våœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½æ›´ä¼˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3ï¸âƒ£ è§†è§‰é—®ç­” (VQA) ä»»åŠ¡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ä¸»è¦æŒ‡æ ‡: VQA Accuracy

VQAç‰¹æ®Šè§„åˆ™:
  å› ä¸ºç­”æ¡ˆå¯èƒ½æœ‰å¤šç§è¡¨è¾¾ï¼Œè€ƒè™‘å¤šä¸ªæ ‡æ³¨è€…

VQA Scoreå…¬å¼:
  score = min(matching_answers / 3, 1.0)

ä¾‹å­:
  é—®é¢˜: "What color is the cat?"
  
  10ä¸ªäººç±»æ ‡æ³¨:
  "orange" Ã— 7
  "ginger" Ã— 2
  "tawny" Ã— 1
  
  æ¨¡å‹å›ç­” "orange":
  score = min(7/3, 1.0) = 1.0 âœ…
  
  æ¨¡å‹å›ç­” "ginger":
  score = min(2/3, 1.0) = 0.67 âš–ï¸
  
  æ¨¡å‹å›ç­” "blue":
  score = min(0/3, 1.0) = 0.0 âŒ

æ€§èƒ½æ ‡å‡†:
  >80%: SOTAæ°´å¹³ â­â­â­
  70-80%: ä¼˜ç§€ âœ…
  60-70%: è‰¯å¥½ âš–ï¸
  <60%: éœ€è¦æ”¹è¿› âŒ

å®é™…æ¡ˆä¾‹ï¼ˆVQA v2ï¼‰:
  CLIP baseline: 45.2%
  BLIP: 65.3%
  LLaVA-7B: 78.5%
  LLaVA-13B: 80.0%
  GPT-4V: 87.2%
  
  äººç±»è¡¨ç°: 91-95%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4ï¸âƒ£ æ–‡ç”Ÿå›¾ä»»åŠ¡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å¸¸ç”¨æŒ‡æ ‡:
  - FID: å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§
  - CLIP Score: å›¾æ–‡ä¸€è‡´æ€§
  - Inception Score: è´¨é‡Ã—å¤šæ ·æ€§

FID (FrÃ©chet Inception Distance):
  åŸç†: æµ‹é‡ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒåœ¨ç‰¹å¾ç©ºé—´çš„è·ç¦»
  
  è®¡ç®—æ­¥éª¤:
  1. ç”¨Inceptionç½‘ç»œæå–ç‰¹å¾
  2. è®¡ç®—çœŸå®å›¾åƒçš„ç‰¹å¾åˆ†å¸ƒ N(Î¼â‚, Î£â‚)
  3. è®¡ç®—ç”Ÿæˆå›¾åƒçš„ç‰¹å¾åˆ†å¸ƒ N(Î¼â‚‚, Î£â‚‚)
  4. FID = ||Î¼â‚-Î¼â‚‚||Â² + Tr(Î£â‚+Î£â‚‚-2âˆš(Î£â‚Î£â‚‚))
  
  ç›´è§‚ç†è§£:
  FID = ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„"è·ç¦»"
  è¶Šå° = ç”Ÿæˆå›¾åƒè¶Šæ¥è¿‘çœŸå®å›¾åƒ
  
  æ€§èƒ½æ ‡å‡†:
  <5: SOTA â­â­â­
  5-15: ä¼˜ç§€ âœ…
  15-50: å¯ç”¨ âš–ï¸
  >50: è¾ƒå·® âŒ
  
  å®é™…æ¡ˆä¾‹:
  DALL-E 3: FID = 5.4
  SDXL: FID = 7.2
  SD 2.1: FID = 9.8
  SD 1.5: FID = 12.6

CLIP Score:
  åŸç†: ç”¨CLIPè¡¡é‡ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„åŒ¹é…åº¦
  
  è®¡ç®—:
  CLIP_Score = cos_similarity(
    CLIP_image(generated_image),
    CLIP_text(text_prompt)
  )
  
  æ€§èƒ½æ ‡å‡†:
  >0.35: éå¸¸åŒ¹é… âœ…
  0.30-0.35: åŒ¹é… âš–ï¸
  <0.30: ä¸åŒ¹é… âŒ
  
  ä¾‹å­:
  Prompt: "a red car in the street"
  Generated: [å›¾åƒ]
  CLIP Score: 0.33 â†’ åŒ¹é…åº¦è‰¯å¥½

Inception Score (IS):
  åŸç†: åŒæ—¶è¡¡é‡è´¨é‡å’Œå¤šæ ·æ€§
  
  å…¬å¼:
  IS = exp(E[KL(p(y|x) || p(y))])
  
  ç›´è§‚ç†è§£:
  - p(y|x): æ¯å¼ å›¾çš„ç±»åˆ«åˆ†å¸ƒï¼ˆæ¸…æ™°åº¦ï¼‰
  - p(y): æ‰€æœ‰å›¾çš„ç±»åˆ«åˆ†å¸ƒï¼ˆå¤šæ ·æ€§ï¼‰
  - ISé«˜ = æ¯å¼ å›¾æ¸…æ™° + æ•´ä½“å¤šæ ·
  
  æ€§èƒ½æ ‡å‡†:
  >15: ä¼˜ç§€ âœ…
  10-15: è‰¯å¥½ âš–ï¸
  <10: è¾ƒå·® âŒ

ğŸ’¡ å®ç”¨å»ºè®®:

è¯„ä¼°æœ€ä½³å®è·µ:
  1. âœ… ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ï¼ˆä¸è¦åªçœ‹ä¸€ä¸ªï¼‰
  2. âœ… ä¸äººç±»è¯„ä¼°ç»“åˆ
  3. âœ… åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šæµ‹è¯•ï¼ˆå¯å¯¹æ¯”ï¼‰
  4. âœ… è®°å½•è®¡ç®—ç¯å¢ƒï¼ˆä¿è¯å¯å¤ç°ï¼‰
  5. âŒ ä¸è¦è¿‡åº¦ä¼˜åŒ–å•ä¸€æŒ‡æ ‡

ä¸åŒä»»åŠ¡çš„æ ¸å¿ƒæŒ‡æ ‡:
  å›¾æ–‡æ£€ç´¢ â†’ Recall@5
  å›¾åƒæè¿° â†’ CIDEr
  VQA â†’ Accuracy
  æ–‡ç”Ÿå›¾ â†’ FID + CLIP Score
```

---

#### Q6: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿå¦‚ä½•åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œå¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ

**A**: å¤šç§ä¼˜åŒ–æŠ€å·§å¯ä»¥æ˜¾è‘—é™ä½æ˜¾å­˜éœ€æ±‚

```python
ğŸ¯ æ˜¾å­˜ä¼˜åŒ–å®Œå…¨æŒ‡å—

é—®é¢˜åˆ†æ:
  LLaVA-13B: éœ€è¦26GBæ˜¾å­˜ï¼ˆFP32ï¼‰
  æ¶ˆè´¹çº§GPU: åªæœ‰8-16GB
  
  å·®è·å¤ªå¤§ï¼Œæ€ä¹ˆåŠï¼Ÿ

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ–¹æ¡ˆ1: é‡åŒ–ï¼ˆæœ€æœ‰æ•ˆï¼ï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

åŸç†: é™ä½æ•°å€¼ç²¾åº¦

FP32 (32ä½æµ®ç‚¹)  â†’ 4å­—èŠ‚/å‚æ•°
FP16 (16ä½æµ®ç‚¹)  â†’ 2å­—èŠ‚/å‚æ•° (å‡åŠï¼)
INT8 (8ä½æ•´æ•°)   â†’ 1å­—èŠ‚/å‚æ•° (å‡75%ï¼)
INT4 (4ä½æ•´æ•°)   â†’ 0.5å­—èŠ‚/å‚æ•° (å‡87.5%ï¼)

å®è·µä»£ç :

# æ–¹æ³•1: FP16åŠç²¾åº¦
model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-13b-hf",
    torch_dtype=torch.float16  # â­ æ˜¾å­˜å‡åŠ
).to("cuda")

æ˜¾å­˜: 26GB â†’ 13GB

# æ–¹æ³•2: 8-bité‡åŒ–ï¼ˆæœ€æ¨èï¼ï¼‰
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-13b-hf",
    quantization_config=quantization_config
)

æ˜¾å­˜: 26GB â†’ 7GB â­â­â­
æ€§èƒ½æŸå¤±: <1%

# æ–¹æ³•3: 4-bité‡åŒ–ï¼ˆæé™ä¼˜åŒ–ï¼‰
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_use_double_quant=True  # åŒé‡é‡åŒ–
)

model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-13b-hf",
    quantization_config=quantization_config
)

æ˜¾å­˜: 26GB â†’ 4GB â­â­â­â­â­
æ€§èƒ½æŸå¤±: 1-2%

æ•ˆæœå¯¹æ¯”:
               æ˜¾å­˜éœ€æ±‚    æ€§èƒ½      æ¨ç†é€Ÿåº¦
FP32 (åŸå§‹)    26GB       100%      1.0x
FP16           13GB       99.9%     1.8x âš¡
INT8           7GB        99.5%     0.9x
INT4           4GB        98.5%     0.7x

ç»“è®º: INT8æ˜¯æœ€ä½³å¹³è¡¡ç‚¹ï¼

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ–¹æ¡ˆ2: CPU Offloadingï¼ˆæ˜¾å­˜ä¸å¤Ÿï¼Œå†…å­˜æ¥å‡‘ï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

åŸç†: éƒ¨åˆ†å‚æ•°æ”¾CPUï¼Œéœ€è¦æ—¶å†ä¼ åˆ°GPU

from accelerate import load_checkpoint_and_dispatch

model = load_checkpoint_and_dispatch(
    model,
    checkpoint_path,
    device_map="auto",  # â­ è‡ªåŠ¨åˆ†é…
    max_memory={
        0: "10GB",  # GPU 0æœ€å¤šç”¨10GB
        "cpu": "30GB"  # CPUå¯ç”¨30GB
    }
)

æ•ˆæœ:
  GPUæ˜¾å­˜: 10GB
  æ€»æ¨¡å‹: 26GB
  å¤šå‡ºçš„16GB â†’ CPUå†…å­˜

ç¼ºç‚¹:
  CPU â†” GPUæ•°æ®ä¼ è¾“æ…¢
  æ¨ç†é€Ÿåº¦é™ä½3-5å€

é€‚ç”¨åœºæ™¯:
  âœ… ç¦»çº¿æ¨ç†ï¼ˆå¯¹é€Ÿåº¦è¦æ±‚ä¸é«˜ï¼‰
  âŒ å®æ—¶åº”ç”¨

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ–¹æ¡ˆ3: Gradient Checkpointingï¼ˆè®­ç»ƒæ—¶ï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

åŸç†: ä¸ä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ï¼Œéœ€è¦æ—¶é‡æ–°è®¡ç®—

model.gradient_checkpointing_enable()

æ•ˆæœ:
  æ˜¾å­˜: å‡å°‘30-50%
  è®­ç»ƒæ—¶é—´: å¢åŠ 20-30%

é€‚ç”¨: ä»…è®­ç»ƒæ—¶ï¼Œæ¨ç†ä¸éœ€è¦

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ–¹æ¡ˆ4: æ‰¹é‡å¤§å°è°ƒæ•´
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# æ¨ç†æ—¶
batch_size = 1  # æœ€å°batch

# è®­ç»ƒæ—¶ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
for batch in dataloader:
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if step % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

æ•ˆæœ:
  batch_size=4 â†’ 1: æ˜¾å­˜å‡å°‘75%
  æ¢¯åº¦ç´¯ç§¯: ä¿æŒç­‰æ•ˆbatch size

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ–¹æ¡ˆ5: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ¨¡å‹å¤§å°å¯¹æ¯”:
  LLaVA-13B: 13GB (FP16)
  LLaVA-7B: 7GB (FP16)  â­ å‡åŠ
  BLIP-2: 4GB (FP16)
  CLIP: 1GB (FP16)

æ€§èƒ½å¯¹æ¯” (VQA v2):
  LLaVA-13B: 80.0%
  LLaVA-7B: 78.5%  (-1.5%)
  BLIP-2: 75.8%  (-4.2%)

å»ºè®®:
  å¦‚æœæ€§èƒ½å·®è·<5%ï¼Œç”¨å°æ¨¡å‹ï¼

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ–¹æ¡ˆ6: Flash Attentionï¼ˆæ³¨æ„åŠ›ä¼˜åŒ–ï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

pip install flash-attn

model = LlavaForConditionalGeneration.from_pretrained(
    model_name,
    attn_implementation="flash_attention_2"
)

æ•ˆæœ:
  æ˜¾å­˜: å‡å°‘20-30%
  é€Ÿåº¦: æå‡2-3å€ âš¡âš¡

é™åˆ¶: éœ€è¦A100/H100 GPU

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆï¼ˆå®æˆ˜ç»„åˆï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# åœºæ™¯1: 8GB GPUæ¨ç†
é…ç½®:
  - 4-bité‡åŒ–
  - batch_size=1
  - FP16è®¡ç®—

model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-7b-hf",  # ç”¨7Bè€Œä¸æ˜¯13B
    quantization_config=BitsAndBytesConfig(load_in_4bit=True),
    torch_dtype=torch.float16
)

æ˜¾å­˜: 4GB âœ…
æ€§èƒ½: 97% ofåŸå§‹

# åœºæ™¯2: 16GB GPUæ¨ç†
é…ç½®:
  - 8-bité‡åŒ–
  - batch_size=1-2

model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-13b-hf",  # å¯ä»¥ç”¨13Bäº†
    quantization_config=BitsAndBytesConfig(load_in_8bit=True)
)

æ˜¾å­˜: 7-10GB âœ…
æ€§èƒ½: 99% ofåŸå§‹

# åœºæ™¯3: 24GB GPUè®­ç»ƒ
é…ç½®:
  - FP16
  - LoRAå¾®è°ƒ
  - Gradient checkpointing
  - æ¢¯åº¦ç´¯ç§¯

model = model.half()
model.gradient_checkpointing_enable()

from peft import get_peft_model, LoraConfig

lora_config = LoraConfig(r=8, lora_alpha=16)
model = get_peft_model(model, lora_config)

# åªè®­ç»ƒ4Må‚æ•°ï¼Œè€Œä¸æ˜¯13Bï¼

æ˜¾å­˜: 18GB âœ…
è®­ç»ƒé€Ÿåº¦: å¯æ¥å—

ğŸ’¡ å®ç”¨å»ºè®®:

1. å…ˆé‡åŒ–ï¼ˆINT8ï¼‰â†’ ç«‹ç«¿è§å½±
2. å†è°ƒbatch size â†’ è¿›ä¸€æ­¥é™ä½
3. å¦‚æœè¿˜ä¸å¤Ÿï¼Œç”¨å°æ¨¡å‹ â†’ LLaVA-7B
4. å®åœ¨ä¸è¡Œï¼Œç”¨API â†’ GPT-4V

è®°ä½: é‡åŒ–å‡ ä¹æ˜¯å…è´¹çš„åˆé¤ï¼
æ€§èƒ½æŸå¤±<2%ï¼Œæ˜¾å­˜å‡å°‘75%
```

---

#### Q7: å¤šæ¨¡æ€æ¨¡å‹æœ‰å“ªäº›å±€é™æ€§ï¼Ÿå¦‚ä½•æ”¹è¿›ï¼Ÿ

**A**: å½“å‰å¤šæ¨¡æ€æ¨¡å‹å­˜åœ¨å¤šä¸ªæŒ‘æˆ˜ï¼Œä½†ç¤¾åŒºæ­£åœ¨ç§¯æè§£å†³

```python
ğŸ¯ å¤šæ¨¡æ€æ¨¡å‹çš„7å¤§å±€é™æ€§åŠæ”¹è¿›æ–¹å‘

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å±€é™1: å¹»è§‰é—®é¢˜ï¼ˆHallucinationï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜æè¿°:
  æ¨¡å‹ä¼š"ç¼–é€ "å›¾ç‰‡ä¸­ä¸å­˜åœ¨çš„å†…å®¹

ä¾‹å­:
  å›¾ç‰‡: [ä¸€åªçŒ«åç€]
  
  é—®: "å›¾ä¸­æœ‰å‡ åªç‹—ï¼Ÿ"
  æ¨¡å‹: "å›¾ä¸­æœ‰ä¸¤åªç‹—ï¼Œä¸€åªæ˜¯æ£•è‰²çš„..."
        â†‘ å®Œå…¨é”™è¯¯ï¼å›¾ä¸­æ ¹æœ¬æ²¡æœ‰ç‹—

åŸå› :
  1. è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆæƒ¯æ€§
  2. è®­ç»ƒæ•°æ®ä¸­çš„åè§
  3. è§†è§‰ç†è§£ä¸å¤Ÿå‡†ç¡®
  4. ç¼ºä¹"ä¸çŸ¥é“"çš„èƒ½åŠ›

æ”¹è¿›æ–¹å‘:
  âœ… å¯¹æ¯”å­¦ä¹ å¢å¼ºè§†è§‰grounding
  âœ… æ·»åŠ "ä¸ç¡®å®š"è¾“å‡º
  âœ… ä½¿ç”¨è§†è§‰æŒ‡é’ˆï¼ˆæŒ‡å‘å›¾åƒåŒºåŸŸï¼‰
  âœ… RLHFå¯¹é½ï¼ˆä¸‹ä¸€ç« å†…å®¹ï¼ï¼‰

ç ”ç©¶è¿›å±•:
  - POPE (Polling-based Object Probing)
  - LRV-Instruction (Liu et al., 2023)
  - è¯†åˆ«ç‡æå‡: 60% â†’ 80%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å±€é™2: ç»†ç²’åº¦ç†è§£ä¸è¶³
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜æè¿°:
  å¯¹ç»†èŠ‚ã€å°ç‰©ä½“ã€æ–‡å­—çš„è¯†åˆ«èƒ½åŠ›å¼±

ä¾‹å­:
  å›¾ç‰‡: [è¡—æ™¯ï¼Œè¿œå¤„æœ‰ä¸ªè·¯ç‰Œ]
  
  é—®: "è·¯ç‰Œä¸Šå†™çš„æ˜¯ä»€ä¹ˆï¼Ÿ"
  æ¨¡å‹: "æŠ±æ­‰ï¼Œæˆ‘çœ‹ä¸æ¸…..."
  
  åŸå› :
  1. å›¾åƒåˆ†è¾¨ç‡é™åˆ¶ï¼ˆ224Ã—224â†’14Ã—14 patchesï¼‰
  2. ViTçš„patchå¤ªå¤§ï¼ˆ16Ã—16åƒç´ ï¼‰
  3. å°ç‰©ä½“ä¿¡æ¯ä¸¢å¤±

æ”¹è¿›æ–¹å‘:
  âœ… æ›´é«˜åˆ†è¾¨ç‡è¾“å…¥ï¼ˆ336Ã—336, 448Ã—448ï¼‰
  âœ… å¤šå°ºåº¦ç‰¹å¾èåˆ
  âœ… ä¸“é—¨çš„OCRæ¨¡å—
  âœ… åŒºåŸŸzoom-inæœºåˆ¶

ç ”ç©¶è¿›å±•:
  - LLaVA-1.5: æ”¯æŒ336Ã—336
  - Qwen-VL: åŠ¨æ€åˆ†è¾¨ç‡
  - mPLUG-Owl: å¤šå°ºåº¦ViT

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å±€é™3: æ—¶åºç†è§£èƒ½åŠ›å¼±ï¼ˆè§†é¢‘ï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜æè¿°:
  éš¾ä»¥ç†è§£åŠ¨ä½œã€å› æœå…³ç³»ã€æ—¶é—´é¡ºåº

ä¾‹å­:
  è§†é¢‘: [äººå…ˆæ‹¿èµ·æ¯å­ï¼Œç„¶åå–æ°´ï¼Œæœ€åæ”¾ä¸‹]
  
  é—®: "äººåœ¨å–æ°´ä¹‹å‰åšäº†ä»€ä¹ˆï¼Ÿ"
  æ¨¡å‹: å¯èƒ½ç­”ä¸å¯¹æˆ–æ··æ·†é¡ºåº

åŸå› :
  1. åªçœ‹å•å¸§æˆ–ç¨€ç–å¸§
  2. ç¼ºä¹æ—¶åºå»ºæ¨¡
  3. è§†é¢‘æ•°æ®ç¨€ç¼º

æ”¹è¿›æ–¹å‘:
  âœ… æ—¶åºTransformer
  âœ… 3Då·ç§¯
  âœ… è®°å¿†æœºåˆ¶
  âœ… æ›´å¤šè§†é¢‘æ•°æ®

ç ”ç©¶è¿›å±•:
  - Video-ChatGPT
  - VideoLLaMA
  - LLaMA-VID

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å±€é™4: æ•°æ®åè§å’Œå…¬å¹³æ€§é—®é¢˜
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜æè¿°:
  è®­ç»ƒæ•°æ®çš„åè§å¯¼è‡´æ¨¡å‹æœ‰åè§

ä¾‹å­:
  CLIPå¯¹ä¸åŒè‚¤è‰²çš„äººè¯†åˆ«å‡†ç¡®ç‡ä¸åŒ
  DALL-Eç”Ÿæˆçš„"CEO"å¤šä¸ºç™½äººç”·æ€§

åŸå› :
  è®­ç»ƒæ•°æ®æ¥è‡ªäº’è”ç½‘ â†’ åæ˜ ç¤¾ä¼šåè§

æ”¹è¿›æ–¹å‘:
  âœ… æ•°æ®å¤šæ ·æ€§å®¡æŸ¥
  âœ… å»åè§ç®—æ³•
  âœ… å…¬å¹³æ€§è¯„ä¼°åŸºå‡†
  âœ… å¯æ§ç”Ÿæˆ

ç ”ç©¶è¿›å±•:
  - FairVLM benchmark
  - Debiasing techniques
  - æ„è¯†æ­£åœ¨æé«˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å±€é™5: è®¡ç®—æˆæœ¬é«˜
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜æè¿°:
  è®­ç»ƒå’Œæ¨ç†éƒ½å¾ˆè´µ

è®­ç»ƒæˆæœ¬:
  CLIP: æ•°ç™¾ä¸‡ç¾å…ƒï¼ˆ4äº¿å›¾æ–‡å¯¹ï¼‰
  LLaVA: æ•°åƒç¾å…ƒï¼ˆä½†éœ€è¦CLIPå’ŒLLaMAï¼‰
  Stable Diffusion: æ•°åä¸‡ç¾å…ƒ

æ¨ç†æˆæœ¬:
  GPT-4V: $0.01-0.03/å›¾
  LLaVA-13B: éœ€è¦24GB GPU

æ”¹è¿›æ–¹å‘:
  âœ… æ¨¡å‹å‹ç¼©ï¼ˆé‡åŒ–ã€å‰ªæï¼‰
  âœ… çŸ¥è¯†è’¸é¦
  âœ… MoEï¼ˆç¨€ç–æ¿€æ´»ï¼‰
  âœ… é«˜æ•ˆæ¶æ„è®¾è®¡

ç ”ç©¶è¿›å±•:
  - TinyLLaVA (3Bå‚æ•°)
  - MobileVLM (ç§»åŠ¨ç«¯)
  - QLoRA (é«˜æ•ˆå¾®è°ƒ)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å±€é™6: ç¼ºä¹æ¨ç†å’Œå¸¸è¯†
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜æè¿°:
  æ— æ³•è¿›è¡Œå¤æ‚æ¨ç†

ä¾‹å­:
  å›¾ç‰‡: [ç©ºæ¯å­å€’æ‰£åœ¨æ¡Œä¸Š]
  
  é—®: "å¦‚æœæˆ‘å¾€è¿™ä¸ªæ¯å­é‡Œå€’æ°´ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ"
  æ¨¡å‹: å¯èƒ½ç­”ä¸å‡º"æ°´ä¼šæ´’"

åŸå› :
  ç¼ºä¹ç‰©ç†å¸¸è¯†å’Œå› æœæ¨ç†èƒ½åŠ›

æ”¹è¿›æ–¹å‘:
  âœ… æ€ç»´é“¾æç¤º
  âœ… å·¥å…·ä½¿ç”¨ï¼ˆè°ƒç”¨ç‰©ç†å¼•æ“ï¼‰
  âœ… çŸ¥è¯†å›¾è°±é›†æˆ
  âœ… å¤šæ­¥æ¨ç†è®­ç»ƒ

ç ”ç©¶è¿›å±•:
  - Visual CoT (Chain of Thought)
  - ViperGPT (ç¨‹åºåŒ–æ¨ç†)
  - è¿›å±•ç¼“æ…¢ï¼Œä»æ˜¯å¼€æ”¾é—®é¢˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
å±€é™7: å®‰å…¨æ€§é—®é¢˜
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜æè¿°:
  å¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹æˆ–è¢«æ¶æ„åˆ©ç”¨

é£é™©:
  1. ç”Ÿæˆè™šå‡ä¿¡æ¯ï¼ˆdeepfakeï¼‰
  2. ä¾µçŠ¯éšç§ï¼ˆè¯†åˆ«äººè„¸ï¼‰
  3. ç»•è¿‡å®¡æ ¸ï¼ˆå¯¹æŠ—æ ·æœ¬ï¼‰
  4. ç‰ˆæƒé—®é¢˜ï¼ˆç”Ÿæˆç±»ä¼¼ä½œå“ï¼‰

æ”¹è¿›æ–¹å‘:
  âœ… å†…å®¹è¿‡æ»¤
  âœ… æ°´å°æŠ€æœ¯
  âœ… å¯¹æŠ—è®­ç»ƒ
  âœ… ä½¿ç”¨è§„èŒƒå’Œæ³•å¾‹

ç ”ç©¶è¿›å±•:
  - Safety benchmarks
  - Red teaming
  - Responsible AI guidelines

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æœªæ¥å±•æœ›
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

çŸ­æœŸï¼ˆ1-2å¹´ï¼‰:
  âœ… å¹»è§‰é—®é¢˜æ˜¾è‘—æ”¹å–„
  âœ… æ›´é«˜åˆ†è¾¨ç‡ã€æ›´é•¿è§†é¢‘
  âœ… æ›´é«˜æ•ˆçš„æ¨¡å‹ï¼ˆåœ¨æ‰‹æœºä¸Šè¿è¡Œï¼‰

ä¸­æœŸï¼ˆ3-5å¹´ï¼‰:
  âœ… çœŸæ­£çš„æ¨ç†èƒ½åŠ›
  âœ… å¤šæ¨¡æ€ç»Ÿä¸€æ¶æ„æˆç†Ÿ
  âœ… å®æ—¶ã€ä½æˆæœ¬æ¨ç†

é•¿æœŸï¼ˆ5-10å¹´ï¼‰:
  âœ… AGIçº§åˆ«çš„å¤šæ¨¡æ€ç†è§£
  âœ… å…·èº«æ™ºèƒ½ï¼ˆæœºå™¨äººï¼‰
  âœ… å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹

ğŸ’¡ å¯¹å¼€å‘è€…çš„å»ºè®®:

1. æ„è¯†åˆ°å±€é™æ€§
   ä¸è¦è¿‡åº¦ä¿¡ä»»æ¨¡å‹è¾“å‡º
   
2. ä½¿ç”¨ç»„åˆæ–¹æ¡ˆ
   å¤šæ¨¡æ€ + ä¼ ç»ŸCV + è§„åˆ™ç³»ç»Ÿ
   
3. æŒç»­å…³æ³¨è¿›å±•
   è¿™ä¸ªé¢†åŸŸå‘å±•æå¿«ï¼
   
4. å‚ä¸ç¤¾åŒº
   å¸®åŠ©å‘ç°å’Œè§£å†³é—®é¢˜
```

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬11ç« ï¼**

ä½ ç°åœ¨å·²ç»æŒæ¡äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä»CLIPåˆ°LLaVAï¼Œä»å›¾æ–‡æ£€ç´¢åˆ°è§†è§‰é—®ç­”ï¼Œä»æ–‡ç”Ÿå›¾åˆ°è§†é¢‘ç†è§£ï¼Œä½ å·²ç»å…·å¤‡äº†æ„å»ºå¤šæ¨¡æ€AIåº”ç”¨çš„èƒ½åŠ›ã€‚

å¤šæ¨¡æ€æ˜¯AIçš„æœªæ¥â€”â€”è®©æœºå™¨åƒäººç±»ä¸€æ ·ï¼Œç”¨çœ¼ç›çœ‹ã€ç”¨è€³æœµå¬ã€ç”¨è¯­è¨€è¡¨è¾¾ã€‚ä½ å·²ç»ç«™åœ¨äº†è¿™ä¸ªæ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸçš„å…¥å£ï¼

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼** â†’ [12_mixture_of_experts.md](12_mixture_of_experts.md)
