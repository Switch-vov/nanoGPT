# ç¬¬11ç« ï¼šå¤šæ¨¡æ€æ¨¡å‹å®Œå…¨æŒ‡å—

> **å­¦ä¹ ç›®æ ‡**: ç†è§£å¦‚ä½•èåˆè§†è§‰ã€è¯­è¨€ç­‰å¤šç§æ¨¡æ€çš„ä¿¡æ¯  
> **éš¾åº¦ç­‰çº§**: ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ é«˜çº§ï¼ˆå‰æ²¿æŠ€æœ¯ï¼‰  
> **é¢„è®¡æ—¶é—´**: 5-6å°æ—¶  
> **å‰ç½®çŸ¥è¯†**: 05æ¨¡å‹æ¶æ„åŸºç¡€

## ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£å¤šæ¨¡æ€å­¦ä¹ çš„åŸºæœ¬åŸç†
- âœ… æŒæ¡CLIPã€ViTç­‰è§†è§‰ç¼–ç å™¨
- âœ… ç†è§£è§†è§‰-è¯­è¨€å¯¹é½æŠ€æœ¯
- âœ… æŒæ¡LLaVAç­‰è§†è§‰è¯­è¨€æ¨¡å‹
- âœ… äº†è§£æ–‡ç”Ÿå›¾ã€è§†é¢‘ç†è§£ç­‰åº”ç”¨
- âœ… èƒ½å¤Ÿæ„å»ºç®€å•çš„å¤šæ¨¡æ€æ¨¡å‹

## ğŸ’­ å¼€å§‹ä¹‹å‰ï¼šä¸ºä»€ä¹ˆè¦å­¦è¿™ä¸ªï¼Ÿ

**åœºæ™¯**ï¼šå•ä¸€æ¨¡æ€æœ‰å±€é™ï¼Œå¤šæ¨¡æ€æ‰èƒ½å®Œæ•´ç†è§£ä¸–ç•Œã€‚

**æ¯”å–»**ï¼šå°±åƒäººçš„æ„Ÿå®˜ï¼š
- ğŸ‘ï¸ çœ¼ç›çœ‹ï¼šè§†è§‰ä¿¡æ¯
- ğŸ‘‚ è€³æœµå¬ï¼šå¬è§‰ä¿¡æ¯
- ğŸ§  å¤§è„‘ï¼šç»¼åˆç†è§£

**å­¦å®Œä¹‹å**ï¼š
- âœ… ç†è§£å¤šæ¨¡æ€èåˆåŸç†
- âœ… èƒ½è¯»æ‡‚GPT-4Vç­‰æ¨¡å‹
- âœ… äº†è§£æœ€æ–°å¤šæ¨¡æ€æŠ€æœ¯
- âœ… èƒ½è®¾è®¡å¤šæ¨¡æ€åº”ç”¨

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€ï¼Ÿ**

```python
å•æ¨¡æ€çš„å±€é™:
  çº¯æ–‡æœ¬: "ä¸€åªçŒ«ååœ¨å«å­ä¸Š"
  â†’ æ— æ³•ç†è§£å…·ä½“é•¿ä»€ä¹ˆæ ·
  
  çº¯å›¾åƒ: [çŒ«çš„å›¾ç‰‡]
  â†’ æ— æ³•ç†è§£ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰

å¤šæ¨¡æ€çš„ä¼˜åŠ¿:
  æ–‡æœ¬ + å›¾åƒ:
  "ä¸€åªæ©˜çŒ«ååœ¨çº¢è‰²å«å­ä¸Š" + [å›¾ç‰‡]
  â†’ å®Œæ•´çš„ç†è§£å’Œæè¿°èƒ½åŠ›
  
  åº”ç”¨åœºæ™¯:
  âœ… å›¾åƒé—®ç­” (VQA)
  âœ… å›¾åƒæè¿° (Image Captioning)
  âœ… æ–‡ç”Ÿå›¾ (Text-to-Image)
  âœ… è§†é¢‘ç†è§£
  âœ… åŒ»å­¦å½±åƒåˆ†æ
```

**å¤šæ¨¡æ€ = èåˆä¸åŒç±»å‹çš„ä¿¡æ¯**

```
è§†è§‰ ğŸ‘ï¸  +  è¯­è¨€ ğŸ“  +  éŸ³é¢‘ ğŸ”Š  =  å®Œæ•´ç†è§£ ğŸ§ 

å°±åƒäººç±»:
  çœ‹å›¾ç‰‡ â†’ ç†è§£å†…å®¹ â†’ ç”¨è¯­è¨€æè¿°
  å¬å£°éŸ³ â†’ ç†è§£æ„æ€ â†’ åšå‡ºååº”
  è¯»æ–‡å­— â†’ æƒ³è±¡åœºæ™¯ â†’ ç”Ÿæˆå›¾åƒ
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šå¤šæ¨¡æ€åŸºç¡€

### ğŸ” æ ¸å¿ƒæ¦‚å¿µ

```python
å¤šæ¨¡æ€å­¦ä¹ çš„ä¸‰å¤§ä»»åŠ¡:

1. è¡¨ç¤ºå­¦ä¹  (Representation)
   å°†ä¸åŒæ¨¡æ€æ˜ å°„åˆ°ç»Ÿä¸€ç©ºé—´
   
   ä¾‹: CLIP
   å›¾åƒ â†’ å›¾åƒembedding (512ç»´)
   æ–‡æœ¬ â†’ æ–‡æœ¬embedding (512ç»´)
   åœ¨åŒä¸€å‘é‡ç©ºé—´ä¸­ï¼

2. è½¬æ¢ (Translation)
   ä»ä¸€ç§æ¨¡æ€ç”Ÿæˆå¦ä¸€ç§æ¨¡æ€
   
   ä¾‹:
   æ–‡æœ¬ â†’ å›¾åƒ (DALL-E, Stable Diffusion)
   å›¾åƒ â†’ æ–‡æœ¬ (Image Captioning)
   è¯­éŸ³ â†’ æ–‡æœ¬ (ASR)

3. å¯¹é½ (Alignment)
   æ‰¾åˆ°ä¸åŒæ¨¡æ€é—´çš„å¯¹åº”å…³ç³»
   
   ä¾‹:
   å›¾åƒä¸­çš„çŒ« â†” æ–‡æœ¬ä¸­çš„"cat"
   è§†é¢‘çš„åŠ¨ä½œ â†” æè¿°çš„å¥å­

4. èåˆ (Fusion)
   ç»¼åˆå¤šä¸ªæ¨¡æ€çš„ä¿¡æ¯
   
   ä¾‹:
   å›¾åƒç‰¹å¾ + æ–‡æœ¬ç‰¹å¾ â†’ VQAç­”æ¡ˆ
   è§†é¢‘ + éŸ³é¢‘ â†’ è§†é¢‘ç†è§£
```

### ğŸ“Š æ¨¡æ€å¯¹æ¯”

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¨¡æ€    â”‚ è¡¨ç¤º     â”‚ ç‰¹ç‚¹     â”‚ æŒ‘æˆ˜     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ–‡æœ¬    â”‚ ç¦»æ•£tokenâ”‚ è¯­ä¹‰ä¸°å¯Œ â”‚ æ­§ä¹‰     â”‚
â”‚ å›¾åƒ    â”‚ åƒç´ çŸ©é˜µ â”‚ è§†è§‰ä¿¡æ¯ â”‚ é«˜ç»´åº¦   â”‚
â”‚ éŸ³é¢‘    â”‚ æ³¢å½¢/é¢‘è°±â”‚ æ—¶åºä¿¡å· â”‚ å™ªå£°     â”‚
â”‚ è§†é¢‘    â”‚ å¸§åºåˆ—   â”‚ æ—¶ç©ºä¿¡æ¯ â”‚ è®¡ç®—é‡å¤§ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

èåˆæ–¹å¼:

æ—©æœŸèåˆ (Early Fusion):
  åŸå§‹æ•°æ® â†’ ç›´æ¥æ‹¼æ¥ â†’ ç»Ÿä¸€å¤„ç†
  
  ä¼˜åŠ¿: ç®€å•
  åŠ£åŠ¿: éš¾ä»¥åˆ©ç”¨æ¨¡æ€ç‰¹æ€§

æ™šæœŸèåˆ (Late Fusion):
  å„æ¨¡æ€ç‹¬ç«‹å¤„ç† â†’ æœ€åèåˆå†³ç­–
  
  ä¼˜åŠ¿: çµæ´»
  åŠ£åŠ¿: å¯èƒ½ä¸¢å¤±äº¤äº’ä¿¡æ¯

æ··åˆèåˆ (Hybrid):
  éƒ¨åˆ†æ—©æœŸ + éƒ¨åˆ†æ™šæœŸ
  
  ä¼˜åŠ¿: å¹³è¡¡
  åŠ£åŠ¿: å¤æ‚
```

---

## ğŸ–¼ï¸ ç¬¬äºŒéƒ¨åˆ†ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹

### 1ï¸âƒ£ CLIP (Contrastive Language-Image Pre-training)

**æ ¸å¿ƒæ€æƒ³ï¼šå¯¹æ¯”å­¦ä¹ **

```python
CLIPåšä»€ä¹ˆï¼Ÿ
  å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬çš„å¯¹é½

è®­ç»ƒæ•°æ®:
  400Mä¸ª(å›¾åƒ, æè¿°)å¯¹
  ä»äº’è”ç½‘æ”¶é›†

è®­ç»ƒç›®æ ‡:
  åŒ¹é…çš„(å›¾åƒ, æ–‡æœ¬)å¯¹ â†’ ç›¸ä¼¼åº¦é«˜
  ä¸åŒ¹é…çš„å¯¹ â†’ ç›¸ä¼¼åº¦ä½
```

**CLIPæ¶æ„ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CLIP Architecture             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å›¾åƒåˆ†æ”¯:
  å›¾åƒ (224Ã—224Ã—3)
    â†“
  Vision Transformer / ResNet
    â†“
  å›¾åƒembedding (512ç»´)

æ–‡æœ¬åˆ†æ”¯:
  æ–‡æœ¬ "A photo of a cat"
    â†“
  Text Transformer
    â†“
  æ–‡æœ¬embedding (512ç»´)

å¯¹æ¯”å­¦ä¹ :
  Similarity = cosine(image_emb, text_emb)
  
  è®­ç»ƒ: æœ€å¤§åŒ–åŒ¹é…å¯¹çš„ç›¸ä¼¼åº¦
       æœ€å°åŒ–ä¸åŒ¹é…å¯¹çš„ç›¸ä¼¼åº¦
```

**CLIPå®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š**

```python
# clip_simple.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class ImageEncoder(nn.Module):
    """å›¾åƒç¼–ç å™¨ï¼ˆç®€åŒ–ç‰ˆViTï¼‰"""
    def __init__(self, image_size=224, patch_size=16, embed_dim=512):
        super().__init__()
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        
        # Patch embedding
        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)
        
        # Position embedding
        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))
        
        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        
        # Transformer
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8),
            num_layers=12
        )
        
        # Projection head
        self.proj = nn.Linear(embed_dim, embed_dim)
    
    def forward(self, x):
        # x: [batch, 3, 224, 224]
        B = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x)  # [B, embed_dim, 14, 14]
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]
        
        # Add CLS token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # Add position embedding
        x = x + self.pos_embed
        
        # Transformer
        x = self.transformer(x)
        
        # Take CLS token
        x = x[:, 0]
        
        # Project
        x = self.proj(x)
        
        # L2 normalize
        x = F.normalize(x, dim=-1)
        
        return x

class TextEncoder(nn.Module):
    """æ–‡æœ¬ç¼–ç å™¨"""
    def __init__(self, vocab_size=50000, embed_dim=512, max_len=77):
        super().__init__()
        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim))
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8),
            num_layers=12
        )
        
        self.proj = nn.Linear(embed_dim, embed_dim)
    
    def forward(self, x):
        # x: [batch, seq_len]
        x = self.token_embed(x)  # [B, seq_len, embed_dim]
        x = x + self.pos_embed[:, :x.size(1), :]
        
        x = self.transformer(x)
        
        # Take last token (or use pooling)
        x = x[:, -1, :]
        
        x = self.proj(x)
        x = F.normalize(x, dim=-1)
        
        return x

class CLIP(nn.Module):
    """CLIPæ¨¡å‹"""
    def __init__(self, embed_dim=512):
        super().__init__()
        self.image_encoder = ImageEncoder(embed_dim=embed_dim)
        self.text_encoder = TextEncoder(embed_dim=embed_dim)
        
        # Learnable temperature
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def forward(self, images, texts):
        # Encode
        image_features = self.image_encoder(images)  # [B, embed_dim]
        text_features = self.text_encoder(texts)     # [B, embed_dim]
        
        # Compute similarity
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()
        
        return logits_per_image, logits_per_text
    
    def contrastive_loss(self, logits_per_image, logits_per_text):
        """å¯¹æ¯”æŸå¤±"""
        batch_size = logits_per_image.shape[0]
        labels = torch.arange(batch_size, device=logits_per_image.device)
        
        loss_i = F.cross_entropy(logits_per_image, labels)
        loss_t = F.cross_entropy(logits_per_text, labels)
        
        loss = (loss_i + loss_t) / 2
        return loss

# è®­ç»ƒ
def train_clip(model, dataloader, epochs=10):
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    for epoch in range(epochs):
        total_loss = 0
        for images, texts in dataloader:
            # å‰å‘
            logits_img, logits_txt = model(images, texts)
            
            # æŸå¤±
            loss = model.contrastive_loss(logits_img, logits_txt)
            
            # æ›´æ–°
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch}: Loss = {total_loss/len(dataloader):.4f}")

# ä½¿ç”¨CLIP
def zero_shot_classification(model, image, candidate_texts):
    """é›¶æ ·æœ¬åˆ†ç±»"""
    with torch.no_grad():
        # ç¼–ç å›¾åƒ
        image_features = model.image_encoder(image.unsqueeze(0))
        
        # ç¼–ç æ‰€æœ‰å€™é€‰æ–‡æœ¬
        text_features = []
        for text in candidate_texts:
            text_ids = tokenize(text)
            feat = model.text_encoder(text_ids.unsqueeze(0))
            text_features.append(feat)
        
        text_features = torch.cat(text_features, dim=0)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = model.logit_scale.exp()
        logits = logit_scale * image_features @ text_features.t()
        probs = F.softmax(logits, dim=-1)
        
        return probs[0].cpu().numpy()

# ç¤ºä¾‹
model = CLIP()
image = load_image("cat.jpg")
texts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
probs = zero_shot_classification(model, image, texts)

print("Probabilities:")
for text, prob in zip(texts, probs):
    print(f"{text}: {prob:.2%}")
# è¾“å‡º:
# a photo of a cat: 92.5%
# a photo of a dog: 5.2%
# a photo of a bird: 2.3%
```

---

### 2ï¸âƒ£ LLaVA (Large Language and Vision Assistant)

**æ ¸å¿ƒæ€æƒ³ï¼šè§†è§‰æŒ‡ä»¤å¾®è°ƒ**

```python
LLaVA = CLIPè§†è§‰ç¼–ç å™¨ + LLaMAè¯­è¨€æ¨¡å‹

æ¶æ„:
  å›¾åƒ â†’ CLIP Visual Encoder â†’ è§†è§‰ç‰¹å¾
                                    â†“
  æ–‡æœ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æŠ•å½±å±‚ â†’ è¯­è¨€æ¨¡å‹è¾“å…¥
                                    â†“
                                LLaMA â†’ ç”Ÿæˆå›ç­”

ç‰¹ç‚¹:
  âœ… å¯ä»¥å¯¹è¯ï¼ˆ"è¿™å›¾é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"ï¼‰
  âœ… å¯ä»¥æ¨ç†ï¼ˆ"ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²ï¼Ÿ"ï¼‰
  âœ… å¯ä»¥æè¿°ç»†èŠ‚
```

**LLaVAå®ç°æ¡†æ¶ï¼š**

```python
# llava_simple.py

import torch
import torch.nn as nn
from transformers import CLIPVisionModel, LlamaForCausalLM

class LLaVA(nn.Module):
    """ç®€åŒ–çš„LLaVAæ¨¡å‹"""
    def __init__(self, vision_model_name='openai/clip-vit-large-patch14',
                 llm_model_name='meta-llama/Llama-2-7b-hf'):
        super().__init__()
        
        # è§†è§‰ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
        self.vision_tower = CLIPVisionModel.from_pretrained(vision_model_name)
        for param in self.vision_tower.parameters():
            param.requires_grad = False
        
        # è¯­è¨€æ¨¡å‹
        self.llm = LlamaForCausalLM.from_pretrained(llm_model_name)
        
        # æŠ•å½±å±‚ï¼ˆå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°LLMç©ºé—´ï¼‰
        vision_hidden_size = self.vision_tower.config.hidden_size  # 1024
        llm_hidden_size = self.llm.config.hidden_size  # 4096
        
        self.mm_projector = nn.Sequential(
            nn.Linear(vision_hidden_size, llm_hidden_size),
            nn.GELU(),
            nn.Linear(llm_hidden_size, llm_hidden_size)
        )
    
    def encode_images(self, images):
        """ç¼–ç å›¾åƒä¸ºç‰¹å¾"""
        with torch.no_grad():
            vision_outputs = self.vision_tower(images)
            image_features = vision_outputs.last_hidden_state  # [B, num_patches, hidden]
        
        # æŠ•å½±åˆ°LLMç©ºé—´
        image_features = self.mm_projector(image_features)
        
        return image_features
    
    def forward(self, images, input_ids, attention_mask=None, labels=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            images: [B, 3, H, W]
            input_ids: [B, seq_len] æ–‡æœ¬tokens
            labels: [B, seq_len] ç”¨äºè®¡ç®—loss
        """
        batch_size = images.shape[0]
        
        # ç¼–ç å›¾åƒ
        image_features = self.encode_images(images)  # [B, num_patches, llm_hidden]
        
        # è·å–æ–‡æœ¬embeddings
        text_embeds = self.llm.model.embed_tokens(input_ids)  # [B, seq_len, llm_hidden]
        
        # æ‹¼æ¥å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
        # å‡è®¾æ ¼å¼: [IMAGE] text...
        # æ‰¾åˆ°[IMAGE] tokençš„ä½ç½®ï¼Œæ›¿æ¢ä¸ºå›¾åƒç‰¹å¾
        
        # ç®€åŒ–ç‰ˆï¼šç›´æ¥åœ¨å¼€å¤´æ‹¼æ¥
        combined_embeds = torch.cat([image_features, text_embeds], dim=1)
        
        # è°ƒæ•´attention_mask
        image_attention_mask = torch.ones(batch_size, image_features.shape[1],
                                         dtype=torch.long, device=images.device)
        if attention_mask is not None:
            combined_attention_mask = torch.cat([image_attention_mask, attention_mask], dim=1)
        else:
            combined_attention_mask = None
        
        # è°ƒæ•´labels
        if labels is not None:
            image_labels = torch.full((batch_size, image_features.shape[1]), -100,
                                     dtype=torch.long, device=images.device)
            combined_labels = torch.cat([image_labels, labels], dim=1)
        else:
            combined_labels = None
        
        # é€šè¿‡LLM
        outputs = self.llm(
            inputs_embeds=combined_embeds,
            attention_mask=combined_attention_mask,
            labels=combined_labels,
            return_dict=True
        )
        
        return outputs
    
    def generate(self, images, input_ids, max_new_tokens=100, temperature=0.7):
        """ç”Ÿæˆå›ç­”"""
        # ç¼–ç å›¾åƒ
        image_features = self.encode_images(images)
        
        # è·å–æ–‡æœ¬embeddings
        text_embeds = self.llm.model.embed_tokens(input_ids)
        
        # æ‹¼æ¥
        combined_embeds = torch.cat([image_features, text_embeds], dim=1)
        
        # ç”Ÿæˆ
        # æ³¨æ„ï¼šéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œç®€åŒ–
        outputs = self.llm.generate(
            inputs_embeds=combined_embeds,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True
        )
        
        return outputs

# è®­ç»ƒLLaVA
def train_llava(model, dataloader, epochs=3):
    """
    è®­ç»ƒæ•°æ®æ ¼å¼:
    {
        'image': image tensor,
        'conversations': [
            {'from': 'human', 'value': 'What is in this image?'},
            {'from': 'gpt', 'value': 'This image shows a cat sitting on a mat...'}
        ]
    }
    """
    # åªè®­ç»ƒæŠ•å½±å±‚å’ŒLLM
    optimizer = torch.optim.AdamW([
        {'params': model.mm_projector.parameters(), 'lr': 2e-5},
        {'params': model.llm.parameters(), 'lr': 2e-6}
    ])
    
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        
        for batch in dataloader:
            images = batch['images']
            input_ids = batch['input_ids']
            labels = batch['labels']
            
            # å‰å‘
            outputs = model(images, input_ids, labels=labels)
            loss = outputs.loss
            
            # æ›´æ–°
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch}: Loss = {total_loss/len(dataloader):.4f}")

# æ¨ç†ç¤ºä¾‹
def chat_with_image(model, image, question, tokenizer):
    """ä¸å›¾åƒå¯¹è¯"""
    # æ ¼å¼åŒ–è¾“å…¥
    prompt = f"### Human: {question}\n### Assistant:"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # ç”Ÿæˆ
    output_ids = model.generate(
        images=image.unsqueeze(0),
        input_ids=input_ids,
        max_new_tokens=200
    )
    
    # è§£ç 
    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return response

# ä½¿ç”¨
model = LLaVA()
image = load_image("scene.jpg")
response = chat_with_image(model, image, "Describe this image in detail.")
print(response)
# è¾“å‡º: "This image shows a beautiful sunset over the ocean..."
```

---

## ğŸ¥ ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†é¢‘ç†è§£

### ğŸ¬ è§†é¢‘æ¨¡å‹æ¶æ„

```python
è§†é¢‘ = æ—¶åºçš„å›¾åƒåºåˆ—

æŒ‘æˆ˜:
  1. æ—¶é—´ç»´åº¦ (Tå¸§)
  2. ç©ºé—´ç»´åº¦ (HÃ—WÃ—C)
  3. è®¡ç®—é‡: TÃ—HÃ—WÃ—C

è§£å†³æ–¹æ¡ˆ:

æ–¹æ³•1: 3D CNN
  æ‰©å±•2Då·ç§¯åˆ°3D
  åŒæ—¶æ•è·æ—¶ç©ºä¿¡æ¯
  
  ä¾‹: I3D, C3D

æ–¹æ³•2: Two-Stream
  ç©ºé—´æµ: å¤„ç†å•å¸§
  æ—¶é—´æµ: å¤„ç†å…‰æµ
  æœ€åèåˆ

æ–¹æ³•3: Transformer
  è§†é¢‘patch â†’ Transformer
  
  ä¾‹: TimeSformer, VideoMAE
```

**ç®€åŒ–è§†é¢‘æ¨¡å‹ï¼š**

```python
# video_model.py

import torch
import torch.nn as nn

class VideoTransformer(nn.Module):
    """è§†é¢‘ç†è§£Transformer"""
    def __init__(self, num_frames=16, image_size=224, patch_size=16,
                 embed_dim=768, num_classes=400):
        super().__init__()
        
        self.num_frames = num_frames
        self.num_patches_per_frame = (image_size // patch_size) ** 2
        
        # Patch embeddingï¼ˆ3Dï¼‰
        self.patch_embed = nn.Conv3d(
            3, embed_dim,
            kernel_size=(1, patch_size, patch_size),
            stride=(1, patch_size, patch_size)
        )
        
        # Position embedding
        num_patches = num_frames * self.num_patches_per_frame
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))
        
        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        
        # Transformer
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=12),
            num_layers=12
        )
        
        # åˆ†ç±»å¤´
        self.head = nn.Linear(embed_dim, num_classes)
    
    def forward(self, x):
        """
        x: [B, T, C, H, W] - batch, time, channels, height, width
        """
        B, T, C, H, W = x.shape
        
        # Reshape for conv3d
        x = x.transpose(1, 2)  # [B, C, T, H, W]
        
        # Patch embedding
        x = self.patch_embed(x)  # [B, embed_dim, T, H', W']
        
        # Flatten patches
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]
        
        # Add CLS token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # Add position embedding
        x = x + self.pos_embed
        
        # Transformer
        x = self.transformer(x)
        
        # Classification
        x = x[:, 0]  # CLS token
        x = self.head(x)
        
        return x

# ä½¿ç”¨
model = VideoTransformer(num_classes=400)  # Kinetics-400
video = torch.randn(2, 16, 3, 224, 224)  # 2 videos, 16 frames each
logits = model(video)
print(logits.shape)  # [2, 400]
```

---

## ğŸµ ç¬¬å››éƒ¨åˆ†ï¼šéŸ³é¢‘-è¯­è¨€æ¨¡å‹

### ğŸ¤ éŸ³é¢‘å¤„ç†åŸºç¡€

```python
éŸ³é¢‘è¡¨ç¤º:

1. æ³¢å½¢ (Waveform)
   åŸå§‹ä¿¡å·: [æ—¶é—´ç‚¹ Ã— å¹…åº¦]
   
2. é¢‘è°± (Spectrogram)
   æ—¶é¢‘è¡¨ç¤º: [æ—¶é—´ Ã— é¢‘ç‡]
   
3. Melé¢‘è°± (Mel-Spectrogram)
   äººè€³æ„ŸçŸ¥: [æ—¶é—´ Ã— Melé¢‘æ®µ]
   æ›´é€‚åˆè¯­éŸ³

æ¨¡å‹:
  Wav2Vec 2.0: è‡ªç›‘ç£éŸ³é¢‘å­¦ä¹ 
  Whisper: è¯­éŸ³è¯†åˆ«
  AudioLM: éŸ³é¢‘ç”Ÿæˆ
```

**éŸ³é¢‘ç¼–ç å™¨ï¼š**

```python
# audio_encoder.py

import torch
import torch.nn as nn
import torchaudio

class AudioEncoder(nn.Module):
    """éŸ³é¢‘ç¼–ç å™¨"""
    def __init__(self, sample_rate=16000, n_mels=80, embed_dim=512):
        super().__init__()
        
        # Mel-Spectrogram transform
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_mels=n_mels,
            n_fft=400,
            hop_length=160
        )
        
        # CNN encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # Projection
        self.proj = nn.Linear(256, embed_dim)
    
    def forward(self, waveform):
        """
        waveform: [B, time]
        """
        # Mel spectrogram
        mel = self.mel_transform(waveform)  # [B, n_mels, time]
        mel = mel.unsqueeze(1)  # [B, 1, n_mels, time]
        
        # Encode
        features = self.encoder(mel)  # [B, 256, 1, 1]
        features = features.squeeze(-1).squeeze(-1)  # [B, 256]
        
        # Project
        embedding = self.proj(features)  # [B, embed_dim]
        
        return embedding

# éŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼ˆç±»ä¼¼CLIPï¼‰
class AudioCLIP(nn.Module):
    """éŸ³é¢‘-æ–‡æœ¬å¯¹é½"""
    def __init__(self, embed_dim=512):
        super().__init__()
        self.audio_encoder = AudioEncoder(embed_dim=embed_dim)
        self.text_encoder = TextEncoder(embed_dim=embed_dim)  # å¤ç”¨CLIPçš„
        
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def forward(self, audio, text):
        audio_features = self.audio_encoder(audio)
        text_features = self.text_encoder(text)
        
        # Normalize
        audio_features = F.normalize(audio_features, dim=-1)
        text_features = F.normalize(text_features, dim=-1)
        
        # Similarity
        logit_scale = self.logit_scale.exp()
        logits = logit_scale * audio_features @ text_features.t()
        
        return logits
```

---

## ğŸ”§ ç¬¬äº”éƒ¨åˆ†ï¼šæ„å»ºå¤šæ¨¡æ€GPT

### ğŸ¯ ç»Ÿä¸€å¤šæ¨¡æ€æ¶æ„

```python
ç›®æ ‡: ä¸€ä¸ªæ¨¡å‹å¤„ç†æ‰€æœ‰æ¨¡æ€

è®¾è®¡æ€è·¯:
  1. å°†æ‰€æœ‰æ¨¡æ€æ˜ å°„åˆ°ç»Ÿä¸€tokenç©ºé—´
  2. ç”¨Transformerå¤„ç†
  3. æ ¹æ®ä»»åŠ¡ç”Ÿæˆç›¸åº”æ¨¡æ€çš„è¾“å‡º

æ¶æ„:
  [å›¾åƒtokens] [æ–‡æœ¬tokens] [éŸ³é¢‘tokens]
           â†“
      Transformer
           â†“
  [è¾“å‡ºtokens] â†’ è§£ç åˆ°å„æ¨¡æ€
```

**å®Œæ•´å¤šæ¨¡æ€æ¨¡å‹ï¼š**

```python
# multimodal_gpt.py

class MultiModalTokenizer:
    """å¤šæ¨¡æ€tokenizer"""
    def __init__(self):
        # ç‰¹æ®Štokens
        self.IMG_START = 50257
        self.IMG_END = 50258
        self.AUD_START = 50259
        self.AUD_END = 50260
        
        # å„æ¨¡æ€çš„codebookå¤§å°
        self.image_vocab_size = 8192  # æ¥è‡ªVQ-VAE
        self.audio_vocab_size = 1024
        self.text_vocab_size = 50257  # GPT-2
    
    def encode_image(self, image):
        """å›¾åƒ â†’ tokens"""
        # ä½¿ç”¨VQ-VAEç¼–ç 
        image_tokens = vqvae.encode(image)  # [H/8, W/8]
        image_tokens = image_tokens.flatten()  # [num_tokens]
        
        # æ·»åŠ ç‰¹æ®Štokens
        tokens = [self.IMG_START] + image_tokens.tolist() + [self.IMG_END]
        return tokens
    
    def encode_text(self, text):
        """æ–‡æœ¬ â†’ tokens"""
        return gpt2_tokenizer.encode(text)
    
    def encode_audio(self, audio):
        """éŸ³é¢‘ â†’ tokens"""
        audio_tokens = audio_vqvae.encode(audio)
        tokens = [self.AUD_START] + audio_tokens.tolist() + [self.AUD_END]
        return tokens

class MultiModalGPT(nn.Module):
    """ç»Ÿä¸€å¤šæ¨¡æ€GPT"""
    def __init__(self, vocab_size=60000, embed_dim=768, num_layers=12):
        super().__init__()
        
        # Token embeddingï¼ˆåŒ…å«æ‰€æœ‰æ¨¡æ€ï¼‰
        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        
        # Position embedding
        self.pos_embed = nn.Embedding(2048, embed_dim)
        
        # Transformer
        self.transformer = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads=12)
            for _ in range(num_layers)
        ])
        
        # å„æ¨¡æ€çš„è¾“å‡ºå¤´
        self.text_head = nn.Linear(embed_dim, 50257)
        self.image_head = nn.Linear(embed_dim, 8192)
        self.audio_head = nn.Linear(embed_dim, 1024)
    
    def forward(self, input_ids, modality_ids=None):
        """
        input_ids: [B, seq_len] æ··åˆçš„å¤šæ¨¡æ€tokens
        modality_ids: [B, seq_len] æŒ‡ç¤ºæ¯ä¸ªtokençš„æ¨¡æ€ç±»å‹
        """
        B, T = input_ids.shape
        
        # Embeddings
        token_emb = self.token_embed(input_ids)
        pos = torch.arange(T, device=input_ids.device)
        pos_emb = self.pos_embed(pos)
        
        x = token_emb + pos_emb
        
        # Transformer
        for block in self.transformer:
            x = block(x)
        
        # æ ¹æ®æ¨¡æ€é€‰æ‹©è¾“å‡ºå¤´
        logits = self.route_to_heads(x, modality_ids)
        
        return logits
    
    def route_to_heads(self, x, modality_ids):
        """æ ¹æ®æ¨¡æ€è·¯ç”±åˆ°ç›¸åº”çš„è¾“å‡ºå¤´"""
        if modality_ids is None:
            # é»˜è®¤ä½¿ç”¨æ–‡æœ¬å¤´
            return self.text_head(x)
        
        # åˆ†åˆ«å¤„ç†å„æ¨¡æ€
        # è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        outputs = []
        for i in range(x.size(0)):
            # åˆ¤æ–­å½“å‰ä½ç½®çš„æ¨¡æ€
            if is_text_position(modality_ids[i]):
                outputs.append(self.text_head(x[i]))
            elif is_image_position(modality_ids[i]):
                outputs.append(self.image_head(x[i]))
            else:
                outputs.append(self.audio_head(x[i]))
        
        return torch.stack(outputs)
    
    def generate_multimodal(self, prompt_tokens, modality_ids, max_length=100):
        """å¤šæ¨¡æ€ç”Ÿæˆ"""
        generated = prompt_tokens
        
        for _ in range(max_length):
            # å‰å‘ä¼ æ’­
            logits = self.forward(generated, modality_ids)
            
            # å–æœ€åä¸€ä¸ªtokençš„logits
            next_token_logits = logits[:, -1, :]
            
            # é‡‡æ ·
            next_token = torch.multinomial(
                F.softmax(next_token_logits, dim=-1),
                num_samples=1
            )
            
            # æ·»åŠ åˆ°åºåˆ—
            generated = torch.cat([generated, next_token], dim=1)
            
            # åœæ­¢æ¡ä»¶
            if next_token.item() in [IMG_END, AUD_END, EOS]:
                break
        
        return generated

# ä½¿ç”¨ç¤ºä¾‹
def image_captioning(model, tokenizer, image):
    """å›¾åƒæè¿°"""
    # ç¼–ç å›¾åƒ
    image_tokens = tokenizer.encode_image(image)
    
    # æ·»åŠ æç¤º
    prompt = image_tokens + tokenizer.encode_text("Caption: ")
    prompt_tensor = torch.tensor([prompt])
    
    # ç”Ÿæˆæè¿°
    output = model.generate_multimodal(prompt_tensor, max_length=50)
    
    # è§£ç 
    caption = tokenizer.decode_text(output[0])
    return caption

def text_to_image(model, tokenizer, text):
    """æ–‡ç”Ÿå›¾"""
    # ç¼–ç æ–‡æœ¬
    text_tokens = tokenizer.encode_text(text)
    
    # æ·»åŠ å›¾åƒå¼€å§‹token
    prompt = text_tokens + [tokenizer.IMG_START]
    prompt_tensor = torch.tensor([prompt])
    
    # ç”Ÿæˆå›¾åƒtokens
    output = model.generate_multimodal(prompt_tensor, max_length=256)
    
    # æå–å›¾åƒtokenså¹¶è§£ç 
    image_tokens = extract_between(output, IMG_START, IMG_END)
    image = vqvae.decode(image_tokens)
    
    return image
```

---

## ğŸ’¡ ç¬¬å…­éƒ¨åˆ†ï¼šè®­ç»ƒæŠ€å·§

### âš¡ 1. æ•°æ®å¯¹é½

```python
æŒ‘æˆ˜: ä¸åŒæ¨¡æ€çš„æ•°æ®ä¸æ€»æ˜¯å¯¹é½çš„

è§£å†³æ–¹æ¡ˆ:

1. å¼±ç›‘ç£å­¦ä¹ 
   åˆ©ç”¨è‡ªç„¶é…å¯¹æ•°æ®
   ä¾‹: ç½‘é¡µçš„å›¾ç‰‡+altæ–‡æœ¬

2. è·¨æ¨¡æ€æ£€ç´¢
   æ‰¾åˆ°ç›¸å…³çš„è·¨æ¨¡æ€æ ·æœ¬
   
3. è‡ªç›‘ç£å­¦ä¹ 
   æ©ç é¢„æµ‹ã€å¯¹æ¯”å­¦ä¹ 

æ•°æ®å¢å¼º:
  å›¾åƒ: è£å‰ªã€ç¿»è½¬ã€é¢œè‰²å˜æ¢
  æ–‡æœ¬: å›è¯‘ã€åŒä¹‰æ›¿æ¢
  éŸ³é¢‘: æ—¶é—´æ‹‰ä¼¸ã€æ·»åŠ å™ªå£°
  
  å…³é”®: ä¿æŒè·¨æ¨¡æ€ä¸€è‡´æ€§ï¼
```

### âš¡ 2. è®­ç»ƒç­–ç•¥

```python
é˜¶æ®µ1: å•æ¨¡æ€é¢„è®­ç»ƒ
  åˆ†åˆ«é¢„è®­ç»ƒå„ç¼–ç å™¨
  æ–‡æœ¬: GPTé¢„è®­ç»ƒ
  å›¾åƒ: ViT on ImageNet
  éŸ³é¢‘: Wav2Vec 2.0

é˜¶æ®µ2: è·¨æ¨¡æ€å¯¹é½
  å†»ç»“ç¼–ç å™¨
  åªè®­ç»ƒæŠ•å½±å±‚
  ä½¿ç”¨å¯¹æ¯”å­¦ä¹ 

é˜¶æ®µ3: ç«¯åˆ°ç«¯å¾®è°ƒ
  è§£å†»æ‰€æœ‰å‚æ•°
  åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒ
  å°å­¦ä¹ ç‡ï¼

è¶…å‚æ•°:
  å­¦ä¹ ç‡: 1e-5 ~ 1e-4
  Batch size: å°½å¯èƒ½å¤§ (256-4096)
  æ¸©åº¦: 0.07 (å¯¹æ¯”å­¦ä¹ )
  æƒé‡è¡°å‡: 0.1
```

### âš¡ 3. è¯„ä¼°æŒ‡æ ‡

```python
å›¾åƒ-æ–‡æœ¬:
  æ£€ç´¢ä»»åŠ¡:
    - Recall@K
    - Mean Rank
  
  ç”Ÿæˆä»»åŠ¡:
    - BLEU, ROUGE (æè¿°è´¨é‡)
    - CLIPScore (è¯­ä¹‰å¯¹é½)
    - FID (å›¾åƒè´¨é‡)

è§†é¢‘ç†è§£:
  - Top-1 Accuracy
  - mAP (å¤šæ ‡ç­¾)

éŸ³é¢‘:
  - WER (è¯é”™è¯¯ç‡, ASR)
  - MOS (å¹³å‡æ„è§åˆ†, ç”Ÿæˆè´¨é‡)
```

---

## ğŸ“ æ€»ç»“

### âœ¨ æ ¸å¿ƒè¦ç‚¹

```python
å¤šæ¨¡æ€å­¦ä¹ çš„æ ¸å¿ƒ:
  1. è¡¨ç¤ºå­¦ä¹ : ç»Ÿä¸€ç©ºé—´
  2. å¯¹é½: è·¨æ¨¡æ€å¯¹åº”
  3. èåˆ: ç»¼åˆä¿¡æ¯
  4. è½¬æ¢: æ¨¡æ€äº’è¯‘

å…³é”®æ¨¡å‹:
  CLIP: å›¾åƒ-æ–‡æœ¬å¯¹é½
  LLaVA: è§†è§‰å¯¹è¯
  Whisper: è¯­éŸ³è¯†åˆ«
  DALL-E/Stable Diffusion: æ–‡ç”Ÿå›¾

è®­ç»ƒæŠ€å·§:
  âœ… å¤§è§„æ¨¡é…å¯¹æ•°æ®
  âœ… å¯¹æ¯”å­¦ä¹ 
  âœ… åˆ†é˜¶æ®µè®­ç»ƒ
  âœ… é€‚å½“çš„æ•°æ®å¢å¼º
```

### ğŸ¯ å®è·µå»ºè®®

```python
ä½ çš„é¡¹ç›® â†’ æ¨èæ–¹æ¡ˆ

å›¾åƒé—®ç­”:
  CLIP + GPT-2
  æˆ–ä½¿ç”¨LLaVA

å›¾åƒç”Ÿæˆ:
  Stable Diffusion
  æˆ–ä»VQGAN+Transformerå¼€å§‹

è§†é¢‘ç†è§£:
  é¢„è®­ç»ƒViT + æ—¶åºå»ºæ¨¡
  æˆ–TimeSformer

è¯­éŸ³è¯†åˆ«:
  Whisper (å¼€ç®±å³ç”¨)
  æˆ–Wav2Vec 2.0å¾®è°ƒ

ç»Ÿä¸€å¤šæ¨¡æ€:
  ä»å°è§„æ¨¡å¼€å§‹
  2ä¸ªæ¨¡æ€ â†’ 3ä¸ªæ¨¡æ€ â†’ ...
```

---

## ğŸ“ æ€»ç»“ä¸æ£€æŸ¥

### âœ… çŸ¥è¯†æ£€æŸ¥æ¸…å•

å®Œæˆå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€æ¦‚å¿µï¼ˆå¿…é¡»æŒæ¡ï¼‰**
- [ ] ç†è§£ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€å­¦ä¹ 
- [ ] çŸ¥é“CLIPçš„åŸºæœ¬åŸç†
- [ ] ç†è§£è§†è§‰ç¼–ç å™¨ï¼ˆViTï¼‰çš„ä½œç”¨
- [ ] çŸ¥é“å¦‚ä½•å¯¹é½ä¸åŒæ¨¡æ€
- [ ] ç†è§£å›¾åƒæè¿°å’ŒVQAçš„åŒºåˆ«
- [ ] èƒ½å¤Ÿä½¿ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹

**è¿›é˜¶ç†è§£ï¼ˆå»ºè®®æŒæ¡ï¼‰**
- [ ] ç†è§£å¯¹æ¯”å­¦ä¹ çš„åŸç†
- [ ] çŸ¥é“LLaVAçš„æ¶æ„è®¾è®¡
- [ ] ç†è§£æ–‡ç”Ÿå›¾çš„å·¥ä½œåŸç†
- [ ] èƒ½å¤Ÿå¾®è°ƒå¤šæ¨¡æ€æ¨¡å‹
- [ ] ç†è§£è·¨æ¨¡æ€æ£€ç´¢çš„æ–¹æ³•
- [ ] çŸ¥é“å¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹

**å®æˆ˜èƒ½åŠ›ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰**
- [ ] èƒ½å¤Ÿæ„å»ºç®€å•çš„å¤šæ¨¡æ€æ¨¡å‹
- [ ] ä¼šä½¿ç”¨CLIPè¿›è¡Œå›¾æ–‡æ£€ç´¢
- [ ] èƒ½å¤Ÿå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹
- [ ] ä¼šå¤„ç†å¤šæ¨¡æ€æ•°æ®
- [ ] èƒ½å¤Ÿè®¾è®¡å¤šæ¨¡æ€åº”ç”¨
- [ ] ç†è§£å¤šæ¨¡æ€æ¨¡å‹çš„å±€é™æ€§

### ğŸ“Š å¤šæ¨¡æ€æ¨¡å‹é€ŸæŸ¥è¡¨

| æ¨¡å‹ | ä»»åŠ¡ | æ¨¡æ€ | å‚æ•°é‡ | ç‰¹ç‚¹ | æ¨èåœºæ™¯ |
|------|------|------|--------|------|---------|
| **CLIP** | å›¾æ–‡åŒ¹é… | å›¾åƒ+æ–‡æœ¬ | 400M | é›¶æ ·æœ¬èƒ½åŠ›å¼º | å›¾æ–‡æ£€ç´¢ â­â­â­â­â­ |
| **ViT** | å›¾åƒåˆ†ç±» | å›¾åƒ | 86M-632M | Transformeræ¶æ„ | å›¾åƒç†è§£ â­â­â­â­ |
| **BLIP** | å›¾æ–‡ç†è§£ | å›¾åƒ+æ–‡æœ¬ | 200M | ç»Ÿä¸€æ¡†æ¶ | å¤šä»»åŠ¡ â­â­â­â­ |
| **LLaVA** | è§†è§‰é—®ç­” | å›¾åƒ+æ–‡æœ¬ | 7B-13B | åŸºäºLLM | å¯¹è¯åº”ç”¨ â­â­â­â­â­ |
| **GPT-4V** | é€šç”¨ç†è§£ | å›¾åƒ+æ–‡æœ¬ | æœªçŸ¥ | æœ€å¼ºæ€§èƒ½ | å•†ä¸šåº”ç”¨ â­â­â­â­â­ |
| **Stable Diffusion** | æ–‡ç”Ÿå›¾ | æ–‡æœ¬â†’å›¾åƒ | 1B | å¼€æºå¯æ§ | å›¾åƒç”Ÿæˆ â­â­â­â­â­ |

### ğŸ¯ å¦‚ä½•é€‰æ‹©å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ

```python
# å†³ç­–æ ‘
if ä»»åŠ¡ == "å›¾æ–‡æ£€ç´¢":
    ä½¿ç”¨ CLIP  # æœ€ç»å…¸ï¼Œæ•ˆæœå¥½
    
elif ä»»åŠ¡ == "å›¾åƒæè¿°":
    if éœ€è¦é«˜è´¨é‡:
        ä½¿ç”¨ BLIP-2 æˆ– LLaVA
    else:
        ä½¿ç”¨ BLIP  # æ›´è½»é‡
        
elif ä»»åŠ¡ == "è§†è§‰é—®ç­”":
    if éœ€è¦å¯¹è¯èƒ½åŠ›:
        ä½¿ç”¨ LLaVA æˆ– GPT-4V  # åŸºäºLLM
    else:
        ä½¿ç”¨ BLIP  # ç®€å•ä»»åŠ¡
        
elif ä»»åŠ¡ == "æ–‡ç”Ÿå›¾":
    if éœ€è¦å¼€æº:
        ä½¿ç”¨ Stable Diffusion  # å¯æ§æ€§å¼º
    elif è¿½æ±‚è´¨é‡:
        ä½¿ç”¨ DALL-E 3  # æœ€å¥½ä½†é—­æº
        
elif ä»»åŠ¡ == "è§†é¢‘ç†è§£":
    ä½¿ç”¨ VideoLLaMA æˆ– Video-ChatGPT
    
# å®é™…ä¾‹å­
ç”µå•†æœç´¢: CLIP âœ…
æ™ºèƒ½å®¢æœ: LLaVA âœ…
å†…å®¹åˆ›ä½œ: Stable Diffusion âœ…
åŒ»å­¦å½±åƒ: è‡ªå®šä¹‰å¤šæ¨¡æ€æ¨¡å‹ âœ…
```

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

ç°åœ¨ä½ å·²ç»æŒæ¡äº†å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¥ä¸‹æ¥åº”è¯¥å­¦ä¹ ï¼š

1. **12_mixture_of_experts.md** - å­¦ä¹ ç¨€ç–æ¨¡å‹MoE
2. **13_rlhf_and_alignment.md** - å­¦ä¹ RLHFä¸æ¨¡å‹å¯¹é½
3. **å®è·µé¡¹ç›®** - æ„å»ºä¸€ä¸ªå¤šæ¨¡æ€åº”ç”¨

### ğŸ’¡ å®è·µå»ºè®®

**ç«‹å³å¯åš**ï¼š
```python
# 1. ä½¿ç”¨CLIPè¿›è¡Œå›¾æ–‡æ£€ç´¢
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å›¾æ–‡ç›¸ä¼¼åº¦
inputs = processor(text=["a cat", "a dog"], images=image, return_tensors="pt")
outputs = model(**inputs)
similarity = outputs.logits_per_image

# 2. ä½¿ç”¨LLaVAè¿›è¡Œè§†è§‰é—®ç­”
from llava import LlavaForConditionalGeneration

model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf")
# é—®ç­”ï¼šWhat's in this image?

# 3. ä½¿ç”¨Stable Diffusionç”Ÿæˆå›¾åƒ
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1")
image = pipe("a cat sitting on a mat").images[0]
```

**ç³»ç»Ÿå®éªŒ**ï¼š
```bash
# å®éªŒ1ï¼šCLIPé›¶æ ·æœ¬åˆ†ç±»
python clip_zero_shot.py \
  --images ./test_images/ \
  --labels "cat,dog,bird,car"
# æµ‹è¯•ï¼šä¸åŒç±»åˆ«çš„å‡†ç¡®ç‡

# å®éªŒ2ï¼šå›¾æ–‡æ£€ç´¢ç³»ç»Ÿ
python image_text_retrieval.py \
  --image_dir ./images/ \
  --queries "red car,cute cat,sunset"
# è¯„ä¼°ï¼šæ£€ç´¢å‡†ç¡®ç‡

# å®éªŒ3ï¼šå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹
python finetune_vlm.py \
  --model llava-7b \
  --dataset custom_vqa \
  --epochs 3
# å¯¹æ¯”ï¼šå¾®è°ƒå‰åçš„æ€§èƒ½

# å®éªŒ4ï¼šæ–‡ç”Ÿå›¾è´¨é‡å¯¹æ¯”
python text_to_image_compare.py \
  --models sd2.1,sdxl,dalle3 \
  --prompts prompts.txt
# è¯„ä¼°ï¼šç”Ÿæˆè´¨é‡
```

**è¿›é˜¶ç ”ç©¶**ï¼š
1. é˜…è¯»CLIPã€LLaVAè®ºæ–‡ï¼Œç†è§£è®¾è®¡æ€æƒ³
2. ç ”ç©¶å¯¹æ¯”å­¦ä¹ çš„ç†è®ºåŸºç¡€
3. æ¢ç´¢æ–°çš„æ¨¡æ€ç»„åˆï¼ˆéŸ³é¢‘+è§†é¢‘+æ–‡æœ¬ï¼‰
4. ç ”ç©¶å¤šæ¨¡æ€åœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨

---

## ğŸ“š æ¨èèµ„æº

### ğŸ“– å¿…è¯»æ–‡æ¡£
- [CLIP Documentation](https://github.com/openai/CLIP) - OpenAIå®˜æ–¹
- [Hugging Face Multimodal](https://huggingface.co/docs/transformers/model_doc/clip) - æœ€å…¨çš„æ¨¡å‹åº“
- [LLaVA Project](https://llava-vl.github.io/) - è§†è§‰è¯­è¨€æ¨¡å‹
- [Stable Diffusion Guide](https://stability.ai/stable-diffusion) - æ–‡ç”Ÿå›¾

### ğŸ“„ é‡è¦è®ºæ–‡

**åŸºç¡€æ¨¡å‹**ï¼š
1. **Learning Transferable Visual Models From Natural Language Supervision (CLIP)** (Radford et al., 2021)
   - https://arxiv.org/abs/2103.00020
   - å¤šæ¨¡æ€å­¦ä¹ çš„é‡Œç¨‹ç¢‘

2. **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)** (Dosovitskiy et al., 2020)
   - https://arxiv.org/abs/2010.11929
   - è§†è§‰Transformer

3. **BLIP: Bootstrapping Language-Image Pre-training** (Li et al., 2022)
   - https://arxiv.org/abs/2201.12086
   - ç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶

**è§†è§‰è¯­è¨€æ¨¡å‹**ï¼š
4. **Visual Instruction Tuning (LLaVA)** (Liu et al., 2023)
   - https://arxiv.org/abs/2304.08485
   - è§†è§‰æŒ‡ä»¤å¾®è°ƒ

5. **Flamingo: a Visual Language Model for Few-Shot Learning** (Alayrac et al., 2022)
   - https://arxiv.org/abs/2204.14198
   - DeepMindçš„å¤šæ¨¡æ€æ¨¡å‹

6. **GPT-4V(ision) System Card** (OpenAI, 2023)
   - https://openai.com/research/gpt-4v-system-card
   - æœ€å¼ºå¤šæ¨¡æ€æ¨¡å‹

**æ–‡ç”Ÿå›¾**ï¼š
7. **High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion)** (Rombach et al., 2022)
   - https://arxiv.org/abs/2112.10752
   - å¼€æºæ–‡ç”Ÿå›¾

8. **DALL-E 2: Hierarchical Text-Conditional Image Generation with CLIP Latents** (Ramesh et al., 2022)
   - https://arxiv.org/abs/2204.06125
   - OpenAIçš„æ–‡ç”Ÿå›¾

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- [CLIP Explained](https://www.youtube.com/watch?v=T9XSU0pKX2E)
- [Stable Diffusion Tutorial](https://www.youtube.com/watch?v=1CIpzeNxIhU)
- [LLaVA Demo](https://www.youtube.com/watch?v=qWB4JgCguHw)

### ğŸ”§ å®ç”¨å·¥å…·

**æ¨¡å‹åº“**ï¼š
```bash
# CLIP - å›¾æ–‡åŒ¹é…
pip install transformers
from transformers import CLIPModel

# LLaVA - è§†è§‰é—®ç­”
pip install llava
# æˆ–ä½¿ç”¨Hugging Faceç‰ˆæœ¬

# Stable Diffusion - æ–‡ç”Ÿå›¾
pip install diffusers
from diffusers import StableDiffusionPipeline

# BLIP - å›¾åƒæè¿°
pip install salesforce-lavis
```

**æ•°æ®é›†**ï¼š
```python
# COCO - å›¾åƒæè¿°
from datasets import load_dataset
coco = load_dataset("coco")

# Conceptual Captions - å¤§è§„æ¨¡å›¾æ–‡å¯¹
cc = load_dataset("conceptual_captions")

# VQA - è§†è§‰é—®ç­”
vqa = load_dataset("vqa_v2")
```

**è¯„ä¼°å·¥å…·**ï¼š
```bash
# CLIP Score - è¯„ä¼°å›¾æ–‡åŒ¹é…
pip install clip-score

# FID - è¯„ä¼°å›¾åƒç”Ÿæˆè´¨é‡
pip install pytorch-fid

# BLEU/CIDEr - è¯„ä¼°å›¾åƒæè¿°
pip install pycocoevalcap
```

---

## ğŸ› å¸¸è§é—®é¢˜ FAQ

### Q1: å¤šæ¨¡æ€æ¨¡å‹å’Œå•æ¨¡æ€æ¨¡å‹æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
**A**: æ ¸å¿ƒæ˜¯ä¿¡æ¯èåˆã€‚
```
å•æ¨¡æ€ï¼ˆå¦‚GPTï¼‰:
  è¾“å…¥ï¼šæ–‡æœ¬
  å¤„ç†ï¼šæ–‡æœ¬ç¼–ç å™¨
  è¾“å‡ºï¼šæ–‡æœ¬
  å±€é™ï¼šåªèƒ½ç†è§£æ–‡å­—

å¤šæ¨¡æ€ï¼ˆå¦‚CLIPï¼‰:
  è¾“å…¥ï¼šå›¾åƒ + æ–‡æœ¬
  å¤„ç†ï¼šå›¾åƒç¼–ç å™¨ + æ–‡æœ¬ç¼–ç å™¨ + èåˆå±‚
  è¾“å‡ºï¼šè·¨æ¨¡æ€è¡¨ç¤º
  ä¼˜åŠ¿ï¼šç†è§£å¤šç§ä¿¡æ¯

ä¾‹å­ï¼š
  é—®é¢˜ï¼š"è¿™å¼ å›¾é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
  å•æ¨¡æ€ï¼šæ— æ³•å›ç­”ï¼ˆçœ‹ä¸åˆ°å›¾ï¼‰
  å¤šæ¨¡æ€ï¼šèƒ½çœ‹å›¾å¹¶æè¿° âœ…

ç»“è®ºï¼šå¤šæ¨¡æ€æ›´æ¥è¿‘äººç±»çš„æ„ŸçŸ¥æ–¹å¼
```

### Q2: CLIPä¸ºä»€ä¹ˆè¿™ä¹ˆå¼ºå¤§ï¼Ÿ
**A**: å¯¹æ¯”å­¦ä¹  + å¤§è§„æ¨¡æ•°æ®ã€‚
```python
# CLIPçš„æ ¸å¿ƒæ€æƒ³
è®­ç»ƒæ•°æ®: 4äº¿å›¾æ–‡å¯¹ï¼ˆä»äº’è”ç½‘æ”¶é›†ï¼‰

è®­ç»ƒç›®æ ‡:
  åŒ¹é…çš„å›¾æ–‡å¯¹ â†’ ç›¸ä¼¼åº¦é«˜
  ä¸åŒ¹é…çš„å›¾æ–‡å¯¹ â†’ ç›¸ä¼¼åº¦ä½

# ä¾‹å­
å›¾ç‰‡: [ä¸€åªçŒ«çš„ç…§ç‰‡]
æ–‡æœ¬1: "a cat" â†’ ç›¸ä¼¼åº¦ 0.95 âœ…
æ–‡æœ¬2: "a dog" â†’ ç›¸ä¼¼åº¦ 0.10 âŒ

ä¼˜åŠ¿:
  1. é›¶æ ·æœ¬èƒ½åŠ›ï¼šä¸éœ€è¦è®­ç»ƒå°±èƒ½åˆ†ç±»
  2. é€šç”¨æ€§å¼ºï¼šé€‚ç”¨äºå„ç§å›¾åƒä»»åŠ¡
  3. å¯è§£é‡Šæ€§ï¼šé€šè¿‡æ–‡æœ¬æè¿°æ§åˆ¶

å®æµ‹:
  ImageNeté›¶æ ·æœ¬: 76.2%å‡†ç¡®ç‡
  ï¼ˆå¾ˆå¤šç›‘ç£å­¦ä¹ æ¨¡å‹æ‰80%+ï¼‰
```

### Q3: å¦‚ä½•å¯¹é½ä¸åŒæ¨¡æ€ï¼Ÿ
**A**: é€šè¿‡å…±äº«è¡¨ç¤ºç©ºé—´ã€‚
```python
# æ–¹æ³•1ï¼šå¯¹æ¯”å­¦ä¹ ï¼ˆCLIPï¼‰
å›¾åƒç¼–ç å™¨(image) â†’ image_embedding
æ–‡æœ¬ç¼–ç å™¨(text) â†’ text_embedding

loss = contrastive_loss(image_embedding, text_embedding)
# åŒ¹é…çš„æ‹‰è¿‘ï¼Œä¸åŒ¹é…çš„æ¨è¿œ

# æ–¹æ³•2ï¼šäº¤å‰æ³¨æ„åŠ›ï¼ˆFlamingoï¼‰
text_features = text_encoder(text)
image_features = image_encoder(image)

# æ–‡æœ¬å…³æ³¨å›¾åƒçš„å“ªäº›éƒ¨åˆ†
attended_features = cross_attention(text_features, image_features)

# æ–¹æ³•3ï¼šé€‚é…å™¨ï¼ˆLLaVAï¼‰
image_features = vision_encoder(image)
# é€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°LLMçš„ç©ºé—´
adapted_features = projection(image_features)
# ç›´æ¥è¾“å…¥åˆ°LLM

# é€‰æ‹©å»ºè®®
ç®€å•ä»»åŠ¡: å¯¹æ¯”å­¦ä¹ ï¼ˆCLIPï¼‰
å¤æ‚ä»»åŠ¡: äº¤å‰æ³¨æ„åŠ›æˆ–é€‚é…å™¨
```

### Q4: è§†è§‰ç¼–ç å™¨ç”¨CNNè¿˜æ˜¯Transformerï¼Ÿ
**A**: ç°åœ¨ä¸»æµæ˜¯Transformerï¼ˆViTï¼‰ã€‚
```
CNNï¼ˆä¼ ç»Ÿï¼‰:
  ä»£è¡¨ï¼šResNet, EfficientNet
  ä¼˜ç‚¹ï¼š
    - å½’çº³åç½®å¼ºï¼ˆå±€éƒ¨æ€§ï¼‰
    - è®­ç»ƒæ•°æ®éœ€æ±‚å°‘
    - é€Ÿåº¦å¿«
  ç¼ºç‚¹ï¼š
    - æ„Ÿå—é‡æœ‰é™
    - éš¾ä»¥æ•è·é•¿è·ç¦»ä¾èµ–

ViTï¼ˆç°ä»£ï¼‰:
  ä»£è¡¨ï¼šViT, CLIP Vision
  ä¼˜ç‚¹ï¼š
    - å…¨å±€æ„Ÿå—é‡
    - å¯æ‰©å±•æ€§å¥½
    - ä¸è¯­è¨€æ¨¡å‹ç»Ÿä¸€æ¶æ„
  ç¼ºç‚¹ï¼š
    - éœ€è¦å¤§é‡æ•°æ®
    - è®¡ç®—é‡å¤§

å®é™…é€‰æ‹©ï¼š
  å°æ•°æ®é›†: CNN âœ…
  å¤§æ•°æ®é›†: ViT âœ…
  å¤šæ¨¡æ€: ViT âœ…ï¼ˆä¸Transformerç»Ÿä¸€ï¼‰

è¶‹åŠ¿ï¼šViTæ­£åœ¨å–ä»£CNN
```

### Q5: å¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ
**A**: å¤šç»´åº¦è¯„ä¼°ã€‚
```python
# 1. å›¾æ–‡æ£€ç´¢ï¼ˆCLIPï¼‰
from sklearn.metrics import accuracy_score

# å›¾åƒæ£€ç´¢æ–‡æœ¬
image_to_text_recall = recall_at_k(predictions, ground_truth, k=5)

# æ–‡æœ¬æ£€ç´¢å›¾åƒ  
text_to_image_recall = recall_at_k(predictions, ground_truth, k=5)

# 2. å›¾åƒæè¿°ï¼ˆBLIPï¼‰
from pycocoevalcap.cider.cider import Cider

cider_score = Cider().compute_score(references, predictions)
# CIDErè¶Šé«˜è¶Šå¥½ï¼ˆé€šå¸¸0-10ï¼‰

# 3. è§†è§‰é—®ç­”ï¼ˆLLaVAï¼‰
accuracy = sum(pred == gt for pred, gt in zip(predictions, ground_truth)) / len(predictions)

# 4. æ–‡ç”Ÿå›¾ï¼ˆStable Diffusionï¼‰
from pytorch_fid import fid_score

fid = fid_score.calculate_fid_given_paths([real_path, generated_path])
# FIDè¶Šä½è¶Šå¥½ï¼ˆ<50ç®—å¥½ï¼‰

# 5. CLIP Scoreï¼ˆå›¾æ–‡ä¸€è‡´æ€§ï¼‰
from clip_score import CLIPScore

clip_score = CLIPScore()(images, texts)
# è¶Šé«˜è¶Šå¥½ï¼ˆ0-100ï¼‰
```

### Q6: å¤šæ¨¡æ€æ¨¡å‹éœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ
**A**: å–å†³äºä»»åŠ¡å’Œæ–¹æ³•ã€‚
```
ä»å¤´è®­ç»ƒï¼ˆå¦‚CLIPï¼‰:
  éœ€è¦ï¼šæ•°äº¿å›¾æ–‡å¯¹
  æ—¶é—´ï¼šæ•°å‘¨åˆ°æ•°æœˆ
  æˆæœ¬ï¼šæ•°ç™¾ä¸‡ç¾å…ƒ
  é€‚åˆï¼šå¤§å…¬å¸

å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹:
  éœ€è¦ï¼šæ•°åƒåˆ°æ•°ä¸‡æ ·æœ¬
  æ—¶é—´ï¼šæ•°å°æ—¶åˆ°æ•°å¤©
  æˆæœ¬ï¼šæ•°ç™¾åˆ°æ•°åƒç¾å…ƒ
  é€‚åˆï¼šå¤§å¤šæ•°åœºæ™¯ âœ…

é›¶æ ·æœ¬ä½¿ç”¨:
  éœ€è¦ï¼š0æ ·æœ¬ï¼
  æ—¶é—´ï¼šç«‹å³
  æˆæœ¬ï¼šå…è´¹
  é€‚åˆï¼šå¿«é€ŸåŸå‹ âœ…

å®é™…å»ºè®®ï¼š
  1. å…ˆå°è¯•é›¶æ ·æœ¬ï¼ˆCLIPï¼‰
  2. å¦‚æœä¸å¤Ÿå¥½ï¼Œæ”¶é›†æ•°æ®å¾®è°ƒ
  3. é€šå¸¸1K-10Kæ ·æœ¬å°±èƒ½æ˜¾è‘—æå‡
```

### Q7: LLaVAå’ŒGPT-4Væœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
**A**: å¼€æºvsé—­æºï¼Œæ€§èƒ½å·®è·ã€‚
```
LLaVAï¼ˆå¼€æºï¼‰:
  å‚æ•°ï¼š7B-13B
  æ€§èƒ½ï¼šå¾ˆå¥½ï¼ˆ80-85åˆ†ï¼‰
  æˆæœ¬ï¼šå…è´¹
  éƒ¨ç½²ï¼šå¯è‡ªå·±éƒ¨ç½²
  å®šåˆ¶ï¼šå¯ä»¥å¾®è°ƒ
  é€‚åˆï¼šç ”ç©¶ã€å®šåˆ¶åŒ–éœ€æ±‚

GPT-4Vï¼ˆé—­æºï¼‰:
  å‚æ•°ï¼šæœªçŸ¥ï¼ˆå¯èƒ½>1Tï¼‰
  æ€§èƒ½ï¼šæœ€å¼ºï¼ˆ95+åˆ†ï¼‰
  æˆæœ¬ï¼šAPIè°ƒç”¨ï¼ˆ$0.01-0.03/imageï¼‰
  éƒ¨ç½²ï¼šåªèƒ½é€šè¿‡API
  å®šåˆ¶ï¼šä¸èƒ½å¾®è°ƒ
  é€‚åˆï¼šå•†ä¸šåº”ç”¨ã€è¿½æ±‚æè‡´æ€§èƒ½

å®æµ‹å¯¹æ¯”ï¼ˆVQAä»»åŠ¡ï¼‰:
  LLaVA-13B: 80.0%
  GPT-4V: 93.1%
  
  å·®è·ï¼š13.1%
  ä½†LLaVAå…è´¹ä¸”å¯å®šåˆ¶ï¼

é€‰æ‹©å»ºè®®ï¼š
  - é¢„ç®—å……è¶³ï¼šGPT-4V
  - éœ€è¦å®šåˆ¶ï¼šLLaVA
  - æ•°æ®æ•æ„Ÿï¼šLLaVAï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
```

### Q8: å¦‚ä½•å¾®è°ƒå¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ
**A**: ç±»ä¼¼å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä½†è¦æ³¨æ„æ¨¡æ€å¯¹é½ã€‚
```python
# å¾®è°ƒLLaVAçš„ä¾‹å­
from transformers import LlavaForConditionalGeneration, Trainer

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf")

# 2. å‡†å¤‡æ•°æ®
# æ ¼å¼ï¼š{"image": image_path, "question": "...", "answer": "..."}
train_dataset = load_custom_dataset("train.json")

# 3. å†»ç»“éƒ¨åˆ†å‚æ•°ï¼ˆå¯é€‰ï¼‰
# åªå¾®è°ƒè¯­è¨€æ¨¡å‹éƒ¨åˆ†ï¼Œå†»ç»“è§†è§‰ç¼–ç å™¨
for param in model.vision_tower.parameters():
    param.requires_grad = False

# 4. è®­ç»ƒ
trainer = Trainer(
    model=model,
    train_dataset=train_dataset,
    args=TrainingArguments(
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-5,
        fp16=True,
    )
)

trainer.train()

# 5. è¯„ä¼°
results = trainer.evaluate(eval_dataset)

# å¾®è°ƒæŠ€å·§ï¼š
# - å­¦ä¹ ç‡è¦å°ï¼ˆ1e-5 to 5e-5ï¼‰
# - å¯ä»¥å†»ç»“è§†è§‰ç¼–ç å™¨
# - ä½¿ç”¨LoRAå‡å°‘æ˜¾å­˜
# - æ•°æ®è´¨é‡æ¯”æ•°é‡é‡è¦
```

### Q9: æ–‡ç”Ÿå›¾æ¨¡å‹å¦‚ä½•å·¥ä½œï¼Ÿ
**A**: æ‰©æ•£æ¨¡å‹ + æ–‡æœ¬æ¡ä»¶ã€‚
```python
# Stable Diffusionçš„å·¥ä½œæµç¨‹

# 1. æ–‡æœ¬ç¼–ç 
text = "a cat sitting on a mat"
text_embedding = text_encoder(text)  # CLIPæ–‡æœ¬ç¼–ç å™¨

# 2. ä»å™ªå£°å¼€å§‹
noise = torch.randn(latent_shape)  # éšæœºå™ªå£°

# 3. é€æ­¥å»å™ªï¼ˆ50æ­¥ï¼‰
for t in range(50, 0, -1):
    # é¢„æµ‹å™ªå£°
    predicted_noise = unet(noise, t, text_embedding)
    
    # å»é™¤ä¸€ç‚¹å™ªå£°
    noise = noise - predicted_noise * step_size

# 4. è§£ç åˆ°å›¾åƒ
image = vae_decoder(noise)

# å…³é”®å‚æ•°ï¼š
# - guidance_scale: æ–‡æœ¬å¼•å¯¼å¼ºåº¦ï¼ˆ7-15ï¼‰
#   å¤ªä½ï¼šå›¾åƒä¸æ–‡æœ¬ä¸ç¬¦
#   å¤ªé«˜ï¼šå›¾åƒè´¨é‡ä¸‹é™
# - num_steps: å»å™ªæ­¥æ•°ï¼ˆ20-50ï¼‰
#   å¤ªå°‘ï¼šè´¨é‡å·®
#   å¤ªå¤šï¼šæ…¢ä½†è´¨é‡å¥½

# å®é™…ä½¿ç”¨
pipe = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1")
image = pipe(
    prompt="a cat",
    negative_prompt="ugly, blurry",  # ä¸æƒ³è¦çš„
    guidance_scale=7.5,
    num_inference_steps=50
).images[0]
```

### Q10: å¤šæ¨¡æ€æ¨¡å‹çš„æœªæ¥æ–¹å‘ï¼Ÿ
**A**: æ›´å¤šæ¨¡æ€ã€æ›´å¼ºèƒ½åŠ›ã€æ›´æ˜“ç”¨ã€‚
```
è¶‹åŠ¿1ï¼šæ›´å¤šæ¨¡æ€
  ç°åœ¨ï¼šå›¾åƒ + æ–‡æœ¬
  æœªæ¥ï¼šå›¾åƒ + æ–‡æœ¬ + éŸ³é¢‘ + è§†é¢‘ + 3D
  ä¾‹å­ï¼šImageBindï¼ˆMetaï¼‰- 6ç§æ¨¡æ€

è¶‹åŠ¿2ï¼šæ›´å¼ºçš„æ¨ç†èƒ½åŠ›
  ç°åœ¨ï¼šè¯†åˆ«å’Œæè¿°
  æœªæ¥ï¼šæ·±åº¦æ¨ç†å’Œè§„åˆ’
  ä¾‹å­ï¼šGPT-4Vèƒ½è§£æ•°å­¦é¢˜ã€å†™ä»£ç 

è¶‹åŠ¿3ï¼šæ›´å¥½çš„ä¸–ç•Œæ¨¡å‹
  ç°åœ¨ï¼šé™æ€ç†è§£
  æœªæ¥ï¼šåŠ¨æ€é¢„æµ‹
  ä¾‹å­ï¼šWorld Modelsã€Soraï¼ˆè§†é¢‘ç”Ÿæˆï¼‰

è¶‹åŠ¿4ï¼šæ›´æ˜“ç”¨
  ç°åœ¨ï¼šéœ€è¦æŠ€æœ¯èƒŒæ™¯
  æœªæ¥ï¼šäººäººå¯ç”¨
  ä¾‹å­ï¼šChatGPTå¼çš„å¤šæ¨¡æ€ç•Œé¢

è¶‹åŠ¿5ï¼šç«¯ä¾§éƒ¨ç½²
  ç°åœ¨ï¼šäº‘ç«¯è¿è¡Œ
  æœªæ¥ï¼šæ‰‹æœºã€çœ¼é•œä¸Šè¿è¡Œ
  ä¾‹å­ï¼šMobileVLMã€TinyLLaVA

ç ”ç©¶çƒ­ç‚¹ï¼š
  - ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¶æ„
  - é«˜æ•ˆçš„æ¨¡æ€èåˆ
  - è§†é¢‘ç†è§£å’Œç”Ÿæˆ
  - 3Dåœºæ™¯ç†è§£
  - å…·èº«æ™ºèƒ½ï¼ˆæœºå™¨äººï¼‰

æœºä¼šï¼š
  - å‚ç›´é¢†åŸŸåº”ç”¨ï¼ˆåŒ»ç–—ã€æ•™è‚²ï¼‰
  - åˆ›ä½œå·¥å…·ï¼ˆè®¾è®¡ã€è§†é¢‘ï¼‰
  - è¾…åŠ©æŠ€æœ¯ï¼ˆç›²äººå¯¼èˆªï¼‰
  - å…ƒå®‡å®™å’ŒAR/VR
```

---

**æ­å–œä½ å®Œæˆç¬¬11ç« ï¼** ğŸ‰

ä½ ç°åœ¨å·²ç»æŒæ¡äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä»CLIPåˆ°LLaVAï¼Œä»å›¾æ–‡æ£€ç´¢åˆ°è§†è§‰é—®ç­”ï¼Œä»æ–‡ç”Ÿå›¾åˆ°è§†é¢‘ç†è§£ï¼Œä½ å·²ç»å…·å¤‡äº†æ„å»ºå¤šæ¨¡æ€AIåº”ç”¨çš„èƒ½åŠ›ã€‚

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼** â†’ [12_mixture_of_experts.md](12_mixture_of_experts.md)
