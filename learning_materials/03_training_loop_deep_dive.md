# ç¬¬03ç« ï¼šè®­ç»ƒå¾ªç¯å®Œå…¨æŒ‡å— - ç†è§£æ¢¯åº¦ä¸‹é™

> **å­¦ä¹ ç›®æ ‡**ï¼šæ·±å…¥ç†è§£GPTå¦‚ä½•é€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹   
> **éš¾åº¦ç­‰çº§**ï¼šğŸŒ¿ è¿›é˜¶  
> **é¢„è®¡æ—¶é—´**ï¼š35-45åˆ†é’Ÿ  
> **å‰ç½®çŸ¥è¯†**ï¼šç¬¬01ç« é…ç½®å‚æ•°ã€ç¬¬02ç« æ•°æ®åŠ è½½

---

## ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†å’Œç›´è§‰
- âœ… æŒæ¡å®Œæ•´è®­ç»ƒå¾ªç¯çš„æ¯ä¸ªæ­¥éª¤
- âœ… ç†è§£å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­çš„å·¥ä½œåŸç†
- âœ… çŸ¥é“æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦è£å‰ªçš„ä½œç”¨
- âœ… ç†è§£AdamWä¼˜åŒ–å™¨å¦‚ä½•æ›´æ–°å‚æ•°
- âœ… ä¼šè°ƒè¯•è®­ç»ƒé—®é¢˜ï¼ˆlossä¸é™ã€NaNç­‰ï¼‰

---

## ğŸ’­ å¼€å§‹ä¹‹å‰ï¼šæœºå™¨å­¦ä¹ åˆ°åº•æ˜¯æ€ä¹ˆ"å­¦ä¹ "çš„ï¼Ÿ

æƒ³è±¡ä½ åœ¨æ•™ä¸€ä¸ªå­¦ç”Ÿåšæ•°å­¦é¢˜ï¼š

```
âŒ ä¼ ç»Ÿç¼–ç¨‹æ–¹å¼ï¼š
  ä½ å‘Šè¯‰ä»–ï¼š"é‡åˆ°è¿™ç§é¢˜ï¼Œç”¨è¿™ä¸ªå…¬å¼"
  â†’ åªèƒ½è§£å†³ä½ æ•™è¿‡çš„é¢˜å‹

âœ… æœºå™¨å­¦ä¹ æ–¹å¼ï¼š
  1. å­¦ç”Ÿå…ˆççŒœç­”æ¡ˆ
  2. ä½ å‘Šè¯‰ä»–å¯¹é”™
  3. ä»–æ ¹æ®é”™è¯¯è°ƒæ•´æ€è·¯
  4. é‡å¤2-3ï¼Œç›´åˆ°å­¦ä¼š
  â†’ å¯ä»¥ä¸¾ä¸€åä¸‰ï¼
```

**è¿™å°±æ˜¯æ¢¯åº¦ä¸‹é™çš„æœ¬è´¨ï¼šé€šè¿‡ä¸æ–­è¯•é”™æ¥å­¦ä¹ ï¼**

åœ¨æ·±å…¥ä»£ç å‰ï¼Œæˆ‘ä»¬å¿…é¡»ç†è§£**æ¢¯åº¦ä¸‹é™**çš„æœ¬è´¨ã€‚

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¢¯åº¦ä¸‹é™çš„ç›´è§‰ç†è§£

### ğŸ”ï¸ æ¯”å–»ï¼šä¸‹å±±æ‰¾æœ€ä½ç‚¹

æƒ³è±¡ä½ åœ¨ä¸€åº§å¤§é›¾å¼¥æ¼«çš„å±±ä¸Šï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°å±±è„šï¼ˆæœ€ä½ç‚¹ï¼‰ï¼š

```
          â›°ï¸
        /    \
       /      \
      /   ä½ ğŸš¶  \
     /          \
    /            \
   /              \
  /________________\
      å±±è„š(ç›®æ ‡)
```

**é—®é¢˜ï¼šå¤§é›¾å¼¥æ¼«ï¼Œä½ çœ‹ä¸åˆ°å±±è„šåœ¨å“ªé‡Œï¼**

**è§£å†³æ–¹æ¡ˆï¼š**
1. æ„Ÿå—è„šä¸‹çš„å¡åº¦ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰
2. å¾€ä¸‹å¡æ–¹å‘èµ°ä¸€å°æ­¥ï¼ˆå‚æ•°æ›´æ–°ï¼‰
3. é‡å¤1-2ï¼Œç›´åˆ°åˆ°è¾¾å¹³åœ°ï¼ˆæ”¶æ•›ï¼‰

**è¿™å°±æ˜¯æ¢¯åº¦ä¸‹é™ï¼**

---

## ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°å­¦åŸç†ï¼ˆç”¨æœ€ç®€å•ä¾‹å­ï¼‰

### ä¾‹å­ï¼šè®­ç»ƒä¸€ä¸ªè¶…ç®€å•çš„"æ¨¡å‹"

å‡è®¾æˆ‘ä»¬è¦é¢„æµ‹ï¼šç»™å®šè¾“å…¥xï¼Œé¢„æµ‹è¾“å‡ºy

```python
# çœŸå®è§„å¾‹ï¼šy = 2x
# ä½†æ¨¡å‹ä¸çŸ¥é“ï¼Œéœ€è¦å­¦ä¹ 

# æ¨¡å‹ï¼šy_pred = w Ã— x ï¼ˆåªæœ‰ä¸€ä¸ªå‚æ•°wï¼‰
# ç›®æ ‡ï¼šæ‰¾åˆ° w = 2
```

**è®­ç»ƒæ•°æ®ï¼š**
```python
x_train = [1, 2, 3, 4, 5]
y_train = [2, 4, 6, 8, 10]
```

### è®­ç»ƒè¿‡ç¨‹ï¼ˆæ‰‹å·¥è®¡ç®—ï¼‰

**åˆå§‹çŠ¶æ€ï¼š**
```python
w = 0.5  # éšæœºåˆå§‹åŒ–ï¼ˆççŒœï¼‰
learning_rate = 0.1
```

**è¿­ä»£1ï¼š**

```python
# 1. å‰å‘ä¼ æ’­ï¼ˆé¢„æµ‹ï¼‰
x = 1
y_true = 2
y_pred = w Ã— x = 0.5 Ã— 1 = 0.5

# 2. è®¡ç®—æŸå¤±
loss = (y_pred - y_true)Â² = (0.5 - 2)Â² = 2.25

# 3. è®¡ç®—æ¢¯åº¦ï¼ˆæ•°å­¦æ¨å¯¼ï¼‰
# loss = (wÃ—x - y)Â²
# âˆ‚loss/âˆ‚w = 2(wÃ—x - y) Ã— x
gradient = 2 Ã— (0.5 - 2) Ã— 1 = -3.0

# 4. æ›´æ–°å‚æ•°
w = w - learning_rate Ã— gradient
w = 0.5 - 0.1 Ã— (-3.0) = 0.5 + 0.3 = 0.8
```

**è¿­ä»£2ï¼š**
```python
x = 2
y_true = 4
y_pred = 0.8 Ã— 2 = 1.6
loss = (1.6 - 4)Â² = 5.76
gradient = 2 Ã— (1.6 - 4) Ã— 2 = -9.6
w = 0.8 - 0.1 Ã— (-9.6) = 0.8 + 0.96 = 1.76
```

**è¿­ä»£3ï¼š**
```python
w = 1.76 â†’ 1.88 â†’ 1.94 â†’ 1.97 â†’ 1.99 â†’ 2.00 âœ…
```

**å¯è§†åŒ–ï¼š**
```
wçš„å˜åŒ–è¿‡ç¨‹ï¼š

     2.0 |                      â­ (ç›®æ ‡)
         |                   â—
     1.5 |              â—
         |         â—
     1.0 |    â—
         | â—
     0.5 |â—
         |
     0.0 |__________________________________
          0   1   2   3   4   5   6   7   (è¿­ä»£æ¬¡æ•°)
```

---

## ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šGPTè®­ç»ƒçš„å®Œæ•´æµç¨‹

### ğŸ”¥ å•æ¬¡è¿­ä»£çš„è¯¦ç»†æ­¥éª¤

è®©æˆ‘ä»¬ç”¨å®é™…æ•°å€¼æ¼”ç¤ºä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒè¿­ä»£ï¼š

```python
# å‡è®¾æˆ‘ä»¬å·²ç»å®Œæˆæ•°æ®åŠ è½½
X.shape = [4, 8]   # batch_size=4, block_size=8
Y.shape = [4, 8]

X = tensor([
    [20, 43,  1, 60, 43,  1, 52, 47],  # "he we ni"
    [18, 43, 44, 53, 43,  1, 56, 60],  # "Before w"
    [45, 53, 47, 52, 45,  1, 47, 52],  # "going in"
    [56, 46, 43,  1, 49, 47, 52, 45],  # "the king"
])

Y = tensor([
    [43,  1, 60, 43,  1, 52, 47, 56],  
    [43, 44, 53, 43,  1, 56, 60, 43],  
    [53, 47, 52, 45,  1, 47, 52, 56],  
    [46, 43,  1, 49, 47, 52, 45, 27],  
])
```

---

#### æ­¥éª¤1ï¼šå‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰

```python
# ä»£ç ï¼ˆtrain.py ç¬¬300è¡Œï¼‰
with ctx:
    logits, loss = model(X, Y)
```

**å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿï¼ˆç®€åŒ–ç‰ˆï¼‰**

```python
# æ¨¡å‹å†…éƒ¨ï¼ˆç®€åŒ–ï¼‰
def model(X, Y):
    # 1. åµŒå…¥å±‚ï¼šæŠŠtoken IDè½¬æ¢ä¸ºå‘é‡
    # X[0,0] = 20 â†’ embedding[20] = [0.23, -0.45, 0.67, ..., 0.12]
    #                                 (384ç»´å‘é‡)
    embeddings = token_embedding(X)  # [4, 8, 384]
    
    # 2. ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡å‹æ¯ä¸ªtokençš„ä½ç½®
    positions = position_embedding(torch.arange(8))  # [8, 384]
    x = embeddings + positions  # [4, 8, 384]
    
    # 3. Transformerå±‚ï¼ˆ6å±‚ï¼‰
    for layer in transformer_blocks:
        x = layer(x)  # è‡ªæ³¨æ„åŠ› + MLP
    
    # 4. è¾“å‡ºå±‚ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡
    logits = output_layer(x)  # [4, 8, 65]
    #         â†‘ å¯¹äº65ä¸ªå­—ç¬¦ï¼Œæ¯ä¸ªçš„æ¦‚ç‡
    
    # 5. è®¡ç®—æŸå¤±
    loss = cross_entropy(logits, Y)
    
    return logits, loss
```

**å…·ä½“æ•°å€¼ç¤ºä¾‹ï¼š**

```python
# ä»¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªä½ç½®ä¸ºä¾‹
è¾“å…¥: X[0, 0] = 20  (å­—ç¬¦ 'h')
ç›®æ ‡: Y[0, 0] = 43  (å­—ç¬¦ 'e')

# ç»è¿‡æ¨¡å‹å
logits[0, 0] = [
    0.1,   # å­—ç¬¦0çš„åˆ†æ•°
    0.3,   # å­—ç¬¦1çš„åˆ†æ•°
    ...
    2.8,   # å­—ç¬¦43çš„åˆ†æ•° (æ­£ç¡®ç­”æ¡ˆï¼ŒæœŸæœ›æœ€é«˜ï¼)
    ...
    0.2,   # å­—ç¬¦64çš„åˆ†æ•°
]  # å…±65ä¸ªåˆ†æ•°

# è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆsoftmaxï¼‰
probs[0, 0] = [
    0.01,   # å­—ç¬¦0: 1%
    0.02,   # å­—ç¬¦1: 2%
    ...
    0.35,   # å­—ç¬¦43: 35% â† æœ€é«˜æ¦‚ç‡
    ...
    0.01,   # å­—ç¬¦64: 1%
]  # æ€»å’Œ = 100%

# ç†æƒ³æƒ…å†µï¼š
ideal_probs = [
    0.00,   # å­—ç¬¦0: 0%
    0.00,   # å­—ç¬¦1: 0%
    ...
    1.00,   # å­—ç¬¦43: 100% â† åº”è¯¥100%ç¡®å®š
    ...
    0.00,   # å­—ç¬¦64: 0%
]
```

**æŸå¤±è®¡ç®—ï¼ˆCross Entropyï¼‰ï¼š**

```python
# äº¤å‰ç†µæŸå¤±å…¬å¼ï¼š
loss = -log(P(æ­£ç¡®ç­”æ¡ˆ))

# è®¡ç®—
loss = -log(0.35) = 1.05

# ç›´è§‰ç†è§£ï¼š
å¦‚æœ P(æ­£ç¡®ç­”æ¡ˆ) = 0.99 â†’ loss = -log(0.99) = 0.01 (å¾ˆå¥½ï¼)
å¦‚æœ P(æ­£ç¡®ç­”æ¡ˆ) = 0.50 â†’ loss = -log(0.50) = 0.69 (ä¸€èˆ¬)
å¦‚æœ P(æ­£ç¡®ç­”æ¡ˆ) = 0.01 â†’ loss = -log(0.01) = 4.61 (å¾ˆå·®ï¼)

# å¯¹æ‰€æœ‰ä½ç½®ã€æ‰€æœ‰æ ·æœ¬æ±‚å¹³å‡
total_loss = average([lossâ‚€â‚€, lossâ‚€â‚, ..., lossâ‚ƒâ‚‡])
           = 2.45  # ç¤ºä¾‹å€¼
```

---

#### æ­¥éª¤2ï¼šæ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰

```python
# ä»£ç ï¼ˆtrain.py ç¬¬292-305è¡Œï¼‰
for micro_step in range(gradient_accumulation_steps):
    with ctx:
        logits, loss = model(X, Y)
        loss = loss / gradient_accumulation_steps
    X, Y = get_batch('train')
    scaler.scale(loss).backward()
```

**ä¸ºä»€ä¹ˆè¦é™¤ä»¥ gradient_accumulation_stepsï¼Ÿ**

```python
# å‡è®¾ gradient_accumulation_steps = 4

# ä¸é™¤çš„è¯ï¼š
micro_step 1: lossâ‚ = 2.5 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 2.5
micro_step 2: lossâ‚‚ = 2.3 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 2.5 + 2.3 = 4.8
micro_step 3: lossâ‚ƒ = 2.4 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 4.8 + 2.4 = 7.2
micro_step 4: lossâ‚„ = 2.6 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 7.2 + 2.6 = 9.8

æœ€ç»ˆæ¢¯åº¦ = 9.8  ï¼ˆå¤ªå¤§äº†ï¼æ˜¯æ­£å¸¸çš„4å€ï¼‰

# é™¤ä»¥4çš„è¯ï¼š
micro_step 1: lossâ‚ = 2.5/4 = 0.625 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 0.625
micro_step 2: lossâ‚‚ = 2.3/4 = 0.575 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 1.200
micro_step 3: lossâ‚ƒ = 2.4/4 = 0.600 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 1.800
micro_step 4: lossâ‚„ = 2.6/4 = 0.650 â†’ backward â†’ ç´¯ç§¯æ¢¯åº¦ = 2.450

æœ€ç»ˆæ¢¯åº¦ = 2.45  ï¼ˆæ­£ç¡®ï¼ç›¸å½“äºbatch_size=64çš„å¹³å‡æ¢¯åº¦ï¼‰
```

---

#### æ­¥éª¤3ï¼šåå‘ä¼ æ’­ï¼ˆBackward Passï¼‰

```python
scaler.scale(loss).backward()
```

**å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿï¼ˆè¶…è¯¦ç»†ï¼‰**

```python
# é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰è®¡ç®—æ¢¯åº¦

# å‡è®¾æ¨¡å‹åªæœ‰3ä¸ªå‚æ•°ï¼šw1, w2, w3
# å‰å‘ä¼ æ’­è·¯å¾„ï¼š
X â†’ w1 â†’ h1 â†’ w2 â†’ h2 â†’ w3 â†’ output â†’ loss

# åå‘ä¼ æ’­è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼š

# 1. ä»losså¼€å§‹
âˆ‚loss/âˆ‚loss = 1  # è‡ªå·±å¯¹è‡ªå·±çš„å¯¼æ•°=1

# 2. outputå±‚çš„æ¢¯åº¦
âˆ‚loss/âˆ‚output = ... (ç”±äº¤å‰ç†µå…¬å¼è®¡ç®—)

# 3. w3çš„æ¢¯åº¦
âˆ‚loss/âˆ‚w3 = âˆ‚loss/âˆ‚output Ã— âˆ‚output/âˆ‚w3

# 4. h2çš„æ¢¯åº¦  
âˆ‚loss/âˆ‚h2 = âˆ‚loss/âˆ‚output Ã— âˆ‚output/âˆ‚h2

# 5. w2çš„æ¢¯åº¦
âˆ‚loss/âˆ‚w2 = âˆ‚loss/âˆ‚h2 Ã— âˆ‚h2/âˆ‚w2

# ... ä¸€ç›´ä¼ æ’­åˆ°w1

# ç»“æœï¼šæ¯ä¸ªå‚æ•°éƒ½çŸ¥é“"æ€ä¹ˆè°ƒæ•´èƒ½é™ä½loss"
```

**å…·ä½“æ•°å€¼ç¤ºä¾‹ï¼š**

```python
# æŸä¸ªå‚æ•°wçš„æ¢¯åº¦è®¡ç®—

å½“å‰å€¼: w = 0.523
loss = 2.45

# åå‘ä¼ æ’­è®¡ç®—å¾—åˆ°ï¼š
âˆ‚loss/âˆ‚w = -0.38

# å«ä¹‰ï¼š
# wå¢åŠ 0.001 â†’ losså‡å°‘ 0.38Ã—0.001 = 0.00038
# wå¢åŠ 0.1   â†’ losså‡å°‘ 0.38Ã—0.1   = 0.038

# æ‰€ä»¥æˆ‘ä»¬åº”è¯¥å¢åŠ wï¼ï¼ˆå› ä¸ºæ¢¯åº¦æ˜¯è´Ÿçš„ï¼‰
```

---

#### æ­¥éª¤4ï¼šæ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰

```python
# ä»£ç ï¼ˆtrain.py ç¬¬307-309è¡Œï¼‰
if grad_clip != 0.0:
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
```

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**

```python
# é—®é¢˜ï¼šæ¢¯åº¦çˆ†ç‚¸

æ­£å¸¸æ¢¯åº¦ï¼š
w1.grad = 0.23
w2.grad = -0.45
w3.grad = 0.67
...

å¼‚å¸¸æƒ…å†µï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼‰ï¼š
w1.grad = 0.23
w2.grad = -0.45
w3.grad = 0.67
w4.grad = 234.56  âš ï¸ çªç„¶å˜å¾—å¾ˆå¤§ï¼
w5.grad = -1892.34 âš ï¸ 
w6.grad = 45629.12 âš ï¸ 

å¦‚æœä¸è£å‰ªï¼Œå‚æ•°æ›´æ–°ï¼š
w4 = w4 - 0.001 Ã— 234.56 = w4 - 0.23  (å˜åŒ–å¤ªå¤§ï¼)
w5 = w5 - 0.001 Ã— (-1892.34) = w5 + 1.89 (å·¨å˜ï¼)
â†’ æ¨¡å‹å´©æºƒï¼Œlosså˜æˆNaN
```

**è£å‰ªè¿‡ç¨‹ï¼š**

```python
# 1. è®¡ç®—æ‰€æœ‰æ¢¯åº¦çš„æ€»èŒƒæ•°
grad_norm = sqrt(sum(gÂ² for g in all_gradients))
          = sqrt(0.23Â² + 0.45Â² + ... + 45629.12Â²)
          = 45630.5  # éå¸¸å¤§ï¼

# 2. è®¾å®šé˜ˆå€¼
grad_clip = 1.0

# 3. å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼ŒæŒ‰æ¯”ä¾‹ç¼©å°
if grad_norm > grad_clip:
    scaling_factor = grad_clip / grad_norm
                   = 1.0 / 45630.5
                   = 0.000022
    
    # æ‰€æœ‰æ¢¯åº¦ä¹˜ä»¥è¿™ä¸ªå› å­
    w1.grad = 0.23 Ã— 0.000022 = 0.0000051
    w2.grad = -0.45 Ã— 0.000022 = -0.0000099
    ...
    w6.grad = 45629.12 Ã— 0.000022 = 1.00
    
    # æ–°çš„grad_normåˆšå¥½ç­‰äº1.0

# æ•ˆæœï¼šä¿æŒæ¢¯åº¦çš„ç›¸å¯¹æ–¹å‘ï¼Œä½†é™åˆ¶å¤§å°
```

---

#### æ­¥éª¤5ï¼šå‚æ•°æ›´æ–°ï¼ˆOptimizer Stepï¼‰

```python
# ä»£ç ï¼ˆtrain.py ç¬¬311-314è¡Œï¼‰
scaler.step(optimizer)
scaler.update()
optimizer.zero_grad(set_to_none=True)
```

**AdamWä¼˜åŒ–å™¨çš„æ›´æ–°è¿‡ç¨‹ï¼š**

```python
# æ ‡å‡†SGDï¼ˆæœ€ç®€å•ï¼‰ï¼š
w = w - learning_rate Ã— gradient
w = 0.523 - 0.001 Ã— (-0.38) = 0.523 + 0.00038 = 0.52338

# AdamWï¼ˆå¤æ‚ä½†å¼ºå¤§ï¼‰ï¼š
# ç»´æŠ¤ä¸¤ä¸ªç§»åŠ¨å¹³å‡ï¼š
# m: æ¢¯åº¦çš„ä¸€é˜¶çŸ©ï¼ˆmomentumï¼ŒåŠ¨é‡ï¼‰
# v: æ¢¯åº¦çš„äºŒé˜¶çŸ©ï¼ˆvarianceï¼Œæ–¹å·®ï¼‰

# åˆå§‹çŠ¶æ€
m = 0
v = 0
beta1 = 0.9
beta2 = 0.95
learning_rate = 0.001
weight_decay = 0.1

# ç¬¬1æ­¥
gradient = -0.38

# æ›´æ–°må’Œv
m = beta1 Ã— m + (1-beta1) Ã— gradient
  = 0.9 Ã— 0 + 0.1 Ã— (-0.38)
  = -0.038

v = beta2 Ã— v + (1-beta2) Ã— gradientÂ²
  = 0.95 Ã— 0 + 0.05 Ã— (0.38)Â²
  = 0.00722

# åå·®ä¿®æ­£ï¼ˆå‰å‡ æ­¥å¾ˆé‡è¦ï¼‰
m_corrected = m / (1 - beta1^t)
            = -0.038 / (1 - 0.9)
            = -0.38

v_corrected = v / (1 - beta2^t)
            = 0.00722 / (1 - 0.95)
            = 0.1444

# å‚æ•°æ›´æ–°ï¼ˆåŠ ä¸Šweight_decayï¼‰
w = w - learning_rate Ã— (m_corrected / sqrt(v_corrected) + weight_decay Ã— w)
  = 0.523 - 0.001 Ã— (-0.38/sqrt(0.1444) + 0.1Ã—0.523)
  = 0.523 - 0.001 Ã— (-1.00 + 0.0523)
  = 0.523 + 0.001 Ã— 0.9477
  = 0.52395

# ç¬¬2æ­¥
gradient = -0.42  # æ–°çš„æ¢¯åº¦

m = 0.9 Ã— (-0.038) + 0.1 Ã— (-0.42) = -0.0762
v = 0.95 Ã— 0.00722 + 0.05 Ã— (0.42)Â² = 0.01567

w = ... (ç»§ç»­æ›´æ–°)
```

**ä¸ºä»€ä¹ˆAdamWæ¯”SGDå¥½ï¼Ÿ**

```
åœºæ™¯1ï¼šç¨³å®šä¸‹é™åŒºåŸŸ
  SGD: gradient = 0.1 â†’ update = 0.1 Ã— lr
  Adam: é€‚åº”æ€§è°ƒæ•´ï¼Œæ­¥é•¿ç¨³å®š

åœºæ™¯2ï¼šé™¡å³­åŒºåŸŸ
  SGD: gradient = 100 â†’ update = 100 Ã— lr (å¤ªå¤§ï¼)
  Adam: æ£€æµ‹åˆ°æ–¹å·®å¤§ â†’ è‡ªåŠ¨å‡å°æ­¥é•¿ âœ…

åœºæ™¯3ï¼šå¹³å¦åŒºåŸŸ  
  SGD: gradient = 0.001 â†’ updateå¾ˆå° (å¤ªæ…¢)
  Adam: ç´¯ç§¯åŠ¨é‡ â†’ ç»§ç»­å‰è¿› âœ…

åœºæ™¯4ï¼šéœ‡è¡åŒºåŸŸ
  SGD: gradient = [+50, -48, +52, -49, ...]
  Adam: åŠ¨é‡ç›¸äº’æŠµæ¶ˆ â†’ ç¨³å®šå‰è¿› âœ…
```

---

## ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šå®Œæ•´è®­ç»ƒå¾ªç¯å¯è§†åŒ–

### ğŸ“Š ä¸€ä¸ªå®Œæ•´epochçš„losså˜åŒ–

```python
# å‡è®¾è®­ç»ƒ1000æ­¥

æ­¥éª¤    Loss    è¯´æ˜
----    ----    ----
0       4.174   éšæœºåˆå§‹åŒ–ï¼Œä¹±çŒœ
10      3.892   å¼€å§‹å­¦ä¹ 
50      3.245   å­¦åˆ°ä¸€äº›æ¨¡å¼
100     2.687   è¶Šæ¥è¶Šå¥½
200     2.234   
500     1.845   æ¥è¿‘æ”¶æ•›
1000    1.469   âœ… è®­ç»ƒå®Œæˆ
```

**å¯è§†åŒ–ï¼š**
```
Loss
4.0 |â—
    |  â—
3.5 |    â—
    |      â—â—
3.0 |         â—â—
    |            â—â—
2.5 |              â—â—â—
    |                  â—â—â—
2.0 |                     â—â—â—â—
    |                         â—â—â—â—â—
1.5 |                              â—â—â—â—â—â—â—
    |_________________________________________
    0    200   400   600   800   1000  (æ­¥æ•°)
```

### ğŸ” å‚æ•°çš„å˜åŒ–è½¨è¿¹

```python
# æŸä¸ªå‚æ•°wåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–

æ­¥éª¤    wå€¼     æ¢¯åº¦     æ›´æ–°é‡
----    ---     ----     -----
0       0.523   -0.38    +0.00038
1       0.52338 -0.42    +0.00042
2       0.52380 -0.39    +0.00039
...
100     0.687   -0.12    +0.00012
...
500     0.823   -0.03    +0.00003
...
1000    0.856   -0.001   +0.000001  â† å‡ ä¹ä¸å˜äº†ï¼ˆæ”¶æ•›ï¼‰
```

---

## ğŸ“š ç¬¬äº”éƒ¨åˆ†ï¼šå®æˆ˜è°ƒè¯•æŠ€å·§

### ğŸ”§ æ‰“å°æ¢¯åº¦ä¿¡æ¯

åˆ›å»ºè°ƒè¯•è„šæœ¬ï¼š

```python
# debug_training.py

import torch
from model import GPT, GPTConfig

# åˆ›å»ºå°æ¨¡å‹
config = GPTConfig(
    vocab_size=65,
    n_layer=2,
    n_head=2,
    n_embd=64,
    block_size=16,
)
model = GPT(config)

# è™šæ‹Ÿæ•°æ®
X = torch.randint(0, 65, (4, 16))
Y = torch.randint(0, 65, (4, 16))

# å‰å‘ä¼ æ’­
logits, loss = model(X, Y)
print(f"Loss: {loss.item():.4f}")

# åå‘ä¼ æ’­
loss.backward()

# æ£€æŸ¥æ¢¯åº¦
print("\nå‚æ•°æ¢¯åº¦ç»Ÿè®¡ï¼š")
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_mean = param.grad.mean().item()
        grad_std = param.grad.std().item()
        grad_max = param.grad.abs().max().item()
        print(f"{name:40s} | mean: {grad_mean:+.6f}, std: {grad_std:.6f}, max: {grad_max:.6f}")

# è¾“å‡ºç¤ºä¾‹ï¼š
# transformer.wte.weight                   | mean: +0.000123, std: 0.002341, max: 0.023451
# transformer.wpe.weight                   | mean: -0.000087, std: 0.001234, max: 0.012345
# transformer.h.0.attn.c_attn.weight       | mean: +0.000456, std: 0.003456, max: 0.034567
# ...
```

### ğŸ¯ ç›‘æ§è®­ç»ƒå¥åº·åº¦

```python
# æ·»åŠ åˆ°train.pyä¸­

# æ¯100æ­¥æ£€æŸ¥ä¸€æ¬¡
if iter_num % 100 == 0:
    # 1. æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
    total_norm = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    print(f"Gradient norm: {total_norm:.4f}")
    
    # 2. æ£€æŸ¥å‚æ•°æ›´æ–°æ¯”ä¾‹
    # æ›´æ–°é‡åº”è¯¥æ˜¯å‚æ•°å€¼çš„0.001-0.01å€
    
    # 3. æ£€æŸ¥lossæ˜¯å¦ä¸ºNaN
    if math.isnan(loss.item()):
        print("âŒ Loss is NaN! Training crashed!")
        break
    
    # 4. æ£€æŸ¥lossæ˜¯å¦åœ¨ä¸‹é™
    # ...
```

### ğŸ› å¸¸è§é—®é¢˜è¯Šæ–­

```python
# é—®é¢˜1: Lossä¸ä¸‹é™
å¯èƒ½åŸå› ï¼š
  - learning_rateå¤ªå° â†’ å¢å¤§åˆ°1e-3
  - æ¨¡å‹å¤ªå° â†’ å¢åŠ n_layer, n_embd
  - æ•°æ®æœ‰é—®é¢˜ â†’ æ£€æŸ¥get_batch()

# é—®é¢˜2: Losså˜æˆNaN
å¯èƒ½åŸå› ï¼š
  - learning_rateå¤ªå¤§ â†’ å‡å°åˆ°1e-4
  - æ¢¯åº¦çˆ†ç‚¸ â†’ å¯ç”¨grad_clip=1.0
  - æ•°å€¼ä¸ç¨³å®š â†’ ä½¿ç”¨float32è€Œä¸æ˜¯float16

# é—®é¢˜3: è¿‡æ‹Ÿåˆï¼ˆtrain loss << val lossï¼‰
è§£å†³æ–¹æ¡ˆï¼š
  - å¢åŠ dropout=0.2
  - å¢åŠ weight_decay=0.1
  - è·å–æ›´å¤šæ•°æ®
  - å‡å°æ¨¡å‹

# é—®é¢˜4: è®­ç»ƒå¤ªæ…¢
ä¼˜åŒ–æ–¹æ¡ˆï¼š
  - å¯ç”¨compile=True
  - å¢å¤§batch_size
  - ä½¿ç”¨å¤šGPU
  - å‡å°eval_interval
```

---

## ğŸ“ æ€»ç»“ï¼šè®­ç»ƒå¾ªç¯å®Œæ•´æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. åˆå§‹åŒ–                                    â”‚
â”‚    - åˆ›å»ºæ¨¡å‹ï¼ˆéšæœºå‚æ•°ï¼‰                      â”‚
â”‚    - åˆ›å»ºä¼˜åŒ–å™¨                               â”‚
â”‚    - è®¾ç½®å­¦ä¹ ç‡                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ä¸»å¾ªç¯ (while iter_num < max_iters)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2.1 è°ƒæ•´å­¦ä¹ ç‡         â”‚
        â”‚  lr = get_lr(iter_num) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2.2 æ¢¯åº¦ç´¯ç§¯å¾ªç¯ (4æ¬¡)                 â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ a) åŠ è½½æ•°æ®: X, Y = get_batch() â”‚  â”‚
        â”‚  â”‚ b) å‰å‘ä¼ æ’­: logits, loss = model(X,Y) â”‚
        â”‚  â”‚ c) åå‘ä¼ æ’­: loss.backward()    â”‚  â”‚
        â”‚  â”‚ d) ç´¯ç§¯æ¢¯åº¦                      â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2.3 æ¢¯åº¦è£å‰ª           â”‚
        â”‚  clip_grad_norm_(...)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2.4 å‚æ•°æ›´æ–°           â”‚
        â”‚  optimizer.step()      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2.5 æ¸…ç©ºæ¢¯åº¦           â”‚
        â”‚  optimizer.zero_grad() â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2.6 å®šæœŸè¯„ä¼° (æ¯2000æ­¥)     â”‚
        â”‚  - è®¡ç®—val loss             â”‚
        â”‚  - ä¿å­˜checkpoint           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2.7 è®°å½•æ—¥å¿—           â”‚
        â”‚  - æ‰“å°loss             â”‚
        â”‚  - è®¡ç®—é€Ÿåº¦             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
              iter_num += 1
                    â†“
           è¾¾åˆ°max_iters? â”€â”€Noâ”€â”€â†’ è¿”å›2.1
                    â”‚
                   Yes
                    â†“
              è®­ç»ƒå®Œæˆï¼âœ…
```

---

---

## ğŸ“ æ€»ç»“ä¸æ£€æŸ¥

### âœ… çŸ¥è¯†æ£€æŸ¥æ¸…å•

å®Œæˆå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€æ¦‚å¿µï¼ˆå¿…é¡»æŒæ¡ï¼‰**
- [ ] ç”¨"ä¸‹å±±"æ¯”å–»è§£é‡Šæ¢¯åº¦ä¸‹é™
- [ ] ç†è§£lossæ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆè¦æœ€å°åŒ–
- [ ] çŸ¥é“å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„åŒºåˆ«
- [ ] ç†è§£æ¢¯åº¦çš„å«ä¹‰ï¼ˆå‚æ•°è°ƒæ•´æ–¹å‘ï¼‰
- [ ] çŸ¥é“learning_rateçš„ä½œç”¨

**è¿›é˜¶ç†è§£ï¼ˆå»ºè®®æŒæ¡ï¼‰**
- [ ] ç†è§£æ¢¯åº¦ç´¯ç§¯çš„å·¥ä½œåŸç†
- [ ] çŸ¥é“ä¸ºä»€ä¹ˆéœ€è¦æ¢¯åº¦è£å‰ª
- [ ] ç†è§£AdamWæ¯”SGDå¥½åœ¨å“ªé‡Œ
- [ ] èƒ½è§£é‡Šäº¤å‰ç†µæŸå¤±çš„è®¡ç®—
- [ ] çŸ¥é“å¦‚ä½•è¯Šæ–­è®­ç»ƒé—®é¢˜

**å®æˆ˜èƒ½åŠ›ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰**
- [ ] èƒ½å†™å‡ºè°ƒè¯•è„šæœ¬æ£€æŸ¥æ¢¯åº¦
- [ ] ä¼šç›‘æ§è®­ç»ƒå¥åº·åº¦
- [ ] èƒ½è§£å†³lossä¸é™ã€NaNç­‰é—®é¢˜
- [ ] ç†è§£å®Œæ•´è®­ç»ƒå¾ªç¯çš„æ¯ä¸ªæ­¥éª¤
- [ ] èƒ½ä¼˜åŒ–è®­ç»ƒé€Ÿåº¦å’Œç¨³å®šæ€§

### ğŸ“Š æ ¸å¿ƒè¦ç‚¹æ€»ç»“

```python
è®­ç»ƒå¾ªç¯ = é‡å¤ä»¥ä¸‹æ­¥éª¤ï¼š

1. å‰å‘ä¼ æ’­ (Forward Pass)
   è¾“å…¥ â†’ æ¨¡å‹ â†’ è¾“å‡º â†’ è®¡ç®—loss
   
2. åå‘ä¼ æ’­ (Backward Pass)
   loss â†’ é“¾å¼æ³•åˆ™ â†’ è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
   
3. æ¢¯åº¦è£å‰ª (Gradient Clipping)
   é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œä¿æŒè®­ç»ƒç¨³å®š
   
4. å‚æ•°æ›´æ–° (Optimizer Step)
   å‚æ•° = å‚æ•° - learning_rate Ã— æ¢¯åº¦
   ï¼ˆAdamWæœ‰è‡ªé€‚åº”è°ƒæ•´ï¼‰
   
5. æ¸…ç©ºæ¢¯åº¦ (Zero Grad)
   å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£
```

### ğŸ¯ å…³é”®å…¬å¼

```python
# æ¢¯åº¦ä¸‹é™
w_new = w_old - learning_rate Ã— gradient

# äº¤å‰ç†µæŸå¤±
loss = -log(P(æ­£ç¡®ç­”æ¡ˆ))

# æ¢¯åº¦è£å‰ª
if grad_norm > threshold:
    gradient = gradient Ã— (threshold / grad_norm)

# AdamWæ›´æ–°
m = Î²1 Ã— m + (1-Î²1) Ã— gradient
v = Î²2 Ã— v + (1-Î²2) Ã— gradientÂ²
w = w - lr Ã— (m / âˆšv + weight_decay Ã— w)
```

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

ç°åœ¨ä½ å®Œå…¨ç†è§£äº†è®­ç»ƒå¾ªç¯ï¼ä½ çŸ¥é“äº†ï¼š

âœ… æ¢¯åº¦ä¸‹é™çš„æ•°å­¦åŸç†  
âœ… å‰å‘ä¼ æ’­è®¡ç®—loss  
âœ… åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦  
âœ… æ¢¯åº¦ç´¯ç§¯çš„å®ç°  
âœ… æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸  
âœ… AdamWä¼˜åŒ–å™¨æ›´æ–°å‚æ•°  
âœ… æ•´ä¸ªå¾ªç¯å¦‚ä½•ä¸²è”  

**æ¥ä¸‹æ¥åº”è¯¥å­¦ä¹ ï¼š**

1. **04_complete_guide_and_experiments.md** - å®Œæ•´æŒ‡å—ä¸å®éªŒ
2. **05_model_architecture_deep_dive.md** - æ·±å…¥ç†è§£Transformer
3. **å®æˆ˜ç»ƒä¹ ** - è¿è¡Œè°ƒè¯•è„šæœ¬ï¼Œç›‘æ§è®­ç»ƒè¿‡ç¨‹

### ğŸ’¡ å®è·µå»ºè®®

1. **è¿è¡Œè°ƒè¯•è„šæœ¬**ï¼šæŸ¥çœ‹å®é™…çš„æ¢¯åº¦å’Œå‚æ•°å˜åŒ–
2. **ç›‘æ§è®­ç»ƒ**ï¼šæ·»åŠ æ—¥å¿—ï¼Œè§‚å¯Ÿlossæ›²çº¿
3. **å®éªŒå‚æ•°**ï¼šå°è¯•ä¸åŒçš„learning_rateå’Œä¼˜åŒ–å™¨
4. **æ•…æ„åˆ¶é€ é—®é¢˜**ï¼šè®¾ç½®å¾ˆå¤§çš„lrï¼Œè§‚å¯Ÿæ¢¯åº¦çˆ†ç‚¸

---

## ğŸ“š æ¨èèµ„æº

### ğŸ“– å»¶ä¼¸é˜…è¯»
- [æ¢¯åº¦ä¸‹é™å¯è§†åŒ–](https://distill.pub/2017/momentum/)
- [Adamä¼˜åŒ–å™¨è®ºæ–‡](https://arxiv.org/abs/1412.6980)
- [åå‘ä¼ æ’­è¯¦è§£](http://colah.github.io/posts/2015-08-Backprop/)

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- [3Blue1Brown: ç¥ç»ç½‘ç»œ](https://www.youtube.com/watch?v=aircAruvnKk)
- [Andrej Karpathy: åå‘ä¼ æ’­](https://www.youtube.com/watch?v=VMj-3S1tku0)

### ğŸ”§ å®ç”¨å·¥å…·
- [TensorBoard](https://www.tensorflow.org/tensorboard) - å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
- [Weights & Biases](https://wandb.ai/) - å®éªŒè¿½è¸ª

---

## ğŸ› å¸¸è§é—®é¢˜ FAQ

### Q1: ä¸ºä»€ä¹ˆlossä¸€å¼€å§‹å¾ˆé«˜ï¼Ÿ

```python
# åˆå§‹çŠ¶æ€ï¼šå‚æ•°éšæœºåˆå§‹åŒ–
# æ¨¡å‹å®Œå…¨ä¸çŸ¥é“è¯¥é¢„æµ‹ä»€ä¹ˆ

åˆå§‹loss â‰ˆ -log(1/vocab_size)
        = -log(1/65)
        = 4.17

# è¿™æ˜¯éšæœºçŒœæµ‹çš„loss
# éšç€è®­ç»ƒï¼Œlossä¼šé€æ¸ä¸‹é™
```

### Q2: æ¢¯åº¦ç´¯ç§¯å’Œå¢å¤§batch_sizeæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

```python
# æ•ˆæœç›¸åŒï¼š
batch_size = 64, grad_accum = 1
batch_size = 16, grad_accum = 4

# åŒºåˆ«ï¼š
æ¢¯åº¦ç´¯ç§¯ï¼š
  âœ… æ˜¾å­˜å ç”¨å°
  âŒ è®­ç»ƒé€Ÿåº¦ç¨æ…¢ï¼ˆå¤šæ¬¡å‰å‘ä¼ æ’­ï¼‰
  
å¤§batch_sizeï¼š
  âœ… è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆä¸€æ¬¡å‰å‘ä¼ æ’­ï¼‰
  âŒ æ˜¾å­˜å ç”¨å¤§
```

### Q3: ä¸ºä»€ä¹ˆè¦ç”¨AdamWè€Œä¸æ˜¯SGDï¼Ÿ

```python
SGDçš„é—®é¢˜ï¼š
  - éœ€è¦ç²¾å¿ƒè°ƒæ•´learning_rate
  - åœ¨ä¸åŒå‚æ•°ä¸Šä½¿ç”¨ç›¸åŒçš„lr
  - å®¹æ˜“å¡åœ¨å¹³å¦åŒºåŸŸ
  
AdamWçš„ä¼˜åŠ¿ï¼š
  âœ… è‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªå‚æ•°çš„lr
  âœ… æœ‰åŠ¨é‡ï¼Œå¯ä»¥è¶Šè¿‡å±€éƒ¨æœ€ä¼˜
  âœ… æ›´ç¨³å®šï¼Œæ›´å®¹æ˜“æ”¶æ•›
  âœ… æ˜¯ç›®å‰è®­ç»ƒLLMçš„æ ‡å‡†é€‰æ‹©
```

### Q4: å¦‚ä½•åˆ¤æ–­è®­ç»ƒæ˜¯å¦æ­£å¸¸ï¼Ÿ

```python
å¥åº·çš„è®­ç»ƒï¼š
  âœ… lossç¨³å®šä¸‹é™
  âœ… æ¢¯åº¦èŒƒæ•°åœ¨0.1-10ä¹‹é—´
  âœ… å‚æ•°æ›´æ–°é‡æ˜¯å‚æ•°å€¼çš„0.001-0.01å€
  âœ… æ²¡æœ‰NaNæˆ–Inf
  
å¼‚å¸¸æƒ…å†µï¼š
  âŒ lossä¸é™æˆ–éœ‡è¡ â†’ è°ƒå°lr
  âŒ losså˜æˆNaN â†’ æ¢¯åº¦çˆ†ç‚¸ï¼Œå¯ç”¨grad_clip
  âŒ lossä¸‹é™å¤ªæ…¢ â†’ è°ƒå¤§lræˆ–å¢å¤§æ¨¡å‹
  âŒ train loss << val loss â†’ è¿‡æ‹Ÿåˆï¼Œå¢åŠ æ­£åˆ™åŒ–
```

---

**æ­å–œä½ å®Œæˆç¬¬03ç« ï¼** ğŸ‰

ä½ ç°åœ¨å·²ç»æŒæ¡äº†è®­ç»ƒå¾ªç¯çš„å®Œæ•´åŸç†ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒï¼Œç†è§£äº†è¿™ä¸€ç« ï¼Œä½ å°±ç†è§£äº†AIå¦‚ä½•"å­¦ä¹ "ã€‚

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼** â†’ [04_complete_guide_and_experiments.md](04_complete_guide_and_experiments.md)
