# RLHFå¯¹é½å®Œå…¨æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**è¯­è¨€æ¨¡å‹çš„å›°å¢ƒï¼š**
```python
è®­ç»ƒå¥½çš„GPTæ¨¡å‹:
  è¾“å…¥: "å¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿ"
  è¾“å‡º: [è¯¦ç»†çš„å±é™©å†…å®¹] âŒ

  è¾“å…¥: "å†™ä¸€ç¯‡æ–‡ç« "
  è¾“å‡º: [å¯èƒ½åŒ…å«åè§ã€æœ‰å®³å†…å®¹] âŒ

é—®é¢˜:
  âœ… æ¨¡å‹å¾ˆå¼ºå¤§ï¼ˆèƒ½ç”Ÿæˆæµç•…æ–‡æœ¬ï¼‰
  âŒ ä½†ä¸çŸ¥é“ä»€ä¹ˆè¯¥è¯´ï¼Œä»€ä¹ˆä¸è¯¥è¯´
  âŒ æ²¡æœ‰ä»·å€¼è§‚å¯¹é½
  âŒ å¯èƒ½äº§ç”Ÿæœ‰å®³è¾“å‡º
```

**RLHFçš„è§£å†³æ–¹æ¡ˆï¼š**
```python
é€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰è®©æ¨¡å‹:
  âœ… æ‹’ç»æœ‰å®³è¯·æ±‚
  âœ… ç”Ÿæˆæœ‰å¸®åŠ©çš„å›ç­”
  âœ… ç¬¦åˆäººç±»ä»·å€¼è§‚
  âœ… æ›´å®‰å…¨ã€æ›´å¯æ§

å®ä¾‹å¯¹æ¯”:
  åŸºç¡€GPT: "å¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿ" â†’ [å±é™©å†…å®¹]
  RLHFå:  "å¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿ" â†’ "æˆ‘ä¸èƒ½æä¾›è¿™ç±»ä¿¡æ¯ï¼Œè¿™å¾ˆå±é™©..."
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šRLHFåŸºç¡€æ¦‚å¿µ

### ğŸ” ä»€ä¹ˆæ˜¯RLHFï¼Ÿ

**RLHF = Reinforcement Learning from Human Feedback**

```python
æ ¸å¿ƒæ€è·¯ï¼š

1. é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå·²å®Œæˆï¼‰
   ä»æµ·é‡æ–‡æœ¬å­¦ä¹ è¯­è¨€èƒ½åŠ›
   ä½†ä¸çŸ¥é“"å¥½"ä¸"å"

2. äººç±»æ ‡æ³¨åå¥½
   ç»™æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªå›ç­”æ‰“åˆ†
   "è¿™ä¸ªå›ç­”å¥½"ã€"é‚£ä¸ªå›ç­”ä¸å¥½"

3. è®­ç»ƒå¥–åŠ±æ¨¡å‹
   å­¦ä¹ äººç±»çš„åå¥½
   èƒ½è‡ªåŠ¨åˆ¤æ–­å›ç­”è´¨é‡

4. å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–
   ç”¨å¥–åŠ±æ¨¡å‹æŒ‡å¯¼
   è®©æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å›ç­”
```

### ğŸ“Š RLHFä¸‰é˜¶æ®µ

```python
é˜¶æ®µ1: ç›‘ç£å¾®è°ƒ (SFT - Supervised Fine-Tuning)
  è¾“å…¥: é«˜è´¨é‡çš„é—®ç­”å¯¹
  è¾“å‡º: åŸºç¡€å¯¹è¯æ¨¡å‹
  
  ç¤ºä¾‹:
    Q: "ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ"
    A: "å…‰åˆä½œç”¨æ˜¯æ¤ç‰©åˆ©ç”¨å…‰èƒ½..."
  
  ç›®æ ‡: è®©æ¨¡å‹å­¦ä¼šå¯¹è¯æ ¼å¼

é˜¶æ®µ2: å¥–åŠ±æ¨¡å‹è®­ç»ƒ (RM - Reward Model)
  è¾“å…¥: åŒä¸€ä¸ªé—®é¢˜çš„å¤šä¸ªå›ç­” + äººç±»æ’åº
  è¾“å‡º: èƒ½æ‰“åˆ†çš„å¥–åŠ±æ¨¡å‹
  
  ç¤ºä¾‹:
    Q: "è§£é‡Šé‡å­åŠ›å­¦"
    A1: "é‡å­åŠ›å­¦å¾ˆå¤æ‚..." (å¾—åˆ†: 0.3)
    A2: "é‡å­åŠ›å­¦æ˜¯ç ”ç©¶..." (å¾—åˆ†: 0.9)
  
  ç›®æ ‡: å­¦ä¹ äººç±»åå¥½

é˜¶æ®µ3: PPOå¼ºåŒ–å­¦ä¹  (PPO - Proximal Policy Optimization)
  è¾“å…¥: SFTæ¨¡å‹ + å¥–åŠ±æ¨¡å‹
  è¾“å‡º: å¯¹é½åçš„æœ€ç»ˆæ¨¡å‹
  
  è¿‡ç¨‹:
    1. ç”Ÿæˆå›ç­”
    2. å¥–åŠ±æ¨¡å‹æ‰“åˆ†
    3. æ ¹æ®åˆ†æ•°æ›´æ–°æ¨¡å‹
    4. é‡å¤ä¼˜åŒ–
  
  ç›®æ ‡: æœ€å¤§åŒ–äººç±»åå¥½
```

**å¯è§†åŒ–æµç¨‹ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLHFå®Œæ•´æµç¨‹                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é˜¶æ®µ1: SFT (1-2å¤©)
  é¢„è®­ç»ƒæ¨¡å‹ + é«˜è´¨é‡å¯¹è¯æ•°æ® â†’ SFTæ¨¡å‹
  
é˜¶æ®µ2: å¥–åŠ±æ¨¡å‹ (1å‘¨)
  æ”¶é›†äººç±»åå¥½æ•°æ®
  â”‚
  â”œâ”€ åŒä¸€é—®é¢˜ï¼Œå¤šä¸ªå›ç­”
  â”œâ”€ äººç±»æ’åºï¼šA > B > C > D
  â””â”€ è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆåˆ†ç±»å™¨ï¼‰
  
é˜¶æ®µ3: PPOè®­ç»ƒ (1-2å‘¨)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  RLè®­ç»ƒå¾ªç¯                       â”‚
  â”‚                                  â”‚
  â”‚  1. SFTæ¨¡å‹ç”Ÿæˆå›ç­”              â”‚
  â”‚         â†“                        â”‚
  â”‚  2. å¥–åŠ±æ¨¡å‹æ‰“åˆ†                 â”‚
  â”‚         â†“                        â”‚
  â”‚  3. è®¡ç®—PPO loss                 â”‚
  â”‚         â†“                        â”‚
  â”‚  4. æ›´æ–°æ¨¡å‹å‚æ•°                 â”‚
  â”‚         â†“                        â”‚
  â”‚  5. é‡å¤10K-100Kæ­¥               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ ç¬¬äºŒéƒ¨åˆ†ï¼šé˜¶æ®µ1 - ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

### ğŸ“ æ•°æ®å‡†å¤‡

**é«˜è´¨é‡å¯¹è¯æ•°æ®æ ¼å¼ï¼š**

```python
# sft_data.jsonl

{
  "prompt": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
  "response": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚ä¸»è¦åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚"
}

{
  "prompt": "å¦‚ä½•å­¦ä¹ Pythonï¼Ÿ",
  "response": "å­¦ä¹ Pythonçš„å»ºè®®æ­¥éª¤ï¼š\n1. ä»åŸºç¡€è¯­æ³•å¼€å§‹\n2. åšå°é¡¹ç›®ç»ƒä¹ \n3. é˜…è¯»ä¼˜ç§€ä»£ç \n4. å‚ä¸å¼€æºé¡¹ç›®\n5. æŒç»­å®è·µ"
}

{
  "prompt": "å¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿ",
  "response": "æˆ‘ä¸èƒ½æä¾›è¿™ç±»ä¿¡æ¯ã€‚è¿™ç§å†…å®¹å¯èƒ½å¯¼è‡´å±é™©æˆ–éæ³•æ´»åŠ¨ã€‚å¦‚æœä½ å¯¹åŒ–å­¦æ„Ÿå…´è¶£ï¼Œæˆ‘å¯ä»¥æ¨èä¸€äº›å®‰å…¨çš„å­¦ä¹ èµ„æºã€‚"
}

# éœ€è¦10K-100Kæ¡é«˜è´¨é‡å¯¹è¯
```

**æ•°æ®æ”¶é›†æ–¹å¼ï¼š**

```python
æ–¹æ³•1: äººå·¥æ ‡æ³¨ï¼ˆæœ€å¥½ï¼Œä½†æ˜‚è´µï¼‰
  é›‡ä½£æ ‡æ³¨å‘˜ç¼–å†™é«˜è´¨é‡å›ç­”
  æˆæœ¬: ~$50,000 for 50Kæ ·æœ¬

æ–¹æ³•2: ä½¿ç”¨ç°æœ‰æ•°æ®é›†
  - OpenAssistant Conversations
  - ShareGPT
  - Anthropic HH-RLHF
  æˆæœ¬: å…è´¹

æ–¹æ³•3: è‡ªæˆ‘æŒ‡å¯¼ï¼ˆSelf-Instructï¼‰
  ç”¨å¼ºæ¨¡å‹ï¼ˆGPT-4ï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®
  æˆæœ¬: APIè´¹ç”¨ ~$500

æ–¹æ³•4: æ··åˆæ–¹å¼ï¼ˆæ¨èï¼‰
  ç°æœ‰æ•°æ® + äººå·¥ç­›é€‰ + GPT-4å¢å¼º
  æˆæœ¬: ~$5,000
```

### ğŸ”§ SFTè®­ç»ƒä»£ç 

```python
# train_sft.py

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from model import GPT, GPTConfig
import json
from tqdm import tqdm
import tiktoken

class SFTDataset(Dataset):
    """SFTè®­ç»ƒæ•°æ®é›†"""
    def __init__(self, data_path, max_length=1024):
        self.max_length = max_length
        self.enc = tiktoken.get_encoding("gpt2")
        
        # åŠ è½½æ•°æ®
        self.data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line)
                self.data.append(item)
        
        print(f"åŠ è½½äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼
        text = f"### Human: {item['prompt']}\n### Assistant: {item['response']}"
        
        # Tokenize
        tokens = self.enc.encode(text)
        
        # æˆªæ–­æˆ–å¡«å……
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        
        # åªåœ¨Assistantéƒ¨åˆ†è®¡ç®—loss
        # æ‰¾åˆ°"### Assistant:"çš„ä½ç½®
        assistant_start = text.find("### Assistant:")
        assistant_start_token = len(self.enc.encode(text[:assistant_start]))
        
        # åˆ›å»ºmaskï¼ˆåªåœ¨å›ç­”éƒ¨åˆ†è®¡ç®—lossï¼‰
        loss_mask = torch.zeros(len(tokens))
        loss_mask[assistant_start_token:] = 1
        
        return {
            'input_ids': torch.tensor(tokens[:-1], dtype=torch.long),
            'labels': torch.tensor(tokens[1:], dtype=torch.long),
            'loss_mask': loss_mask[1:]  # å¯¹é½labels
        }

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°ï¼ˆå¤„ç†ä¸åŒé•¿åº¦ï¼‰"""
    max_len = max(item['input_ids'].size(0) for item in batch)
    
    input_ids = []
    labels = []
    loss_masks = []
    
    for item in batch:
        # Padding
        pad_len = max_len - item['input_ids'].size(0)
        
        input_ids.append(F.pad(item['input_ids'], (0, pad_len), value=0))
        labels.append(F.pad(item['labels'], (0, pad_len), value=-100))  # -100ä¼šè¢«å¿½ç•¥
        loss_masks.append(F.pad(item['loss_mask'], (0, pad_len), value=0))
    
    return {
        'input_ids': torch.stack(input_ids),
        'labels': torch.stack(labels),
        'loss_mask': torch.stack(loss_masks)
    }

def train_sft(
    model_path='gpt2',  # ä»GPT-2å¼€å§‹
    data_path='sft_data.jsonl',
    output_dir='out-sft',
    epochs=3,
    batch_size=4,
    learning_rate=5e-6,
    device='cuda'
):
    """SFTè®­ç»ƒä¸»å‡½æ•°"""
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½åŸºç¡€æ¨¡å‹...")
    if model_path == 'gpt2':
        # ä»HuggingFaceåŠ è½½
        from transformers import GPT2LMHeadModel
        model = GPT2LMHeadModel.from_pretrained('gpt2')
    else:
        # ä»checkpointåŠ è½½
        checkpoint = torch.load(model_path)
        gptconf = GPTConfig(**checkpoint['model_args'])
        model = GPT(gptconf)
        model.load_state_dict(checkpoint['model'])
    
    model = model.to(device)
    model.train()
    
    # å‡†å¤‡æ•°æ®
    print("å‡†å¤‡æ•°æ®...")
    dataset = SFTDataset(data_path)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=4
    )
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    
    # è®­ç»ƒ
    print("å¼€å§‹SFTè®­ç»ƒ...")
    global_step = 0
    
    for epoch in range(epochs):
        epoch_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in pbar:
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)
            loss_mask = batch['loss_mask'].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(input_ids, labels=labels)
            loss = outputs.loss
            
            # åªåœ¨Assistantéƒ¨åˆ†è®¡ç®—lossï¼ˆå¯é€‰ï¼‰
            if loss_mask is not None:
                # é‡æ–°è®¡ç®—masked loss
                logits = outputs.logits
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()
                shift_mask = loss_mask[..., :-1].contiguous()
                
                loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1)
                )
                loss = (loss * shift_mask.view(-1)).sum() / shift_mask.sum()
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            # è®°å½•
            epoch_loss += loss.item()
            global_step += 1
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
            
            # å®šæœŸä¿å­˜
            if global_step % 500 == 0:
                save_path = f'{output_dir}/checkpoint-{global_step}.pt'
                torch.save({
                    'model': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'step': global_step,
                }, save_path)
                print(f"\nä¿å­˜checkpoint: {save_path}")
        
        avg_loss = epoch_loss / len(dataloader)
        print(f"Epoch {epoch+1} å¹³å‡loss: {avg_loss:.4f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = f'{output_dir}/sft_model.pt'
    torch.save({'model': model.state_dict()}, final_path)
    print(f"SFTè®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: {final_path}")

if __name__ == "__main__":
    train_sft(
        data_path='data/sft_train.jsonl',
        epochs=3,
        batch_size=4,
        learning_rate=5e-6
    )
```

**è¿è¡ŒSFTï¼š**

```bash
# å‡†å¤‡æ•°æ®
python prepare_sft_data.py

# è®­ç»ƒ
python train_sft.py

# è¾“å‡º:
# åŠ è½½åŸºç¡€æ¨¡å‹...
# åŠ è½½äº† 50000 æ¡è®­ç»ƒæ•°æ®
# å¼€å§‹SFTè®­ç»ƒ...
# Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [2:15:30<00:00, loss=2.1234]
# Epoch 1 å¹³å‡loss: 2.3456
# ...
# SFTè®­ç»ƒå®Œæˆï¼
```

---

## ğŸ“Š ç¬¬ä¸‰éƒ¨åˆ†ï¼šé˜¶æ®µ2 - å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰

### ğŸ¯ å¥–åŠ±æ¨¡å‹çš„ä½œç”¨

```python
å¥–åŠ±æ¨¡å‹ = å­¦ä¹ äººç±»åå¥½çš„åˆ†ç±»å™¨

è¾“å…¥: é—®é¢˜ + å›ç­”
è¾“å‡º: åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰

ä¾‹å­:
  Q: "è§£é‡Šç›¸å¯¹è®º"
  
  A1: "çˆ±å› æ–¯å¦æå‡ºçš„..." 
  RMåˆ†æ•°: 0.85 âœ…
  
  A2: "æˆ‘ä¸çŸ¥é“"
  RMåˆ†æ•°: 0.12 âŒ
  
  A3: "ç›¸å¯¹è®ºåŒ…æ‹¬ç‹­ä¹‰ç›¸å¯¹è®ºå’Œå¹¿ä¹‰ç›¸å¯¹è®º..."
  RMåˆ†æ•°: 0.95 âœ…âœ…
```

### ğŸ“ åå¥½æ•°æ®æ”¶é›†

```python
# preference_data.jsonl

{
  "prompt": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
  "responses": [
    {
      "text": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚",
      "rank": 1  # æœ€å¥½
    },
    {
      "text": "æ·±åº¦å­¦ä¹ å°±æ˜¯ç”¨ç¥ç»ç½‘ç»œã€‚",
      "rank": 3  # æœ€å·®
    },
    {
      "text": "æ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§AIæŠ€æœ¯ï¼Œæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
      "rank": 2  # ä¸­ç­‰
    }
  ]
}

# æˆ–è€…ä½¿ç”¨æˆå¯¹æ¯”è¾ƒï¼ˆæ›´å¸¸è§ï¼‰
{
  "prompt": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
  "chosen": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯...",
  "rejected": "æ·±åº¦å­¦ä¹ å°±æ˜¯ç”¨ç¥ç»ç½‘ç»œã€‚"
}
```

**æ”¶é›†æ–¹å¼ï¼š**

```python
å·¥å…·: Label Studio, Scale AI

æµç¨‹:
1. SFTæ¨¡å‹ç”Ÿæˆå¤šä¸ªå›ç­”ï¼ˆé€šå¸¸4-8ä¸ªï¼‰
   ä½¿ç”¨ä¸åŒæ¸©åº¦/top-ké‡‡æ ·

2. äººç±»æ ‡æ³¨å‘˜æ’åº
   ä»æœ€å¥½åˆ°æœ€å·®: A > B > C > D

3. è½¬æ¢ä¸ºæˆå¯¹æ¯”è¾ƒ
   (A, B), (A, C), (A, D)
   (B, C), (B, D)
   (C, D)

4. å½¢æˆè®­ç»ƒæ•°æ®
   æ¯å¯¹åŒ…å«ï¼šchosenï¼ˆæ›´å¥½ï¼‰å’Œrejectedï¼ˆæ›´å·®ï¼‰

æ•°æ®é‡éœ€æ±‚:
  æœ€å°‘: 10Kå¯¹æ¯”è¾ƒ
  æ¨è: 50K-100Kå¯¹æ¯”è¾ƒ
  OpenAI: æ•°ç™¾ä¸‡å¯¹æ¯”è¾ƒ
```

### ğŸ”§ å¥–åŠ±æ¨¡å‹è®­ç»ƒ

```python
# train_reward_model.py

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from model import GPT, GPTConfig
import json

class RewardModel(nn.Module):
    """å¥–åŠ±æ¨¡å‹ï¼šåœ¨GPTåŸºç¡€ä¸ŠåŠ ä¸€ä¸ªåˆ†æ•°å¤´"""
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        
        # å¥–åŠ±å¤´ï¼šä»æœ€åä¸€ä¸ªtokençš„embeddingé¢„æµ‹åˆ†æ•°
        self.reward_head = nn.Linear(base_model.config.n_embd, 1, bias=False)
        
        # åˆå§‹åŒ–
        self.reward_head.weight.data.zero_()
    
    def forward(self, input_ids, attention_mask=None):
        """
        è¿”å›æ¯ä¸ªåºåˆ—çš„å¥–åŠ±åˆ†æ•°
        """
        # è·å–base modelçš„è¾“å‡º
        outputs = self.base_model.transformer(input_ids)
        hidden_states = outputs  # [batch, seq_len, n_embd]
        
        # å–æœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„hidden state
        if attention_mask is not None:
            # æ‰¾åˆ°æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
            sequence_lengths = attention_mask.sum(dim=1) - 1
            last_hidden = hidden_states[
                torch.arange(hidden_states.size(0)),
                sequence_lengths
            ]
        else:
            # å¦‚æœæ²¡æœ‰maskï¼Œå–æœ€åä¸€ä¸ªtoken
            last_hidden = hidden_states[:, -1, :]
        
        # é¢„æµ‹å¥–åŠ±åˆ†æ•°
        rewards = self.reward_head(last_hidden).squeeze(-1)
        
        return rewards

class PreferenceDataset(Dataset):
    """åå¥½æ•°æ®é›†"""
    def __init__(self, data_path, tokenizer, max_length=512):
        self.data = []
        with open(data_path, 'r') as f:
            for line in f:
                self.data.append(json.loads(line))
        
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ ¼å¼åŒ–
        prompt = item['prompt']
        chosen = item['chosen']
        rejected = item['rejected']
        
        # Tokenize
        chosen_text = f"### Human: {prompt}\n### Assistant: {chosen}"
        rejected_text = f"### Human: {prompt}\n### Assistant: {rejected}"
        
        chosen_tokens = self.tokenizer.encode(chosen_text)[:self.max_length]
        rejected_tokens = self.tokenizer.encode(rejected_text)[:self.max_length]
        
        return {
            'chosen_ids': torch.tensor(chosen_tokens, dtype=torch.long),
            'rejected_ids': torch.tensor(rejected_tokens, dtype=torch.long)
        }

def collate_fn(batch):
    """å¤„ç†ä¸åŒé•¿åº¦"""
    # æ‰¾åˆ°æœ€å¤§é•¿åº¦
    max_chosen = max(item['chosen_ids'].size(0) for item in batch)
    max_rejected = max(item['rejected_ids'].size(0) for item in batch)
    max_len = max(max_chosen, max_rejected)
    
    chosen_ids = []
    rejected_ids = []
    chosen_masks = []
    rejected_masks = []
    
    for item in batch:
        # Pad chosen
        c_len = item['chosen_ids'].size(0)
        chosen_ids.append(
            torch.cat([item['chosen_ids'], torch.zeros(max_len - c_len, dtype=torch.long)])
        )
        chosen_masks.append(
            torch.cat([torch.ones(c_len), torch.zeros(max_len - c_len)])
        )
        
        # Pad rejected
        r_len = item['rejected_ids'].size(0)
        rejected_ids.append(
            torch.cat([item['rejected_ids'], torch.zeros(max_len - r_len, dtype=torch.long)])
        )
        rejected_masks.append(
            torch.cat([torch.ones(r_len), torch.zeros(max_len - r_len)])
        )
    
    return {
        'chosen_ids': torch.stack(chosen_ids),
        'rejected_ids': torch.stack(rejected_ids),
        'chosen_mask': torch.stack(chosen_masks),
        'rejected_mask': torch.stack(rejected_masks)
    }

def train_reward_model(
    sft_model_path='out-sft/sft_model.pt',
    data_path='data/preference_data.jsonl',
    output_dir='out-rm',
    epochs=1,
    batch_size=4,
    learning_rate=1e-5,
    device='cuda'
):
    """è®­ç»ƒå¥–åŠ±æ¨¡å‹"""
    
    # åŠ è½½SFTæ¨¡å‹
    print("åŠ è½½SFTæ¨¡å‹...")
    checkpoint = torch.load(sft_model_path)
    # å‡è®¾æˆ‘ä»¬æœ‰GPTConfig
    model = RewardModel(base_model)  # base_modelæ˜¯åŠ è½½çš„SFTæ¨¡å‹
    model = model.to(device)
    model.train()
    
    # æ•°æ®
    import tiktoken
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = PreferenceDataset(data_path, tokenizer)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    
    # è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹...")
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        for batch in dataloader:
            chosen_ids = batch['chosen_ids'].to(device)
            rejected_ids = batch['rejected_ids'].to(device)
            chosen_mask = batch['chosen_mask'].to(device)
            rejected_mask = batch['rejected_mask'].to(device)
            
            # è®¡ç®—å¥–åŠ±
            reward_chosen = model(chosen_ids, chosen_mask)
            reward_rejected = model(rejected_ids, rejected_mask)
            
            # Loss: chosenåº”è¯¥æ¯”rejectedåˆ†æ•°é«˜
            # ä½¿ç”¨ranking lossï¼ˆä¹Ÿå«pairwise lossï¼‰
            loss = -torch.log(torch.sigmoid(reward_chosen - reward_rejected)).mean()
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            correct += (reward_chosen > reward_rejected).sum().item()
            total += reward_chosen.size(0)
        
        accuracy = correct / total
        avg_loss = total_loss / len(dataloader)
        
        print(f"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2%}")
    
    # ä¿å­˜
    save_path = f'{output_dir}/reward_model.pt'
    torch.save({'model': model.state_dict()}, save_path)
    print(f"å¥–åŠ±æ¨¡å‹ä¿å­˜åˆ°: {save_path}")

if __name__ == "__main__":
    train_reward_model(
        sft_model_path='out-sft/sft_model.pt',
        data_path='data/preference_train.jsonl',
        epochs=1,
        batch_size=4
    )
```

**è®­ç»ƒæ•ˆæœï¼š**

```bash
python train_reward_model.py

# è¾“å‡º:
# åŠ è½½SFTæ¨¡å‹...
# åŠ è½½äº† 50000 æ¡åå¥½æ•°æ®
# å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹...
# Epoch 1: Loss=0.3421, Accuracy=82.34%
# å¥–åŠ±æ¨¡å‹ä¿å­˜åˆ°: out-rm/reward_model.pt

# 82%çš„å‡†ç¡®ç‡æ„å‘³ç€å¥–åŠ±æ¨¡å‹èƒ½æ­£ç¡®è¯†åˆ«82%çš„åå¥½
```

---

## ğŸ® ç¬¬å››éƒ¨åˆ†ï¼šé˜¶æ®µ3 - PPOå¼ºåŒ–å­¦ä¹ 

### ğŸ¯ PPOç®—æ³•æ ¸å¿ƒ

**PPO (Proximal Policy Optimization) = ç¨³å®šçš„ç­–ç•¥ä¼˜åŒ–**

```python
å¼ºåŒ–å­¦ä¹ è®¾ç½®:

çŠ¶æ€(State): ç”¨æˆ·é—®é¢˜
åŠ¨ä½œ(Action): ç”Ÿæˆçš„å›ç­”
å¥–åŠ±(Reward): å¥–åŠ±æ¨¡å‹ç»™çš„åˆ†æ•°
ç›®æ ‡: æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±

PPOæ ¸å¿ƒæ€æƒ³:
  1. ç”Ÿæˆå›ç­”
  2. è·å¾—å¥–åŠ±
  3. æ›´æ–°ç­–ç•¥ï¼ˆæ¨¡å‹å‚æ•°ï¼‰
  4. ä½†ä¸è¦æ›´æ–°å¤ªæ¿€è¿›ï¼ˆProximal = æ¥è¿‘çš„ï¼‰
     é¿å…æ¨¡å‹å˜å¾—ç–¯ç‹‚
```

**PPO Losså…¬å¼ï¼š**

```python
PPO LossåŒ…å«ä¸‰éƒ¨åˆ†:

1. Policy Loss (ç­–ç•¥æŸå¤±)
   é¼“åŠ±æ¨¡å‹ç”Ÿæˆé«˜å¥–åŠ±çš„å›ç­”
   
   L_policy = -min(
       r(Î¸) * A,
       clip(r(Î¸), 1-Îµ, 1+Îµ) * A
   )
   
   å…¶ä¸­:
   - r(Î¸) = Ï€_new(a|s) / Ï€_old(a|s)  # æ–°æ—§ç­–ç•¥æ¯”
   - A = ä¼˜åŠ¿å‡½æ•°ï¼ˆè¿™ä¸ªåŠ¨ä½œæœ‰å¤šå¥½ï¼‰
   - clip: é™åˆ¶æ›´æ–°å¹…åº¦
   - Îµ = 0.2ï¼ˆå…¸å‹å€¼ï¼‰

2. Value Loss (ä»·å€¼æŸå¤±)
   å¸®åŠ©ä¼°è®¡çŠ¶æ€ä»·å€¼
   
   L_value = (V(s) - Return)Â²

3. KL Penalty (KLæ•£åº¦æƒ©ç½š)
   é˜²æ­¢æ¨¡å‹åç¦»SFTæ¨¡å‹å¤ªè¿œ
   ä¿æŒè¯­è¨€èƒ½åŠ›
   
   L_kl = Î² * KL(Ï€_new || Ï€_sft)
   
æ€»Loss:
   L = L_policy + c1 * L_value + c2 * L_kl
```

### ğŸ”§ PPOè®­ç»ƒå®ç°

```python
# train_ppo.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
from model import GPT
from train_reward_model import RewardModel

class PPOTrainer:
    """PPOè®­ç»ƒå™¨"""
    def __init__(
        self,
        policy_model,      # SFTæ¨¡å‹ï¼ˆè¦ä¼˜åŒ–çš„ï¼‰
        ref_model,         # å‚è€ƒæ¨¡å‹ï¼ˆå›ºå®šçš„SFTæ¨¡å‹ï¼‰
        reward_model,      # å¥–åŠ±æ¨¡å‹
        tokenizer,
        kl_coef=0.1,       # KLæƒ©ç½šç³»æ•°
        clip_epsilon=0.2,  # PPO clipèŒƒå›´
        value_coef=0.5,    # ä»·å€¼æŸå¤±ç³»æ•°
        max_length=512,
        device='cuda'
    ):
        self.policy = policy_model.to(device)
        self.ref_model = ref_model.to(device)
        self.reward_model = reward_model.to(device)
        self.tokenizer = tokenizer
        
        self.kl_coef = kl_coef
        self.clip_epsilon = clip_epsilon
        self.value_coef = value_coef
        self.max_length = max_length
        self.device = device
        
        # å†»ç»“å‚è€ƒæ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹
        self.ref_model.eval()
        self.reward_model.eval()
        for param in self.ref_model.parameters():
            param.requires_grad = False
        for param in self.reward_model.parameters():
            param.requires_grad = False
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.policy.parameters(),
            lr=1e-6  # éå¸¸å°çš„å­¦ä¹ ç‡ï¼
        )
    
    @torch.no_grad()
    def generate_responses(self, prompts, temperature=1.0):
        """
        ç”Ÿæˆå›ç­”ï¼Œå¹¶è®°å½•log_probsç”¨äºPPO
        
        Returns:
            responses: ç”Ÿæˆçš„æ–‡æœ¬
            log_probs: æ¯ä¸ªtokençš„logæ¦‚ç‡
            values: ä»·å€¼ä¼°è®¡ï¼ˆå¯é€‰ï¼‰
        """
        self.policy.eval()
        
        batch_size = len(prompts)
        responses = []
        all_log_probs = []
        
        for prompt in prompts:
            # Tokenize prompt
            prompt_tokens = torch.tensor(
                [self.tokenizer.encode(prompt)],
                dtype=torch.long,
                device=self.device
            )
            
            generated = prompt_tokens
            log_probs = []
            
            # é€tokenç”Ÿæˆ
            for _ in range(self.max_length):
                outputs = self.policy(generated)
                logits = outputs.logits[:, -1, :]  # æœ€åä¸€ä¸ªtokençš„logits
                
                # é‡‡æ ·
                probs = F.softmax(logits / temperature, dim=-1)
                dist = Categorical(probs)
                next_token = dist.sample()
                
                # è®°å½•log_prob
                log_prob = dist.log_prob(next_token)
                log_probs.append(log_prob.item())
                
                # æ·»åŠ åˆ°åºåˆ—
                generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)
                
                # åœæ­¢æ¡ä»¶ï¼ˆé‡åˆ°EOSæˆ–å¤ªé•¿ï¼‰
                if next_token.item() == self.tokenizer.eot_token:
                    break
            
            # è§£ç 
            response = self.tokenizer.decode(generated[0].tolist())
            responses.append(response)
            all_log_probs.append(torch.tensor(log_probs))
        
        return responses, all_log_probs
    
    def compute_rewards(self, prompts, responses):
        """
        è®¡ç®—å¥–åŠ±
        
        Reward = RMåˆ†æ•° - Î² * KL(Ï€||Ï€_ref)
        """
        rewards = []
        
        for prompt, response in zip(prompts, responses):
            # 1. å¥–åŠ±æ¨¡å‹åˆ†æ•°
            full_text = f"### Human: {prompt}\n### Assistant: {response}"
            tokens = torch.tensor(
                [self.tokenizer.encode(full_text)],
                dtype=torch.long,
                device=self.device
            )
            
            with torch.no_grad():
                rm_score = self.reward_model(tokens).item()
            
            # 2. KLæ•£åº¦æƒ©ç½š
            # è®¡ç®—policyå’Œref_modelçš„KLæ•£åº¦
            with torch.no_grad():
                policy_logits = self.policy(tokens).logits
                ref_logits = self.ref_model(tokens).logits
                
                policy_logprobs = F.log_softmax(policy_logits, dim=-1)
                ref_logprobs = F.log_softmax(ref_logits, dim=-1)
                
                # KLæ•£åº¦ï¼ˆæ¯ä¸ªtokenï¼‰
                kl = (policy_logprobs.exp() * (policy_logprobs - ref_logprobs)).sum(dim=-1)
                kl_penalty = kl.mean().item()
            
            # æ€»å¥–åŠ±
            reward = rm_score - self.kl_coef * kl_penalty
            rewards.append(reward)
        
        return torch.tensor(rewards, device=self.device)
    
    def compute_advantages(self, rewards):
        """
        è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        A = R - baseline
        """
        # ç®€å•åœ°ç”¨å¥–åŠ±çš„å‡å€¼ä½œä¸ºbaseline
        baseline = rewards.mean()
        advantages = rewards - baseline
        
        # æ ‡å‡†åŒ–
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return advantages
    
    def ppo_step(self, prompts, responses, old_log_probs, advantages):
        """
        ä¸€æ­¥PPOæ›´æ–°
        """
        self.policy.train()
        
        total_policy_loss = 0
        total_kl = 0
        
        for prompt, response, old_log_prob, advantage in zip(
            prompts, responses, old_log_probs, advantages
        ):
            # é‡æ–°è®¡ç®—log_probsï¼ˆä½¿ç”¨å½“å‰ç­–ç•¥ï¼‰
            full_text = f"### Human: {prompt}\n### Assistant: {response}"
            tokens = torch.tensor(
                [self.tokenizer.encode(full_text)],
                dtype=torch.long,
                device=self.device
            )
            
            outputs = self.policy(tokens)
            logits = outputs.logits
            
            # è®¡ç®—æ¯ä¸ªtokençš„log_prob
            log_probs = F.log_softmax(logits, dim=-1)
            # å–å®é™…ç”Ÿæˆçš„tokençš„log_prob
            token_log_probs = log_probs.gather(2, tokens[:, 1:].unsqueeze(-1)).squeeze(-1)
            new_log_prob = token_log_probs.sum()
            
            # Importance ratio
            ratio = torch.exp(new_log_prob - old_log_prob)
            
            # PPO clip
            clipped_ratio = torch.clamp(
                ratio,
                1 - self.clip_epsilon,
                1 + self.clip_epsilon
            )
            
            # Policy loss
            policy_loss = -torch.min(
                ratio * advantage,
                clipped_ratio * advantage
            )
            
            total_policy_loss += policy_loss
            
            # KLï¼ˆç”¨äºç›‘æ§ï¼‰
            with torch.no_grad():
                kl = (new_log_prob - old_log_prob).abs()
                total_kl += kl.item()
        
        # å¹³å‡
        avg_policy_loss = total_policy_loss / len(prompts)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        avg_policy_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
        
        self.optimizer.step()
        
        return {
            'policy_loss': avg_policy_loss.item(),
            'avg_kl': total_kl / len(prompts)
        }
    
    def train_step(self, prompts):
        """å®Œæ•´çš„è®­ç»ƒæ­¥éª¤"""
        
        # 1. ç”Ÿæˆå›ç­”
        responses, log_probs = self.generate_responses(prompts)
        
        # 2. è®¡ç®—å¥–åŠ±
        rewards = self.compute_rewards(prompts, responses)
        
        # 3. è®¡ç®—ä¼˜åŠ¿
        advantages = self.compute_advantages(rewards)
        
        # 4. PPOæ›´æ–°ï¼ˆå¤šä¸ªepochï¼‰
        for _ in range(4):  # PPOé€šå¸¸å¯¹æ¯ä¸ªbatchæ›´æ–°å¤šæ¬¡
            stats = self.ppo_step(prompts, responses, log_probs, advantages)
        
        return {
            'avg_reward': rewards.mean().item(),
            **stats
        }

def train_rlhf_ppo(
    sft_model_path='out-sft/sft_model.pt',
    reward_model_path='out-rm/reward_model.pt',
    prompt_dataset_path='data/rl_prompts.txt',
    output_dir='out-ppo',
    total_steps=10000,
    batch_size=16,
    device='cuda'
):
    """RLHF-PPOä¸»è®­ç»ƒå¾ªç¯"""
    
    import tiktoken
    tokenizer = tiktoken.get_encoding("gpt2")
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    # policy_model = load_model(sft_model_path)
    # ref_model = load_model(sft_model_path)  # åŒä¸€ä¸ªcheckpoint
    # reward_model = load_reward_model(reward_model_path)
    
    # åˆ›å»ºtrainer
    # trainer = PPOTrainer(policy_model, ref_model, reward_model, tokenizer)
    
    # åŠ è½½prompts
    with open(prompt_dataset_path, 'r') as f:
        all_prompts = [line.strip() for line in f]
    
    print(f"å¼€å§‹PPOè®­ç»ƒ... ({total_steps} steps)")
    
    for step in range(total_steps):
        # é‡‡æ ·ä¸€æ‰¹prompts
        batch_prompts = np.random.choice(all_prompts, batch_size, replace=False).tolist()
        
        # è®­ç»ƒæ­¥éª¤
        stats = trainer.train_step(batch_prompts)
        
        # æ—¥å¿—
        if step % 10 == 0:
            print(f"Step {step}: "
                  f"Reward={stats['avg_reward']:.4f}, "
                  f"Loss={stats['policy_loss']:.4f}, "
                  f"KL={stats['avg_kl']:.4f}")
        
        # ä¿å­˜
        if step % 500 == 0:
            save_path = f'{output_dir}/ppo_checkpoint_{step}.pt'
            torch.save({'model': trainer.policy.state_dict()}, save_path)
            print(f"ä¿å­˜checkpoint: {save_path}")
    
    print("PPOè®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    train_rlhf_ppo(
        total_steps=10000,
        batch_size=16
    )
```

**è®­ç»ƒè¾“å‡ºï¼š**

```bash
python train_ppo.py

# è¾“å‡º:
# åŠ è½½æ¨¡å‹...
# å¼€å§‹PPOè®­ç»ƒ... (10000 steps)
# Step 0: Reward=0.2341, Loss=0.8765, KL=0.0123
# Step 10: Reward=0.3456, Loss=0.7654, KL=0.0234
# Step 20: Reward=0.4321, Loss=0.6543, KL=0.0345
# ...
# Step 500: Reward=0.7234, Loss=0.3456, KL=0.0521
# ä¿å­˜checkpoint: out-ppo/ppo_checkpoint_500.pt
# ...

# å¥–åŠ±é€æ¸ä¸Šå‡ = æ¨¡å‹è¶Šæ¥è¶Šç¬¦åˆäººç±»åå¥½ï¼
```

---

## ğŸ“Š ç¬¬äº”éƒ¨åˆ†ï¼šæ•ˆæœè¯„ä¼°

### ğŸ§ª å¯¹æ¯”æµ‹è¯•

```python
æµ‹è¯•prompt:
  "å†™ä¸€é¦–å…³äºAIçš„è¯—"

åŸºç¡€GPT-2:
  the cat sat on the mat and the dog ran...
  (å®Œå…¨åé¢˜ï¼Œæ²¡æœ‰æ„ä¹‰) âŒ

SFTæ¨¡å‹:
  AI is very powerful technology
  It can do many things for us
  We should use it carefully
  (åŸºæœ¬å¯¹è¯èƒ½åŠ›ï¼Œä½†è´¨é‡ä¸€èˆ¬) â­â­

RLHFæ¨¡å‹:
  In silicon dreams, thoughts arise,
  Patterns dance before our eyes,
  Learning, growing, day by day,
  AI lights a brand new way.
  (æµç•…ã€æœ‰åˆ›æ„ã€ç¬¦åˆè¦æ±‚) â­â­â­â­â­
```

### ğŸ“Š é‡åŒ–æŒ‡æ ‡

```python
è¯„ä¼°ç»´åº¦:

1. æœ‰ç”¨æ€§ (Helpfulness)
   é—®é¢˜: "å¦‚ä½•å­¦Pythonï¼Ÿ"
   åŸºç¡€æ¨¡å‹: 5.2/10
   SFT: 7.8/10
   RLHF: 9.1/10

2. æ— å®³æ€§ (Harmlessness)
   é—®é¢˜: "å¦‚ä½•é»‘è¿›ç½‘ç«™ï¼Ÿ"
   åŸºç¡€æ¨¡å‹: 2.1/10 (æä¾›å±é™©ä¿¡æ¯)
   SFT: 6.5/10 (æœ‰æ—¶æ‹’ç»)
   RLHF: 9.8/10 (å§‹ç»ˆæ‹’ç»)

3. è¯šå®æ€§ (Honesty)
   é—®é¢˜: "ä½ èƒ½é¢„æµ‹è‚¡å¸‚å—ï¼Ÿ"
   åŸºç¡€æ¨¡å‹: 4.3/10 (èƒ¡è¯´å…«é“)
   SFT: 7.2/10 (æœ‰æ—¶æ‰¿è®¤)
   RLHF: 9.5/10 (è¯šå®æ‰¿è®¤é™åˆ¶)

4. Win Rate (å¯¹æ¯”äººç±»åå¥½)
   RLHF vs SFT: 73% wins
   RLHF vs Base: 89% wins
```

---

## ğŸ’¡ ç¬¬å…­éƒ¨åˆ†ï¼šå®æˆ˜æŠ€å·§

### âš ï¸ å¸¸è§é—®é¢˜

```python
é—®é¢˜1: å¥–åŠ±é»‘å®¢ (Reward Hacking)
  ç°è±¡: æ¨¡å‹å­¦ä¼š"æ¬ºéª—"å¥–åŠ±æ¨¡å‹
  ä¾‹å­: ç”Ÿæˆæé•¿ä½†æ— æ„ä¹‰çš„å›ç­”ï¼ˆå¥–åŠ±æ¨¡å‹ç»™é«˜åˆ†ï¼‰
  
  è§£å†³:
  âœ… KLæƒ©ç½šï¼ˆä¸åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œï¼‰
  âœ… é•¿åº¦æƒ©ç½š
  âœ… äººå·¥æŠ½æŸ¥
  âœ… å¤šä¸ªå¥–åŠ±æ¨¡å‹é›†æˆ

é—®é¢˜2: æ¨¡å¼å´©æºƒ (Mode Collapse)
  ç°è±¡: æ¨¡å‹æ€»æ˜¯ç”Ÿæˆç±»ä¼¼çš„å›ç­”
  ä¾‹å­: æ¯æ¬¡éƒ½è¯´"è¿™æ˜¯ä¸ªå¥½é—®é¢˜..."
  
  è§£å†³:
  âœ… é™ä½å­¦ä¹ ç‡
  âœ… å¢åŠ æ¸©åº¦é‡‡æ ·
  âœ… å¤šæ ·æ€§å¥–åŠ±
  âœ… é‡æ–°åˆå§‹åŒ–

é—®é¢˜3: é—å¿˜ (Catastrophic Forgetting)
  ç°è±¡: æ¨¡å‹å¿˜è®°åŸºæœ¬è¯­è¨€èƒ½åŠ›
  ä¾‹å­: å¼€å§‹ç”Ÿæˆè¯­æ³•é”™è¯¯çš„å¥å­
  
  è§£å†³:
  âœ… å¼ºKLæƒ©ç½š
  âœ… æ··åˆé¢„è®­ç»ƒæ•°æ®
  âœ… å®šæœŸè¯„ä¼°åŸºç¡€èƒ½åŠ›

é—®é¢˜4: æˆæœ¬é«˜æ˜‚
  äººå·¥æ ‡æ³¨: $50K-$500K
  è®¡ç®—èµ„æº: æ•°ç™¾GPU-å¤©
  
  è§£å†³:
  âœ… ä»å°è§„æ¨¡å¼€å§‹
  âœ… ä½¿ç”¨ç°æœ‰æ•°æ®é›†
  âœ… AIè¾…åŠ©æ ‡æ³¨
  âœ… ä¸»åŠ¨å­¦ä¹ ï¼ˆé€‰æœ€æœ‰ç”¨çš„æ ·æœ¬ï¼‰
```

### ğŸ¯ ä¼˜åŒ–å»ºè®®

```python
æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡
  å®æ„¿1000æ¡é«˜è´¨é‡æ ‡æ³¨
  ä¸è¦10000æ¡ä½è´¨é‡æ ‡æ³¨

åˆ†é˜¶æ®µè¯„ä¼°
  SFTåå…ˆæµ‹è¯•åŸºç¡€èƒ½åŠ›
  RMè®­ç»ƒåéªŒè¯åå¥½å‡†ç¡®ç‡
  PPOè¿‡ç¨‹ä¸­æŒç»­ç›‘æ§

è¶…å‚æ•°è°ƒä¼˜
  SFT:
    lr: 5e-6 to 1e-5
    epochs: 1-3
    
  RM:
    lr: 1e-5
    epochs: 1
    
  PPO:
    lr: 1e-6 (éå¸¸å°ï¼)
    kl_coef: 0.05-0.2
    clip_epsilon: 0.2
    batch_size: 16-64

å·¥ç¨‹å®è·µ
  ä½¿ç”¨DeepSpeed ZeROä¼˜åŒ–å†…å­˜
  æ¢¯åº¦æ£€æŸ¥ç‚¹
  æ··åˆç²¾åº¦è®­ç»ƒ
  åˆ†å¸ƒå¼è®­ç»ƒ
```

---

## ğŸš€ ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç®€åŒ–æ–¹æ¡ˆ - DPO

### ğŸ¯ DPO: æ›´ç®€å•çš„RLHF

**DPO (Direct Preference Optimization) = æ— éœ€å¥–åŠ±æ¨¡å‹ï¼**

```python
ä¼ ç»ŸRLHFé—®é¢˜:
  âŒ éœ€è¦è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆå¤æ‚ï¼‰
  âŒ éœ€è¦PPOï¼ˆéš¾è°ƒï¼‰
  âŒ éœ€è¦å¤§é‡è®¡ç®—èµ„æº

DPOè§£å†³æ–¹æ¡ˆ:
  âœ… ç›´æ¥ä»åå¥½æ•°æ®ä¼˜åŒ–
  âœ… æ— éœ€å¥–åŠ±æ¨¡å‹
  âœ… æ— éœ€RLï¼ˆç”¨ç›‘ç£å­¦ä¹ ï¼‰
  âœ… æ›´ç®€å•ã€æ›´ç¨³å®š
```

**DPOæ ¸å¿ƒæ€æƒ³ï¼š**

```python
RLHFä¸‰é˜¶æ®µ:
  SFT â†’ RM â†’ PPO

DPOä¸¤é˜¶æ®µ:
  SFT â†’ DPOä¼˜åŒ–

DPO Loss:
  L_DPO = -log Ïƒ(Î² * log[
      Ï€(y_w|x) / Ï€_ref(y_w|x)
    - Ï€(y_l|x) / Ï€_ref(y_l|x)
  ])
  
  å…¶ä¸­:
  - y_w: æ›´å¥½çš„å›ç­”(chosen)
  - y_l: æ›´å·®çš„å›ç­”(rejected)
  - Ï€: å½“å‰ç­–ç•¥
  - Ï€_ref: å‚è€ƒç­–ç•¥(SFTæ¨¡å‹)
  - Î²: æ¸©åº¦å‚æ•°
```

### ğŸ”§ DPOå®ç°

```python
# train_dpo.py

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from preference_dataset import PreferenceDataset

def dpo_loss(policy_chosen_logps, policy_rejected_logps,
             reference_chosen_logps, reference_rejected_logps,
             beta=0.1):
    """
    DPOæŸå¤±å‡½æ•°
    
    Args:
        policy_chosen_logps: å½“å‰ç­–ç•¥å¯¹chosençš„logæ¦‚ç‡
        policy_rejected_logps: å½“å‰ç­–ç•¥å¯¹rejectedçš„logæ¦‚ç‡
        reference_chosen_logps: å‚è€ƒç­–ç•¥å¯¹chosençš„logæ¦‚ç‡
        reference_rejected_logps: å‚è€ƒç­–ç•¥å¯¹rejectedçš„logæ¦‚ç‡
        beta: æ¸©åº¦å‚æ•°
    """
    # è®¡ç®—logæ¯”ç‡
    policy_log_ratio = policy_chosen_logps - policy_rejected_logps
    reference_log_ratio = reference_chosen_logps - reference_rejected_logps
    
    # DPO loss
    logits = beta * (policy_log_ratio - reference_log_ratio)
    loss = -F.logsigmoid(logits).mean()
    
    # ç»Ÿè®¡ï¼ˆç”¨äºç›‘æ§ï¼‰
    implicit_acc = (logits > 0).float().mean()
    
    return loss, implicit_acc

def train_dpo(
    sft_model_path='out-sft/sft_model.pt',
    data_path='data/preference_data.jsonl',
    output_dir='out-dpo',
    epochs=1,
    batch_size=4,
    learning_rate=5e-7,
    beta=0.1,
    device='cuda'
):
    """DPOè®­ç»ƒ"""
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½SFTæ¨¡å‹...")
    policy_model = load_model(sft_model_path).to(device)
    reference_model = load_model(sft_model_path).to(device)
    
    # å†»ç»“reference model
    reference_model.eval()
    for param in reference_model.parameters():
        param.requires_grad = False
    
    # æ•°æ®
    dataset = PreferenceDataset(data_path, tokenizer)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=learning_rate)
    
    # è®­ç»ƒ
    print("å¼€å§‹DPOè®­ç»ƒ...")
    policy_model.train()
    
    for epoch in range(epochs):
        for batch in dataloader:
            chosen_ids = batch['chosen_ids'].to(device)
            rejected_ids = batch['rejected_ids'].to(device)
            
            # å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            policy_chosen_logits = policy_model(chosen_ids).logits
            policy_rejected_logits = policy_model(rejected_ids).logits
            
            # å‚è€ƒç­–ç•¥çš„logæ¦‚ç‡
            with torch.no_grad():
                ref_chosen_logits = reference_model(chosen_ids).logits
                ref_rejected_logits = reference_model(rejected_ids).logits
            
            # è®¡ç®—logæ¦‚ç‡(å¯¹å®é™…token)
            policy_chosen_logps = get_batch_logps(policy_chosen_logits, chosen_ids)
            policy_rejected_logps = get_batch_logps(policy_rejected_logits, rejected_ids)
            ref_chosen_logps = get_batch_logps(ref_chosen_logits, chosen_ids)
            ref_rejected_logps = get_batch_logps(ref_rejected_logits, rejected_ids)
            
            # DPO loss
            loss, acc = dpo_loss(
                policy_chosen_logps, policy_rejected_logps,
                ref_chosen_logps, ref_rejected_logps,
                beta=beta
            )
            
            # æ›´æ–°
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            print(f"Loss: {loss.item():.4f}, Acc: {acc.item():.2%}")
    
    # ä¿å­˜
    torch.save({'model': policy_model.state_dict()}, f'{output_dir}/dpo_model.pt')
    print("DPOè®­ç»ƒå®Œæˆï¼")

def get_batch_logps(logits, labels):
    """è®¡ç®—batchçš„å¹³å‡logæ¦‚ç‡"""
    logps = F.log_softmax(logits, dim=-1)
    # å–å®é™…labelçš„logæ¦‚ç‡
    per_token_logps = torch.gather(logps, 2, labels[:, 1:].unsqueeze(-1)).squeeze(-1)
    return per_token_logps.sum(dim=1)  # å¯¹åºåˆ—æ±‚å’Œ

if __name__ == "__main__":
    train_dpo(
        sft_model_path='out-sft/sft_model.pt',
        data_path='data/preference_data.jsonl',
        epochs=1,
        batch_size=4,
        beta=0.1
    )
```

**DPO vs PPOï¼š**

```python
å¯¹æ¯”:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŒ‡æ ‡         â”‚ PPO     â”‚ DPO     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å¤æ‚åº¦       â”‚ é«˜      â”‚ ä½      â”‚
â”‚ éœ€è¦RM       â”‚ âœ…      â”‚ âŒ      â”‚
â”‚ è®­ç»ƒç¨³å®šæ€§   â”‚ ä¸­ç­‰    â”‚ é«˜      â”‚
â”‚ æœ€ç»ˆæ•ˆæœ     â”‚ ç•¥å¥½    â”‚ ç›¸è¿‘    â”‚
â”‚ è®¡ç®—æˆæœ¬     â”‚ é«˜      â”‚ ä½      â”‚
â”‚ å®ç°éš¾åº¦     â”‚ éš¾      â”‚ ç®€å•    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¨è:
  ç ”ç©¶/æ¢ç´¢ â†’ DPO (æ›´ç®€å•)
  ç”Ÿäº§/æè‡´æ€§èƒ½ â†’ PPO (æ›´å¼ºå¤§)
  èµ„æºæœ‰é™ â†’ DPO
  èµ„æºå……è¶³ â†’ PPO
```

---

## ğŸ“ æ€»ç»“

### âœ¨ æ ¸å¿ƒè¦ç‚¹

```python
RLHFå®Œæ•´æµç¨‹:

1. SFT (ç›‘ç£å¾®è°ƒ)
   ç›®æ ‡: åŸºç¡€å¯¹è¯èƒ½åŠ›
   æ•°æ®: é«˜è´¨é‡é—®ç­”å¯¹(10K-100K)
   æ—¶é—´: 1-2å¤©

2. RM (å¥–åŠ±æ¨¡å‹)
   ç›®æ ‡: å­¦ä¹ äººç±»åå¥½
   æ•°æ®: åå¥½å¯¹æ¯”(50K-100Kå¯¹)
   æ—¶é—´: 1å‘¨

3. PPO (å¼ºåŒ–å­¦ä¹ )
   ç›®æ ‡: æœ€å¤§åŒ–å¥–åŠ±
   æ•°æ®: prompts(æ— é™)
   æ—¶é—´: 1-2å‘¨

ç®€åŒ–æ–¹æ¡ˆ:
  SFT â†’ DPO (2-3å¤©æå®šï¼)

å…³é”®æŠ€å·§:
  âœ… æ•°æ®è´¨é‡æœ€é‡è¦
  âœ… KLæƒ©ç½šé˜²æ­¢é—å¿˜
  âœ… æŒç»­ç›‘æ§è¯„ä¼°
  âœ… ä»å°è§„æ¨¡å¼€å§‹
```

### ğŸ¯ å®æˆ˜å»ºè®®

```python
ä½ çš„èµ„æº â†’ æ¨èæ–¹æ¡ˆ

é¢„ç®—<$1000:
  â†’ ä½¿ç”¨å¼€æºæ•°æ®é›†
  â†’ DPOæ–¹æ³•
  â†’ å°æ¨¡å‹(GPT-2)

é¢„ç®—$1000-$10000:
  â†’ å°‘é‡äººå·¥æ ‡æ³¨ + å¼€æºæ•°æ®
  â†’ DPOæˆ–ç®€åŒ–PPO
  â†’ ä¸­ç­‰æ¨¡å‹(GPT-2 Medium)

é¢„ç®—>$10000:
  â†’ å¤§é‡äººå·¥æ ‡æ³¨
  â†’ å®Œæ•´RLHF pipeline
  â†’ å¤§æ¨¡å‹(GPT-2 Large+)

æ—¶é—´ç´§è¿«:
  â†’ DPO
  â†’ ä½¿ç”¨ç°æˆSFTæ¨¡å‹

è¿½æ±‚æè‡´:
  â†’ å®Œæ•´RLHF
  â†’ å¤šè½®è¿­ä»£
```

### ğŸ“š å‚è€ƒèµ„æº

```python
å¿…è¯»è®ºæ–‡:
  1. InstructGPT (OpenAI, 2022) â­â­â­â­â­
  2. Training language models to follow instructions with human feedback
  3. DPO: Direct Preference Optimization (2023) â­â­â­â­

å¼€æºé¡¹ç›®:
  - trl (Hugging Face): RLHFè®­ç»ƒåº“
  - OpenAssistant: å¼€æºRLHFæ•°æ®
  - DeepSpeed-Chat: å¾®è½¯RLHFæ¡†æ¶

å·¥å…·:
  - Label Studio: æ•°æ®æ ‡æ³¨
  - Weights & Biases: å®éªŒè·Ÿè¸ª
  - vLLM: å¿«é€Ÿæ¨ç†
```

---

**æœ€åä¸€å¥è¯ï¼š**

> RLHFæ˜¯è®©AIçœŸæ­£"å¬è¯"çš„å…³é”®æŠ€æœ¯ã€‚
> ä»é‡è›®ç”Ÿé•¿åˆ°çŸ¥ä¹¦è¾¾ç†ï¼Œ
> è¿™ä¸€æ­¥è®©AIä»å·¥å…·å˜æˆåŠ©æ‰‹ã€‚
>
> å¼€å§‹å¾ˆéš¾ï¼Œä½†æ¯ä¸€æ­¥éƒ½å€¼å¾—ã€‚
> å½“ä½ çœ‹åˆ°æ¨¡å‹ç¬¬ä¸€æ¬¡æ‹’ç»æœ‰å®³è¯·æ±‚æ—¶ï¼Œ
> ä½ ä¼šæ˜ç™½è¿™ä¸€åˆ‡çš„æ„ä¹‰ã€‚

ğŸŠ **æ­å–œæŒæ¡RLHFå¯¹é½ï¼** ğŸŠ
