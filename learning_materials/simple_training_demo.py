#!/usr/bin/env python3
"""
è®­ç»ƒå¾ªç¯æ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º - çº¯Pythonç‰ˆæœ¬
ä¸éœ€è¦ä»»ä½•å¤–éƒ¨åº“ï¼Œå¸®åŠ©ç†è§£æ ¸å¿ƒåŸç†
"""

import math
import random

print("="*80)
print("ğŸ“ è®­ç»ƒå¾ªç¯æ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º")
print("="*80)
print()

# ============================================================================
# æ¼”ç¤º1: æœ€ç®€å•çš„æ¢¯åº¦ä¸‹é™
# ============================================================================
print("ğŸ“ æ¼”ç¤º1: ç†è§£æ¢¯åº¦ä¸‹é™")
print("-" * 80)
print()
print("é—®é¢˜: å­¦ä¹ å‡½æ•° y = 2x")
print("ç›®æ ‡: æ‰¾åˆ°å‚æ•° w = 2")
print()

# è®­ç»ƒæ•°æ®
x_data = [1.0, 2.0, 3.0, 4.0, 5.0]
y_data = [2.0, 4.0, 6.0, 8.0, 10.0]

# åˆå§‹å‚æ•°ï¼ˆéšæœºçŒœæµ‹ï¼‰
w = 0.5
learning_rate = 0.1

print(f"åˆå§‹çŠ¶æ€: w = {w:.4f}")
print(f"å­¦ä¹ ç‡: {learning_rate}")
print()
print("å¼€å§‹è®­ç»ƒ...")
print()

for epoch in range(20):
    # å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹å’ŒæŸå¤±
    total_loss = 0
    total_gradient = 0
    
    for x, y_true in zip(x_data, y_data):
        # é¢„æµ‹
        y_pred = w * x
        
        # æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
        loss = (y_pred - y_true) ** 2
        total_loss += loss
        
        # æ¢¯åº¦ï¼ˆæ‰‹åŠ¨è®¡ç®—ï¼‰
        # loss = (w*x - y)^2
        # d(loss)/d(w) = 2*(w*x - y)*x
        gradient = 2 * (y_pred - y_true) * x
        total_gradient += gradient
    
    # å¹³å‡
    avg_loss = total_loss / len(x_data)
    avg_gradient = total_gradient / len(x_data)
    
    # æ›´æ–°å‚æ•°
    w = w - learning_rate * avg_gradient
    
    # æ‰“å°è¿›åº¦
    if epoch % 4 == 0 or epoch == 19:
        print(f"  Epoch {epoch:2d}: w = {w:.6f}, loss = {avg_loss:.6f}, gradient = {avg_gradient:+.6f}")

print()
print(f"âœ… æœ€ç»ˆç»“æœ: w = {w:.6f} (ç›®æ ‡: 2.0)")
print(f"   è¯¯å·®: {abs(w - 2.0):.6f}")
print()

# ============================================================================
# æ¼”ç¤º2: Batch Sizeçš„å½±å“
# ============================================================================
print("ğŸ“ æ¼”ç¤º2: Batch Sizeçš„å½±å“")
print("-" * 80)
print()

def train_with_batch(batch_size, epochs=10):
    """ç”¨æŒ‡å®šbatch sizeè®­ç»ƒ"""
    w = 0.5
    lr = 0.1
    
    for epoch in range(epochs):
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = list(range(len(x_data)))
        random.shuffle(indices)
        
        # æŒ‰batchå¤„ç†
        for i in range(0, len(x_data), batch_size):
            batch_indices = indices[i:i+batch_size]
            
            # è®¡ç®—batchçš„æ¢¯åº¦
            gradient = 0
            for idx in batch_indices:
                x = x_data[idx]
                y_true = y_data[idx]
                y_pred = w * x
                gradient += 2 * (y_pred - y_true) * x
            
            gradient /= len(batch_indices)
            
            # æ›´æ–°
            w -= lr * gradient
    
    return w

print("æ¯”è¾ƒä¸åŒbatch sizeçš„è®­ç»ƒç»“æœï¼š")
print()

for bs in [1, 2, 5]:
    random.seed(42)  # å›ºå®šéšæœºç§å­ä»¥ä¾¿æ¯”è¾ƒ
    final_w = train_with_batch(bs, epochs=20)
    print(f"  Batch Size = {bs}: w = {final_w:.6f}")

print()
print("è§‚å¯Ÿï¼šbatch sizeå½±å“è®­ç»ƒè¿‡ç¨‹ï¼Œä½†æœ€ç»ˆéƒ½èƒ½æ”¶æ•›")
print()

# ============================================================================
# æ¼”ç¤º3: æ¢¯åº¦ç´¯ç§¯
# ============================================================================
print("ğŸ“ æ¼”ç¤º3: æ¢¯åº¦ç´¯ç§¯ = æ¨¡æ‹Ÿå¤§Batch")
print("-" * 80)
print()

# æ–¹æ³•1: ç›´æ¥ç”¨å¤§batch
def train_large_batch():
    w = 0.5
    lr = 0.1
    
    # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦
    gradient = 0
    for x, y_true in zip(x_data, y_data):
        y_pred = w * x
        gradient += 2 * (y_pred - y_true) * x
    gradient /= len(x_data)
    
    # æ›´æ–°
    w -= lr * gradient
    return w

# æ–¹æ³•2: å°batch + æ¢¯åº¦ç´¯ç§¯
def train_with_accumulation():
    w = 0.5
    lr = 0.1
    accumulation_steps = 5
    
    accumulated_gradient = 0
    
    # ç´¯ç§¯5æ¬¡
    for i in range(accumulation_steps):
        x = x_data[i]
        y_true = y_data[i]
        y_pred = w * x
        
        # è®¡ç®—æ¢¯åº¦å¹¶ç´¯åŠ 
        gradient = 2 * (y_pred - y_true) * x
        accumulated_gradient += gradient
    
    # å¹³å‡
    accumulated_gradient /= accumulation_steps
    
    # æ›´æ–°
    w -= lr * accumulated_gradient
    return w

w1 = train_large_batch()
w2 = train_with_accumulation()

print(f"æ–¹æ³•1 (å¤§batch):      w = {w1:.6f}")
print(f"æ–¹æ³•2 (æ¢¯åº¦ç´¯ç§¯):     w = {w2:.6f}")
print(f"å·®å¼‚:                {abs(w1 - w2):.8f}")
print()
print("âœ… ä¸¤ç§æ–¹æ³•ç»“æœå‡ ä¹ç›¸åŒï¼")
print()

# ============================================================================
# æ¼”ç¤º4: å­¦ä¹ ç‡è°ƒåº¦
# ============================================================================
print("ğŸ“ æ¼”ç¤º4: å­¦ä¹ ç‡è°ƒåº¦")
print("-" * 80)
print()

def get_lr_with_warmup(step, max_lr=0.1, warmup_steps=10, total_steps=50):
    """å¸¦warmupçš„å­¦ä¹ ç‡è°ƒåº¦"""
    if step < warmup_steps:
        # Warmup: çº¿æ€§å¢é•¿
        return max_lr * (step + 1) / warmup_steps
    else:
        # Cosine decay
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return max_lr * 0.5 * (1 + math.cos(math.pi * progress))

print("å­¦ä¹ ç‡å˜åŒ–ï¼š")
print()
print("Step |  Learning Rate  | é˜¶æ®µ")
print("-----|-----------------|--------")

steps_to_show = [0, 5, 10, 15, 25, 40, 49]
for step in steps_to_show:
    lr = get_lr_with_warmup(step)
    if step < 10:
        phase = "Warmup"
    elif step < 40:
        phase = "Training"
    else:
        phase = "Decay"
    print(f"{step:4d} |  {lr:.6f}      | {phase}")

print()
print("å¯è§†åŒ–ï¼ˆASCIIå›¾ï¼‰ï¼š")
print()

# ç®€å•çš„ASCIIå›¾
max_height = 20
for height in range(max_height, 0, -1):
    line = ""
    for step in range(0, 50, 2):
        lr = get_lr_with_warmup(step)
        normalized_height = int(lr / 0.1 * max_height)
        if normalized_height >= height:
            line += "â–ˆ"
        else:
            line += " "
    print(f"{height:2d} | {line}")

print("    " + "-" * 25)
print("     0    10   20   30   40  (steps)")
print()

# ============================================================================
# æ¼”ç¤º5: æ¢¯åº¦è£å‰ª
# ============================================================================
print("ğŸ“ æ¼”ç¤º5: æ¢¯åº¦è£å‰ª")
print("-" * 80)
print()

def clip_gradient(gradient, max_norm=1.0):
    """æ¢¯åº¦è£å‰ª"""
    # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆç»å¯¹å€¼ï¼‰
    norm = abs(gradient)
    
    if norm > max_norm:
        # ç¼©æ”¾
        return gradient * (max_norm / norm)
    else:
        return gradient

print("æ¢¯åº¦è£å‰ªç¤ºä¾‹ï¼š")
print()
print("åŸå§‹æ¢¯åº¦ | è£å‰ªå (max_norm=1.0)")
print("---------|----------------------")

test_gradients = [-5.0, -2.0, -0.5, 0.8, 2.5, 10.0]
for g in test_gradients:
    clipped = clip_gradient(g, max_norm=1.0)
    print(f"{g:8.2f} | {clipped:8.2f}")

print()
print("âœ… å¤§æ¢¯åº¦è¢«ç¼©å°ï¼Œé˜²æ­¢å‚æ•°æ›´æ–°è¿‡å¤§")
print()

# ============================================================================
# æ¼”ç¤º6: å®Œæ•´è®­ç»ƒå¾ªç¯æ¨¡æ‹Ÿ
# ============================================================================
print("ğŸ“ æ¼”ç¤º6: å®Œæ•´è®­ç»ƒå¾ªç¯ï¼ˆæ•´åˆæ‰€æœ‰æŠ€å·§ï¼‰")
print("-" * 80)
print()

class SimpleTrainer:
    def __init__(self):
        self.w = 0.5
        self.best_loss = float('inf')
        self.best_w = self.w
    
    def train_step(self, batch_data, lr, use_grad_clip=True):
        """å•æ­¥è®­ç»ƒ"""
        # æ¢¯åº¦ç´¯ç§¯
        accumulated_grad = 0
        total_loss = 0
        
        for x, y_true in batch_data:
            y_pred = self.w * x
            loss = (y_pred - y_true) ** 2
            gradient = 2 * (y_pred - y_true) * x
            
            accumulated_grad += gradient
            total_loss += loss
        
        # å¹³å‡
        accumulated_grad /= len(batch_data)
        avg_loss = total_loss / len(batch_data)
        
        # æ¢¯åº¦è£å‰ª
        if use_grad_clip:
            accumulated_grad = clip_gradient(accumulated_grad, max_norm=2.0)
        
        # æ›´æ–°å‚æ•°
        self.w -= lr * accumulated_grad
        
        return avg_loss
    
    def train(self, epochs=30, batch_size=2):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print(f"é…ç½®: epochs={epochs}, batch_size={batch_size}")
        print()
        
        data = list(zip(x_data, y_data))
        
        for epoch in range(epochs):
            # å­¦ä¹ ç‡è°ƒåº¦
            lr = get_lr_with_warmup(epoch, max_lr=0.1, warmup_steps=5, total_steps=epochs)
            
            # éšæœºæ‰“ä¹±
            random.shuffle(data)
            
            # æ‰¹æ¬¡è®­ç»ƒ
            epoch_loss = 0
            num_batches = 0
            
            for i in range(0, len(data), batch_size):
                batch = data[i:i+batch_size]
                loss = self.train_step(batch, lr)
                epoch_loss += loss
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            
            # ä¿å­˜æœ€ä½³
            if avg_loss < self.best_loss:
                self.best_loss = avg_loss
                self.best_w = self.w
            
            # æ‰“å°
            if epoch % 5 == 0 or epoch == epochs - 1:
                print(f"Epoch {epoch:2d}: lr={lr:.6f}, w={self.w:.6f}, loss={avg_loss:.6f}")
        
        print()
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   æœ€ä½³å‚æ•°: w = {self.best_w:.6f}")
        print(f"   æœ€ä½³æŸå¤±: {self.best_loss:.6f}")

# è¿è¡Œè®­ç»ƒ
random.seed(42)
trainer = SimpleTrainer()
trainer.train()

print()

# ============================================================================
# æ€»ç»“
# ============================================================================
print("="*80)
print("ğŸ“ æ€»ç»“")
print("="*80)
print()
print("ä½ åˆšåˆšå­¦åˆ°äº†ï¼š")
print()
print("1. âœ… æ¢¯åº¦ä¸‹é™åŸºç¡€")
print("   - è®¡ç®—æ¢¯åº¦ï¼ˆæŸå¤±å¯¹å‚æ•°çš„å¯¼æ•°ï¼‰")
print("   - æ›´æ–°å‚æ•°ï¼ˆw = w - lr * gradientï¼‰")
print()
print("2. âœ… Batchè®­ç»ƒ")
print("   - å°batch: æ›´æ–°é¢‘ç¹ï¼Œæ¢¯åº¦å™ªå£°å¤§")
print("   - å¤§batch: æ›´æ–°ç¨³å®šï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜")
print()
print("3. âœ… æ¢¯åº¦ç´¯ç§¯")
print("   - ç”¨å°batchæ¨¡æ‹Ÿå¤§batch")
print("   - å…³é”®: ç´¯ç§¯å¤šä¸ªå°batchçš„æ¢¯åº¦ï¼Œç„¶åä¸€æ¬¡æ›´æ–°")
print()
print("4. âœ… å­¦ä¹ ç‡è°ƒåº¦")
print("   - Warmup: å¼€å§‹æ—¶å­¦ä¹ ç‡ä»å°åˆ°å¤§")
print("   - Decay: åæœŸé€æ¸é™ä½å­¦ä¹ ç‡")
print()
print("5. âœ… æ¢¯åº¦è£å‰ª")
print("   - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
print("   - é™åˆ¶æ¢¯åº¦çš„æœ€å¤§å€¼")
print()
print("6. âœ… å®Œæ•´è®­ç»ƒå¾ªç¯")
print("   - æ•´åˆæ‰€æœ‰æŠ€å·§")
print("   - è¿™å°±æ˜¯train.pyçš„æ ¸å¿ƒï¼")
print()
print("="*80)
print("ğŸš€ ç°åœ¨ä½ å®Œå…¨ç†è§£è®­ç»ƒå¾ªç¯äº†ï¼")
print("="*80)
print()
print("ä¸‹ä¸€æ­¥ï¼š")
print("1. è¿è¡ŒçœŸå®è®­ç»ƒ: python train.py config/train_shakespeare_char.py")
print("2. å°è¯•ä¸åŒçš„è¶…å‚æ•°")
print("3. è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹")
print("4. ç”Ÿæˆæ–‡æœ¬çœ‹æ•ˆæœ: python sample.py --out_dir=out-shakespeare-char")
print()
