# ğŸš€ NanoGPT è¿›é˜¶å­¦ä¹ è·¯çº¿å›¾

æ­å–œä½ ï¼ä½ å·²ç»æŒæ¡äº† NanoGPT çš„æ ¸å¿ƒå†…å®¹ï¼š
- âœ… é…ç½®å‚æ•°è¯¦è§£
- âœ… æ•°æ®åŠ è½½æœºåˆ¶
- âœ… è®­ç»ƒå¾ªç¯åŸç†
- âœ… Transformeræ¨¡å‹æ¶æ„

ä½†è¿™åªæ˜¯å¼€å§‹ï¼è¿™ä¸ªé¡¹ç›®è¿˜æœ‰å¾ˆå¤šæ·±åº¦å†…å®¹å€¼å¾—å­¦ä¹ ã€‚

---

## ğŸ“š å­¦ä¹ è·¯å¾„æ€»è§ˆ

```
ä½ ç°åœ¨çš„ä½ç½® âœ…
â”œâ”€â”€ åŸºç¡€æ¦‚å¿µ âœ…
â”œâ”€â”€ è®­ç»ƒæµç¨‹ âœ…
â””â”€â”€ æ¨¡å‹æ¶æ„ âœ…

æ¥ä¸‹æ¥å¯ä»¥å­¦ä¹  â¬‡ï¸
â”œâ”€â”€ ğŸ¯ å®æˆ˜åº”ç”¨
â”‚   â”œâ”€â”€ æ–‡æœ¬ç”ŸæˆæŠ€å·§
â”‚   â”œâ”€â”€ æ¨¡å‹å¾®è°ƒ
â”‚   â””â”€â”€ æ•°æ®å‡†å¤‡
â”‚
â”œâ”€â”€ âš¡ æ€§èƒ½ä¼˜åŒ–
â”‚   â”œâ”€â”€ æ··åˆç²¾åº¦è®­ç»ƒ
â”‚   â”œâ”€â”€ åˆ†å¸ƒå¼è®­ç»ƒ (DDP)
â”‚   â”œâ”€â”€ æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
â”‚   â””â”€â”€ æ€§èƒ½åˆ†æ
â”‚
â”œâ”€â”€ ğŸ”¬ é«˜çº§ä¸»é¢˜
â”‚   â”œâ”€â”€ Scaling Lawsï¼ˆç¼©æ”¾å®šå¾‹ï¼‰
â”‚   â”œâ”€â”€ æ¨¡å‹è¯„ä¼°
â”‚   â”œâ”€â”€ Tokenizationæ·±å…¥
â”‚   â””â”€â”€ ä¸åŒæ¶æ„å¯¹æ¯”
â”‚
â””â”€â”€ ğŸ“ ç ”ç©¶æ–¹å‘
    â”œâ”€â”€ PEFT/LoRA
    â”œâ”€â”€ RLHF
    â”œâ”€â”€ Multi-modal
    â””â”€â”€ å¼€æºè´¡çŒ®
```

---

## ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šå®æˆ˜åº”ç”¨ï¼ˆå¿…å­¦ï¼‰

### 1. æ–‡æœ¬ç”Ÿæˆè¯¦è§£ (sample.py)

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
è®­ç»ƒæ¨¡å‹çš„æœ€ç»ˆç›®çš„æ˜¯ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ã€‚

**å­¦ä¹ å†…å®¹ï¼š**

#### ğŸ“ ä¸»é¢˜ 1.1: é‡‡æ ·ç­–ç•¥æ·±åº¦è§£æ

```python
# sample.py çš„æ ¸å¿ƒå‚æ•°

temperature = 0.8      # æ§åˆ¶éšæœºæ€§
top_k = 200           # Top-K é‡‡æ ·
num_samples = 10      # ç”Ÿæˆæ•°é‡
max_new_tokens = 500  # æœ€å¤§é•¿åº¦
```

**è¯¦ç»†å¯¹æ¯”å®éªŒï¼š**

```python
# å®éªŒ1: Temperature çš„å½±å“
temperatures = [0.1, 0.5, 0.8, 1.0, 1.5, 2.0]

æç¤ºè¯: "Once upon a time"

temperature=0.1 (å‡ ä¹ç¡®å®šæ€§):
  è¾“å‡º: "Once upon a time, the company said it would be the first to..."
  ç‰¹ç‚¹: é‡å¤ã€æ— èŠã€ä½†è¯­æ³•å®Œç¾

temperature=0.8 (æ¨è):
  è¾“å‡º: "Once upon a time there was a little girl who lived in a forest..."
  ç‰¹ç‚¹: å¹³è¡¡åˆ›é€ æ€§å’Œè¿è´¯æ€§

temperature=2.0 (éå¸¸éšæœº):
  è¾“å‡º: "Once upon a time zebra! Mathematics $%^ incredible journeys..."
  ç‰¹ç‚¹: åˆ›é€ æ€§å¼ºï¼Œä½†å¯èƒ½ä¸è¿è´¯

# å®éªŒ2: Top-K çš„å½±å“
top_k = None    # ä»æ‰€æœ‰tokenä¸­é‡‡æ ·
top_k = 50      # åªä»æœ€å¯èƒ½çš„50ä¸ªä¸­é€‰
top_k = 10      # åªä»æœ€å¯èƒ½çš„10ä¸ªä¸­é€‰

è§‚å¯Ÿ: Top-Kè¶Šå°ï¼Œç”Ÿæˆè¶Šä¿å®ˆä½†è´¨é‡è¶Šç¨³å®š
```

**å®æˆ˜ä»»åŠ¡ï¼š**

```bash
# ä»»åŠ¡1: åˆ›æ„å†™ä½œ
python sample.py \
  --start="In a world where AI rules," \
  --temperature=1.2 \
  --top_k=100 \
  --num_samples=5

# ä»»åŠ¡2: æŠ€æœ¯æ–‡æ¡£ï¼ˆéœ€è¦æ›´ç²¾ç¡®ï¼‰
python sample.py \
  --start="To install TensorFlow, first" \
  --temperature=0.3 \
  --top_k=20

# ä»»åŠ¡3: ä»£ç ç”Ÿæˆ
python sample.py \
  --start="def fibonacci(n):" \
  --temperature=0.5 \
  --max_new_tokens=200
```

#### ğŸ“ ä¸»é¢˜ 1.2: ä»æ–‡ä»¶è¯»å–æç¤ºè¯

```python
# åˆ›å»ºæç¤ºè¯æ–‡ä»¶
cat > prompt.txt << EOF
Write a detailed explanation of the Transformer architecture.
Include:
1. Self-attention mechanism
2. Position encoding
3. Feed-forward networks
EOF

# ä½¿ç”¨æ–‡ä»¶ä½œä¸ºæç¤º
python sample.py --start=FILE:prompt.txt
```

#### ğŸ“ ä¸»é¢˜ 1.3: æ‰¹é‡ç”Ÿæˆå’Œåå¤„ç†

```python
# åˆ›å»ºæ‰¹é‡ç”Ÿæˆè„šæœ¬
# batch_generate.py

import subprocess
import json

prompts = [
    "The future of AI is",
    "Climate change solutions include",
    "The best way to learn programming is",
]

results = {}
for prompt in prompts:
    output = subprocess.check_output([
        'python', 'sample.py',
        f'--start={prompt}',
        '--num_samples=3',
        '--temperature=0.8'
    ])
    results[prompt] = output.decode()

# ä¿å­˜ç»“æœ
with open('generation_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

**å­¦ä¹ èµ„æºï¼š**
- æ–‡ä»¶: `sample.py` (90è¡Œï¼Œæ˜“è¯»)
- å…³é”®å‡½æ•°: `model.generate()`
- ç›¸å…³æ¨¡å‹ä»£ç : `model.py` ç¬¬306-330è¡Œ

---

### 2. æ¨¡å‹å¾®è°ƒ (Fine-tuning)

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
ä»å¤´è®­ç»ƒå¤ªæ˜‚è´µï¼Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ˜¯æœ€å®ç”¨çš„æ–¹æ³•ã€‚

#### ğŸ“ ä¸»é¢˜ 2.1: å¾®è°ƒ vs ä»å¤´è®­ç»ƒ

```python
å¯¹æ¯”ï¼š

ä»å¤´è®­ç»ƒ (Train from scratch):
  âœ… å®Œå…¨æ§åˆ¶
  âœ… é€‚åˆç‰¹å®šé¢†åŸŸï¼ˆå¦‚ä»£ç ã€æ•°å­¦ï¼‰
  âŒ éœ€è¦å¤§é‡æ•°æ®ï¼ˆå‡ åGBï¼‰
  âŒ è®­ç»ƒæ—¶é—´é•¿ï¼ˆæ•°å¤©åˆ°æ•°å‘¨ï¼‰
  âŒ è®¡ç®—æˆæœ¬é«˜

å¾®è°ƒ (Fine-tuning):
  âœ… å¿«é€Ÿï¼ˆå‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ï¼‰
  âœ… æ•°æ®éœ€æ±‚å°ï¼ˆå‡ MBå°±å¤Ÿï¼‰
  âœ… æˆæœ¬ä½
  âœ… ä¿ç•™é€šç”¨çŸ¥è¯†
  âŒ å—é™äºé¢„è®­ç»ƒæ¨¡å‹çš„æ¶æ„
```

#### ğŸ“ ä¸»é¢˜ 2.2: å¾®è°ƒå®æˆ˜ - èå£«æ¯”äºšé£æ ¼

```bash
# æ­¥éª¤1: å‡†å¤‡æ•°æ®
cd data/shakespeare
python prepare.py
# ç”Ÿæˆ: train.bin (çº¦300KB), val.bin (çº¦36KB)

# æ­¥éª¤2: æŸ¥çœ‹é…ç½®
cat ../../config/finetune_shakespeare.py
```

```python
# config/finetune_shakespeare.py è§£æ

# å…³é”®é…ç½®
init_from = 'gpt2'  # ä»GPT-2å¼€å§‹ï¼Œè€Œä¸æ˜¯éšæœºåˆå§‹åŒ–
learning_rate = 3e-5  # æ¯”ä»å¤´è®­ç»ƒå°å¾ˆå¤šï¼ˆé€šå¸¸æ˜¯1e-3ï¼‰
max_iters = 5000      # çŸ­å¾—å¤š
```

**ä¸ºä»€ä¹ˆå­¦ä¹ ç‡è¦å°ï¼Ÿ**

```
æƒ³è±¡å¾®è°ƒæ˜¯"ç²¾ä¿®"ä¸€ä¸ªå·²ç»å¾ˆå¥½çš„é›•å¡‘ï¼š

ä»å¤´è®­ç»ƒ (learning_rate=1e-3):
  åƒä»ä¸€å—çŸ³å¤´å¼€å§‹é›•åˆ»
  éœ€è¦å¤§åˆ€é˜”æ–§åœ°æ”¹å˜
  
å¾®è°ƒ (learning_rate=3e-5):
  é›•å¡‘å·²ç»å¾ˆåƒäº†
  åªéœ€è¦ç»†å¾®è°ƒæ•´
  å­¦ä¹ ç‡å¤ªå¤§ä¼šç ´åå·²å­¦åˆ°çš„çŸ¥è¯†ï¼
```

```bash
# æ­¥éª¤3: å¼€å§‹å¾®è°ƒ
python train.py config/finetune_shakespeare.py

# è§‚å¯Ÿæ—¥å¿—:
# iter 0: loss 3.2145  â† åˆå§‹lossï¼ˆé¢„è®­ç»ƒæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šï¼‰
# iter 100: loss 1.8234
# iter 500: loss 1.2456
# iter 5000: loss 0.8912 â† å·²ç»éå¸¸å¥½äº†ï¼

# æ­¥éª¤4: ç”Ÿæˆèå£«æ¯”äºšé£æ ¼æ–‡æœ¬
python sample.py --out_dir=out-shakespeare
```

#### ğŸ“ ä¸»é¢˜ 2.3: å¾®è°ƒè‡ªå·±çš„æ•°æ®é›†

**å®æˆ˜é¡¹ç›®ï¼šå¾®è°ƒä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹**

```bash
# 1. å‡†å¤‡æ•°æ®
mkdir -p data/my_code
cd data/my_code

# åˆ›å»ºæ•°æ®å‡†å¤‡è„šæœ¬
cat > prepare.py << 'EOF'
import os
import tiktoken
import numpy as np

# æ”¶é›†Pythonä»£ç 
code_files = []
for root, dirs, files in os.walk('/path/to/your/code'):
    for file in files:
        if file.endswith('.py'):
            with open(os.path.join(root, file), 'r') as f:
                code_files.append(f.read())

# åˆå¹¶
data = '\n\n'.join(code_files)

# 90/10 åˆ†å‰²
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# Tokenize
enc = tiktoken.get_encoding("gpt2")
train_ids = enc.encode_ordinary(train_data)
val_ids = enc.encode_ordinary(val_data)

# ä¿å­˜
np.array(train_ids, dtype=np.uint16).tofile('train.bin')
np.array(val_ids, dtype=np.uint16).tofile('val.bin')

print(f"è®­ç»ƒé›†: {len(train_ids):,} tokens")
print(f"éªŒè¯é›†: {len(val_ids):,} tokens")
EOF

python prepare.py

# 2. åˆ›å»ºé…ç½®æ–‡ä»¶
cat > ../../config/finetune_code.py << 'EOF'
# å¾®è°ƒä»£ç åŠ©æ‰‹
import time

out_dir = 'out-code-assistant'
eval_interval = 250
eval_iters = 200
log_interval = 10

wandb_log = True
wandb_project = 'code-assistant'
wandb_run_name = 'ft-' + str(time.time())

dataset = 'my_code'
init_from = 'gpt2'  # æˆ– 'gpt2-medium' å¦‚æœæ˜¾å­˜å¤Ÿ

# å¾®è°ƒå‚æ•°
batch_size = 16
block_size = 512  # ä»£ç é€šå¸¸éœ€è¦æ›´é•¿çš„ä¸Šä¸‹æ–‡
gradient_accumulation_steps = 4

# å­¦ä¹ ç‡è°ƒåº¦
learning_rate = 1e-5  # æ›´å°ï¼Œå› ä¸ºæ˜¯ä»£ç 
max_iters = 3000
lr_decay_iters = 3000
min_lr = 1e-6

# æ­£åˆ™åŒ–
weight_decay = 1e-1
dropout = 0.1
EOF

# 3. å¼€å§‹å¾®è°ƒ
python train.py config/finetune_code.py

# 4. æµ‹è¯•
python sample.py \
  --out_dir=out-code-assistant \
  --start="def quick_sort(arr):" \
  --num_samples=3
```

#### ğŸ“ ä¸»é¢˜ 2.4: å¾®è°ƒçš„é«˜çº§æŠ€å·§

**æŠ€å·§1: å­¦ä¹ ç‡æŸ¥æ‰¾å™¨**

```python
# æ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡
learning_rates = [1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4]

for lr in learning_rates:
    print(f"\næµ‹è¯• lr={lr}")
    # ä¿®æ”¹configæ–‡ä»¶
    # è®­ç»ƒ1000æ­¥
    # è§‚å¯Ÿlossä¸‹é™é€Ÿåº¦
    # é€‰æ‹©lossä¸‹é™æœ€å¿«ä½†ä¸å‘æ•£çš„lr
```

**æŠ€å·§2: æ¸è¿›å¼è§£å†»**

```python
# å…ˆåªè®­ç»ƒæœ€åå‡ å±‚ï¼Œç„¶åé€æ­¥è§£å†»æ›´å¤šå±‚

# é˜¶æ®µ1: åªè®­ç»ƒè¾“å‡ºå±‚ (1000æ­¥)
for param in model.transformer.parameters():
    param.requires_grad = False
for param in model.lm_head.parameters():
    param.requires_grad = True

# é˜¶æ®µ2: è§£å†»æœ€å3å±‚ (1000æ­¥)
for i in range(-3, 0):
    for param in model.transformer.h[i].parameters():
        param.requires_grad = True

# é˜¶æ®µ3: å…¨éƒ¨è§£å†» (å‰©ä½™æ­¥æ•°)
for param in model.parameters():
    param.requires_grad = True
```

**å­¦ä¹ èµ„æºï¼š**
- é…ç½®: `config/finetune_shakespeare.py`
- æ•°æ®å‡†å¤‡: `data/shakespeare/prepare.py`
- æ¨èé˜…è¯»: [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)

---

### 3. æ•°æ®å‡†å¤‡æ·±å…¥

#### ğŸ“ ä¸»é¢˜ 3.1: Tokenization è¯¦è§£

**ä»€ä¹ˆæ˜¯Tokenizationï¼Ÿ**

```python
# å­—ç¬¦çº§ (Shakespeare Char)
æ–‡æœ¬: "Hello World"
Tokens: ['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd']
ä¼˜ç‚¹: è¯æ±‡è¡¨å°ï¼ˆ65ä¸ªå­—ç¬¦ï¼‰
ç¼ºç‚¹: åºåˆ—é•¿ã€æ•ˆç‡ä½

# BPE (GPT-2 Tiktoken)  
æ–‡æœ¬: "Hello World"
Tokens: ['Hello', ' World']  # æˆ– [15496, 2159]
ä¼˜ç‚¹: æ•ˆç‡é«˜ã€è¯æ±‡è¡¨é€‚ä¸­ï¼ˆ50257ï¼‰
ç¼ºç‚¹: ä¸èƒ½å¤„ç†æœªè§è¿‡çš„è¯­è¨€
```

**å®æˆ˜ï¼šå¯¹æ¯”ä¸åŒTokenization**

```python
import tiktoken

text = "The quick brown fox jumps over the lazy dog. é€Ÿåº¦å¾ˆå¿«çš„æ£•è‰²ç‹ç‹¸è·³è¿‡æ‡’ç‹—ã€‚"

# GPT-2 BPE
enc_gpt2 = tiktoken.get_encoding("gpt2")
tokens_gpt2 = enc_gpt2.encode(text)
print(f"GPT-2: {len(tokens_gpt2)} tokens")
print(tokens_gpt2)

# GPT-4 (æ›´é«˜æ•ˆ)
enc_gpt4 = tiktoken.get_encoding("cl100k_base")
tokens_gpt4 = enc_gpt4.encode(text)
print(f"GPT-4: {len(tokens_gpt4)} tokens")

# è§‚å¯Ÿä¸­æ–‡å¤„ç†çš„å·®å¼‚
```

#### ğŸ“ ä¸»é¢˜ 3.2: è‡ªå®šä¹‰æ•°æ®é›†

**é¡¹ç›®ç¤ºä¾‹ï¼šè®­ç»ƒä¸€ä¸ªSQLç”Ÿæˆå™¨**

```python
# data/sql_dataset/prepare.py

import tiktoken
import numpy as np
import json

# 1. æ”¶é›†SQLæ•°æ®
# æ ¼å¼: 
# é—®é¢˜: "æŸ¥æ‰¾æ‰€æœ‰å¹´é¾„å¤§äº30çš„ç”¨æˆ·"
# SQL: "SELECT * FROM users WHERE age > 30"

with open('sql_pairs.json', 'r') as f:
    data = json.load(f)

# 2. æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ•°æ®
formatted_data = []
for item in data:
    prompt = f"# Question: {item['question']}\n# SQL:\n"
    response = item['sql']
    formatted_data.append(prompt + response + "\n\n")

full_text = ''.join(formatted_data)

# 3. Tokenize
enc = tiktoken.get_encoding("gpt2")
tokens = enc.encode_ordinary(full_text)

# 4. åˆ†å‰²
n = len(tokens)
train_tokens = tokens[:int(n*0.9)]
val_tokens = tokens[int(n*0.9):]

# 5. ä¿å­˜
np.array(train_tokens, dtype=np.uint16).tofile('train.bin')
np.array(val_tokens, dtype=np.uint16).tofile('val.bin')
```

**æ•°æ®è´¨é‡æ£€æŸ¥æ¸…å•ï¼š**

```python
âœ… æ•°æ®æ¸…æ´—
  - å»é™¤é‡å¤
  - ä¿®æ­£ç¼–ç é”™è¯¯
  - ç»Ÿä¸€æ ¼å¼

âœ… æ•°æ®å¹³è¡¡
  - å„ç±»åˆ«æ¯”ä¾‹åˆç†
  - ä¸åŒéš¾åº¦çš„æ ·æœ¬éƒ½æœ‰

âœ… æ•°æ®é‡è¯„ä¼°
  å°è§„æ¨¡å¾®è°ƒ: 1-10MB (çº¦100ä¸‡tokens)
  ä¸­ç­‰è®­ç»ƒ: 100MB-1GB
  å¤§è§„æ¨¡è®­ç»ƒ: 10GB+

âœ… è´¨é‡éªŒè¯
  - éšæœºæŠ½æ ·æ£€æŸ¥
  - è‡ªåŠ¨åŒ–æ£€æµ‹å¼‚å¸¸
  - A/Bæµ‹è¯•
```

---

## âš¡ ç¬¬äºŒé˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆè¿›é˜¶ï¼‰

### 4. æ··åˆç²¾åº¦è®­ç»ƒ

**ä»€ä¹ˆæ˜¯æ··åˆç²¾åº¦ï¼Ÿ**

```python
ä¼ ç»Ÿè®­ç»ƒ (FP32 - 32ä½æµ®ç‚¹):
  æ¯ä¸ªå‚æ•°: 4 bytes
  10Må‚æ•°æ¨¡å‹: 40MB
  ç²¾åº¦: å¾ˆé«˜ï¼Œä½†æ…¢

æ··åˆç²¾åº¦ (FP16/BF16 - 16ä½):
  æ¯ä¸ªå‚æ•°: 2 bytes  
  10Må‚æ•°æ¨¡å‹: 20MB
  ç²¾åº¦: å¤Ÿç”¨
  é€Ÿåº¦: 2-3x å¿«
  æ˜¾å­˜: å‡åŠ
```

**åœ¨ NanoGPT ä¸­å·²ç»å®ç°ï¼š**

```python
# train.py ç¬¬70-72è¡Œ
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)
```

**FP16 vs BF16 å¯¹æ¯”ï¼š**

```
FP16 (Float16):
  èŒƒå›´: Â±65,504
  ç²¾åº¦: é«˜
  é—®é¢˜: å®¹æ˜“æº¢å‡ºï¼ˆéœ€è¦loss scalingï¼‰
  
BF16 (BFloat16):  
  èŒƒå›´: Â±3.4Ã—10Â³â¸ (å’ŒFP32ä¸€æ ·)
  ç²¾åº¦: ç¨ä½
  ä¼˜åŠ¿: ä¸å®¹æ˜“æº¢å‡ºï¼Œæ›´ç¨³å®š
  æ¨è: A100, H100ç­‰æ–°GPU
```

**å®éªŒï¼š**

```bash
# å¯¹æ¯”ä¸åŒç²¾åº¦
python train.py config/train_shakespeare_char.py --dtype='float32' --compile=False
# è®°å½•: æ—¶é—´, æ˜¾å­˜, æœ€ç»ˆloss

python train.py config/train_shakespeare_char.py --dtype='float16'
# å¯¹æ¯”å·®å¼‚

python train.py config/train_shakespeare_char.py --dtype='bfloat16'
# é€šå¸¸æœ€ä¼˜é€‰æ‹©
```

---

### 5. åˆ†å¸ƒå¼è®­ç»ƒ (DDP)

**ä¸ºä»€ä¹ˆéœ€è¦åˆ†å¸ƒå¼ï¼Ÿ**

```
å•GPUæé™:
  GPU: A100 40GB
  æœ€å¤§æ¨¡å‹: ~1Bå‚æ•° (éœ€è¦ä¼˜åŒ–)
  è®­ç»ƒæ—¶é—´: GPT-2 éœ€è¦1-2å‘¨

å¤šGPU (8Ã—A100):
  å¹¶è¡Œè®­ç»ƒ
  çº¿æ€§åŠ é€Ÿï¼ˆæ¥è¿‘8xï¼‰
  è®­ç»ƒæ—¶é—´: GPT-2 åªéœ€è¦4å¤©ï¼
```

**NanoGPT çš„ DDP å®ç°ï¼š**

```bash
# å•æœºå¤šå¡ï¼ˆ8ä¸ªGPUï¼‰
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py

# å¤šæœºå¤šå¡ï¼ˆ2å°æœºå™¨ï¼Œæ¯å°8ä¸ªGPUï¼‰
# ä¸»èŠ‚ç‚¹:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
  --master_addr=192.168.1.100 --master_port=29500 \
  train.py config/train_gpt2.py

# ä»èŠ‚ç‚¹:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 \
  --master_addr=192.168.1.100 --master_port=29500 \
  train.py config/train_gpt2.py
```

**DDP å…³é”®ä»£ç è§£æï¼š**

```python
# train.py ç¬¬94-105è¡Œ

# åˆå§‹åŒ–DDP
ddp = int(os.environ.get('RANK', -1)) != -1
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0
else:
    master_process = True

# åŒ…è£…æ¨¡å‹
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])
```

**DDP å·¥ä½œåŸç†ï¼š**

```
ä¸ç”¨DDP:
  GPU 0: å¤„ç†batch 0
  ç­‰å¾…...
  GPU 0: å¤„ç†batch 1
  æ€»æ—¶é—´: Nä¸ªbatch Ã— æ¯batchæ—¶é—´

ç”¨DDP:
  GPU 0: å¤„ç†batch 0  |
  GPU 1: å¤„ç†batch 1  | åŒæ—¶è¿›è¡Œï¼
  GPU 2: å¤„ç†batch 2  |
  ...
  
  æ¯è½®åŒæ­¥æ¢¯åº¦ï¼ˆå¹³å‡ï¼‰
  æ‰€æœ‰GPUæ›´æ–°ç›¸åŒçš„å‚æ•°
  
  æ€»æ—¶é—´: Nä¸ªbatch Ã— æ¯batchæ—¶é—´ / GPUæ•°é‡
```

---

### 6. æ¨¡å‹ç¼–è¯‘ (torch.compile)

**PyTorch 2.0 çš„é©å‘½æ€§åŠŸèƒ½ï¼**

```python
# train.py ç¬¬267-269è¡Œ
if compile:
    print("compiling the model... (takes a ~minute)")
    model = torch.compile(model)
```

**æ•ˆæœï¼š**

```
æœªç¼–è¯‘:
  è¿­ä»£æ—¶é—´: ~250ms
  
ç¼–è¯‘å:
  é¦–æ¬¡è¿è¡Œ: éœ€è¦ç¼–è¯‘ï¼ˆ1-2åˆ†é’Ÿï¼‰
  è¿­ä»£æ—¶é—´: ~135ms
  
åŠ é€Ÿ: 1.8xï¼
```

**å·¥ä½œåŸç†ï¼š**

```python
ä¼ ç»ŸPyTorch:
  Python â†’ PyTorch ops â†’ é€ä¸ªæ‰§è¡Œ
  å¼€é”€: Pythonè§£é‡Šã€å†…æ ¸å¯åŠ¨

torch.compile:
  Python â†’ åˆ†æè®¡ç®—å›¾ â†’ ä¼˜åŒ– â†’ èåˆæ“ä½œ â†’ ç”Ÿæˆä¼˜åŒ–ä»£ç 
  
ä¼˜åŒ–åŒ…æ‹¬:
  - æ“ä½œèåˆ (LayerNorm + Dropout â†’ å•ä¸ªkernel)
  - å†…å­˜å¸ƒå±€ä¼˜åŒ–
  - è‡ªåŠ¨è°ƒä¼˜
```

**å®éªŒå¯¹æ¯”ï¼š**

```bash
# ä¸ç¼–è¯‘
python train.py config/train_shakespeare_char.py --compile=False

# ç¼–è¯‘
python train.py config/train_shakespeare_char.py --compile=True

# è§‚å¯Ÿæ—¥å¿—ä¸­çš„ "time per iteration"
```

---

### 7. æ€§èƒ½åˆ†æå’ŒåŸºå‡†æµ‹è¯•

**ä½¿ç”¨ bench.pyï¼š**

```bash
python bench.py

# è¾“å‡º:
# Compiling model...
# 0/10 loss: 10.9876
# ...
# time per iteration: 145.23ms, MFU: 42.35%
```

**MFU (Model FLOPs Utilization) è§£é‡Šï¼š**

```python
# model.py ç¬¬289-303è¡Œ

def estimate_mfu(self, fwdbwd_per_iter, dt):
    """ä¼°ç®—æ¨¡å‹FLOPåˆ©ç”¨ç‡"""
    N = self.get_num_params()
    cfg = self.config
    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
    
    # Transformerçš„ç†è®ºFLOPs
    flops_per_token = 6*N + 12*L*H*Q*T
    flops_per_fwdbwd = flops_per_token * T
    flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
    
    # å®é™…è¾¾åˆ°çš„FLOPs
    flops_achieved = flops_per_iter * (1.0/dt)
    
    # A100çš„å³°å€¼æ€§èƒ½
    flops_promised = 312e12  # 312 TFLOPS
    
    # åˆ©ç”¨ç‡
    mfu = flops_achieved / flops_promised
    return mfu
```

**è§£è¯»MFUï¼š**

```
MFU < 20%: æœ‰å¤§é‡ä¼˜åŒ–ç©ºé—´
  - æ£€æŸ¥æ•°æ®åŠ è½½
  - å¢å¤§batch_size
  - ä½¿ç”¨compile

MFU 30-40%: ä¸é”™
  - å¸¸è§„ä¼˜åŒ–åˆ°ä½

MFU 50-60%: éå¸¸å¥½
  - æ¥è¿‘ç¡¬ä»¶æé™
  - A100åœ¨è‰¯å¥½æ¡ä»¶ä¸‹å¯ä»¥è¾¾åˆ°

MFU > 60%: æè‡´ä¼˜åŒ–
  - é€šå¸¸éœ€è¦ä¸“é—¨è°ƒä¼˜
```

**æ€§èƒ½Profilingï¼š**

```bash
# ä½¿ç”¨PyTorch Profiler
python bench.py --profile=True

# ç”Ÿæˆæ—¥å¿—åˆ° ./bench_log
# ä½¿ç”¨TensorBoardæŸ¥çœ‹:
tensorboard --logdir=./bench_log
```

---

## ğŸ”¬ ç¬¬ä¸‰é˜¶æ®µï¼šé«˜çº§ä¸»é¢˜ï¼ˆä¸“å®¶çº§ï¼‰

### 8. Scaling Lawsï¼ˆç¼©æ”¾å®šå¾‹ï¼‰

**Jupyter Notebook: scaling_laws.ipynb**

**æ ¸å¿ƒé—®é¢˜ï¼š**
- ç»™å®šè®¡ç®—é¢„ç®—ï¼Œåº”è¯¥è®­ç»ƒå¤šå¤§çš„æ¨¡å‹ï¼Ÿ
- éœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ
- é¢„æœŸçš„æ€§èƒ½æ˜¯å¤šå°‘ï¼Ÿ

**Chinchillaè®ºæ–‡çš„å‘ç°ï¼š**

```python
ä¼ ç»Ÿè§‚ç‚¹ (GPT-3):
  å¤§æ¨¡å‹ + å°‘æ•°æ®
  175Bå‚æ•°ï¼Œ300B tokens
  
Chinchillaå‘ç°:
  ä¸­ç­‰æ¨¡å‹ + æ›´å¤šæ•°æ® æ›´ä¼˜
  70Bå‚æ•°ï¼Œ1.4T tokens
  æ€§èƒ½æ›´å¥½ï¼Œæˆæœ¬æ›´ä½ï¼
  
ç»“è®º: å‚æ•°é‡å’Œæ•°æ®é‡åº”è¯¥åŒæ­¥å¢é•¿
  æœ€ä¼˜: N_params âˆ C^0.5
       N_tokens âˆ C^0.5
  å…¶ä¸­Cæ˜¯è®¡ç®—é¢„ç®—
```

**å®æˆ˜ï¼šè®¡ç®—ä½ çš„æœ€ä¼˜æ¨¡å‹ï¼š**

```python
# å‡è®¾ä½ æœ‰çš„èµ„æº
compute_budget_flops = 1e20  # 100 PetaFLOPs

# Chinchillaå…¬å¼
N_params_optimal = (compute_budget_flops / 6) ** 0.5 / 20
N_tokens_optimal = 20 * N_params_optimal

print(f"æœ€ä¼˜å‚æ•°é‡: {N_params_optimal/1e9:.1f}B")
print(f"æœ€ä¼˜è®­ç»ƒtokens: {N_tokens_optimal/1e9:.1f}B")
```

**å­¦ä¹ èµ„æºï¼š**
- Notebook: `scaling_laws.ipynb`
- Notebook: `transformer_sizing.ipynb`
- è®ºæ–‡: [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/abs/2203.15556)

---

### 9. æ¨¡å‹è¯„ä¼°

**è¶…è¶Šå•ä¸€LossæŒ‡æ ‡ï¼š**

#### è¯„ä¼°ç»´åº¦ï¼š

```python
1. Perplexity (å›°æƒ‘åº¦)
   PPL = exp(loss)
   è¶Šä½è¶Šå¥½
   
   ä¾‹å­:
   loss = 2.85 â†’ PPL = 17.3
   å«ä¹‰: å¹³å‡æ¯ä¸ªä½ç½®æœ‰17.3ä¸ªå€™é€‰token

2. Zero-shotèƒ½åŠ›
   ç›´æ¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæµ‹è¯•
   - é—®ç­”
   - æ–‡æœ¬åˆ†ç±»
   - æ‘˜è¦ç”Ÿæˆ

3. Few-shotèƒ½åŠ›
   ç»™å‡ ä¸ªä¾‹å­ï¼Œçœ‹èƒ½å¦å­¦ä¼š
   
4. ç”Ÿæˆè´¨é‡
   - äººå·¥è¯„ä¼°
   - è‡ªåŠ¨æŒ‡æ ‡ (BLEU, ROUGE)
```

**å®ç°è¯„ä¼°è„šæœ¬ï¼š**

```python
# eval_downstream.py

import torch
from model import GPT

model = GPT.from_pretrained('gpt2')
model.eval()

# ä»»åŠ¡1: æƒ…æ„Ÿåˆ†ç±»
test_cases = [
    ("This movie is amazing!", "positive"),
    ("Terrible experience, waste of money.", "negative"),
]

for text, true_label in test_cases:
    # è®¡ç®—ä¸¤ç§å»¶ç»­çš„æ¦‚ç‡
    pos_prob = model_prob(text + " Great!")
    neg_prob = model_prob(text + " Terrible!")
    
    pred_label = "positive" if pos_prob > neg_prob else "negative"
    print(f"çœŸå®: {true_label}, é¢„æµ‹: {pred_label}")
```

---

### 10. é…ç½®ç³»ç»Ÿè¯¦è§£ (configurator.py)

**NanoGPT çš„"ç©·äººé…ç½®å™¨"ï¼š**

```python
# ä½¿ç”¨æ–¹å¼
python train.py config/train_shakespeare_char.py --batch_size=64 --learning_rate=1e-4
```

**å·¥ä½œåŸç†ï¼š**

```python
# configurator.py çš„é­”æ³•

for arg in sys.argv[1:]:
    if '=' not in arg:
        # é…ç½®æ–‡ä»¶
        exec(open(config_file).read())
    else:
        # å‘½ä»¤è¡Œè¦†ç›–
        key, val = arg.split('=')
        globals()[key] = val
```

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**

```python
ä¼˜ç‚¹:
  âœ… ç®€å•ï¼ä¸éœ€è¦å¤æ‚çš„é…ç½®åº“
  âœ… çµæ´»ï¼šå¯ä»¥è¦†ç›–ä»»ä½•å˜é‡
  âœ… å¯è¯»ï¼šé…ç½®æ–‡ä»¶å°±æ˜¯Pythonä»£ç 

ç¼ºç‚¹:
  âŒ ä½¿ç”¨exec() (æœ‰å®‰å…¨é£é™©)
  âŒ ç±»å‹æ£€æŸ¥ç®€é™‹
  âŒ ä¸é€‚åˆå¤æ‚é¡¹ç›®

Andrejçš„å“²å­¦:
  "ç®€å•æ¯”å¤æ‚å¥½ï¼Œå³ä½¿æœ‰äº›hack"
```

**åˆ›å»ºä½ è‡ªå·±çš„é…ç½®ï¼š**

```python
# config/my_experiment.py

# åŸºç¡€è®¾ç½®
out_dir = 'out-my-experiment'
eval_interval = 500
eval_iters = 200

# æ¨¡å‹å¤§å°
n_layer = 8
n_head = 8
n_embd = 512

# è®­ç»ƒå‚æ•°
batch_size = 32
block_size = 256
learning_rate = 1e-3
max_iters = 10000

# æ­£åˆ™åŒ–
dropout = 0.2
weight_decay = 0.1
```

---

## ğŸ“ ç¬¬å››é˜¶æ®µï¼šå‰æ²¿ç ”ç©¶ï¼ˆç ”ç©¶è€…ï¼‰

### 11. PEFT - å‚æ•°é«˜æ•ˆå¾®è°ƒ

**é—®é¢˜ï¼š**
```
å®Œæ•´å¾®è°ƒGPT-2 (124Må‚æ•°):
  - éœ€è¦æ›´æ–°æ‰€æœ‰å‚æ•°
  - æ˜¾å­˜éœ€æ±‚: çº¦500MB (æ¨¡å‹+ä¼˜åŒ–å™¨)
  - æ¯ä¸ªä»»åŠ¡éœ€è¦ä¿å­˜å®Œæ•´æ¨¡å‹
```

**è§£å†³æ–¹æ¡ˆï¼šLoRA**

```python
LoRA (Low-Rank Adaptation):
  åŸå§‹æƒé‡: W (frozen)
  æ·»åŠ : Î”W = A Ã— B
  å…¶ä¸­ A: [d, r], B: [r, d], r << d
  
  ä¾‹å­: 
    W: [768, 768] = 589,824 å‚æ•°
    A: [768, 8], B: [8, 768] = 12,288 å‚æ•°
    å‡å°‘: 98% ï¼

å®ç°(éœ€è¦æ·»åŠ åˆ°model.py):
import loralib as lora

# æ›¿æ¢Linearå±‚
self.c_attn = lora.Linear(n_embd, 3*n_embd, r=8)

# åªè®­ç»ƒLoRAå‚æ•°
for n, p in model.named_parameters():
    if 'lora_' not in n:
        p.requires_grad = False
```

**å­¦ä¹ èµ„æºï¼š**
- è®ºæ–‡: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- åº“: [microsoft/LoRA](https://github.com/microsoft/LoRA)

---

### 12. å¤šæ¨¡æ€æ‰©å±•

**GPTåªæ˜¯æ–‡æœ¬ï¼Œèƒ½å¦å¤„ç†å›¾åƒï¼Ÿ**

**æ€è·¯ï¼š**

```python
# å›¾åƒ â†’ æ–‡æœ¬çš„æ¡¥æ¢

1. å›¾åƒç¼–ç å™¨ (å¦‚ CLIP)
   å›¾åƒ â†’ [196, 768] çš„tokenåºåˆ—

2. æŠ•å½±å±‚
   [196, 768] â†’ [196, n_embd]

3. æ‹¼æ¥åˆ°GPT
   [image_tokens] + [text_tokens] â†’ GPT

4. è®­ç»ƒç›®æ ‡
   ç»™å®šå›¾åƒï¼Œç”Ÿæˆæè¿°
```

**ç¤ºä¾‹ï¼š**

```python
class MultimodalGPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.gpt = GPT(config)
        self.image_encoder = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
        self.image_projection = nn.Linear(768, config.n_embd)
    
    def forward(self, image, text):
        # ç¼–ç å›¾åƒ
        img_features = self.image_encoder(image).last_hidden_state
        img_tokens = self.image_projection(img_features)
        
        # ç¼–ç æ–‡æœ¬
        text_emb = self.gpt.transformer.wte(text)
        
        # æ‹¼æ¥
        combined = torch.cat([img_tokens, text_emb], dim=1)
        
        # GPTå¤„ç†
        return self.gpt(combined)
```

---

### 13. æ¶æ„æ”¹è¿›

**NanoGPTä½¿ç”¨æ ‡å‡†Transformerï¼Œä½†æœ‰å¾ˆå¤šå˜ä½“ï¼š**

#### æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ï¼š

```python
1. Rotary Position Embedding (RoPE)
   ç”¨äº: LLaMA, PaLM
   ä¼˜åŠ¿: æ›´å¥½çš„é•¿åº¦å¤–æ¨

2. ALiBi (Attention with Linear Biases)
   ç”¨äº: BLOOM
   ä¼˜åŠ¿: è®­ç»ƒçŸ­åºåˆ—ï¼Œæ¨ç†é•¿åºåˆ—

3. Flash Attention
   å·²é›†æˆåœ¨NanoGPT!
   ä¼˜åŠ¿: å†…å­˜é«˜æ•ˆï¼Œé€Ÿåº¦å¿«

4. Sparse Attention
   åªè®¡ç®—éƒ¨åˆ†attention
   ä¼˜åŠ¿: O(nâˆšn) è€Œä¸æ˜¯ O(nÂ²)
```

#### å®ç°RoPEç¤ºä¾‹ï¼š

```python
# æ›¿ä»£ä¼ ç»Ÿposition embedding

class RoPEAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ä¸éœ€è¦position embedding!
        # self.wpe = nn.Embedding(config.block_size, config.n_embd)  # åˆ é™¤
    
    def apply_rotary_emb(self, q, k):
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        seq_len = q.shape[1]
        freqs = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))
        pos = torch.arange(seq_len)
        emb = pos[:, None] * freqs[None, :]
        
        cos, sin = emb.cos(), emb.sin()
        # æ—‹è½¬qå’Œk
        q_rot = (q * cos) + (self.rotate_half(q) * sin)
        k_rot = (k * cos) + (self.rotate_half(k) * sin)
        return q_rot, k_rot
```

---

## ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•

### åŸºç¡€ç¯‡ (ä½ å·²ç»å®Œæˆï¼)
- [x] ç†è§£é…ç½®å‚æ•°
- [x] æŒæ¡æ•°æ®åŠ è½½
- [x] ç†è§£è®­ç»ƒå¾ªç¯
- [x] ç†è§£Transformeræ¶æ„

### å®æˆ˜ç¯‡
- [ ] æˆåŠŸè®­ç»ƒä¸€ä¸ªå­—ç¬¦çº§æ¨¡å‹
- [ ] å¾®è°ƒGPT-2åœ¨è‡ªå·±çš„æ•°æ®ä¸Š
- [ ] å®ç°ä¸åŒçš„é‡‡æ ·ç­–ç•¥
- [ ] å‡†å¤‡å¹¶æ¸…æ´—è‡ªå·±çš„æ•°æ®é›†

### ä¼˜åŒ–ç¯‡
- [ ] ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- [ ] å°è¯•DDPå¤šGPUè®­ç»ƒ
- [ ] ä½¿ç”¨torch.compileåŠ é€Ÿ
- [ ] è¿›è¡Œæ€§èƒ½profiling

### é«˜çº§ç¯‡
- [ ] ç†è§£Scaling Laws
- [ ] å®ç°æ¨¡å‹è¯„ä¼°è„šæœ¬
- [ ] å°è¯•LoRAå¾®è°ƒ
- [ ] å®éªŒä¸åŒçš„æ¶æ„æ”¹è¿›

---

## ğŸ¯ æ¨èå­¦ä¹ è·¯å¾„

### è·¯å¾„A: å®è·µè€… (åº”ç”¨ä¸ºä¸»)

```
Week 1: æ–‡æœ¬ç”Ÿæˆå®éªŒ
  - ç†è§£é‡‡æ ·ç­–ç•¥
  - è°ƒæ•´temperatureå’Œtop_k
  - ç”Ÿæˆä¸åŒé£æ ¼çš„æ–‡æœ¬

Week 2: å¾®è°ƒå®æˆ˜
  - å‡†å¤‡è‡ªå·±çš„æ•°æ®
  - å¾®è°ƒæ¨¡å‹
  - è¯„ä¼°å’Œè¿­ä»£

Week 3: æ€§èƒ½ä¼˜åŒ–
  - æ··åˆç²¾åº¦
  - å¢å¤§batch_size
  - ä½¿ç”¨compile

Week 4: å®é™…åº”ç”¨
  - éƒ¨ç½²æ¨¡å‹
  - APIå°è£…
  - ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
```

### è·¯å¾„B: ç ”ç©¶è€… (ç†è®ºä¸ºä¸»)

```
Week 1-2: æ·±å…¥ç†è®º
  - è¯¦ç»†ç ”è¯»Transformerè®ºæ–‡
  - ç†è§£Scaling Laws
  - å¯¹æ¯”ä¸åŒæ¶æ„

Week 3-4: å®ç°æ”¹è¿›
  - å®ç°æ–°çš„æ³¨æ„åŠ›æœºåˆ¶
  - å°è¯•æ¶æ„å˜ä½“
  - æ¶ˆèå®éªŒ

Week 5-6: å‰æ²¿æ¢ç´¢
  - RLHF
  - Constitutional AI
  - å¤šæ¨¡æ€
```

### è·¯å¾„C: å·¥ç¨‹å¸ˆ (ç³»ç»Ÿä¸ºä¸»)

```
Week 1: åˆ†å¸ƒå¼è®­ç»ƒ
  - å•æœºå¤šå¡
  - å¤šæœºå¤šå¡
  - é€šä¿¡ä¼˜åŒ–

Week 2: å¤§è§„æ¨¡è®­ç»ƒ
  - FSDP
  - Pipeline Parallelism
  - Gradient Checkpointing

Week 3: ç”Ÿäº§éƒ¨ç½²
  - æ¨¡å‹é‡åŒ–
  - æ¨ç†ä¼˜åŒ–
  - æœåŠ¡åŒ–

Week 4: ç›‘æ§å’Œè¿ç»´
  - è®­ç»ƒç›‘æ§
  - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
  - A/Bæµ‹è¯•
```

---

## ğŸ“š æ¨èèµ„æº

### å¿…è¯»è®ºæ–‡
1. **Attention is All You Need** - TransformeråŸå§‹è®ºæ–‡
2. **Language Models are Few-Shot Learners (GPT-3)** - å¤§æ¨¡å‹çš„åŠ›é‡
3. **Training Compute-Optimal LLMs (Chinchilla)** - Scaling Laws
4. **LoRA** - é«˜æ•ˆå¾®è°ƒ
5. **InstructGPT** - RLHF

### ä¼˜ç§€æ•™ç¨‹
1. **Andrej Karpathyçš„è§†é¢‘**:
   - "Let's build GPT: from scratch"
   - "Zero to Hero"ç³»åˆ—

2. **åšå®¢**:
   - Jay Alammarçš„"The Illustrated Transformer"
   - Lilian Wengçš„åšå®¢

3. **è¯¾ç¨‹**:
   - Stanford CS224N
   - Hugging Face Course

### å®ç”¨å·¥å…·
1. **Weights & Biases** - å®éªŒè·Ÿè¸ª
2. **Hugging Face** - æ¨¡å‹å’Œæ•°æ®é›†
3. **TensorBoard** - å¯è§†åŒ–
4. **DeepSpeed** - å¤§è§„æ¨¡è®­ç»ƒ

---

## ğŸ’¡ é¡¹ç›®å»ºè®®

### åˆçº§é¡¹ç›®
1. **è¯—æ­Œç”Ÿæˆå™¨** - è®­ç»ƒç”Ÿæˆå”è¯—æˆ–ç°ä»£è¯—
2. **ä»£ç è¡¥å…¨** - å¾®è°ƒä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹
3. **å¯¹è¯bot** - ç‰¹å®šé¢†åŸŸçš„èŠå¤©æœºå™¨äºº

### ä¸­çº§é¡¹ç›®
1. **SQLç”Ÿæˆå™¨** - è‡ªç„¶è¯­è¨€â†’SQL
2. **æ–‡æ¡£æ‘˜è¦** - è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
3. **é£æ ¼è¿ç§»** - æ”¹å˜æ–‡æœ¬é£æ ¼

### é«˜çº§é¡¹ç›®
1. **å¤šè¯­è¨€æ¨¡å‹** - è®­ç»ƒæ”¯æŒå¤šç§è¯­è¨€
2. **ä»£ç è°ƒè¯•åŠ©æ‰‹** - æ‰¾bugå’Œä¿®å¤
3. **å¤šæ¨¡æ€åº”ç”¨** - å›¾æ–‡ç»“åˆ

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³å¯ä»¥åšçš„3ä»¶äº‹ï¼š**

1. **è¿è¡Œä¸€æ¬¡å®Œæ•´è®­ç»ƒ**
   ```bash
   python train.py config/train_shakespeare_char.py
   python sample.py --out_dir=out-shakespeare-char
   ```

2. **å¾®è°ƒè‡ªå·±çš„æ¨¡å‹**
   - æ”¶é›†ä½ æ„Ÿå…´è¶£çš„æ–‡æœ¬æ•°æ®
   - å‡†å¤‡æ•°æ®é›†
   - å¼€å§‹å¾®è°ƒ

3. **æ·±å…¥ä¸€ä¸ªé«˜çº§ä¸»é¢˜**
   - é€‰æ‹©ä¸€ä¸ªä½ æœ€æ„Ÿå…´è¶£çš„
   - é˜…è¯»ç›¸å…³è®ºæ–‡
   - åŠ¨æ‰‹å®ç°

---

## ğŸ“¬ ç»§ç»­å­¦ä¹ 

**ä½ æƒ³æ·±å…¥å­¦ä¹ å“ªä¸ªæ–¹å‘ï¼Ÿ**

1. **"æˆ‘æƒ³åšå®æˆ˜é¡¹ç›®"** â†’ æˆ‘å¯ä»¥æä¾›è¯¦ç»†çš„é¡¹ç›®æŒ‡å¯¼
2. **"æˆ‘æƒ³ç†è§£DDP"** â†’ æˆ‘å¯ä»¥è¯¦ç»†è®²è§£åˆ†å¸ƒå¼è®­ç»ƒ
3. **"æˆ‘æƒ³å®ç°LoRA"** â†’ æˆ‘å¯ä»¥æä¾›å®Œæ•´çš„ä»£ç å®ç°
4. **"æˆ‘æƒ³ä¼˜åŒ–æ€§èƒ½"** â†’ æˆ‘å¯ä»¥æä¾›æ€§èƒ½è°ƒä¼˜æŒ‡å—
5. **"æˆ‘æœ‰å…¶ä»–é—®é¢˜"** â†’ ç›´æ¥é—®æˆ‘ï¼

---

**è®°ä½ï¼š**

> æœ€å¥½çš„å­¦ä¹ æ–¹å¼æ˜¯åŠ¨æ‰‹å®è·µã€‚
> ç†è®º+å®æˆ˜ï¼Œæ‰èƒ½çœŸæ­£æŒæ¡ï¼
> ä¸è¦å®³æ€•çŠ¯é”™ï¼Œæ¯ä¸ªbugéƒ½æ˜¯å­¦ä¹ æœºä¼šã€‚

ç¥ä½ åœ¨AIå­¦ä¹ ä¹‹è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼ğŸ‰
