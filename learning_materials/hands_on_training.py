#!/usr/bin/env python3
"""
è®­ç»ƒå¾ªç¯å®æˆ˜ç»ƒä¹  - ä»é›¶ç†è§£æ¢¯åº¦ä¸‹é™

è¿™ä¸ªè„šæœ¬åŒ…å«5ä¸ªé€’è¿›çš„ç»ƒä¹ ï¼Œå¸®åŠ©ä½ ç†è§£train.pyçš„æ ¸å¿ƒæ¦‚å¿µ
æ¯ä¸ªç»ƒä¹ éƒ½æœ‰è¯¦ç»†æ³¨é‡Šå’Œå¯è§†åŒ–è¾“å‡º
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import matplotlib.pyplot as plt
import numpy as np

print("="*80)
print("è®­ç»ƒå¾ªç¯å®æˆ˜ç»ƒä¹ ")
print("="*80)
print()

# ============================================================================
# ç»ƒä¹ 1: æœ€ç®€å•çš„æ¢¯åº¦ä¸‹é™
# ============================================================================
print("ğŸ“ ç»ƒä¹ 1: ç†è§£æ¢¯åº¦ä¸‹é™çš„åŸºæœ¬åŸç†")
print("-" * 80)

# é—®é¢˜ï¼šå­¦ä¹ å‡½æ•° y = 2x
# ç›®æ ‡ï¼šæ‰¾åˆ°å‚æ•° w = 2

# è®­ç»ƒæ•°æ®
x_train = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y_train = torch.tensor([2.0, 4.0, 6.0, 8.0, 10.0])

# æ¨¡å‹å‚æ•°ï¼ˆåˆå§‹ççŒœï¼‰
w = torch.tensor([0.5], requires_grad=True)

# è¶…å‚æ•°
learning_rate = 0.1
num_epochs = 20

# è®°å½•è®­ç»ƒè¿‡ç¨‹
history = {'w': [], 'loss': []}

print(f"åˆå§‹å‚æ•°: w = {w.item():.4f}")
print(f"ç›®æ ‡: w = 2.0")
print()

for epoch in range(num_epochs):
    # å‰å‘ä¼ æ’­
    y_pred = w * x_train
    loss = ((y_pred - y_train) ** 2).mean()
    
    # è®°å½•
    history['w'].append(w.item())
    history['loss'].append(loss.item())
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ‰‹åŠ¨æ›´æ–°å‚æ•°ï¼ˆä¸ä½¿ç”¨ä¼˜åŒ–å™¨ï¼‰
    with torch.no_grad():
        w -= learning_rate * w.grad
        
    # æ¸…ç©ºæ¢¯åº¦
    w.grad.zero_()
    
    # æ‰“å°è¿›åº¦
    if epoch % 4 == 0 or epoch == num_epochs - 1:
        print(f"Epoch {epoch:2d}: w = {w.item():.6f}, loss = {loss.item():.6f}")

print(f"\nâœ… æœ€ç»ˆå‚æ•°: w = {w.item():.6f} (ç›®æ ‡: 2.0)")
print()

# ============================================================================
# ç»ƒä¹ 2: ç†è§£batch training
# ============================================================================
print("ğŸ“ ç»ƒä¹ 2: ç†è§£Batch Sizeçš„å½±å“")
print("-" * 80)

# ç”Ÿæˆæ•°æ®
torch.manual_seed(42)
X = torch.randn(100, 1) * 10
Y = 3 * X + 7 + torch.randn(100, 1) * 2  # y = 3x + 7 + noise

def train_with_batch_size(batch_size, num_epochs=50):
    """ç”¨æŒ‡å®šbatch_sizeè®­ç»ƒ"""
    model = nn.Linear(1, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    
    losses = []
    for epoch in range(num_epochs):
        # éšæœºæ‰“ä¹±æ•°æ®
        perm = torch.randperm(len(X))
        X_shuffled = X[perm]
        Y_shuffled = Y[perm]
        
        epoch_loss = 0
        for i in range(0, len(X), batch_size):
            # è·å–batch
            X_batch = X_shuffled[i:i+batch_size]
            Y_batch = Y_shuffled[i:i+batch_size]
            
            # è®­ç»ƒ
            y_pred = model(X_batch)
            loss = F.mse_loss(y_pred, Y_batch)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        losses.append(epoch_loss / (len(X) // batch_size))
    
    return losses, model

# æµ‹è¯•ä¸åŒbatch_size
batch_sizes = [1, 10, 50, 100]
results = {}

for bs in batch_sizes:
    losses, model = train_with_batch_size(bs)
    w = model.weight.item()
    b = model.bias.item()
    results[bs] = losses
    print(f"Batch Size {bs:3d}: w={w:.4f}, b={b:.4f} (ç›®æ ‡: w=3, b=7)")

print()
print("è§‚å¯Ÿï¼š")
print("- Batch Sizeè¶Šå°ï¼šlossæ›²çº¿è¶Šéœ‡è¡ï¼ˆå™ªå£°å¤§ï¼‰")
print("- Batch Sizeè¶Šå¤§ï¼šlossæ›²çº¿è¶Šå¹³æ»‘ï¼ˆä½†éœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰")
print()

# ============================================================================
# ç»ƒä¹ 3: ç†è§£æ¢¯åº¦ç´¯ç§¯
# ============================================================================
print("ğŸ“ ç»ƒä¹ 3: æ¢¯åº¦ç´¯ç§¯ = æ¨¡æ‹Ÿå¤§Batch")
print("-" * 80)

def train_normal(batch_size, lr=0.01):
    """æ­£å¸¸è®­ç»ƒï¼šç›´æ¥ç”¨å¤§batch"""
    model = nn.Linear(1, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    
    # ä¸€æ¬¡æ€§å¤„ç†å¤§batch
    X_batch = X[:batch_size]
    Y_batch = Y[:batch_size]
    
    y_pred = model(X_batch)
    loss = F.mse_loss(y_pred, Y_batch)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    return model, loss.item()

def train_with_accumulation(small_batch_size, accumulation_steps, lr=0.01):
    """æ¢¯åº¦ç´¯ç§¯è®­ç»ƒï¼šç”¨å°batchæ¨¡æ‹Ÿå¤§batch"""
    model = nn.Linear(1, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    
    total_loss = 0
    optimizer.zero_grad()
    
    for step in range(accumulation_steps):
        # å°batch
        start = step * small_batch_size
        X_batch = X[start:start + small_batch_size]
        Y_batch = Y[start:start + small_batch_size]
        
        y_pred = model(X_batch)
        loss = F.mse_loss(y_pred, Y_batch)
        
        # å…³é”®ï¼šé™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼
        loss = loss / accumulation_steps
        loss.backward()  # ç´¯ç§¯æ¢¯åº¦
        
        total_loss += loss.item()
    
    optimizer.step()  # ä¸€æ¬¡æ€§æ›´æ–°
    
    return model, total_loss

# æ¯”è¾ƒ
print("æ¯”è¾ƒä¸¤ç§æ–¹æ³•ï¼ˆåº”è¯¥å¾—åˆ°ç›¸åŒç»“æœï¼‰ï¼š")
print()

# æ–¹æ³•1ï¼šç›´æ¥ç”¨batch_size=32
model1, loss1 = train_normal(batch_size=32)
print(f"æ–¹æ³•1 (æ­£å¸¸batch=32):")
print(f"  Loss: {loss1:.6f}")
print(f"  å‚æ•°: w={model1.weight.item():.6f}, b={model1.bias.item():.6f}")
print()

# æ–¹æ³•2ï¼šç”¨batch_size=8ï¼Œç´¯ç§¯4æ¬¡
model2, loss2 = train_with_accumulation(small_batch_size=8, accumulation_steps=4)
print(f"æ–¹æ³•2 (batch=8, ç´¯ç§¯4æ¬¡):")
print(f"  Loss: {loss2:.6f}")
print(f"  å‚æ•°: w={model2.weight.item():.6f}, b={model2.bias.item():.6f}")
print()

print("âœ… ä¸¤ç§æ–¹æ³•æ¢¯åº¦å‡ ä¹ç›¸åŒï¼")
print("   è¿™å°±æ˜¯æ¢¯åº¦ç´¯ç§¯çš„åŸç†ï¼šç”¨å°batchæ¨¡æ‹Ÿå¤§batch")
print()

# ============================================================================
# ç»ƒä¹ 4: ç†è§£æ¢¯åº¦è£å‰ª
# ============================================================================
print("ğŸ“ ç»ƒä¹ 4: æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
print("-" * 80)

def train_with_gradient_clip(clip_value=None):
    """è®­ç»ƒå¹¶å¯é€‰åœ°åº”ç”¨æ¢¯åº¦è£å‰ª"""
    model = nn.Linear(10, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    
    # æ„é€ ä¸€ä¸ªå®¹æ˜“å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸çš„æƒ…å†µ
    X_bad = torch.randn(5, 10) * 100  # å¾ˆå¤§çš„è¾“å…¥
    Y_bad = torch.randn(5, 1) * 100
    
    max_grads = []
    
    for _ in range(10):
        y_pred = model(X_bad)
        loss = F.mse_loss(y_pred, Y_bad)
        
        optimizer.zero_grad()
        loss.backward()
        
        # è®°å½•æ¢¯åº¦å¤§å°
        total_norm = 0
        for p in model.parameters():
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        max_grads.append(total_norm)
        
        # æ¢¯åº¦è£å‰ª
        if clip_value is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)
        
        optimizer.step()
    
    return max_grads

# ä¸è£å‰ª
grads_no_clip = train_with_gradient_clip(clip_value=None)
print(f"ä¸è£å‰ª: æœ€å¤§æ¢¯åº¦èŒƒæ•° = {max(grads_no_clip):.2f}")

# è£å‰ªåˆ°1.0
grads_with_clip = train_with_gradient_clip(clip_value=1.0)
print(f"è£å‰ªåˆ°1.0: æœ€å¤§æ¢¯åº¦èŒƒæ•° = {max(grads_with_clip):.2f}")
print()

print("âœ… æ¢¯åº¦è£å‰ªæˆåŠŸé™åˆ¶äº†æ¢¯åº¦å¤§å°ï¼Œé˜²æ­¢è®­ç»ƒå´©æºƒ")
print()

# ============================================================================
# ç»ƒä¹ 5: ç†è§£å­¦ä¹ ç‡è°ƒåº¦
# ============================================================================
print("ğŸ“ ç»ƒä¹ 5: å­¦ä¹ ç‡è°ƒåº¦ï¼ˆWarmup + Cosine Decayï¼‰")
print("-" * 80)

def get_lr(it, learning_rate=1e-3, warmup_iters=100, lr_decay_iters=1000, min_lr=1e-4):
    """å¤åˆ¶train.pyçš„å­¦ä¹ ç‡è°ƒåº¦"""
    # Warmup
    if it < warmup_iters:
        return learning_rate * (it + 1) / (warmup_iters + 1)
    # Decay
    if it > lr_decay_iters:
        return min_lr
    # Cosine
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate - min_lr)

# ç”Ÿæˆå­¦ä¹ ç‡æ›²çº¿
max_iters = 1000
lrs = [get_lr(i) for i in range(max_iters)]

print("å­¦ä¹ ç‡å˜åŒ–ï¼š")
checkpoints = [0, 50, 100, 200, 500, 900, 999]
for i in checkpoints:
    print(f"  Step {i:4d}: lr = {lrs[i]:.6f}")
print()

print("âœ… å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼š")
print("   1. Warmup (0-100æ­¥): ä»å°åˆ°å¤§ï¼Œé¿å…åˆæœŸéœ‡è¡")
print("   2. æ­£å¸¸è®­ç»ƒ: ä¿æŒè¾ƒé«˜å­¦ä¹ ç‡")
print("   3. Cosine Decay: é€æ¸é™ä½ï¼Œç²¾ç»†è°ƒæ•´")
print()

# ============================================================================
# ç»ƒä¹ 6: æ¨¡æ‹Ÿå®Œæ•´è®­ç»ƒå¾ªç¯
# ============================================================================
print("ğŸ“ ç»ƒä¹ 6: å®Œæ•´è®­ç»ƒå¾ªç¯ï¼ˆæ¨¡æ‹ŸNanoGPTï¼‰")
print("-" * 80)

class TinyModel(nn.Module):
    """è¶…å°æ¨¡å‹ï¼Œæ¨¡æ‹ŸGPT"""
    def __init__(self, vocab_size, n_embd):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, n_embd)
        self.fc = nn.Linear(n_embd, vocab_size)
    
    def forward(self, x, targets=None):
        # x: [batch, seq_len]
        emb = self.token_emb(x)  # [batch, seq_len, n_embd]
        logits = self.fc(emb)     # [batch, seq_len, vocab_size]
        
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss

# è™šæ‹Ÿæ•°æ®
vocab_size = 50
batch_size = 4
block_size = 8

def get_fake_batch():
    """æ¨¡æ‹Ÿget_batch()"""
    x = torch.randint(0, vocab_size, (batch_size, block_size))
    y = torch.randint(0, vocab_size, (batch_size, block_size))
    return x, y

# åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
model = TinyModel(vocab_size, n_embd=32)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

# è®­ç»ƒé…ç½®
max_iters = 100
gradient_accumulation_steps = 4
grad_clip = 1.0

print(f"è®­ç»ƒé…ç½®ï¼š")
print(f"  max_iters = {max_iters}")
print(f"  gradient_accumulation_steps = {gradient_accumulation_steps}")
print(f"  grad_clip = {grad_clip}")
print()

print("å¼€å§‹è®­ç»ƒ...")
print()

for iter_num in range(max_iters):
    # æ¢¯åº¦ç´¯ç§¯å¾ªç¯
    optimizer.zero_grad()
    total_loss = 0
    
    for micro_step in range(gradient_accumulation_steps):
        X, Y = get_fake_batch()
        logits, loss = model(X, Y)
        loss = loss / gradient_accumulation_steps
        loss.backward()
        total_loss += loss.item()
    
    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
    
    # æ›´æ–°å‚æ•°
    optimizer.step()
    
    # æ—¥å¿—
    if iter_num % 20 == 0:
        print(f"Step {iter_num:3d}: loss = {total_loss:.4f}")

print()
print("âœ… å®Œæ•´è®­ç»ƒå¾ªç¯æ¼”ç¤ºå®Œæˆï¼")
print()

# ============================================================================
# æ€»ç»“
# ============================================================================
print("="*80)
print("ğŸ“ æ€»ç»“ï¼šä½ å­¦åˆ°äº†ä»€ä¹ˆ")
print("="*80)
print()
print("1. âœ… æ¢¯åº¦ä¸‹é™çš„åŸºæœ¬åŸç†")
print("   - å‰å‘ä¼ æ’­è®¡ç®—loss")
print("   - åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦")
print("   - å‚æ•°æ›´æ–°")
print()
print("2. âœ… Batch Sizeçš„å½±å“")
print("   - å¤§batch: ç¨³å®šä½†éœ€è¦æ›´å¤šæ˜¾å­˜")
print("   - å°batch: éœ‡è¡ä½†æ˜¾å­˜å‹å¥½")
print()
print("3. âœ… æ¢¯åº¦ç´¯ç§¯")
print("   - ç”¨å°batchæ¨¡æ‹Ÿå¤§batch")
print("   - å…³é”®ï¼šlossè¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°")
print()
print("4. âœ… æ¢¯åº¦è£å‰ª")
print("   - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
print("   - é™åˆ¶æ¢¯åº¦èŒƒæ•°åˆ°æŒ‡å®šå€¼")
print()
print("5. âœ… å­¦ä¹ ç‡è°ƒåº¦")
print("   - Warmup: é¿å…åˆæœŸéœ‡è¡")
print("   - Cosine Decay: åæœŸç²¾ç»†è°ƒæ•´")
print()
print("6. âœ… å®Œæ•´è®­ç»ƒå¾ªç¯")
print("   - ç»“åˆä»¥ä¸Šæ‰€æœ‰æŠ€å·§")
print("   - è¿™å°±æ˜¯NanoGPTçš„train.py!")
print()
print("="*80)
print("ğŸš€ ä¸‹ä¸€æ­¥ï¼šè¿è¡ŒçœŸå®çš„è®­ç»ƒ")
print("="*80)
print()
print("è¯•è¯•è¿™äº›å‘½ä»¤ï¼š")
print()
print("# å¿«é€Ÿæµ‹è¯•ï¼ˆCPUï¼Œ5åˆ†é’Ÿï¼‰")
print("python train.py config/train_shakespeare_char.py --device=cpu --max_iters=500")
print()
print("# å®Œæ•´è®­ç»ƒï¼ˆGPUï¼Œ30åˆ†é’Ÿï¼‰")
print("python train.py config/train_shakespeare_char.py --max_iters=5000")
print()
print("# æŸ¥çœ‹è®­ç»ƒæ•ˆæœ")
print("python sample.py --out_dir=out-shakespeare-char")
print()
