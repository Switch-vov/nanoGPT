# é…ç½®å‚æ•°è¯¦è§£ - åˆå­¦è€…æŒ‡å—

## 1. æ•°æ®ç›¸å…³å‚æ•°

### batch_size (æ‰¹æ¬¡å¤§å°)

**æ˜¯ä»€ä¹ˆï¼Ÿ**
æ¯æ¬¡è®­ç»ƒæ—¶ï¼ŒåŒæ—¶ç»™æ¨¡å‹çœ‹å¤šå°‘ä¸ªæ ·æœ¬ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼š**
- batch_size = 1ï¼šè€å¸ˆä¸€æ¬¡åªæ•™ä¸€ä¸ªå­¦ç”Ÿï¼ˆæ…¢ï¼Œä½†ç²¾ç¡®ï¼‰
- batch_size = 64ï¼šè€å¸ˆä¸€æ¬¡æ•™64ä¸ªå­¦ç”Ÿï¼ˆå¿«ï¼Œä½†å¹³å‡åŒ–ï¼‰

**å…·ä½“ä¾‹å­ï¼š**
```python
batch_size = 4
block_size = 8  # æ¯ä¸ªæ ·æœ¬8ä¸ªå­—ç¬¦

# è®­ç»ƒæ•°æ®å¯èƒ½é•¿è¿™æ ·ï¼š
æ ·æœ¬1: "To be or" â†’ é¢„æµ‹ "not"
æ ·æœ¬2: "not to b" â†’ é¢„æµ‹ "e"
æ ·æœ¬3: "be that " â†’ é¢„æµ‹ "is"
æ ·æœ¬4: "is the q" â†’ é¢„æµ‹ "u"

# æ¨¡å‹ä¸€æ¬¡æ€§å¤„ç†è¿™4ä¸ªæ ·æœ¬
```

**æ•°å€¼å½±å“ï¼š**
| batch_size | è®­ç»ƒé€Ÿåº¦ | æ˜¾å­˜ä½¿ç”¨ | æ¢¯åº¦å™ªå£° | å»ºè®®åœºæ™¯ |
|-----------|---------|---------|---------|----------|
| 1-8       | æ…¢      | ä½      | é«˜      | è°ƒè¯•/å°æ•°æ®é›† |
| 16-32     | ä¸­      | ä¸­      | ä¸­      | **åˆå­¦è€…æ¨è** |
| 64-128    | å¿«      | é«˜      | ä½      | å¤§è§„æ¨¡è®­ç»ƒ |

### block_size (ä¸Šä¸‹æ–‡é•¿åº¦)

**æ˜¯ä»€ä¹ˆï¼Ÿ**
æ¨¡å‹ä¸€æ¬¡èƒ½çœ‹åˆ°å¤šå°‘ä¸ªå­—ç¬¦/è¯ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼š**
- block_size = 8ï¼šåªè®°å¾—æœ€è¿‘8ä¸ªè¯ï¼ˆå¥å¿˜ï¼‰
- block_size = 256ï¼šèƒ½è®°å¾—256ä¸ªè¯ï¼ˆè®°å¿†åŠ›å¥½ï¼‰
- block_size = 1024ï¼šèƒ½è®°å¾—1024ä¸ªè¯ï¼ˆè¿‡ç›®ä¸å¿˜ï¼‰

**å®é™…ä¾‹å­ï¼š**

å‡è®¾æœ‰å¥å­ï¼š"The quick brown fox jumps over the lazy dog"

```python
# block_size = 4
è¾“å…¥: "The quick brown" â†’ åªçœ‹å‰4ä¸ªè¯ï¼Œé¢„æµ‹ç¬¬5ä¸ª
æ¨¡å‹çœ‹ä¸åˆ°ï¼š"fox jumps over the lazy dog"

# block_size = 10
è¾“å…¥: "The quick brown fox jumps over the lazy" â†’ çœ‹å‰10ä¸ªè¯
æ¨¡å‹å¯ä»¥çœ‹åˆ°æ›´å¤šä¸Šä¸‹æ–‡ï¼
```

**æƒè¡¡ï¼š**
```
block_size â†‘ â†’ æ˜¾å­˜ä½¿ç”¨ â†‘â†‘ (å¹³æ–¹å¢é•¿!)
           â†’ è®­ç»ƒæ—¶é—´ â†‘â†‘
           â†’ æ¨¡å‹ç†è§£åŠ› â†‘

è®¡ç®—å¤æ‚åº¦ = O(block_sizeÂ²) å› ä¸ºself-attentionæœºåˆ¶
```

### gradient_accumulation_steps (æ¢¯åº¦ç´¯ç§¯)

**è¿™æ˜¯æœ€éš¾ç†è§£çš„æ¦‚å¿µï¼è®©æˆ‘ç”¨è¯¦ç»†ä¾‹å­è¯´æ˜ï¼š**

**é—®é¢˜èƒŒæ™¯ï¼š**
ä½ çš„æ˜¾å¡åªæœ‰8GBæ˜¾å­˜ï¼Œä½†ä½ æƒ³ç”¨ç›¸å½“äº32GBæ˜¾å­˜çš„batch sizeã€‚

**è§£å†³æ–¹æ¡ˆï¼šåˆ†æœŸä»˜æ¬¾è®­ç»ƒæ³•**

```python
# æ–¹æ³•1ï¼šç›´æ¥ç”¨å¤§batchï¼ˆæ˜¾å­˜çˆ†ç‚¸ï¼ï¼‰
batch_size = 64  # éœ€è¦32GBæ˜¾å­˜ âŒ

# æ–¹æ³•2ï¼šæ¢¯åº¦ç´¯ç§¯ï¼ˆå·§å¦™ï¼ï¼‰
batch_size = 16  # åªéœ€8GBæ˜¾å­˜
gradient_accumulation_steps = 4  # ç´¯ç§¯4æ¬¡
# ç­‰æ•ˆbatch_size = 16 Ã— 4 = 64 âœ…
```

**è¯¦ç»†è¿‡ç¨‹æ¨¡æ‹Ÿï¼š**

```
=== ä¼ ç»Ÿè®­ç»ƒ (batch_size=64) ===
æ­¥éª¤1: è¯»å–64ä¸ªæ ·æœ¬ â†’ å‰å‘ä¼ æ’­ â†’ è®¡ç®—loss â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°
æ˜¾å­˜: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (16GB) âŒ æ˜¾å­˜ä¸å¤Ÿï¼

=== æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ ===
æ­¥éª¤1: è¯»å–16ä¸ªæ ·æœ¬ â†’ å‰å‘ â†’ åå‘ â†’ æ¢¯åº¦æš‚å­˜ (ä¸æ›´æ–°å‚æ•°)
æ˜¾å­˜: â–ˆâ–ˆâ–ˆâ–ˆ (4GB)

æ­¥éª¤2: è¯»å–16ä¸ªæ ·æœ¬ â†’ å‰å‘ â†’ åå‘ â†’ æ¢¯åº¦ç´¯åŠ 
æ˜¾å­˜: â–ˆâ–ˆâ–ˆâ–ˆ (4GB)
ç´¯ç§¯æ¢¯åº¦ = æ¢¯åº¦1 + æ¢¯åº¦2

æ­¥éª¤3: è¯»å–16ä¸ªæ ·æœ¬ â†’ å‰å‘ â†’ åå‘ â†’ æ¢¯åº¦ç´¯åŠ 
æ˜¾å­˜: â–ˆâ–ˆâ–ˆâ–ˆ (4GB)
ç´¯ç§¯æ¢¯åº¦ = æ¢¯åº¦1 + æ¢¯åº¦2 + æ¢¯åº¦3

æ­¥éª¤4: è¯»å–16ä¸ªæ ·æœ¬ â†’ å‰å‘ â†’ åå‘ â†’ æ¢¯åº¦ç´¯åŠ  â†’ æ›´æ–°å‚æ•°ï¼
æ˜¾å­˜: â–ˆâ–ˆâ–ˆâ–ˆ (4GB)
ç´¯ç§¯æ¢¯åº¦ = æ¢¯åº¦1 + æ¢¯åº¦2 + æ¢¯åº¦3 + æ¢¯åº¦4

æ•ˆæœï¼šç­‰åŒäºbatch_size=64ï¼Œä½†æ˜¾å­˜åªç”¨äº†4GBï¼
```

**æ•°å­¦åŸç†ï¼š**
```
å¹³å‡æ¢¯åº¦ = (æ¢¯åº¦1 + æ¢¯åº¦2 + æ¢¯åº¦3 + æ¢¯åº¦4) / 4
ç­‰ä»·äºä¸€æ¬¡æ€§è®¡ç®—64ä¸ªæ ·æœ¬çš„å¹³å‡æ¢¯åº¦
```

---

## 2. æ¨¡å‹æ¶æ„å‚æ•°

### n_layer (å±‚æ•°)

**æ˜¯ä»€ä¹ˆï¼Ÿ**
æ¨¡å‹æœ‰å¤šå°‘å±‚Transformerå—ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼š**
```
1å±‚ï¼šå°å­¦ç”Ÿçš„ç†è§£åŠ›
6å±‚ï¼šé«˜ä¸­ç”Ÿçš„ç†è§£åŠ›ï¼ˆèå£«æ¯”äºšæ¨¡å‹ç”¨è¿™ä¸ªï¼‰
12å±‚ï¼šå¤§å­¦ç”Ÿçš„ç†è§£åŠ›ï¼ˆGPT-2ç”¨è¿™ä¸ªï¼‰
96å±‚ï¼šåšå£«çš„ç†è§£åŠ›ï¼ˆGPT-3ç”¨è¿™ä¸ªï¼‰
```

**å…·ä½“å½±å“ï¼š**
```python
n_layer = 6   # å‚æ•°é‡ï¼š~40Mï¼Œè®­ç»ƒæ—¶é—´ï¼š3åˆ†é’Ÿ
n_layer = 12  # å‚æ•°é‡ï¼š~124Mï¼Œè®­ç»ƒæ—¶é—´ï¼š4å¤©
n_layer = 24  # å‚æ•°é‡ï¼š~350Mï¼Œè®­ç»ƒæ—¶é—´ï¼š2å‘¨
```

### n_head (æ³¨æ„åŠ›å¤´æ•°)

**æ˜¯ä»€ä¹ˆï¼Ÿ**
æ¨¡å‹åŒæ—¶å…³æ³¨å¤šå°‘ä¸ªä¸åŒçš„æ–¹é¢ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼šé˜…è¯»ç†è§£çš„å¤šä¸ªè§’åº¦**
```
n_head = 1ï¼šåªä»ä¸€ä¸ªè§’åº¦çœ‹é—®é¢˜
  "To be or not to be"
  åªå…³æ³¨ï¼šè¯­æ³•ç»“æ„

n_head = 6ï¼šä»6ä¸ªè§’åº¦çœ‹é—®é¢˜
  è§’åº¦1ï¼šè¯­æ³•ç»“æ„
  è§’åº¦2ï¼šè¯­ä¹‰å…³ç³»
  è§’åº¦3ï¼šéŸµå¾‹èŠ‚å¥
  è§’åº¦4ï¼šæƒ…æ„Ÿè‰²å½©
  è§’åº¦5ï¼šä¸Šä¸‹æ–‡è¿è´¯
  è§’åº¦6ï¼šé£æ ¼ç‰¹å¾
```

**å®é™…è®¡ç®—ï¼š**
```python
n_embd = 384  # æ€»ç»´åº¦
n_head = 6    # 6ä¸ªå¤´
æ¯ä¸ªå¤´çš„ç»´åº¦ = 384 / 6 = 64

# æ¯ä¸ªå¤´ç‹¬ç«‹å¤„ç†ä¸€éƒ¨åˆ†ä¿¡æ¯
å¤´1å¤„ç†ï¼šç»´åº¦0-63
å¤´2å¤„ç†ï¼šç»´åº¦64-127
å¤´3å¤„ç†ï¼šç»´åº¦128-191
...
æœ€ååˆå¹¶æ‰€æœ‰å¤´çš„ç»“æœ
```

### n_embd (åµŒå…¥ç»´åº¦)

**æ˜¯ä»€ä¹ˆï¼Ÿ**
ç”¨å¤šå°‘ä¸ªæ•°å­—æ¥è¡¨ç¤ºä¸€ä¸ªè¯ã€‚

**ç±»æ¯”ï¼šæè¿°ä¸€ä¸ªäºº**
```
n_embd = 2ï¼š(èº«é«˜, ä½“é‡)  
  â†’ ä¿¡æ¯å¤ªå°‘ï¼Œæ— æ³•åŒºåˆ†å¾ˆå¤šäºº

n_embd = 384ï¼š(èº«é«˜, ä½“é‡, å¹´é¾„, è‚¤è‰², å‘è‰², ..., 384ä¸ªç‰¹å¾)
  â†’ ä¿¡æ¯ä¸°å¯Œï¼Œå¯ä»¥ç²¾ç¡®æè¿°

n_embd = 768ï¼š(GPT-2ç”¨è¿™ä¸ª)
  â†’ å¯ä»¥æ•æ‰éå¸¸ç»†å¾®çš„å·®å¼‚
```

**å­˜å‚¨ç©ºé—´ï¼š**
```python
vocab_size = 50000  # è¯æ±‡è¡¨å¤§å°
n_embd = 768

# åµŒå…¥å±‚å‚æ•°é‡
embedding_params = vocab_size Ã— n_embd 
                 = 50000 Ã— 768 
                 = 38,400,000 ä¸ªå‚æ•°
                 â‰ˆ 150MB (float32)
```

---

## 3. ä¼˜åŒ–å™¨å‚æ•°

### learning_rate (å­¦ä¹ ç‡)

**æœ€é‡è¦çš„è¶…å‚æ•°ï¼**

**ç”Ÿæ´»æ¯”å–»ï¼šå­¦ä¹ çš„æ­¥ä¼**
```
learning_rate = 1.0ï¼š  
  å­¦ç”Ÿå­¦å¾—å¤ªæ¿€è¿›ï¼Œä¸€ä¼šå„¿å…¨æ‡‚ï¼Œä¸€ä¼šå„¿å…¨å¿˜
  ç»“æœï¼šæ— æ³•æ”¶æ•›

learning_rate = 0.001ï¼š
  å­¦ç”Ÿå­¦å¾—ç¨³å¥ï¼Œæ¯æ¬¡è¿›æ­¥ä¸€ç‚¹ç‚¹
  ç»“æœï¼šâœ… ç¨³å®šå­¦ä¹ 

learning_rate = 0.000001ï¼š
  å­¦ç”Ÿå­¦å¾—å¤ªæ…¢ï¼Œä¸€è¾ˆå­éƒ½å­¦ä¸å®Œ
  ç»“æœï¼šè®­ç»ƒå¤ªæ…¢
```

**å¯è§†åŒ–ï¼š**
```
Loss (æŸå¤±)
  ^
  |  lrå¤ªå¤§ â†’ éœ‡è¡
  |    /\  /\  /\
  |   /  \/  \/  \
  |  
  |  lråˆšå¥½ â†’ å¹³æ»‘ä¸‹é™
  |  \
  |   \___
  |       \___
  |           \___
  |
  |  lrå¤ªå° â†’ ä¸‹é™å¤ªæ…¢
  |  \
  |   \
  |    \
  |     \
  +------------------------> è®­ç»ƒæ­¥æ•°
```

**ç»éªŒå€¼ï¼š**
```python
# ä»é›¶è®­ç»ƒ
learning_rate = 1e-3  # 0.001ï¼Œå°æ¨¡å‹
learning_rate = 6e-4  # 0.0006ï¼ŒGPT-2

# å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
learning_rate = 1e-4  # 0.0001ï¼Œé¿å…ç ´åå·²å­¦çŸ¥è¯†
learning_rate = 5e-5  # 0.00005ï¼Œæ›´ä¿å®ˆ
```

### weight_decay (æƒé‡è¡°å‡)

**æ˜¯ä»€ä¹ˆï¼Ÿ**
é˜²æ­¢æ¨¡å‹"æ­»è®°ç¡¬èƒŒ"çš„æœºåˆ¶ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼š**
```
æ²¡æœ‰weight_decayï¼š
  å­¦ç”ŸæŠŠæ¯ä¸ªä¾‹å­éƒ½æ­»è®°ç¡¬èƒŒ
  "To be" â†’ "or not to be"
  "To go" â†’ ï¼Ÿï¼ˆä¸çŸ¥é“ï¼Œæ²¡èƒŒè¿‡ï¼‰
  ç»“æœï¼šè¿‡æ‹Ÿåˆ

æœ‰weight_decayï¼š
  å­¦ç”Ÿç†è§£äº†è§„å¾‹ï¼Œä¸æ˜¯æ­»è®°
  å­¦åˆ°ï¼š"To" åé¢é€šå¸¸è·ŸåŠ¨è¯åŸå½¢
  "To be" â†’ "or not to be"
  "To go" â†’ "or not to go" âœ…
  ç»“æœï¼šæ³›åŒ–èƒ½åŠ›å¼º
```

**æ•°å­¦åŸç†ï¼š**
```python
# æ¯æ¬¡æ›´æ–°å‚æ•°æ—¶
æ–°å‚æ•° = æ—§å‚æ•° - learning_rate Ã— æ¢¯åº¦ - weight_decay Ã— æ—§å‚æ•°
                                        â†‘
                                è¿™ä¸€é¡¹è®©å‚æ•°ä¸è¦å¤ªå¤§
```

### grad_clip (æ¢¯åº¦è£å‰ª)

**æ˜¯ä»€ä¹ˆï¼Ÿ**
é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼š**
```
æ²¡æœ‰grad_clipï¼š
  è€å¸ˆçªç„¶å¤§å–Šï¼š"ä½ å…¨é”™äº†ï¼ï¼ï¼ä»å¤´å­¦ï¼ï¼ï¼"
  å­¦ç”Ÿï¼šğŸ˜± å“å‚»äº†ï¼Œå¿˜è®°ä¹‹å‰å­¦çš„
  ç»“æœï¼šæ¢¯åº¦çˆ†ç‚¸ï¼Œè®­ç»ƒå´©æºƒ

æœ‰grad_clipï¼š
  è€å¸ˆæ¸©å’Œè¯´ï¼š"ä½ æœ‰äº›é”™è¯¯ï¼Œæ…¢æ…¢æ”¹è¿›"
  å­¦ç”Ÿï¼šğŸ˜Š ç¨³æ­¥æ”¹è¿›
  ç»“æœï¼šè®­ç»ƒç¨³å®š
```

**å…·ä½“å®ç°ï¼š**
```python
grad_clip = 1.0

# å‡è®¾è®¡ç®—å‡ºçš„æ¢¯åº¦å¾ˆå¤§
gradient = [100, 200, 300]  # æ¢¯åº¦çˆ†ç‚¸äº†ï¼
gradient_norm = sqrt(100Â² + 200Â² + 300Â²) = 374

# è£å‰ª
if gradient_norm > grad_clip:
    gradient = gradient * (grad_clip / gradient_norm)
    gradient = [0.27, 0.53, 0.80]  # è¢«ç¼©å°äº†
```

---

## 4. å­¦ä¹ ç‡è°ƒåº¦

### warmup_iters (é¢„çƒ­æ­¥æ•°)

**ä¸ºä»€ä¹ˆéœ€è¦warmupï¼Ÿ**

**æ²¡æœ‰warmupçš„é—®é¢˜ï¼š**
```
è®­ç»ƒå¼€å§‹ï¼š
  æ­¥éª¤1: å‚æ•°éšæœºåˆå§‹åŒ–
  æ­¥éª¤2: ç”¨å¤§learning_rateæ›´æ–°
  æ­¥éª¤3: å‚æ•°å‰§çƒˆå˜åŒ– ğŸ’¥
  æ­¥éª¤4: æ¨¡å‹å´©æºƒ
```

**æœ‰warmupï¼š**
```
è®­ç»ƒå¼€å§‹ï¼š
  æ­¥éª¤1-100: learning_rateä»0æ…¢æ…¢å¢åŠ åˆ°0.001
    æ­¥éª¤1:  lr = 0.00001  (å°å¿ƒç¿¼ç¿¼)
    æ­¥éª¤50: lr = 0.0005   (é€æ¸åŠ é€Ÿ)
    æ­¥éª¤100: lr = 0.001   (è¾¾åˆ°æ­£å¸¸é€Ÿåº¦)
  
  æ­¥éª¤100+: å¼€å§‹æ­£å¸¸è®­ç»ƒ
```

**æ›²çº¿å›¾ï¼š**
```
Learning Rate
  ^
  |          /â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\
  |         /           \
  |        /             \
  |       /               \___
  |      /                    \___
  |     /                         \___
  |    /                              \___
  +-------|---------|--------------|---------> Steps
        warmup    æ­£å¸¸è®­ç»ƒ        decay
       (2000)                   (600000)
```

---

## 5. å®æˆ˜è®¡ç®—ç¤ºä¾‹

è®©æˆ‘ä»¬ç”¨èå£«æ¯”äºšé…ç½®è®¡ç®—ä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒè¿­ä»£ï¼š

```python
# é…ç½®ï¼ˆæ¥è‡ª train_shakespeare_char.pyï¼‰
batch_size = 64
block_size = 256
gradient_accumulation_steps = 1
vocab_size = 65  # èå£«æ¯”äºšæ•°æ®é›†æœ‰65ä¸ªä¸åŒå­—ç¬¦

# ä¸€æ¬¡è¿­ä»£å¤„ç†çš„tokenæ•°é‡
tokens_per_iter = batch_size Ã— block_size Ã— gradient_accumulation_steps
                = 64 Ã— 256 Ã— 1
                = 16,384 tokens

# æ˜¾å­˜ä½¿ç”¨ä¼°ç®—ï¼ˆç®€åŒ–ï¼‰
# å‡è®¾æ¯ä¸ªå‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ â‰ˆ 12 bytes
model_params = 10_000_000  # 10Må‚æ•°çš„å°æ¨¡å‹
memory_model = 10_000_000 Ã— 12 = 120MB

# æ¿€æ´»å€¼ï¼ˆå‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœï¼‰
memory_activation = batch_size Ã— block_size Ã— n_embd Ã— n_layer Ã— 4
                  = 64 Ã— 256 Ã— 384 Ã— 6 Ã— 4 bytes
                  = 150MB

# æ€»æ˜¾å­˜ â‰ˆ 270MB (å®é™…ä¼šæ›´å¤š)
```

---

## 6. é…ç½®è°ƒä¼˜ç­–ç•¥

### ğŸš¦ æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼š**

```python
# ç­–ç•¥1: å‡å°batch_sizeï¼ˆæœ€æœ‰æ•ˆï¼‰
batch_size = 32  # å‡åŠ â†’ æ˜¾å­˜å‡åŠ

# ç­–ç•¥2: å‡å°block_size
block_size = 128  # å‡åŠ â†’ æ˜¾å­˜å‡å°‘60%

# ç­–ç•¥3: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps = 2  # æ˜¾å­˜å‡åŠï¼Œæ•ˆæœä¸å˜

# ç­–ç•¥4: å‡å°æ¨¡å‹
n_layer = 4
n_embd = 256

# ç­–ç•¥5: ä½¿ç”¨æ··åˆç²¾åº¦
dtype = 'float16'  # æ˜¾å­˜å‡åŠ
```

### âš¡ è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

```python
# ç­–ç•¥1: å¯ç”¨ç¼–è¯‘ï¼ˆæœ€æœ‰æ•ˆï¼‰
compile = True  # æé€Ÿ1.5-2x

# ç­–ç•¥2: å¢å¤§batch_size
batch_size = 128  # GPUåˆ©ç”¨ç‡æ›´é«˜

# ç­–ç•¥3: å‡å°eval_interval
eval_interval = 500  # å°‘åšè¯„ä¼°ï¼Œä¸“å¿ƒè®­ç»ƒ

# ç­–ç•¥4: å…³é—­ä¸€äº›æ—¥å¿—
log_interval = 100  # å‡å°‘æ‰“å°
```

### ğŸ¯ è¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ

```python
# ç­–ç•¥1: å¢åŠ dropout
dropout = 0.2  # éšæœºå…³é—­20%çš„ç¥ç»å…ƒ

# ç­–ç•¥2: å¢åŠ weight_decay
weight_decay = 0.1  # æ›´å¼ºçš„æ­£åˆ™åŒ–

# ç­–ç•¥3: å‡å°æ¨¡å‹
n_layer = 4  # æ›´å°çš„æ¨¡å‹ï¼Œæ³›åŒ–æ›´å¥½

# ç­–ç•¥4: æ›´å¤šæ•°æ®
# è·å–æ›´å¤šè®­ç»ƒæ•°æ®ï¼
```

---

## ğŸ§ª å®éªŒå»ºè®®

åˆ›å»ºä¸€ä¸ªå®éªŒè„šæœ¬æ¥ç†è§£æ¯ä¸ªå‚æ•°çš„å½±å“ï¼š

```bash
# å®éªŒ1ï¼šbatch_sizeçš„å½±å“
python train.py config/train_shakespeare_char.py --batch_size=16 --max_iters=1000
python train.py config/train_shakespeare_char.py --batch_size=32 --max_iters=1000
python train.py config/train_shakespeare_char.py --batch_size=64 --max_iters=1000
# è§‚å¯Ÿï¼šè®­ç»ƒé€Ÿåº¦å’Œæœ€ç»ˆloss

# å®éªŒ2ï¼šlearning_rateçš„å½±å“
python train.py config/train_shakespeare_char.py --learning_rate=1e-4 --max_iters=1000
python train.py config/train_shakespeare_char.py --learning_rate=1e-3 --max_iters=1000
python train.py config/train_shakespeare_char.py --learning_rate=1e-2 --max_iters=1000
# è§‚å¯Ÿï¼šæ˜¯å¦æ”¶æ•›

# å®éªŒ3ï¼šæ¨¡å‹å¤§å°çš„å½±å“
python train.py config/train_shakespeare_char.py --n_layer=2 --n_embd=128 --max_iters=2000
python train.py config/train_shakespeare_char.py --n_layer=4 --n_embd=256 --max_iters=2000
python train.py config/train_shakespeare_char.py --n_layer=6 --n_embd=384 --max_iters=2000
# è§‚å¯Ÿï¼šæ¨¡å‹èƒ½åŠ›å’Œè®­ç»ƒæ—¶é—´
```

---

## ğŸ“Š æ€»ç»“è¡¨æ ¼

| å‚æ•° | å¢å¤§åæœ | å‡å°åæœ | åˆå­¦è€…æ¨èå€¼ |
|------|---------|---------|-------------|
| batch_size | æ˜¾å­˜â†‘, é€Ÿåº¦â†‘, å™ªå£°â†“ | æ˜¾å­˜â†“, é€Ÿåº¦â†“, å™ªå£°â†‘ | 32-64 |
| block_size | æ˜¾å­˜â†‘â†‘, ç†è§£åŠ›â†‘ | æ˜¾å­˜â†“â†“, ç†è§£åŠ›â†“ | 128-256 |
| learning_rate | ä¸æ”¶æ•› | è®­ç»ƒæ…¢ | 1e-3 (å°æ¨¡å‹) |
| n_layer | èƒ½åŠ›â†‘, æ—¶é—´â†‘â†‘ | èƒ½åŠ›â†“, æ—¶é—´â†“ | 4-6 |
| n_embd | èƒ½åŠ›â†‘, æ˜¾å­˜â†‘ | èƒ½åŠ›â†“, æ˜¾å­˜â†“ | 256-384 |
| dropout | æ³›åŒ–â†‘, æ‹Ÿåˆâ†“ | æ³›åŒ–â†“, æ‹Ÿåˆâ†‘ | 0.1-0.2 |

