# ç¬¬10ç« ï¼šç”Ÿäº§çº§éƒ¨ç½²å®æˆ˜æŒ‡å—

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´å·¥ç¨‹æµç¨‹  
> **éš¾åº¦ç­‰çº§**: ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ é«˜çº§ï¼ˆå·¥ç¨‹å®æˆ˜ï¼‰  
> **é¢„è®¡æ—¶é—´**: 6-8å°æ—¶  
> **å‰ç½®çŸ¥è¯†**: 01-09ç« å…¨éƒ¨å†…å®¹

## ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… æŒæ¡ç«¯åˆ°ç«¯çš„éƒ¨ç½²æµç¨‹
- âœ… èƒ½å¤Ÿæ„å»ºé«˜æ€§èƒ½APIæœåŠ¡
- âœ… ç†è§£Dockerå’ŒKuberneteséƒ¨ç½²
- âœ… æŒæ¡ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
- âœ… èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½ä¼˜åŒ–å’Œæ•…éšœæ’æŸ¥
- âœ… ç†è§£ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µ

## ğŸ’­ å¼€å§‹ä¹‹å‰ï¼šä¸ºä»€ä¹ˆè¦å­¦è¿™ä¸ªï¼Ÿ

**åœºæ™¯**ï¼šè®­ç»ƒå‡ºæ¨¡å‹åªæ˜¯ç¬¬ä¸€æ­¥ï¼Œéƒ¨ç½²åˆ°ç”Ÿäº§æ‰æ˜¯çœŸæ­£çš„æŒ‘æˆ˜ã€‚

**æ¯”å–»**ï¼šå°±åƒå¼€é¤å…ï¼š
- ğŸ³ åšå‡ºå¥½èœï¼šè®­ç»ƒå¥½æ¨¡å‹
- ğŸª é«˜æ•ˆæœåŠ¡ï¼šéƒ¨ç½²å’Œä¼˜åŒ–
- ğŸ’° èµšé’±ï¼šçœŸæ­£åˆ›é€ ä»·å€¼

**å­¦å®Œä¹‹å**ï¼š
- âœ… èƒ½ç‹¬ç«‹å®Œæˆæ¨¡å‹éƒ¨ç½²
- âœ… ç†è§£ç”Ÿäº§ç¯å¢ƒçš„æŒ‘æˆ˜
- âœ… ä¼šä¼˜åŒ–æœåŠ¡æ€§èƒ½
- âœ… èƒ½å¤„ç†å®é™…é—®é¢˜

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ª**ç”Ÿäº§çº§çš„ä»£ç è¡¥å…¨åŠ©æ‰‹**ï¼Œä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚

```
é¡¹ç›®æµç¨‹ï¼š

æ•°æ®å‡†å¤‡ â†’ æ¨¡å‹è®­ç»ƒ â†’ åˆ†å¸ƒå¼åŠ é€Ÿ â†’ æ¨¡å‹ä¼˜åŒ– â†’ APIæœåŠ¡ â†’ å®¹å™¨åŒ– â†’ ç›‘æ§è¿ç»´

é¢„æœŸæˆæœï¼š
  âœ… è®­ç»ƒä¸€ä¸ªä»£ç è¡¥å…¨æ¨¡å‹
  âœ… éƒ¨ç½²ä¸ºé«˜æ€§èƒ½APIæœåŠ¡
  âœ… æ”¯æŒ100+å¹¶å‘ç”¨æˆ·
  âœ… å»¶è¿Ÿ < 200ms
  âœ… æˆæœ¬ < $0.01/1K tokens
```

---

## ğŸ“š é˜¶æ®µ1ï¼šæ•°æ®å‡†å¤‡

### ğŸ”§ æ”¶é›†Pythonä»£ç æ•°æ®

```python
# collect_code.py
import os
import tiktoken
import numpy as np

def collect_python_files(root_dir):
    """æ”¶é›†æ‰€æœ‰Pythonæ–‡ä»¶"""
    code_files = []
    for root, dirs, files in os.walk(root_dir):
        # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜
        dirs[:] = [d for d in dirs if d not in ['.venv', '__pycache__', 'node_modules']]
        
        for file in files:
            if file.endswith('.py'):
                path = os.path.join(root, file)
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        code = f.read()
                        if len(code) > 100:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡ä»¶
                            code_files.append(code)
                except:
                    continue
    
    return code_files

def prepare_dataset(code_files, output_dir='data/python_code'):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. åˆå¹¶æ‰€æœ‰ä»£ç 
    data = '\n\n# ================\n\n'.join(code_files)
    print(f"æ€»å­—ç¬¦æ•°: {len(data):,}")
    
    # 2. åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
    n = len(data)
    train_data = data[:int(n*0.9)]
    val_data = data[int(n*0.9):]
    
    # 3. Tokenize
    enc = tiktoken.get_encoding("gpt2")
    train_ids = enc.encode_ordinary(train_data)
    val_ids = enc.encode_ordinary(val_data)
    
    print(f"è®­ç»ƒtokens: {len(train_ids):,}")
    print(f"éªŒè¯tokens: {len(val_ids):,}")
    
    # 4. ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
    np.array(train_ids, dtype=np.uint16).tofile(f'{output_dir}/train.bin')
    np.array(val_ids, dtype=np.uint16).tofile(f'{output_dir}/val.bin')
    
    print(f"æ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")

# è¿è¡Œ
if __name__ == '__main__':
    # æ”¶é›†ä»£ç ï¼ˆæ›¿æ¢ä¸ºä½ çš„é¡¹ç›®è·¯å¾„ï¼‰
    code_files = collect_python_files('/path/to/your/python/projects')
    print(f"æ”¶é›†äº† {len(code_files)} ä¸ªPythonæ–‡ä»¶")
    
    # å‡†å¤‡æ•°æ®é›†
    prepare_dataset(code_files)
```

**è¿è¡Œï¼š**
```bash
python collect_code.py

# è¾“å‡ºï¼š
# æ”¶é›†äº† 1,234 ä¸ªPythonæ–‡ä»¶
# æ€»å­—ç¬¦æ•°: 5,678,901
# è®­ç»ƒtokens: 1,234,567
# éªŒè¯tokens: 137,174
# æ•°æ®å·²ä¿å­˜åˆ° data/python_code/
```

---

## ğŸ“š é˜¶æ®µ2ï¼šæ¨¡å‹è®­ç»ƒ

### ğŸ”§ åˆ›å»ºè®­ç»ƒé…ç½®

```python
# config/train_code_assistant.py

import time

# è¾“å‡ºç›®å½•
out_dir = 'out-code-assistant'
eval_interval = 500
eval_iters = 100
log_interval = 10

# æ•°æ®é›†
dataset = 'python_code'
gradient_accumulation_steps = 4
batch_size = 8
block_size = 512  # ä»£ç éœ€è¦é•¿ä¸Šä¸‹æ–‡

# æ¨¡å‹ï¼ˆä¸­ç­‰å¤§å°ï¼‰
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.1

# å¾®è°ƒè®¾ç½®
init_from = 'gpt2'  # ä»GPT-2å¼€å§‹
learning_rate = 5e-5  # å°å­¦ä¹ ç‡
max_iters = 5000
lr_decay_iters = 5000
min_lr = 5e-6

# ä¼˜åŒ–
weight_decay = 0.1
grad_clip = 1.0
decay_lr = True
warmup_iters = 100

# ç³»ç»Ÿ
device = 'cuda'
dtype = 'float16'
compile = True
```

### ğŸš€ å¼€å§‹è®­ç»ƒ

```bash
# å•GPUè®­ç»ƒ
python train.py config/train_code_assistant.py

# é¢„æœŸè¾“å‡ºï¼š
# iter 0: loss 3.2145, time 1234.56ms
# iter 500: train loss 2.1234, val loss 2.3456
# iter 1000: train loss 1.8765, val loss 2.1234
# ...
# iter 5000: train loss 1.2345, val loss 1.5678
# è®­ç»ƒå®Œæˆï¼æœ€ä½³val loss: 1.5234
```

---

## ğŸ“š é˜¶æ®µ3ï¼šåˆ†å¸ƒå¼åŠ é€Ÿ

### ğŸ”§ å¤šGPUè®­ç»ƒé…ç½®

```python
# config/train_code_assistant_ddp.py

# ç»§æ‰¿å•GPUé…ç½®
exec(open('config/train_code_assistant.py').read())

# DDPä¼˜åŒ–
batch_size = 16  # æ¯ä¸ªGPUæ›´å¤§batch
gradient_accumulation_steps = 2  # å‡å°‘ç´¯ç§¯æ­¥æ•°

# æ€»batch_size = 16 Ã— 2 Ã— 4 = 128 (å’Œå•GPUçš„8Ã—4Ã—4ä¸€æ ·)
```

### ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# 4 GPUè®­ç»ƒ
torchrun --standalone --nproc_per_node=4 \
  train.py config/train_code_assistant_ddp.py

# åŠ é€Ÿæ•ˆæœï¼š
# å•GPU: 2-3å°æ—¶
# 4 GPU: 30-40åˆ†é’Ÿ (çº¦4xåŠ é€Ÿ)
```

---

## ğŸ“š é˜¶æ®µ4ï¼šæ¨¡å‹ä¼˜åŒ–

### ğŸ”§ é‡åŒ–æ¨¡å‹

```python
# quantize_model.py

import torch
from model import GPT, GPTConfig

def quantize_model(checkpoint_path, output_path):
    """é‡åŒ–æ¨¡å‹åˆ°INT8"""
    # 1. åŠ è½½æ¨¡å‹
    checkpoint = torch.load(checkpoint_path)
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    
    print(f"åŸå§‹æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")
    
    # 2. åŠ¨æ€é‡åŒ–
    model_quantized = torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8
    )
    
    # 3. ä¿å­˜
    torch.save({
        'model': model_quantized.state_dict(),
        'model_args': checkpoint['model_args'],
        'quantized': True,
    }, output_path)
    
    # 4. å¯¹æ¯”
    import os
    orig_size = os.path.getsize(checkpoint_path) / 1e6
    quant_size = os.path.getsize(output_path) / 1e6
    
    print(f"\né‡åŒ–ç»“æœï¼š")
    print(f"  åŸå§‹å¤§å°: {orig_size:.2f} MB")
    print(f"  é‡åŒ–å¤§å°: {quant_size:.2f} MB")
    print(f"  å‹ç¼©æ¯”: {orig_size/quant_size:.2f}x")

if __name__ == '__main__':
    quantize_model(
        'out-code-assistant/ckpt.pt',
        'out-code-assistant/ckpt_int8.pt'
    )
```

**è¿è¡Œï¼š**
```bash
python quantize_model.py

# è¾“å‡ºï¼š
# åŸå§‹æ¨¡å‹å‚æ•°: 124.44M
# 
# é‡åŒ–ç»“æœï¼š
#   åŸå§‹å¤§å°: 497.35 MB
#   é‡åŒ–å¤§å°: 124.67 MB
#   å‹ç¼©æ¯”: 3.99x
```

---

## ğŸ“š é˜¶æ®µ5ï¼šAPIæœåŠ¡

### ğŸ”§ åˆ›å»ºFastAPIæœåŠ¡

```python
# serve_api.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from model import GPT, GPTConfig
import tiktoken
from contextlib import asynccontextmanager
from typing import Optional
import time

# å…¨å±€å˜é‡
model = None
tokenizer = None
device = 'cuda' if torch.cuda.is_available() else 'cpu'

@asynccontextmanager
async def lifespan(app: FastAPI):
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model, tokenizer
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load('out-code-assistant/ckpt_int8.pt', map_location=device)
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    model.to(device)
    
    tokenizer = tiktoken.get_encoding("gpt2")
    print(f"æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    
    yield
    
    # æ¸…ç†
    del model
    del tokenizer

app = FastAPI(lifespan=lifespan)

class CompletionRequest(BaseModel):
    code: str
    max_tokens: int = 50
    temperature: float = 0.8
    top_k: Optional[int] = 200

class CompletionResponse(BaseModel):
    completion: str
    tokens: int
    latency_ms: float

@app.post("/complete", response_model=CompletionResponse)
async def complete_code(request: CompletionRequest):
    """ä»£ç è¡¥å…¨API"""
    try:
        start_time = time.time()
        
        # 1. Tokenizeè¾“å…¥
        input_ids = tokenizer.encode(request.code)
        if len(input_ids) > 512:
            input_ids = input_ids[-512:]  # æˆªæ–­
        
        x = torch.tensor([input_ids], dtype=torch.long, device=device)
        
        # 2. ç”Ÿæˆ
        with torch.no_grad():
            y = model.generate(
                x,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_k=request.top_k
            )
        
        # 3. Decode
        output_ids = y[0].tolist()
        completion = tokenizer.decode(output_ids[len(input_ids):])
        
        latency = (time.time() - start_time) * 1000
        
        return CompletionResponse(
            completion=completion,
            tokens=len(output_ids) - len(input_ids),
            latency_ms=latency
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": device
    }

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### ğŸš€ å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨APIæœåŠ¡
python serve_api.py

# è¾“å‡ºï¼š
# æ­£åœ¨åŠ è½½æ¨¡å‹...
# æ¨¡å‹å·²åŠ è½½åˆ° cuda
# INFO:     Started server process [12345]
# INFO:     Waiting for application startup.
# INFO:     Application startup complete.
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

### ğŸ§ª æµ‹è¯•API

```bash
# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# è¾“å‡ºï¼š
# {"status":"healthy","model_loaded":true,"device":"cuda"}

# æµ‹è¯•ä»£ç è¡¥å…¨
curl -X POST http://localhost:8000/complete \
  -H "Content-Type: application/json" \
  -d '{
    "code": "def fibonacci(n):\n    if n <= 1:\n        return n\n    ",
    "max_tokens": 50,
    "temperature": 0.8
  }'

# è¾“å‡ºï¼š
# {
#   "completion": "return fibonacci(n-1) + fibonacci(n-2)\n\n# Test\nprint(fibonacci(10))",
#   "tokens": 23,
#   "latency_ms": 156.78
# }
```

---

## ğŸ“š é˜¶æ®µ6ï¼šå®¹å™¨åŒ–éƒ¨ç½²

### ğŸ³ åˆ›å»ºDockerfile

```dockerfile
# Dockerfile

FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# å®‰è£…Python
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç å’Œæ¨¡å‹
COPY model.py .
COPY serve_api.py .
COPY out-code-assistant/ckpt_int8.pt ./out-code-assistant/

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨æœåŠ¡
CMD ["python3", "serve_api.py"]
```

### ğŸ“¦ requirements.txt

```txt
torch==2.0.1
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
tiktoken==0.5.1
numpy==1.24.3
```

### ğŸš€ æ„å»ºå’Œè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t code-assistant:v1 .

# è¿è¡Œå®¹å™¨
docker run -d \
  --gpus all \
  -p 8000:8000 \
  --name code-assistant \
  code-assistant:v1

# æŸ¥çœ‹æ—¥å¿—
docker logs -f code-assistant

# æµ‹è¯•
curl http://localhost:8000/health
```

---

## ğŸ“š é˜¶æ®µ7ï¼šKuberneteséƒ¨ç½²

### â˜¸ï¸ éƒ¨ç½²é…ç½®

```yaml
# k8s/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: code-assistant
spec:
  replicas: 3  # 3ä¸ªå‰¯æœ¬
  selector:
    matchLabels:
      app: code-assistant
  template:
    metadata:
      labels:
        app: code-assistant
    spec:
      containers:
      - name: code-assistant
        image: code-assistant:v1
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1  # æ¯ä¸ªpodä¸€ä¸ªGPU
            memory: "8Gi"
            cpu: "4"
          requests:
            nvidia.com/gpu: 1
            memory: "4Gi"
            cpu: "2"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: code-assistant-service
spec:
  selector:
    app: code-assistant
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: code-assistant-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: code-assistant
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### ğŸš€ éƒ¨ç½²åˆ°K8s

```bash
# éƒ¨ç½²
kubectl apply -f k8s/deployment.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods
kubectl get services

# æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/code-assistant

# æµ‹è¯•æœåŠ¡
kubectl port-forward service/code-assistant-service 8000:80
curl http://localhost:8000/health
```

---

## ğŸ“š é˜¶æ®µ8ï¼šç›‘æ§ä¸è¿ç»´

### ğŸ“Š æ·»åŠ Prometheusç›‘æ§

```python
# serve_api_with_metrics.py

from prometheus_client import Counter, Histogram, Gauge, make_asgi_app
from fastapi import FastAPI
import time

# å®šä¹‰æŒ‡æ ‡
request_count = Counter(
    'code_completion_requests_total',
    'Total code completion requests',
    ['status']
)

request_duration = Histogram(
    'code_completion_duration_seconds',
    'Code completion request duration',
    buckets=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
)

tokens_generated = Counter(
    'tokens_generated_total',
    'Total tokens generated'
)

active_requests = Gauge(
    'active_requests',
    'Number of active requests'
)

app = FastAPI()

# æ·»åŠ Prometheus metrics endpoint
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

@app.post("/complete")
async def complete_code(request: CompletionRequest):
    active_requests.inc()
    start_time = time.time()
    
    try:
        # ... ç”Ÿæˆä»£ç  ...
        
        # è®°å½•æŒ‡æ ‡
        duration = time.time() - start_time
        request_duration.observe(duration)
        request_count.labels(status='success').inc()
        tokens_generated.inc(len(output_ids) - len(input_ids))
        
        return response
    
    except Exception as e:
        request_count.labels(status='error').inc()
        raise
    
    finally:
        active_requests.dec()
```

### ğŸ“ˆ Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "Code Assistant Metrics",
    "panels": [
      {
        "title": "Requests per Second",
        "targets": [{
          "expr": "rate(code_completion_requests_total[1m])"
        }]
      },
      {
        "title": "P95 Latency",
        "targets": [{
          "expr": "histogram_quantile(0.95, code_completion_duration_seconds)"
        }]
      },
      {
        "title": "Tokens per Second",
        "targets": [{
          "expr": "rate(tokens_generated_total[1m])"
        }]
      },
      {
        "title": "Active Requests",
        "targets": [{
          "expr": "active_requests"
        }]
      }
    ]
  }
}
```

---

## ğŸ“š é˜¶æ®µ9ï¼šæ€§èƒ½ä¼˜åŒ–

### ğŸ¯ ä¼˜åŒ–æ¸…å•

```python
ä¼˜åŒ–é¡¹ç›®ï¼š

1. æ¨¡å‹ä¼˜åŒ–
â”œâ”€â”€ âœ… INT8é‡åŒ– (4xå‹ç¼©)
â”œâ”€â”€ â¬œ INT4é‡åŒ– (8xå‹ç¼©, éœ€è¦GPTQ)
â”œâ”€â”€ â¬œ æ¨¡å‹å‰ªæ (å‡å°‘å‚æ•°)
â””â”€â”€ â¬œ çŸ¥è¯†è’¸é¦ (è®­ç»ƒå°æ¨¡å‹)

2. æ¨ç†ä¼˜åŒ–
â”œâ”€â”€ âœ… KV Cache (å·²åœ¨model.pyä¸­)
â”œâ”€â”€ â¬œ vLLM (20xåŠ é€Ÿ)
â”œâ”€â”€ â¬œ TensorRT (æè‡´æ€§èƒ½)
â””â”€â”€ â¬œ Continuous Batching

3. æœåŠ¡ä¼˜åŒ–
â”œâ”€â”€ âœ… å¼‚æ­¥API (FastAPI)
â”œâ”€â”€ â¬œ è¯·æ±‚ç¼“å­˜ (Redis)
â”œâ”€â”€ â¬œ è´Ÿè½½å‡è¡¡ (Nginx)
â””â”€â”€ â¬œ CDNåŠ é€Ÿ

4. åŸºç¡€è®¾æ–½ä¼˜åŒ–
â”œâ”€â”€ âœ… Dockerå®¹å™¨åŒ–
â”œâ”€â”€ âœ… K8sç¼–æ’
â”œâ”€â”€ âœ… è‡ªåŠ¨æ‰©ç¼©å®¹ (HPA)
â””â”€â”€ â¬œ Spotå®ä¾‹ (é™ä½æˆæœ¬70%)
```

### ğŸ’° æˆæœ¬åˆ†æ

```python
# æˆæœ¬è®¡ç®—
def calculate_monthly_cost():
    # é…ç½®
    gpu_type = "A10G"  # $1.00/hour
    num_gpus = 3  # 3ä¸ªå‰¯æœ¬
    hours_per_month = 730
    
    # GPUæˆæœ¬
    gpu_cost = 1.00 * num_gpus * hours_per_month
    
    # é¢„ä¼°è¯·æ±‚é‡
    requests_per_second = 10
    total_requests = requests_per_second * 3600 * hours_per_month
    
    # æ¯åƒæ¬¡è¯·æ±‚æˆæœ¬
    cost_per_1k = (gpu_cost / total_requests) * 1000
    
    print(f"""
æˆæœ¬åˆ†æï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GPUé…ç½®: {num_gpus}x {gpu_type}
æœˆåº¦æˆæœ¬: ${gpu_cost:,.2f}
æœˆåº¦è¯·æ±‚: {total_requests:,.0f}
æ¯1Kè¯·æ±‚æˆæœ¬: ${cost_per_1k:.4f}

ä¼˜åŒ–å»ºè®®:
1. ä½¿ç”¨Spotå®ä¾‹ â†’ èŠ‚çœ70% â†’ ${gpu_cost*0.3:,.2f}/æœˆ
2. ä½¿ç”¨INT4é‡åŒ– â†’ GPUå‡åŠ â†’ ${gpu_cost*0.5:,.2f}/æœˆ
3. ä½¿ç”¨vLLM â†’ ååé‡5x â†’ GPUå‡å°‘80% â†’ ${gpu_cost*0.2:,.2f}/æœˆ

ç»¼åˆä¼˜åŒ–å: ${gpu_cost*0.3*0.5*0.2:,.2f}/æœˆ (èŠ‚çœ97%!)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """)

calculate_monthly_cost()
```

---

## ğŸ¯ æ€»ç»“ï¼šå®Œæ•´éƒ¨ç½²æµç¨‹

```python
ç«¯åˆ°ç«¯æµç¨‹å›é¡¾ï¼š

âœ… é˜¶æ®µ1: æ•°æ®å‡†å¤‡
  â””â”€â”€ æ”¶é›†Pythonä»£ç ï¼Œå‡†å¤‡è®­ç»ƒæ•°æ®

âœ… é˜¶æ®µ2: æ¨¡å‹è®­ç»ƒ
  â””â”€â”€ å•GPUè®­ç»ƒï¼Œ2-3å°æ—¶

âœ… é˜¶æ®µ3: åˆ†å¸ƒå¼åŠ é€Ÿ
  â””â”€â”€ 4 GPUè®­ç»ƒï¼Œ30-40åˆ†é’Ÿ (4xåŠ é€Ÿ)

âœ… é˜¶æ®µ4: æ¨¡å‹ä¼˜åŒ–
  â””â”€â”€ INT8é‡åŒ–ï¼Œ4xå‹ç¼©

âœ… é˜¶æ®µ5: APIæœåŠ¡
  â””â”€â”€ FastAPIæœåŠ¡ï¼Œå»¶è¿Ÿ<200ms

âœ… é˜¶æ®µ6: å®¹å™¨åŒ–
  â””â”€â”€ Dockeré•œåƒï¼Œä¾¿äºéƒ¨ç½²

âœ… é˜¶æ®µ7: K8séƒ¨ç½²
  â””â”€â”€ 3å‰¯æœ¬ï¼Œè‡ªåŠ¨æ‰©ç¼©å®¹

âœ… é˜¶æ®µ8: ç›‘æ§è¿ç»´
  â””â”€â”€ Prometheus + Grafana

âœ… é˜¶æ®µ9: æ€§èƒ½ä¼˜åŒ–
  â””â”€â”€ æˆæœ¬ä¼˜åŒ–ï¼ŒèŠ‚çœ97%

æœ€ç»ˆæˆæœï¼š
  âœ… ç”Ÿäº§çº§ä»£ç è¡¥å…¨æœåŠ¡
  âœ… æ”¯æŒ100+å¹¶å‘ç”¨æˆ·
  âœ… å»¶è¿Ÿ < 200ms
  âœ… æˆæœ¬ < $0.01/1K tokens
  âœ… 99.9% å¯ç”¨æ€§
```

---

---

## ğŸ“ æ€»ç»“ä¸æ£€æŸ¥

### âœ… çŸ¥è¯†æ£€æŸ¥æ¸…å•

å®Œæˆå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€æ¦‚å¿µï¼ˆå¿…é¡»æŒæ¡ï¼‰**
- [ ] ç†è§£ç«¯åˆ°ç«¯éƒ¨ç½²çš„å®Œæ•´æµç¨‹
- [ ] çŸ¥é“å¦‚ä½•å‡†å¤‡å’Œå¤„ç†æ•°æ®
- [ ] èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªåŸºç¡€æ¨¡å‹
- [ ] ç†è§£å¦‚ä½•æ„å»ºAPIæœåŠ¡
- [ ] çŸ¥é“Dockerå®¹å™¨åŒ–çš„åŸºæœ¬æ­¥éª¤
- [ ] èƒ½å¤Ÿéƒ¨ç½²ä¸€ä¸ªç®€å•çš„æœåŠ¡

**è¿›é˜¶ç†è§£ï¼ˆå»ºè®®æŒæ¡ï¼‰**
- [ ] ç†è§£åˆ†å¸ƒå¼è®­ç»ƒçš„é…ç½®
- [ ] çŸ¥é“å¦‚ä½•é‡åŒ–å’Œä¼˜åŒ–æ¨¡å‹
- [ ] ç†è§£Kubernetesçš„åŸºæœ¬æ¦‚å¿µ
- [ ] èƒ½å¤Ÿé…ç½®ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
- [ ] çŸ¥é“å¦‚ä½•è¿›è¡Œæ€§èƒ½ä¼˜åŒ–
- [ ] ç†è§£è´Ÿè½½å‡è¡¡å’Œè‡ªåŠ¨æ‰©ç¼©å®¹

**å®æˆ˜èƒ½åŠ›ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰**
- [ ] èƒ½å¤Ÿç‹¬ç«‹å®Œæˆç«¯åˆ°ç«¯éƒ¨ç½²
- [ ] ä¼šå¤„ç†ç”Ÿäº§ç¯å¢ƒçš„é—®é¢˜
- [ ] èƒ½å¤Ÿä¼˜åŒ–æœåŠ¡æ€§èƒ½å’Œæˆæœ¬
- [ ] ä¼šè®¾è®¡é«˜å¯ç”¨æ¶æ„
- [ ] èƒ½å¤Ÿç›‘æ§å’Œæ’æŸ¥æ•…éšœ
- [ ] ç†è§£å¦‚ä½•æŒç»­æ”¹è¿›ç³»ç»Ÿ

### ğŸ“Š éƒ¨ç½²é˜¶æ®µé€ŸæŸ¥è¡¨

| é˜¶æ®µ | ä¸»è¦ä»»åŠ¡ | å…³é”®æŠ€æœ¯ | éš¾åº¦ | é‡è¦æ€§ | é¢„è®¡æ—¶é—´ |
|------|---------|---------|------|--------|---------|
| **æ•°æ®å‡†å¤‡** | æ”¶é›†ã€æ¸…æ´—ã€åˆ†è¯ | Python, tiktoken | â­ ç®€å• | â­â­â­â­â­ | 1-2å¤© |
| **æ¨¡å‹è®­ç»ƒ** | è®­ç»ƒåŸºç¡€æ¨¡å‹ | PyTorch, NanoGPT | â­â­ ä¸­ç­‰ | â­â­â­â­â­ | 3-7å¤© |
| **åˆ†å¸ƒå¼è®­ç»ƒ** | å¤šGPUåŠ é€Ÿ | DDP, DeepSpeed | â­â­â­ è¾ƒéš¾ | â­â­â­â­ | 1-2å¤© |
| **æ¨¡å‹ä¼˜åŒ–** | é‡åŒ–ã€åŠ é€Ÿ | INT8, vLLM | â­â­ ä¸­ç­‰ | â­â­â­â­â­ | 1-2å¤© |
| **APIæœåŠ¡** | æ„å»ºREST API | FastAPI | â­â­ ä¸­ç­‰ | â­â­â­â­â­ | 1-2å¤© |
| **å®¹å™¨åŒ–** | Dockeræ‰“åŒ… | Docker | â­ ç®€å• | â­â­â­â­ | 0.5å¤© |
| **K8séƒ¨ç½²** | ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² | Kubernetes | â­â­â­ è¾ƒéš¾ | â­â­â­â­ | 2-3å¤© |
| **ç›‘æ§è¿ç»´** | ç›‘æ§å’Œæ—¥å¿— | Prometheus, Grafana | â­â­ ä¸­ç­‰ | â­â­â­â­â­ | 1-2å¤© |
| **æ€§èƒ½ä¼˜åŒ–** | é™ä½æˆæœ¬ | å„ç§ä¼˜åŒ–æŠ€æœ¯ | â­â­â­ è¾ƒéš¾ | â­â­â­â­ | æŒç»­è¿›è¡Œ |

### ğŸ¯ å¦‚ä½•è§„åˆ’éƒ¨ç½²é¡¹ç›®ï¼Ÿ

```python
# å†³ç­–æ ‘
if ä½ æ˜¯åˆå­¦è€…:
    # ç¬¬1å‘¨ï¼šåŸºç¡€
    å­¦ä¹ æ•°æ®å‡†å¤‡å’Œæ¨¡å‹è®­ç»ƒ
    ç›®æ ‡ï¼šèƒ½è·‘é€šè®­ç»ƒæµç¨‹
    
    # ç¬¬2å‘¨ï¼šéƒ¨ç½²
    å­¦ä¹ FastAPIå’ŒDocker
    ç›®æ ‡ï¼šèƒ½éƒ¨ç½²ä¸€ä¸ªç®€å•æœåŠ¡
    
    # ç¬¬3-4å‘¨ï¼šä¼˜åŒ–
    å­¦ä¹ æ¨¡å‹ä¼˜åŒ–å’Œç›‘æ§
    ç›®æ ‡ï¼šèƒ½ä¼˜åŒ–æ€§èƒ½
    
elif ä½ æœ‰ç»éªŒ:
    # ç¬¬1å‘¨ï¼šç«¯åˆ°ç«¯
    å¿«é€Ÿæ­å»ºå®Œæ•´æµç¨‹
    
    # ç¬¬2å‘¨ï¼šä¼˜åŒ–
    æ€§èƒ½ä¼˜åŒ–å’Œæˆæœ¬ä¼˜åŒ–
    
    # ç¬¬3å‘¨ï¼šç”Ÿäº§åŒ–
    é«˜å¯ç”¨ã€ç›‘æ§ã€è¿ç»´

# é¡¹ç›®è§„æ¨¡ä¼°ç®—
å°é¡¹ç›®ï¼ˆä¸ªäºº/å­¦ä¹ ï¼‰:
  - å•GPUè®­ç»ƒï¼š1-2å¤©
  - ç®€å•éƒ¨ç½²ï¼š1å¤©
  - æ€»è®¡ï¼š3-5å¤©

ä¸­é¡¹ç›®ï¼ˆå°å›¢é˜Ÿï¼‰:
  - å¤šGPUè®­ç»ƒï¼š3-5å¤©
  - K8séƒ¨ç½²ï¼š2-3å¤©
  - ç›‘æ§è¿ç»´ï¼š2å¤©
  - æ€»è®¡ï¼š1-2å‘¨

å¤§é¡¹ç›®ï¼ˆä¼ä¸šçº§ï¼‰:
  - å¤§è§„æ¨¡è®­ç»ƒï¼š1-2å‘¨
  - å®Œæ•´éƒ¨ç½²ï¼š1å‘¨
  - ä¼˜åŒ–è°ƒè¯•ï¼š1-2å‘¨
  - æ€»è®¡ï¼š1-2ä¸ªæœˆ
```

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

ç°åœ¨ä½ å·²ç»æŒæ¡äº†ç”Ÿäº§çº§éƒ¨ç½²ï¼Œæ¥ä¸‹æ¥åº”è¯¥å­¦ä¹ ï¼š

1. **11_multimodal_models.md** - å­¦ä¹ å¤šæ¨¡æ€æ¨¡å‹
2. **12_mixture_of_experts.md** - å­¦ä¹ ç¨€ç–æ¨¡å‹MoE
3. **13_rlhf_and_alignment.md** - å­¦ä¹ RLHFä¸æ¨¡å‹å¯¹é½
4. **å®è·µé¡¹ç›®** - éƒ¨ç½²ä¸€ä¸ªçœŸå®çš„ç”Ÿäº§æœåŠ¡

### ğŸ’¡ å®è·µå»ºè®®

**ç«‹å³å¯åš**ï¼š
```bash
# 1. éƒ¨ç½²ä¸€ä¸ªæœ€å°å¯è¡Œäº§å“ï¼ˆMVPï¼‰
# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¿«é€ŸéªŒè¯
python deploy_mvp.py --model gpt2

# 2. æµ‹è¯•APIæ€§èƒ½
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "max_tokens": 50}'

# 3. ç›‘æ§èµ„æºä½¿ç”¨
docker stats
nvidia-smi
```

**ç³»ç»Ÿå®éªŒ**ï¼š
```bash
# å®éªŒ1ï¼šç«¯åˆ°ç«¯éƒ¨ç½²æµç¨‹
# ä»æ•°æ®å‡†å¤‡åˆ°æœåŠ¡ä¸Šçº¿ï¼Œå®Œæ•´èµ°ä¸€é
./deploy_end_to_end.sh

# å®éªŒ2ï¼šæ€§èƒ½å‹æµ‹
# æµ‹è¯•ä¸åŒå¹¶å‘ä¸‹çš„æ€§èƒ½
for concurrency in 1 10 50 100; do
    ab -n 1000 -c $concurrency http://localhost:8000/generate
done

# å®éªŒ3ï¼šæˆæœ¬ä¼˜åŒ–
# å¯¹æ¯”ä¸åŒé…ç½®çš„æˆæœ¬
python cost_analysis.py \
  --configs fp32,fp16,int8 \
  --instances t4,a10,a100

# å®éªŒ4ï¼šæ•…éšœæ¼”ç»ƒ
# æ¨¡æ‹Ÿå„ç§æ•…éšœåœºæ™¯
kubectl delete pod api-server-xxx  # æµ‹è¯•è‡ªåŠ¨æ¢å¤
```

**è¿›é˜¶ç ”ç©¶**ï¼š
1. ç ”ç©¶å¤§å‚çš„LLMéƒ¨ç½²æ¶æ„ï¼ˆOpenAIã€Anthropicï¼‰
2. å­¦ä¹ æ›´å¤šä¼˜åŒ–æŠ€æœ¯ï¼ˆæ¨¡å‹å¹¶è¡Œã€æµå¼æ¨ç†ï¼‰
3. æ¢ç´¢è¾¹ç¼˜éƒ¨ç½²ï¼ˆç§»åŠ¨ç«¯ã€æµè§ˆå™¨ï¼‰
4. ç ”ç©¶æˆæœ¬ä¼˜åŒ–ç­–ç•¥ï¼ˆSpotå®ä¾‹ã€æ··åˆäº‘ï¼‰

---

## ğŸ“š æ¨èèµ„æº

### ğŸ“– å¿…è¯»æ–‡æ¡£
- [FastAPI Documentation](https://fastapi.tiangolo.com/) - æœ€å¥½çš„Python APIæ¡†æ¶
- [Docker Documentation](https://docs.docker.com/) - å®¹å™¨åŒ–å¿…å¤‡
- [Kubernetes Documentation](https://kubernetes.io/docs/) - å®¹å™¨ç¼–æ’
- [vLLM Documentation](https://docs.vllm.ai/) - é«˜æ€§èƒ½æ¨ç†å¼•æ“

### ğŸ“„ é‡è¦æ–‡ç« 

**éƒ¨ç½²æ¶æ„**ï¼š
1. **Building LLM applications for production** (Chip Huyen)
   - https://huyenchip.com/2023/04/11/llm-engineering.html
   - LLMå·¥ç¨‹åŒ–æœ€ä½³å®è·µ

2. **Patterns for Building LLM-based Systems** (Eugene Yan)
   - https://eugeneyan.com/writing/llm-patterns/
   - LLMç³»ç»Ÿè®¾è®¡æ¨¡å¼

3. **How to Deploy Large Language Models** (Hugging Face)
   - https://huggingface.co/blog/deploy-llms
   - éƒ¨ç½²æŒ‡å—

**æ€§èƒ½ä¼˜åŒ–**ï¼š
4. **Optimizing LLMs for Speed and Memory** (Hugging Face)
   - https://huggingface.co/docs/transformers/llm_tutorial_optimization
   - ä¼˜åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—

5. **Cost-Effective LLM Serving** (Anyscale)
   - https://www.anyscale.com/blog/cost-effective-llm-serving
   - æˆæœ¬ä¼˜åŒ–ç­–ç•¥

**ç›‘æ§è¿ç»´**ï¼š
6. **Monitoring Machine Learning Models in Production** (Google)
   - https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
   - MLOpsæœ€ä½³å®è·µ

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- [FastAPI Tutorial](https://www.youtube.com/watch?v=0sOvCWFmrtA)
- [Kubernetes Crash Course](https://www.youtube.com/watch?v=X48VuDVv0do)
- [Docker Tutorial for Beginners](https://www.youtube.com/watch?v=fqMOX6JJhGo)

### ğŸ”§ å®ç”¨å·¥å…·

**å¼€å‘å·¥å…·**ï¼š
```bash
# FastAPI - APIæ¡†æ¶
pip install fastapi uvicorn

# vLLM - æ¨ç†å¼•æ“
pip install vllm

# Locust - å‹æµ‹å·¥å…·
pip install locust

# Prometheus Client - ç›‘æ§
pip install prometheus-client
```

**éƒ¨ç½²å·¥å…·**ï¼š
```bash
# Docker - å®¹å™¨åŒ–
docker build -t my-llm-service .
docker run -p 8000:8000 my-llm-service

# Kubernetes - ç¼–æ’
kubectl apply -f deployment.yaml
kubectl get pods

# Helm - K8såŒ…ç®¡ç†
helm install my-release ./chart
```

**ç›‘æ§å·¥å…·**ï¼š
```bash
# Prometheus - æŒ‡æ ‡æ”¶é›†
docker run -p 9090:9090 prom/prometheus

# Grafana - å¯è§†åŒ–
docker run -p 3000:3000 grafana/grafana

# Jaeger - åˆ†å¸ƒå¼è¿½è¸ª
docker run -p 16686:16686 jaegertracing/all-in-one
```

---

## ğŸ› å¸¸è§é—®é¢˜ FAQ

### Q1: å¦‚ä½•é€‰æ‹©éƒ¨ç½²æ–¹å¼ï¼Ÿ
**A**: æ ¹æ®è§„æ¨¡å’Œéœ€æ±‚é€‰æ‹©ã€‚
```
ä¸ªäººé¡¹ç›®/åŸå‹:
  âœ… å•æœºéƒ¨ç½²
  âœ… Docker Compose
  âœ… ç®€å•å¿«é€Ÿ
  âŒ ä¸é€‚åˆç”Ÿäº§
  
å°å›¢é˜Ÿ/MVP:
  âœ… äº‘æœåŠ¡ï¼ˆAWS/GCPï¼‰
  âœ… æ‰˜ç®¡K8sï¼ˆEKS/GKEï¼‰
  âœ… æ˜“äºæ‰©å±•
  âš ï¸ æˆæœ¬è¾ƒé«˜
  
ä¼ä¸šçº§/å¤§è§„æ¨¡:
  âœ… è‡ªå»ºK8sé›†ç¾¤
  âœ… å¤šåŒºåŸŸéƒ¨ç½²
  âœ… å®Œå…¨æ§åˆ¶
  âŒ è¿ç»´å¤æ‚

æ¨èè·¯å¾„ï¼š
  1. å¼€å‘ï¼šæœ¬åœ°Docker
  2. æµ‹è¯•ï¼šå•æœºéƒ¨ç½²
  3. ç”Ÿäº§ï¼šK8sæ‰˜ç®¡æœåŠ¡
```

### Q2: APIå»¶è¿Ÿå¤šå°‘ç®—æ­£å¸¸ï¼Ÿ
**A**: å–å†³äºæ¨¡å‹å¤§å°å’Œç¡¬ä»¶ã€‚
```python
# å»¶è¿ŸåŸºå‡†ï¼ˆç”Ÿæˆ50 tokensï¼‰

å°æ¨¡å‹ï¼ˆ<1Bå‚æ•°ï¼‰:
  CPU: 5-10ç§’ âš ï¸ å¤ªæ…¢
  T4 GPU: 500-1000ms âœ… å¯æ¥å—
  A100 GPU: 100-200ms âœ… å¾ˆå¥½

ä¸­æ¨¡å‹ï¼ˆ1-10Bå‚æ•°ï¼‰:
  T4 GPU: 2-5ç§’ âš ï¸ è¾ƒæ…¢
  A10 GPU: 500-1000ms âœ… å¯æ¥å—
  A100 GPU: 200-500ms âœ… å¾ˆå¥½

å¤§æ¨¡å‹ï¼ˆ>10Bå‚æ•°ï¼‰:
  A10 GPU: 2-5ç§’ âš ï¸ è¾ƒæ…¢
  A100 GPU: 500-1000ms âœ… å¯æ¥å—
  A100Ã—4: 200-300ms âœ… å¾ˆå¥½

ä¼˜åŒ–ç›®æ ‡ï¼š
  - äº¤äº’å¼åº”ç”¨ï¼š< 500ms
  - æ‰¹å¤„ç†ï¼š< 5ç§’
  - ç¦»çº¿ä»»åŠ¡ï¼šä¸é™åˆ¶

å¦‚æœå»¶è¿Ÿè¿‡é«˜ï¼š
  1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨KV Cache
  2. è€ƒè™‘æ¨¡å‹é‡åŒ–
  3. ä½¿ç”¨æ›´å¿«çš„GPU
  4. å‡å°batch_size
```

### Q3: å¦‚ä½•ä¼°ç®—éƒ¨ç½²æˆæœ¬ï¼Ÿ
**A**: è®¡ç®—å…¬å¼å’Œå®é™…æ¡ˆä¾‹ã€‚
```python
# æˆæœ¬ = GPUæˆæœ¬ + å…¶ä»–æˆæœ¬

# 1. GPUæˆæœ¬ï¼ˆä¸»è¦ï¼‰
GPUå°æ—¶æˆæœ¬:
  T4: $0.35/å°æ—¶
  A10: $1.28/å°æ—¶
  A100: $3.67/å°æ—¶

æ¯æœˆæˆæœ¬ï¼ˆ24Ã—30ï¼‰:
  T4: $252/æœˆ
  A10: $922/æœˆ
  A100: $2,642/æœˆ

# 2. è¯·æ±‚é‡ä¼°ç®—
å‡è®¾ï¼š
  - 1000 req/å¤©
  - æ¯ä¸ªè¯·æ±‚50 tokens
  - T4 GPUï¼Œ100 req/å°æ—¶

GPUåˆ©ç”¨ç‡: 1000/24/100 = 42%
å®é™…æˆæœ¬: $252 Ã— 0.42 = $106/æœˆ

# 3. ä¼˜åŒ–å
ä½¿ç”¨INT8é‡åŒ– + vLLM:
  - ååé‡æå‡3x
  - åŒæ ·1000 req/å¤©
  - åˆ©ç”¨ç‡: 14%
  - æˆæœ¬: $252 Ã— 0.14 = $35/æœˆ

èŠ‚çœ: $106 - $35 = $71/æœˆ (67%)

# 4. å®é™…æ¡ˆä¾‹
å°é¡¹ç›®ï¼ˆ1K req/å¤©ï¼‰:
  - 1Ã—T4: $35-50/æœˆ
  
ä¸­é¡¹ç›®ï¼ˆ10K req/å¤©ï¼‰:
  - 2Ã—A10: $200-300/æœˆ
  
å¤§é¡¹ç›®ï¼ˆ100K req/å¤©ï¼‰:
  - 4Ã—A100: $2000-3000/æœˆ
```

### Q4: Dockeré•œåƒå¤ªå¤§æ€ä¹ˆåŠï¼Ÿ
**A**: å¤šç§ä¼˜åŒ–æ–¹æ³•ã€‚
```dockerfile
# é—®é¢˜ï¼šåŸºç¡€é•œåƒå¤ªå¤§
FROM pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
# é•œåƒå¤§å°ï¼š8GB+

# ä¼˜åŒ–1ï¼šä½¿ç”¨æ›´å°çš„åŸºç¡€é•œåƒ
FROM nvidia/cuda:11.7.1-cudnn8-runtime-ubuntu22.04
# é•œåƒå¤§å°ï¼š2GB

# ä¼˜åŒ–2ï¼šå¤šé˜¶æ®µæ„å»º
FROM python:3.10 as builder
RUN pip install --user torch transformers
# åªå¤åˆ¶éœ€è¦çš„æ–‡ä»¶

FROM python:3.10-slim
COPY --from=builder /root/.local /root/.local
# æœ€ç»ˆé•œåƒï¼š1GB

# ä¼˜åŒ–3ï¼šæ¸…ç†ç¼“å­˜
RUN pip install --no-cache-dir torch
RUN apt-get clean && rm -rf /var/lib/apt/lists/*

# ä¼˜åŒ–4ï¼šåªå®‰è£…éœ€è¦çš„åŒ…
# ä¸è¦ï¼špip install transformersï¼ˆåŒ…å«æ‰€æœ‰ä¾èµ–ï¼‰
# è€Œæ˜¯ï¼špip install torch tokenizersï¼ˆåªè£…å¿…éœ€çš„ï¼‰

# æ•ˆæœå¯¹æ¯”
åŸå§‹é•œåƒ: 8GB
ä¼˜åŒ–å: 1-2GB
å‡å°‘: 75%+
```

### Q5: å¦‚ä½•å¤„ç†çªå‘æµé‡ï¼Ÿ
**A**: è‡ªåŠ¨æ‰©ç¼©å®¹å’Œé™æµã€‚
```yaml
# 1. Kubernetes HPAï¼ˆæ°´å¹³è‡ªåŠ¨æ‰©ç¼©å®¹ï¼‰
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-api
  minReplicas: 2      # æœ€å°‘2ä¸ª
  maxReplicas: 10     # æœ€å¤š10ä¸ª
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # CPU>70%æ—¶æ‰©å®¹

# 2. é™æµï¼ˆRate Limitingï¼‰
from fastapi import FastAPI
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)
app = FastAPI()

@app.post("/generate")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡
async def generate(request: Request):
    ...

# 3. é˜Ÿåˆ—ç¼“å†²
# çªå‘æµé‡è¿›å…¥é˜Ÿåˆ—ï¼Œæ…¢æ…¢å¤„ç†
from celery import Celery
app = Celery('tasks')

@app.task
def generate_task(prompt):
    return model.generate(prompt)

# 4. CDNç¼“å­˜
# å¯¹äºç›¸åŒçš„è¯·æ±‚ï¼Œè¿”å›ç¼“å­˜ç»“æœ
@app.post("/generate")
@cache(expire=3600)  # ç¼“å­˜1å°æ—¶
async def generate(prompt: str):
    ...

# æ•ˆæœ
çªå‘æµé‡: 1000 req/s
é™æµå: 100 req/sï¼ˆå¯å¤„ç†ï¼‰
å…¶ä»–900: æ’é˜Ÿæˆ–è¿”å›429
```

### Q6: å¦‚ä½•ç›‘æ§æ¨¡å‹è´¨é‡ï¼Ÿ
**A**: å¤šç»´åº¦ç›‘æ§ã€‚
```python
# 1. è¾“å‡ºè´¨é‡æŒ‡æ ‡
from prometheus_client import Histogram

response_length = Histogram('response_length', 'Response length')
response_time = Histogram('response_time', 'Response time')

@app.post("/generate")
async def generate(prompt: str):
    start = time.time()
    output = model.generate(prompt)
    
    # è®°å½•æŒ‡æ ‡
    response_length.observe(len(output))
    response_time.observe(time.time() - start)
    
    return output

# 2. å†…å®¹å®‰å…¨æ£€æŸ¥
def check_safety(text):
    # æ£€æŸ¥æœ‰å®³å†…å®¹
    if contains_harmful_content(text):
        alert("Harmful content detected!")
        return False
    return True

# 3. ç”¨æˆ·åé¦ˆ
@app.post("/feedback")
async def feedback(request_id: str, rating: int):
    # æ”¶é›†ç”¨æˆ·è¯„åˆ†
    store_feedback(request_id, rating)
    
    # ä½åˆ†å‘Šè­¦
    if rating < 3:
        alert(f"Low rating: {rating}")

# 4. A/Bæµ‹è¯•
def get_model_version(user_id):
    # 10%ç”¨æˆ·ä½¿ç”¨æ–°ç‰ˆæœ¬
    if hash(user_id) % 10 == 0:
        return "model_v2"
    return "model_v1"

# å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„æŒ‡æ ‡

# 5. å®šæœŸè¯„ä¼°
# æ¯å¤©åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
@scheduler.scheduled_job('cron', hour=2)
def daily_eval():
    metrics = evaluate_model(test_dataset)
    if metrics['accuracy'] < threshold:
        alert("Model quality degraded!")
```

### Q7: å¦‚ä½•å¤„ç†æ¨¡å‹æ›´æ–°ï¼Ÿ
**A**: è“ç»¿éƒ¨ç½²æˆ–é‡‘ä¸é›€å‘å¸ƒã€‚
```yaml
# è“ç»¿éƒ¨ç½²ï¼ˆBlue-Green Deploymentï¼‰

# æ­¥éª¤1ï¼šéƒ¨ç½²æ–°ç‰ˆæœ¬ï¼ˆGreenï¼‰
kubectl apply -f deployment-v2.yaml
# æ­¤æ—¶v1ï¼ˆBlueï¼‰è¿˜åœ¨è¿è¡Œ

# æ­¥éª¤2ï¼šæµ‹è¯•æ–°ç‰ˆæœ¬
kubectl port-forward svc/llm-api-v2 8001:8000
# å†…éƒ¨æµ‹è¯•

# æ­¥éª¤3ï¼šåˆ‡æ¢æµé‡
kubectl patch service llm-api \
  -p '{"spec":{"selector":{"version":"v2"}}}'
# æµé‡ä»v1åˆ‡åˆ°v2

# æ­¥éª¤4ï¼šå¦‚æœæœ‰é—®é¢˜ï¼Œç«‹å³å›æ»š
kubectl patch service llm-api \
  -p '{"spec":{"selector":{"version":"v1"}}}'
# ç§’çº§å›æ»š

# é‡‘ä¸é›€å‘å¸ƒï¼ˆCanary Deploymentï¼‰

# æ­¥éª¤1ï¼šéƒ¨ç½²æ–°ç‰ˆæœ¬ï¼Œåªç»™5%æµé‡
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: llm-api
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: llm-api-v2
      weight: 5    # 5%æµé‡
  - route:
    - destination:
        host: llm-api-v1
      weight: 95   # 95%æµé‡

# æ­¥éª¤2ï¼šè§‚å¯ŸæŒ‡æ ‡ï¼Œé€æ­¥å¢åŠ 
# 5% â†’ 10% â†’ 25% â†’ 50% â†’ 100%

# æ­¥éª¤3ï¼šå¦‚æœæœ‰é—®é¢˜ï¼Œåœæ­¢å‘å¸ƒ
# æµé‡å›åˆ°v1
```

### Q8: æ—¥å¿—åº”è¯¥è®°å½•ä»€ä¹ˆï¼Ÿ
**A**: ç»“æ„åŒ–æ—¥å¿—ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯ã€‚
```python
import logging
import json

# é…ç½®ç»“æ„åŒ–æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

@app.post("/generate")
async def generate(request: GenerateRequest):
    request_id = str(uuid.uuid4())
    
    # 1. è¯·æ±‚æ—¥å¿—
    logging.info(json.dumps({
        "event": "request_received",
        "request_id": request_id,
        "prompt_length": len(request.prompt),
        "max_tokens": request.max_tokens,
        "user_id": request.user_id,
        "timestamp": time.time()
    }))
    
    try:
        # 2. å¤„ç†æ—¥å¿—
        start = time.time()
        output = model.generate(request.prompt)
        duration = time.time() - start
        
        logging.info(json.dumps({
            "event": "request_completed",
            "request_id": request_id,
            "duration": duration,
            "output_length": len(output),
            "tokens_per_second": len(output) / duration
        }))
        
        return {"output": output}
        
    except Exception as e:
        # 3. é”™è¯¯æ—¥å¿—
        logging.error(json.dumps({
            "event": "request_failed",
            "request_id": request_id,
            "error": str(e),
            "traceback": traceback.format_exc()
        }))
        raise

# å…³é”®æŒ‡æ ‡ï¼š
# - request_id: è¿½è¸ªè¯·æ±‚
# - duration: æ€§èƒ½ç›‘æ§
# - error: é”™è¯¯è¿½è¸ª
# - user_id: ç”¨æˆ·åˆ†æ
```

### Q9: å¦‚ä½•åšç¾éš¾æ¢å¤ï¼Ÿ
**A**: å¤šå±‚å¤‡ä»½å’Œæ¢å¤è®¡åˆ’ã€‚
```bash
# 1. æ¨¡å‹å¤‡ä»½
# å®šæœŸå¤‡ä»½æ¨¡å‹åˆ°å¤šä¸ªä½ç½®
aws s3 sync ./models s3://backup-bucket/models/
gsutil rsync -r ./models gs://backup-bucket/models/

# 2. æ•°æ®åº“å¤‡ä»½
# æ¯å¤©è‡ªåŠ¨å¤‡ä»½
0 2 * * * pg_dump mydb > backup_$(date +%Y%m%d).sql

# 3. é…ç½®å¤‡ä»½
# ç‰ˆæœ¬æ§åˆ¶æ‰€æœ‰é…ç½®
git commit -am "Update config"
git push origin main

# 4. å¤šåŒºåŸŸéƒ¨ç½²
# åœ¨å¤šä¸ªåŒºåŸŸéƒ¨ç½²æœåŠ¡
kubectl apply -f deployment-us-east.yaml
kubectl apply -f deployment-eu-west.yaml

# 5. æ¢å¤æ¼”ç»ƒ
# å®šæœŸæµ‹è¯•æ¢å¤æµç¨‹
./disaster_recovery_test.sh

# æ¢å¤æ—¶é—´ç›®æ ‡ï¼ˆRTOï¼‰:
# - æ•°æ®åº“ï¼š< 1å°æ—¶
# - æ¨¡å‹æœåŠ¡ï¼š< 15åˆ†é’Ÿ
# - å®Œæ•´ç³»ç»Ÿï¼š< 4å°æ—¶

# æ¢å¤ç‚¹ç›®æ ‡ï¼ˆRPOï¼‰:
# - æ•°æ®ä¸¢å¤±ï¼š< 1å°æ—¶
# - æ¨¡å‹ç‰ˆæœ¬ï¼šæœ€æ–°ç‰ˆæœ¬
```

### Q10: å¦‚ä½•é™ä½éƒ¨ç½²æˆæœ¬ï¼Ÿ
**A**: å¤šæ–¹é¢ä¼˜åŒ–ã€‚
```python
# 1. æ¨¡å‹ä¼˜åŒ–ï¼ˆæœ€æœ‰æ•ˆï¼‰
é‡åŒ–åˆ°INT8: æˆæœ¬é™ä½75%
é‡åŒ–åˆ°INT4: æˆæœ¬é™ä½87.5%

# 2. ä½¿ç”¨Spotå®ä¾‹
AWS Spot: æˆæœ¬é™ä½70%
ä½†éœ€è¦å¤„ç†ä¸­æ–­

# 3. è‡ªåŠ¨æ‰©ç¼©å®¹
é«˜å³°æœŸ: 10ä¸ªå®ä¾‹
ä½å³°æœŸ: 2ä¸ªå®ä¾‹
å¹³å‡æˆæœ¬: é™ä½60%

# 4. æ‰¹å¤„ç†
å®æ—¶è¯·æ±‚: ç«‹å³å¤„ç†ï¼ˆè´µï¼‰
ç¦»çº¿è¯·æ±‚: æ‰¹å¤„ç†ï¼ˆä¾¿å®œ80%ï¼‰

# 5. ç¼“å­˜
ç›¸åŒè¯·æ±‚: è¿”å›ç¼“å­˜
ç¼“å­˜å‘½ä¸­ç‡30%: æˆæœ¬é™ä½30%

# 6. é€‰æ‹©åˆé€‚çš„GPU
ä¸è¦æ€»ç”¨A100:
  - å¼€å‘æµ‹è¯•: T4 ($0.35/h)
  - ç”Ÿäº§å°æ¨¡å‹: A10 ($1.28/h)
  - ç”Ÿäº§å¤§æ¨¡å‹: A100 ($3.67/h)

# 7. åŒºåŸŸé€‰æ‹©
ä¸åŒåŒºåŸŸä»·æ ¼ä¸åŒ:
  - us-east-1: æœ€ä¾¿å®œ
  - eu-west-1: è´µ10-20%
  - ap-southeast: è´µ20-30%

# å®é™…æ¡ˆä¾‹
åŸå§‹æˆæœ¬: $5000/æœˆ
  - A100Ã—4, 24/7è¿è¡Œ
  
ä¼˜åŒ–å: $800/æœˆ
  - INT8é‡åŒ–: -75%
  - è‡ªåŠ¨æ‰©ç¼©å®¹: -60%
  - Spotå®ä¾‹: -70%
  - ç»¼åˆ: -84%

èŠ‚çœ: $4200/æœˆï¼
```

---

**æ­å–œä½ å®Œæˆç¬¬10ç« ï¼** ğŸ‰

ä½ ç°åœ¨å·²ç»æŒæ¡äº†ç”Ÿäº§çº§éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚ä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹è®­ç»ƒï¼Œä»APIæœåŠ¡åˆ°å®¹å™¨åŒ–éƒ¨ç½²ï¼Œä»ç›‘æ§è¿ç»´åˆ°æˆæœ¬ä¼˜åŒ–ï¼Œä½ å·²ç»å…·å¤‡äº†æ„å»ºç”Ÿäº§çº§AIæœåŠ¡çš„èƒ½åŠ›ã€‚

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼** â†’ [11_multimodal_models.md](11_multimodal_models.md)

