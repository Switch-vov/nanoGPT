# ç”Ÿäº§çº§éƒ¨ç½²å®æˆ˜ï¼šç«¯åˆ°ç«¯é¡¹ç›®

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ª**ç”Ÿäº§çº§çš„ä»£ç è¡¥å…¨åŠ©æ‰‹**ï¼Œä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚

```
é¡¹ç›®æµç¨‹ï¼š

æ•°æ®å‡†å¤‡ â†’ æ¨¡å‹è®­ç»ƒ â†’ åˆ†å¸ƒå¼åŠ é€Ÿ â†’ æ¨¡å‹ä¼˜åŒ– â†’ APIæœåŠ¡ â†’ å®¹å™¨åŒ– â†’ ç›‘æ§è¿ç»´

é¢„æœŸæˆæœï¼š
  âœ… è®­ç»ƒä¸€ä¸ªä»£ç è¡¥å…¨æ¨¡å‹
  âœ… éƒ¨ç½²ä¸ºé«˜æ€§èƒ½APIæœåŠ¡
  âœ… æ”¯æŒ100+å¹¶å‘ç”¨æˆ·
  âœ… å»¶è¿Ÿ < 200ms
  âœ… æˆæœ¬ < $0.01/1K tokens
```

---

## ğŸ“š é˜¶æ®µ1ï¼šæ•°æ®å‡†å¤‡

### ğŸ”§ æ”¶é›†Pythonä»£ç æ•°æ®

```python
# collect_code.py
import os
import tiktoken
import numpy as np

def collect_python_files(root_dir):
    """æ”¶é›†æ‰€æœ‰Pythonæ–‡ä»¶"""
    code_files = []
    for root, dirs, files in os.walk(root_dir):
        # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜
        dirs[:] = [d for d in dirs if d not in ['.venv', '__pycache__', 'node_modules']]
        
        for file in files:
            if file.endswith('.py'):
                path = os.path.join(root, file)
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        code = f.read()
                        if len(code) > 100:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡ä»¶
                            code_files.append(code)
                except:
                    continue
    
    return code_files

def prepare_dataset(code_files, output_dir='data/python_code'):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. åˆå¹¶æ‰€æœ‰ä»£ç 
    data = '\n\n# ================\n\n'.join(code_files)
    print(f"æ€»å­—ç¬¦æ•°: {len(data):,}")
    
    # 2. åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
    n = len(data)
    train_data = data[:int(n*0.9)]
    val_data = data[int(n*0.9):]
    
    # 3. Tokenize
    enc = tiktoken.get_encoding("gpt2")
    train_ids = enc.encode_ordinary(train_data)
    val_ids = enc.encode_ordinary(val_data)
    
    print(f"è®­ç»ƒtokens: {len(train_ids):,}")
    print(f"éªŒè¯tokens: {len(val_ids):,}")
    
    # 4. ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
    np.array(train_ids, dtype=np.uint16).tofile(f'{output_dir}/train.bin')
    np.array(val_ids, dtype=np.uint16).tofile(f'{output_dir}/val.bin')
    
    print(f"æ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")

# è¿è¡Œ
if __name__ == '__main__':
    # æ”¶é›†ä»£ç ï¼ˆæ›¿æ¢ä¸ºä½ çš„é¡¹ç›®è·¯å¾„ï¼‰
    code_files = collect_python_files('/path/to/your/python/projects')
    print(f"æ”¶é›†äº† {len(code_files)} ä¸ªPythonæ–‡ä»¶")
    
    # å‡†å¤‡æ•°æ®é›†
    prepare_dataset(code_files)
```

**è¿è¡Œï¼š**
```bash
python collect_code.py

# è¾“å‡ºï¼š
# æ”¶é›†äº† 1,234 ä¸ªPythonæ–‡ä»¶
# æ€»å­—ç¬¦æ•°: 5,678,901
# è®­ç»ƒtokens: 1,234,567
# éªŒè¯tokens: 137,174
# æ•°æ®å·²ä¿å­˜åˆ° data/python_code/
```

---

## ğŸ“š é˜¶æ®µ2ï¼šæ¨¡å‹è®­ç»ƒ

### ğŸ”§ åˆ›å»ºè®­ç»ƒé…ç½®

```python
# config/train_code_assistant.py

import time

# è¾“å‡ºç›®å½•
out_dir = 'out-code-assistant'
eval_interval = 500
eval_iters = 100
log_interval = 10

# æ•°æ®é›†
dataset = 'python_code'
gradient_accumulation_steps = 4
batch_size = 8
block_size = 512  # ä»£ç éœ€è¦é•¿ä¸Šä¸‹æ–‡

# æ¨¡å‹ï¼ˆä¸­ç­‰å¤§å°ï¼‰
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.1

# å¾®è°ƒè®¾ç½®
init_from = 'gpt2'  # ä»GPT-2å¼€å§‹
learning_rate = 5e-5  # å°å­¦ä¹ ç‡
max_iters = 5000
lr_decay_iters = 5000
min_lr = 5e-6

# ä¼˜åŒ–
weight_decay = 0.1
grad_clip = 1.0
decay_lr = True
warmup_iters = 100

# ç³»ç»Ÿ
device = 'cuda'
dtype = 'float16'
compile = True
```

### ğŸš€ å¼€å§‹è®­ç»ƒ

```bash
# å•GPUè®­ç»ƒ
python train.py config/train_code_assistant.py

# é¢„æœŸè¾“å‡ºï¼š
# iter 0: loss 3.2145, time 1234.56ms
# iter 500: train loss 2.1234, val loss 2.3456
# iter 1000: train loss 1.8765, val loss 2.1234
# ...
# iter 5000: train loss 1.2345, val loss 1.5678
# è®­ç»ƒå®Œæˆï¼æœ€ä½³val loss: 1.5234
```

---

## ğŸ“š é˜¶æ®µ3ï¼šåˆ†å¸ƒå¼åŠ é€Ÿ

### ğŸ”§ å¤šGPUè®­ç»ƒé…ç½®

```python
# config/train_code_assistant_ddp.py

# ç»§æ‰¿å•GPUé…ç½®
exec(open('config/train_code_assistant.py').read())

# DDPä¼˜åŒ–
batch_size = 16  # æ¯ä¸ªGPUæ›´å¤§batch
gradient_accumulation_steps = 2  # å‡å°‘ç´¯ç§¯æ­¥æ•°

# æ€»batch_size = 16 Ã— 2 Ã— 4 = 128 (å’Œå•GPUçš„8Ã—4Ã—4ä¸€æ ·)
```

### ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# 4 GPUè®­ç»ƒ
torchrun --standalone --nproc_per_node=4 \
  train.py config/train_code_assistant_ddp.py

# åŠ é€Ÿæ•ˆæœï¼š
# å•GPU: 2-3å°æ—¶
# 4 GPU: 30-40åˆ†é’Ÿ (çº¦4xåŠ é€Ÿ)
```

---

## ğŸ“š é˜¶æ®µ4ï¼šæ¨¡å‹ä¼˜åŒ–

### ğŸ”§ é‡åŒ–æ¨¡å‹

```python
# quantize_model.py

import torch
from model import GPT, GPTConfig

def quantize_model(checkpoint_path, output_path):
    """é‡åŒ–æ¨¡å‹åˆ°INT8"""
    # 1. åŠ è½½æ¨¡å‹
    checkpoint = torch.load(checkpoint_path)
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    
    print(f"åŸå§‹æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")
    
    # 2. åŠ¨æ€é‡åŒ–
    model_quantized = torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8
    )
    
    # 3. ä¿å­˜
    torch.save({
        'model': model_quantized.state_dict(),
        'model_args': checkpoint['model_args'],
        'quantized': True,
    }, output_path)
    
    # 4. å¯¹æ¯”
    import os
    orig_size = os.path.getsize(checkpoint_path) / 1e6
    quant_size = os.path.getsize(output_path) / 1e6
    
    print(f"\né‡åŒ–ç»“æœï¼š")
    print(f"  åŸå§‹å¤§å°: {orig_size:.2f} MB")
    print(f"  é‡åŒ–å¤§å°: {quant_size:.2f} MB")
    print(f"  å‹ç¼©æ¯”: {orig_size/quant_size:.2f}x")

if __name__ == '__main__':
    quantize_model(
        'out-code-assistant/ckpt.pt',
        'out-code-assistant/ckpt_int8.pt'
    )
```

**è¿è¡Œï¼š**
```bash
python quantize_model.py

# è¾“å‡ºï¼š
# åŸå§‹æ¨¡å‹å‚æ•°: 124.44M
# 
# é‡åŒ–ç»“æœï¼š
#   åŸå§‹å¤§å°: 497.35 MB
#   é‡åŒ–å¤§å°: 124.67 MB
#   å‹ç¼©æ¯”: 3.99x
```

---

## ğŸ“š é˜¶æ®µ5ï¼šAPIæœåŠ¡

### ğŸ”§ åˆ›å»ºFastAPIæœåŠ¡

```python
# serve_api.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from model import GPT, GPTConfig
import tiktoken
from contextlib import asynccontextmanager
from typing import Optional
import time

# å…¨å±€å˜é‡
model = None
tokenizer = None
device = 'cuda' if torch.cuda.is_available() else 'cpu'

@asynccontextmanager
async def lifespan(app: FastAPI):
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model, tokenizer
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load('out-code-assistant/ckpt_int8.pt', map_location=device)
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    model.to(device)
    
    tokenizer = tiktoken.get_encoding("gpt2")
    print(f"æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    
    yield
    
    # æ¸…ç†
    del model
    del tokenizer

app = FastAPI(lifespan=lifespan)

class CompletionRequest(BaseModel):
    code: str
    max_tokens: int = 50
    temperature: float = 0.8
    top_k: Optional[int] = 200

class CompletionResponse(BaseModel):
    completion: str
    tokens: int
    latency_ms: float

@app.post("/complete", response_model=CompletionResponse)
async def complete_code(request: CompletionRequest):
    """ä»£ç è¡¥å…¨API"""
    try:
        start_time = time.time()
        
        # 1. Tokenizeè¾“å…¥
        input_ids = tokenizer.encode(request.code)
        if len(input_ids) > 512:
            input_ids = input_ids[-512:]  # æˆªæ–­
        
        x = torch.tensor([input_ids], dtype=torch.long, device=device)
        
        # 2. ç”Ÿæˆ
        with torch.no_grad():
            y = model.generate(
                x,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_k=request.top_k
            )
        
        # 3. Decode
        output_ids = y[0].tolist()
        completion = tokenizer.decode(output_ids[len(input_ids):])
        
        latency = (time.time() - start_time) * 1000
        
        return CompletionResponse(
            completion=completion,
            tokens=len(output_ids) - len(input_ids),
            latency_ms=latency
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": device
    }

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### ğŸš€ å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨APIæœåŠ¡
python serve_api.py

# è¾“å‡ºï¼š
# æ­£åœ¨åŠ è½½æ¨¡å‹...
# æ¨¡å‹å·²åŠ è½½åˆ° cuda
# INFO:     Started server process [12345]
# INFO:     Waiting for application startup.
# INFO:     Application startup complete.
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

### ğŸ§ª æµ‹è¯•API

```bash
# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# è¾“å‡ºï¼š
# {"status":"healthy","model_loaded":true,"device":"cuda"}

# æµ‹è¯•ä»£ç è¡¥å…¨
curl -X POST http://localhost:8000/complete \
  -H "Content-Type: application/json" \
  -d '{
    "code": "def fibonacci(n):\n    if n <= 1:\n        return n\n    ",
    "max_tokens": 50,
    "temperature": 0.8
  }'

# è¾“å‡ºï¼š
# {
#   "completion": "return fibonacci(n-1) + fibonacci(n-2)\n\n# Test\nprint(fibonacci(10))",
#   "tokens": 23,
#   "latency_ms": 156.78
# }
```

---

## ğŸ“š é˜¶æ®µ6ï¼šå®¹å™¨åŒ–éƒ¨ç½²

### ğŸ³ åˆ›å»ºDockerfile

```dockerfile
# Dockerfile

FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# å®‰è£…Python
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç å’Œæ¨¡å‹
COPY model.py .
COPY serve_api.py .
COPY out-code-assistant/ckpt_int8.pt ./out-code-assistant/

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨æœåŠ¡
CMD ["python3", "serve_api.py"]
```

### ğŸ“¦ requirements.txt

```txt
torch==2.0.1
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
tiktoken==0.5.1
numpy==1.24.3
```

### ğŸš€ æ„å»ºå’Œè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t code-assistant:v1 .

# è¿è¡Œå®¹å™¨
docker run -d \
  --gpus all \
  -p 8000:8000 \
  --name code-assistant \
  code-assistant:v1

# æŸ¥çœ‹æ—¥å¿—
docker logs -f code-assistant

# æµ‹è¯•
curl http://localhost:8000/health
```

---

## ğŸ“š é˜¶æ®µ7ï¼šKuberneteséƒ¨ç½²

### â˜¸ï¸ éƒ¨ç½²é…ç½®

```yaml
# k8s/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: code-assistant
spec:
  replicas: 3  # 3ä¸ªå‰¯æœ¬
  selector:
    matchLabels:
      app: code-assistant
  template:
    metadata:
      labels:
        app: code-assistant
    spec:
      containers:
      - name: code-assistant
        image: code-assistant:v1
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1  # æ¯ä¸ªpodä¸€ä¸ªGPU
            memory: "8Gi"
            cpu: "4"
          requests:
            nvidia.com/gpu: 1
            memory: "4Gi"
            cpu: "2"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: code-assistant-service
spec:
  selector:
    app: code-assistant
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: code-assistant-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: code-assistant
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### ğŸš€ éƒ¨ç½²åˆ°K8s

```bash
# éƒ¨ç½²
kubectl apply -f k8s/deployment.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods
kubectl get services

# æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/code-assistant

# æµ‹è¯•æœåŠ¡
kubectl port-forward service/code-assistant-service 8000:80
curl http://localhost:8000/health
```

---

## ğŸ“š é˜¶æ®µ8ï¼šç›‘æ§ä¸è¿ç»´

### ğŸ“Š æ·»åŠ Prometheusç›‘æ§

```python
# serve_api_with_metrics.py

from prometheus_client import Counter, Histogram, Gauge, make_asgi_app
from fastapi import FastAPI
import time

# å®šä¹‰æŒ‡æ ‡
request_count = Counter(
    'code_completion_requests_total',
    'Total code completion requests',
    ['status']
)

request_duration = Histogram(
    'code_completion_duration_seconds',
    'Code completion request duration',
    buckets=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
)

tokens_generated = Counter(
    'tokens_generated_total',
    'Total tokens generated'
)

active_requests = Gauge(
    'active_requests',
    'Number of active requests'
)

app = FastAPI()

# æ·»åŠ Prometheus metrics endpoint
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

@app.post("/complete")
async def complete_code(request: CompletionRequest):
    active_requests.inc()
    start_time = time.time()
    
    try:
        # ... ç”Ÿæˆä»£ç  ...
        
        # è®°å½•æŒ‡æ ‡
        duration = time.time() - start_time
        request_duration.observe(duration)
        request_count.labels(status='success').inc()
        tokens_generated.inc(len(output_ids) - len(input_ids))
        
        return response
    
    except Exception as e:
        request_count.labels(status='error').inc()
        raise
    
    finally:
        active_requests.dec()
```

### ğŸ“ˆ Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "Code Assistant Metrics",
    "panels": [
      {
        "title": "Requests per Second",
        "targets": [{
          "expr": "rate(code_completion_requests_total[1m])"
        }]
      },
      {
        "title": "P95 Latency",
        "targets": [{
          "expr": "histogram_quantile(0.95, code_completion_duration_seconds)"
        }]
      },
      {
        "title": "Tokens per Second",
        "targets": [{
          "expr": "rate(tokens_generated_total[1m])"
        }]
      },
      {
        "title": "Active Requests",
        "targets": [{
          "expr": "active_requests"
        }]
      }
    ]
  }
}
```

---

## ğŸ“š é˜¶æ®µ9ï¼šæ€§èƒ½ä¼˜åŒ–

### ğŸ¯ ä¼˜åŒ–æ¸…å•

```python
ä¼˜åŒ–é¡¹ç›®ï¼š

1. æ¨¡å‹ä¼˜åŒ–
â”œâ”€â”€ âœ… INT8é‡åŒ– (4xå‹ç¼©)
â”œâ”€â”€ â¬œ INT4é‡åŒ– (8xå‹ç¼©, éœ€è¦GPTQ)
â”œâ”€â”€ â¬œ æ¨¡å‹å‰ªæ (å‡å°‘å‚æ•°)
â””â”€â”€ â¬œ çŸ¥è¯†è’¸é¦ (è®­ç»ƒå°æ¨¡å‹)

2. æ¨ç†ä¼˜åŒ–
â”œâ”€â”€ âœ… KV Cache (å·²åœ¨model.pyä¸­)
â”œâ”€â”€ â¬œ vLLM (20xåŠ é€Ÿ)
â”œâ”€â”€ â¬œ TensorRT (æè‡´æ€§èƒ½)
â””â”€â”€ â¬œ Continuous Batching

3. æœåŠ¡ä¼˜åŒ–
â”œâ”€â”€ âœ… å¼‚æ­¥API (FastAPI)
â”œâ”€â”€ â¬œ è¯·æ±‚ç¼“å­˜ (Redis)
â”œâ”€â”€ â¬œ è´Ÿè½½å‡è¡¡ (Nginx)
â””â”€â”€ â¬œ CDNåŠ é€Ÿ

4. åŸºç¡€è®¾æ–½ä¼˜åŒ–
â”œâ”€â”€ âœ… Dockerå®¹å™¨åŒ–
â”œâ”€â”€ âœ… K8sç¼–æ’
â”œâ”€â”€ âœ… è‡ªåŠ¨æ‰©ç¼©å®¹ (HPA)
â””â”€â”€ â¬œ Spotå®ä¾‹ (é™ä½æˆæœ¬70%)
```

### ğŸ’° æˆæœ¬åˆ†æ

```python
# æˆæœ¬è®¡ç®—
def calculate_monthly_cost():
    # é…ç½®
    gpu_type = "A10G"  # $1.00/hour
    num_gpus = 3  # 3ä¸ªå‰¯æœ¬
    hours_per_month = 730
    
    # GPUæˆæœ¬
    gpu_cost = 1.00 * num_gpus * hours_per_month
    
    # é¢„ä¼°è¯·æ±‚é‡
    requests_per_second = 10
    total_requests = requests_per_second * 3600 * hours_per_month
    
    # æ¯åƒæ¬¡è¯·æ±‚æˆæœ¬
    cost_per_1k = (gpu_cost / total_requests) * 1000
    
    print(f"""
æˆæœ¬åˆ†æï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GPUé…ç½®: {num_gpus}x {gpu_type}
æœˆåº¦æˆæœ¬: ${gpu_cost:,.2f}
æœˆåº¦è¯·æ±‚: {total_requests:,.0f}
æ¯1Kè¯·æ±‚æˆæœ¬: ${cost_per_1k:.4f}

ä¼˜åŒ–å»ºè®®:
1. ä½¿ç”¨Spotå®ä¾‹ â†’ èŠ‚çœ70% â†’ ${gpu_cost*0.3:,.2f}/æœˆ
2. ä½¿ç”¨INT4é‡åŒ– â†’ GPUå‡åŠ â†’ ${gpu_cost*0.5:,.2f}/æœˆ
3. ä½¿ç”¨vLLM â†’ ååé‡5x â†’ GPUå‡å°‘80% â†’ ${gpu_cost*0.2:,.2f}/æœˆ

ç»¼åˆä¼˜åŒ–å: ${gpu_cost*0.3*0.5*0.2:,.2f}/æœˆ (èŠ‚çœ97%!)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """)

calculate_monthly_cost()
```

---

## ğŸ¯ æ€»ç»“ï¼šå®Œæ•´éƒ¨ç½²æµç¨‹

```python
ç«¯åˆ°ç«¯æµç¨‹å›é¡¾ï¼š

âœ… é˜¶æ®µ1: æ•°æ®å‡†å¤‡
  â””â”€â”€ æ”¶é›†Pythonä»£ç ï¼Œå‡†å¤‡è®­ç»ƒæ•°æ®

âœ… é˜¶æ®µ2: æ¨¡å‹è®­ç»ƒ
  â””â”€â”€ å•GPUè®­ç»ƒï¼Œ2-3å°æ—¶

âœ… é˜¶æ®µ3: åˆ†å¸ƒå¼åŠ é€Ÿ
  â””â”€â”€ 4 GPUè®­ç»ƒï¼Œ30-40åˆ†é’Ÿ (4xåŠ é€Ÿ)

âœ… é˜¶æ®µ4: æ¨¡å‹ä¼˜åŒ–
  â””â”€â”€ INT8é‡åŒ–ï¼Œ4xå‹ç¼©

âœ… é˜¶æ®µ5: APIæœåŠ¡
  â””â”€â”€ FastAPIæœåŠ¡ï¼Œå»¶è¿Ÿ<200ms

âœ… é˜¶æ®µ6: å®¹å™¨åŒ–
  â””â”€â”€ Dockeré•œåƒï¼Œä¾¿äºéƒ¨ç½²

âœ… é˜¶æ®µ7: K8séƒ¨ç½²
  â””â”€â”€ 3å‰¯æœ¬ï¼Œè‡ªåŠ¨æ‰©ç¼©å®¹

âœ… é˜¶æ®µ8: ç›‘æ§è¿ç»´
  â””â”€â”€ Prometheus + Grafana

âœ… é˜¶æ®µ9: æ€§èƒ½ä¼˜åŒ–
  â””â”€â”€ æˆæœ¬ä¼˜åŒ–ï¼ŒèŠ‚çœ97%

æœ€ç»ˆæˆæœï¼š
  âœ… ç”Ÿäº§çº§ä»£ç è¡¥å…¨æœåŠ¡
  âœ… æ”¯æŒ100+å¹¶å‘ç”¨æˆ·
  âœ… å»¶è¿Ÿ < 200ms
  âœ… æˆæœ¬ < $0.01/1K tokens
  âœ… 99.9% å¯ç”¨æ€§
```

---

## ğŸ“š æ¨èèµ„æº

### å·¥å…·å’Œæ¡†æ¶
- [FastAPI](https://fastapi.tiangolo.com/)
- [vLLM](https://docs.vllm.ai/)
- [Kubernetes](https://kubernetes.io/)
- [Prometheus](https://prometheus.io/)

### æœ€ä½³å®è·µ
- [12-Factor App](https://12factor.net/)
- [Google SRE Book](https://sre.google/books/)

---

**æ­å–œï¼** ä½ å·²ç»å®Œæˆäº†ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼ğŸ‰

