# ç¨€ç–æ¨¡å‹ï¼šMixture of Experts (MoE) å®Œå…¨æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**ä¼ ç»Ÿå¯†é›†æ¨¡å‹çš„å›°å¢ƒï¼š**
```python
GPT-3 (175Bå‚æ•°):
  âœ… æ€§èƒ½å¼ºå¤§
  âŒ è®­ç»ƒæˆæœ¬ï¼š$4.6M
  âŒ æ¨ç†æ…¢ï¼šéœ€è¦A100 Ã— 8
  âŒ æ˜¾å­˜å ç”¨ï¼š700GB
  
é—®é¢˜ï¼šèƒ½å¦ç”¨æ›´å°‘çš„æˆæœ¬è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Ÿ
```

**MoEçš„è§£å†³æ–¹æ¡ˆï¼š**
```python
Switch Transformer (1.6Tå‚æ•°):
  âœ… å‚æ•°é‡ï¼š10å€äºGPT-3
  âœ… è®­ç»ƒæˆæœ¬ï¼š1/7 ($650K)
  âœ… æ¨ç†é€Ÿåº¦ï¼šæ›´å¿«
  âœ… æ€§èƒ½ï¼šæ›´å¥½
  
ç§˜å¯†ï¼šç¨€ç–æ¿€æ´» - æ¯æ¬¡åªç”¨ä¸€å°éƒ¨åˆ†å‚æ•°ï¼
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šMoEåŸºç¡€

### ğŸ” ä»€ä¹ˆæ˜¯MoEï¼Ÿ

```python
ä¼ ç»ŸDenseæ¨¡å‹ï¼ˆå¯†é›†æ¨¡å‹ï¼‰:
  è¾“å…¥ â†’ æ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®¡ç®— â†’ è¾“å‡º
  
  ä¾‹ï¼šGPT-3 175B
  æ¯ä¸ªtokenéƒ½è¦ç»è¿‡å…¨éƒ¨175Bå‚æ•°
  è®¡ç®—é‡ = 175B Ã— åºåˆ—é•¿åº¦

MoEæ¨¡å‹ï¼ˆç¨€ç–æ¨¡å‹ï¼‰:
  è¾“å…¥ â†’ è·¯ç”±å™¨é€‰æ‹©ä¸“å®¶ â†’ åªæœ‰é€‰ä¸­çš„ä¸“å®¶å‚ä¸è®¡ç®— â†’ è¾“å‡º
  
  ä¾‹ï¼šSwitch Transformer 1.6T
  æ¯ä¸ªtokenåªç»è¿‡çº¦10Bå‚æ•°ï¼ˆ1/160ï¼‰
  è®¡ç®—é‡ = 10B Ã— åºåˆ—é•¿åº¦
  
ç»“æœï¼šå‚æ•°å¤š10å€ï¼Œä½†è®¡ç®—é‡ç›¸åŒï¼
```

### ğŸ“Š MoEæ¶æ„

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Mixture of Experts Layer        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥ x
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Router    â”‚  è·¯ç”±å™¨ï¼šå†³å®šç”¨å“ªä¸ªä¸“å®¶
â”‚  (Gating)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  åˆ†å‘åˆ°ä¸åŒä¸“å®¶
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚Expertâ”‚Expertâ”‚Expertâ”‚Expertâ”‚Expertâ”‚  Nä¸ªä¸“å®¶
â”‚  1   â”‚  2   â”‚  3   â”‚  4   â”‚  5   â”‚  (é€šå¸¸8-128ä¸ª)
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
  â†“
  èšåˆä¸“å®¶è¾“å‡º
  â†“
è¾“å‡º

å…³é”®ï¼šæ¯æ¬¡åªæ¿€æ´»1-2ä¸ªä¸“å®¶ï¼ˆç¨€ç–æ¿€æ´»ï¼‰
```

### ğŸ¯ MoEçš„æ ¸å¿ƒç»„ä»¶

```python
1. ä¸“å®¶ï¼ˆExpertsï¼‰
   - æ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„FFN
   - é€šå¸¸æœ‰8-128ä¸ªä¸“å®¶
   - æ¯ä¸ªä¸“å®¶å­¦ä¹ ä¸åŒçš„"ä¸“é•¿"

2. è·¯ç”±å™¨ï¼ˆRouter/Gatingï¼‰
   - å†³å®šæ¯ä¸ªtokenç”¨å“ªä¸ªä¸“å®¶
   - è¾“å‡ºï¼šæ¯ä¸ªä¸“å®¶çš„æƒé‡
   - å…³é”®ï¼šTop-Ké€‰æ‹©ï¼ˆé€šå¸¸K=1æˆ–2ï¼‰

3. è´Ÿè½½å‡è¡¡ï¼ˆLoad Balancingï¼‰
   - ç¡®ä¿æ¯ä¸ªä¸“å®¶éƒ½è¢«å……åˆ†ä½¿ç”¨
   - é˜²æ­¢æ‰€æœ‰tokenéƒ½é€‰åŒä¸€ä¸ªä¸“å®¶
   - ä½¿ç”¨è¾…åŠ©æŸå¤±å‡½æ•°
```

---

## ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šMoEæ•°å­¦åŸç†

### ğŸ“ è·¯ç”±æœºåˆ¶

```python
# åŸºç¡€MoEå…¬å¼
y = Î£ G(x)_i Â· E_i(x)
    i=1..N

å…¶ä¸­ï¼š
  x = è¾“å…¥token
  N = ä¸“å®¶æ•°é‡
  G(x)_i = è·¯ç”±å™¨ç»™ä¸“å®¶içš„æƒé‡
  E_i(x) = ä¸“å®¶içš„è¾“å‡º
  y = æœ€ç»ˆè¾“å‡º

# è·¯ç”±å™¨ï¼ˆSoftmax Gatingï¼‰
G(x) = Softmax(x Â· W_g)

å…¶ä¸­ï¼š
  W_g = è·¯ç”±å™¨æƒé‡çŸ©é˜µ
  G(x) = [g_1, g_2, ..., g_N]  # Nä¸ªä¸“å®¶çš„æƒé‡
  Î£ g_i = 1  # æƒé‡å’Œä¸º1
```

### âš¡ Top-Kè·¯ç”±ï¼ˆç¨€ç–æ¿€æ´»ï¼‰

```python
# Top-K MoEï¼ˆåªé€‰Kä¸ªä¸“å®¶ï¼‰
y = Î£ G(x)_i Â· E_i(x)
    iâˆˆTopK(G(x))

å®ç°ï¼š
def top_k_gating(x, W_g, k=2):
    # 1. è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„logits
    logits = x @ W_g  # [batch, num_experts]
    
    # 2. é€‰æ‹©Top-K
    top_k_logits, top_k_indices = torch.topk(logits, k)
    
    # 3. è®¡ç®—Top-Kçš„æƒé‡ï¼ˆSoftmaxï¼‰
    top_k_gates = F.softmax(top_k_logits, dim=-1)
    
    # 4. åˆ›å»ºç¨€ç–é—¨æ§å‘é‡
    gates = torch.zeros_like(logits)
    gates.scatter_(1, top_k_indices, top_k_gates)
    
    return gates, top_k_indices

# ä½¿ç”¨
gates, indices = top_k_gating(x, W_g, k=2)
output = sum(gates[:, i] * experts[i](x) for i in indices)
```

### ğŸ¯ è´Ÿè½½å‡è¡¡æŸå¤±

```python
# é—®é¢˜ï¼šæ‰€æœ‰tokenå¯èƒ½éƒ½é€‰åŒä¸€ä¸ªä¸“å®¶
# è§£å†³ï¼šæ·»åŠ è¾…åŠ©æŸå¤±ï¼Œé¼“åŠ±å‡åŒ€åˆ†å¸ƒ

# è¾…åŠ©æŸå¤±å‡½æ•°
L_aux = Î± Â· Î£ f_i Â· P_i
        i=1..N

å…¶ä¸­ï¼š
  f_i = ä¸“å®¶iè¢«é€‰ä¸­çš„é¢‘ç‡
  P_i = è·¯ç”±åˆ°ä¸“å®¶içš„æ€»æ¦‚ç‡
  Î± = å¹³è¡¡ç³»æ•°ï¼ˆé€šå¸¸0.01ï¼‰

ç›®æ ‡ï¼šæœ€å°åŒ– L_auxï¼Œä½¿ä¸“å®¶ä½¿ç”¨å‡åŒ€

# å®ç°
def load_balancing_loss(gates, num_experts, alpha=0.01):
    # gates: [batch, seq_len, num_experts]
    
    # 1. è®¡ç®—æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨é¢‘ç‡
    # f_i = è¢«é€‰ä¸­çš„æ¬¡æ•° / æ€»æ¬¡æ•°
    importance = gates.sum(dim=[0, 1])  # [num_experts]
    load = (gates > 0).float().sum(dim=[0, 1])  # [num_experts]
    
    # 2. å½’ä¸€åŒ–
    importance = importance / importance.sum()
    load = load / load.sum()
    
    # 3. è®¡ç®—æŸå¤±
    loss = (importance * load).sum() * num_experts
    
    return alpha * loss

# æ€»æŸå¤±
total_loss = task_loss + load_balancing_loss
```

---

## ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šMoEå®ç°

### ğŸ”§ åŸºç¡€MoEå±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MoELayer(nn.Module):
    def __init__(
        self,
        hidden_size=768,
        num_experts=8,
        expert_capacity=None,
        top_k=2,
        dropout=0.1
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.top_k = top_k
        
        # è·¯ç”±å™¨
        self.router = nn.Linear(hidden_size, num_experts, bias=False)
        
        # ä¸“å®¶ç½‘ç»œï¼ˆæ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªFFNï¼‰
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size * 4),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_size * 4, hidden_size),
                nn.Dropout(dropout)
            )
            for _ in range(num_experts)
        ])
        
        # ä¸“å®¶å®¹é‡ï¼ˆå¯é€‰ï¼Œç”¨äºé™åˆ¶æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°ï¼‰
        self.expert_capacity = expert_capacity
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, hidden_size]
        """
        batch_size, seq_len, hidden_size = x.shape
        
        # 1. è·¯ç”±ï¼šè®¡ç®—æ¯ä¸ªtokenåº”è¯¥ç”¨å“ªä¸ªä¸“å®¶
        router_logits = self.router(x)  # [batch, seq_len, num_experts]
        
        # 2. Top-Ké€‰æ‹©
        top_k_logits, top_k_indices = torch.topk(
            router_logits, self.top_k, dim=-1
        )  # [batch, seq_len, top_k]
        
        # 3. è®¡ç®—é—¨æ§æƒé‡
        top_k_gates = F.softmax(top_k_logits, dim=-1)
        
        # 4. ä¸ºæ¯ä¸ªä¸“å®¶æ”¶é›†token
        # ç®€åŒ–ç‰ˆï¼šç›´æ¥è®¡ç®—ï¼ˆå®é™…åº”è¯¥ç”¨æ›´é«˜æ•ˆçš„åˆ†ç»„æ–¹æ³•ï¼‰
        output = torch.zeros_like(x)
        
        for k in range(self.top_k):
            # è·å–ç¬¬kä¸ªé€‰æ‹©çš„ä¸“å®¶ç´¢å¼•
            expert_indices = top_k_indices[:, :, k]  # [batch, seq_len]
            gates = top_k_gates[:, :, k]  # [batch, seq_len]
            
            # å¯¹æ¯ä¸ªä¸“å®¶
            for expert_id in range(self.num_experts):
                # æ‰¾åˆ°é€‰æ‹©äº†è¿™ä¸ªä¸“å®¶çš„token
                mask = (expert_indices == expert_id)
                
                if mask.any():
                    # æå–è¿™äº›token
                    expert_input = x[mask]  # [num_tokens, hidden_size]
                    
                    # é€šè¿‡ä¸“å®¶ç½‘ç»œ
                    expert_output = self.experts[expert_id](expert_input)
                    
                    # åŠ æƒå¹¶å†™å›
                    expert_gates = gates[mask].unsqueeze(-1)
                    output[mask] += expert_gates * expert_output
        
        # 5. è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆä½œä¸ºè¾…åŠ©è¾“å‡ºï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
        
        return output
```

### ğŸš€ ä¼˜åŒ–çš„MoEå®ç°

```python
class EfficientMoELayer(nn.Module):
    """
    æ›´é«˜æ•ˆçš„MoEå®ç°ï¼Œä½¿ç”¨åˆ†ç»„æ“ä½œ
    """
    def __init__(
        self,
        hidden_size=768,
        num_experts=8,
        top_k=2,
        capacity_factor=1.25,
        dropout=0.1
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.top_k = top_k
        self.capacity_factor = capacity_factor
        
        # è·¯ç”±å™¨
        self.router = nn.Linear(hidden_size, num_experts, bias=False)
        
        # ä¸“å®¶ï¼ˆä½¿ç”¨æ›´é«˜æ•ˆçš„å®ç°ï¼‰
        self.experts = nn.ModuleList([
            Expert(hidden_size, dropout) for _ in range(num_experts)
        ])
    
    def forward(self, x):
        batch_size, seq_len, hidden_size = x.shape
        num_tokens = batch_size * seq_len
        
        # Reshape: [batch, seq_len, hidden] -> [num_tokens, hidden]
        x_flat = x.view(-1, hidden_size)
        
        # 1. è·¯ç”±
        router_logits = self.router(x_flat)  # [num_tokens, num_experts]
        router_probs = F.softmax(router_logits, dim=-1)
        
        # 2. Top-Ké€‰æ‹©
        top_k_probs, top_k_indices = torch.topk(
            router_probs, self.top_k, dim=-1
        )
        
        # 3. é‡æ–°å½’ä¸€åŒ–
        top_k_gates = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)
        
        # 4. è®¡ç®—ä¸“å®¶å®¹é‡
        capacity = int(self.capacity_factor * num_tokens / self.num_experts)
        
        # 5. åˆ†å‘åˆ°ä¸“å®¶å¹¶è®¡ç®—
        output = torch.zeros_like(x_flat)
        
        # ä½¿ç”¨einsumè¿›è¡Œé«˜æ•ˆè®¡ç®—
        for i in range(self.top_k):
            expert_mask = F.one_hot(
                top_k_indices[:, i], self.num_experts
            ).float()  # [num_tokens, num_experts]
            
            for expert_id in range(self.num_experts):
                # è·å–åˆ†é…ç»™è¿™ä¸ªä¸“å®¶çš„token
                token_mask = expert_mask[:, expert_id].bool()
                
                if token_mask.any():
                    # é™åˆ¶å®¹é‡
                    token_indices = torch.where(token_mask)[0]
                    if len(token_indices) > capacity:
                        token_indices = token_indices[:capacity]
                    
                    # ä¸“å®¶è®¡ç®—
                    expert_input = x_flat[token_indices]
                    expert_output = self.experts[expert_id](expert_input)
                    
                    # åŠ æƒè¾“å‡º
                    gates = top_k_gates[token_indices, i].unsqueeze(-1)
                    output[token_indices] += gates * expert_output
        
        # Reshape back
        output = output.view(batch_size, seq_len, hidden_size)
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        aux_loss = self.load_balancing_loss(router_probs, top_k_indices)
        
        return output, aux_loss
    
    def load_balancing_loss(self, router_probs, top_k_indices):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
        # ä¸“å®¶ä½¿ç”¨é¢‘ç‡
        expert_mask = F.one_hot(top_k_indices, self.num_experts).float()
        expert_usage = expert_mask.sum(dim=[0, 1])  # [num_experts]
        expert_usage = expert_usage / expert_usage.sum()
        
        # è·¯ç”±æ¦‚ç‡
        router_prob_per_expert = router_probs.mean(dim=0)  # [num_experts]
        
        # è´Ÿè½½å‡è¡¡æŸå¤±
        loss = (expert_usage * router_prob_per_expert).sum() * self.num_experts
        
        return loss

class Expert(nn.Module):
    """å•ä¸ªä¸“å®¶ç½‘ç»œ"""
    def __init__(self, hidden_size, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        return self.net(x)
```

---

## ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šå°†MoEé›†æˆåˆ°Transformer

### ğŸ”§ MoE Transformer Block

```python
class MoETransformerBlock(nn.Module):
    """
    å¸¦MoEçš„Transformer Block
    """
    def __init__(
        self,
        hidden_size=768,
        num_heads=12,
        num_experts=8,
        top_k=2,
        dropout=0.1
    ):
        super().__init__()
        
        # Self-Attentionï¼ˆä¿æŒä¸å˜ï¼‰
        self.ln1 = nn.LayerNorm(hidden_size)
        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)
        
        # MoE FFNï¼ˆæ›¿æ¢ä¼ ç»ŸFFNï¼‰
        self.ln2 = nn.LayerNorm(hidden_size)
        self.moe = EfficientMoELayer(
            hidden_size, num_experts, top_k, dropout=dropout
        )
    
    def forward(self, x):
        # Self-Attention
        x = x + self.attn(self.ln1(x))
        
        # MoE FFN
        moe_out, aux_loss = self.moe(self.ln2(x))
        x = x + moe_out
        
        return x, aux_loss

class MoEGPT(nn.Module):
    """
    å®Œæ•´çš„MoE GPTæ¨¡å‹
    """
    def __init__(
        self,
        vocab_size=50257,
        block_size=1024,
        n_layer=12,
        n_head=12,
        n_embd=768,
        num_experts=8,
        top_k=2,
        dropout=0.1
    ):
        super().__init__()
        
        # Token + Position Embeddings
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        
        # MoE Transformer Blocks
        self.blocks = nn.ModuleList([
            MoETransformerBlock(n_embd, n_head, num_experts, top_k, dropout)
            for _ in range(n_layer)
        ])
        
        # Output
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
        
        # æƒé‡å…±äº«
        self.token_embedding.weight = self.lm_head.weight
    
    def forward(self, idx, targets=None):
        B, T = idx.shape
        
        # Embeddings
        tok_emb = self.token_embedding(idx)
        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))
        x = tok_emb + pos_emb
        
        # Transformer blocks + æ”¶é›†è¾…åŠ©æŸå¤±
        aux_losses = []
        for block in self.blocks:
            x, aux_loss = block(x)
            aux_losses.append(aux_loss)
        
        # Output
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        # è®¡ç®—æŸå¤±
        loss = None
        if targets is not None:
            # ä¸»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1)
            )
            
            # åŠ ä¸Šè¾…åŠ©æŸå¤±
            aux_loss_total = sum(aux_losses) / len(aux_losses)
            loss = loss + 0.01 * aux_loss_total
        
        return logits, loss
```

---

## ğŸ“š ç¬¬äº”éƒ¨åˆ†ï¼šè®­ç»ƒMoEæ¨¡å‹

### ğŸ”§ è®­ç»ƒè„šæœ¬

```python
import torch
from torch.utils.data import DataLoader

def train_moe_model():
    # 1. åˆ›å»ºæ¨¡å‹
    model = MoEGPT(
        vocab_size=50257,
        block_size=1024,
        n_layer=12,
        n_head=12,
        n_embd=768,
        num_experts=8,
        top_k=2,
        dropout=0.1
    )
    
    # 2. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=3e-4,
        betas=(0.9, 0.95),
        weight_decay=0.1
    )
    
    # 3. è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(num_epochs):
        for batch in dataloader:
            x, y = batch
            
            # å‰å‘ä¼ æ’­
            logits, loss = model(x, targets=y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # æ›´æ–°
            optimizer.step()
            
            # æ—¥å¿—
            if step % 100 == 0:
                print(f"Step {step}, Loss: {loss.item():.4f}")
```

### ğŸ“Š MoEç‰¹æ®Šè€ƒè™‘

```python
è®­ç»ƒMoEçš„å…³é”®ç‚¹ï¼š

1. è´Ÿè½½å‡è¡¡
   - ç›‘æ§æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨ç‡
   - è°ƒæ•´è¾…åŠ©æŸå¤±ç³»æ•°ï¼ˆé€šå¸¸0.01ï¼‰
   - ä½¿ç”¨ä¸“å®¶å®¹é‡é™åˆ¶

2. é€šä¿¡ä¼˜åŒ–ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰
   - ä¸“å®¶å¹¶è¡Œï¼šä¸åŒGPUè´Ÿè´£ä¸åŒä¸“å®¶
   - All-to-Allé€šä¿¡ï¼štokenåˆ†å‘åˆ°ä¸“å®¶
   - éœ€è¦é«˜é€Ÿäº’è”ï¼ˆInfiniBandï¼‰

3. æ˜¾å­˜ç®¡ç†
   - ä¸“å®¶å‚æ•°å¯ä»¥offloadåˆ°CPU
   - åªåŠ è½½éœ€è¦çš„ä¸“å®¶
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

4. è°ƒè¯•æŠ€å·§
   - æ£€æŸ¥ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
   - ç›‘æ§è·¯ç”±å™¨çš„ç†µ
   - å¯è§†åŒ–ä¸“å®¶ä¸“é•¿
```

---

## ğŸ“š ç¬¬å…­éƒ¨åˆ†ï¼šMoEå˜ä½“

### âš¡ Switch Transformer

```python
# Googleçš„Switch Transformer
# ç‰¹ç‚¹ï¼šæ¯ä¸ªtokenåªè·¯ç”±åˆ°1ä¸ªä¸“å®¶ï¼ˆtop_k=1ï¼‰

ä¼˜åŠ¿ï¼š
  âœ… æ›´ç®€å•
  âœ… æ›´å¿«
  âœ… æ›´å®¹æ˜“è®­ç»ƒ

é…ç½®ï¼š
  - ä¸“å®¶æ•°ï¼š128-256
  - Top-Kï¼š1
  - å®¹é‡å› å­ï¼š1.25
  - è¾…åŠ©æŸå¤±ï¼š0.01
```

### ğŸ¯ Expert Choice Routing

```python
# åå‘è·¯ç”±ï¼šä¸“å®¶é€‰æ‹©tokenï¼Œè€Œä¸æ˜¯tokené€‰æ‹©ä¸“å®¶

ä¼ ç»ŸMoEï¼š
  æ¯ä¸ªtokené€‰Top-Kä¸ªä¸“å®¶
  é—®é¢˜ï¼šè´Ÿè½½ä¸å‡è¡¡

Expert Choiceï¼š
  æ¯ä¸ªä¸“å®¶é€‰Top-Kä¸ªtoken
  ä¼˜åŠ¿ï¼šå®Œç¾çš„è´Ÿè½½å‡è¡¡

å®ç°ï¼š
def expert_choice_routing(x, router, k):
    # x: [num_tokens, hidden]
    # router: [hidden, num_experts]
    
    # 1. è®¡ç®—äº²å’Œåº¦
    affinity = x @ router  # [num_tokens, num_experts]
    
    # 2. æ¯ä¸ªä¸“å®¶é€‰æ‹©Top-Kä¸ªtoken
    top_k_tokens = torch.topk(affinity, k, dim=0)
    
    # 3. åˆ†é…
    for expert_id in range(num_experts):
        selected_tokens = top_k_tokens.indices[:, expert_id]
        # å¤„ç†è¿™äº›token
```

### ğŸ”§ Soft MoE

```python
# è½¯è·¯ç”±ï¼šä¸æ˜¯ç¡¬é€‰æ‹©ï¼Œè€Œæ˜¯åŠ æƒå¹³å‡

ä¼ ç»ŸMoEï¼š
  output = Î£ gate_i Â· expert_i(x)  # åªå¯¹Top-K
  
Soft MoEï¼š
  output = Î£ softmax(logits)_i Â· expert_i(x)  # å¯¹æ‰€æœ‰ä¸“å®¶
  
ä¼˜åŠ¿ï¼š
  âœ… å¯å¾®åˆ†
  âœ… ä¸éœ€è¦è´Ÿè½½å‡è¡¡
  
åŠ£åŠ¿ï¼š
  âŒ è®¡ç®—é‡å¤§ï¼ˆä¸ç¨€ç–ï¼‰
  âŒ å¤±å»MoEçš„ä¸»è¦ä¼˜åŠ¿
```

---

## ğŸ“š ç¬¬ä¸ƒéƒ¨åˆ†ï¼šMoEå®æˆ˜æ¡ˆä¾‹

### ğŸ¯ æ¡ˆä¾‹ï¼šè®­ç»ƒ8ä¸“å®¶MoEæ¨¡å‹

```python
# config_moe.py

# æ¨¡å‹é…ç½®
n_layer = 12
n_head = 12
n_embd = 768
block_size = 1024
vocab_size = 50257

# MoEé…ç½®
num_experts = 8
top_k = 2
capacity_factor = 1.25
aux_loss_coef = 0.01

# è®­ç»ƒé…ç½®
batch_size = 4
gradient_accumulation_steps = 16
max_iters = 10000

# ä¼˜åŒ–å™¨
learning_rate = 3e-4
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95

# å­¦ä¹ ç‡è°ƒåº¦
warmup_iters = 1000
lr_decay_iters = 10000
min_lr = 3e-5
```

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

```python
# å¯¹æ¯”å®éªŒï¼šDense vs MoE

Dense GPT (768Må‚æ•°):
  è®­ç»ƒæ—¶é—´: 100 hours
  è®­ç»ƒæˆæœ¬: $1,000
  éªŒè¯Loss: 2.50
  æ¨ç†é€Ÿåº¦: 100 tokens/s

MoE GPT (2.4Bå‚æ•°, 8ä¸“å®¶):
  è®­ç»ƒæ—¶é—´: 120 hours (+20%)
  è®­ç»ƒæˆæœ¬: $1,200 (+20%)
  éªŒè¯Loss: 2.35 (æ›´å¥½!)
  æ¨ç†é€Ÿåº¦: 90 tokens/s (-10%)
  
ç»“è®ºï¼š
  âœ… ç”¨ç›¸ä¼¼çš„æˆæœ¬è·å¾—æ›´å¥½çš„æ€§èƒ½
  âœ… å‚æ•°å¤š3å€ï¼Œä½†è®¡ç®—é‡ç›¸è¿‘
  âš ï¸ æ¨ç†ç¨æ…¢ï¼ˆéœ€è¦è·¯ç”±å¼€é”€ï¼‰
```

---

## ğŸ“š ç¬¬å…«éƒ¨åˆ†ï¼šMoEçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜

### âœ… ä¼˜åŠ¿

```python
1. å‚æ•°æ•ˆç‡
   - å‚æ•°å¤š10å€ï¼Œè®¡ç®—é‡ç›¸åŒ
   - æ›´å¥½çš„æ€§èƒ½/æˆæœ¬æ¯”

2. ä¸“å®¶ä¸“é•¿
   - ä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæŠ€èƒ½
   - è‡ªåŠ¨ä»»åŠ¡åˆ†è§£

3. å¯æ‰©å±•æ€§
   - å®¹æ˜“æ‰©å±•åˆ°è¶…å¤§è§„æ¨¡
   - ä¸“å®¶å¹¶è¡Œè®­ç»ƒ

4. æ¡ä»¶è®¡ç®—
   - æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©è®¡ç®—
   - æ›´çµæ´»
```

### âŒ æŒ‘æˆ˜

```python
1. è®­ç»ƒå¤æ‚åº¦
   - è´Ÿè½½å‡è¡¡å›°éš¾
   - éœ€è¦è¾…åŠ©æŸå¤±
   - è°ƒå‚æ›´å›°éš¾

2. é€šä¿¡å¼€é”€
   - All-to-Allé€šä¿¡
   - éœ€è¦é«˜é€Ÿäº’è”
   - åˆ†å¸ƒå¼è®­ç»ƒæ›´å¤æ‚

3. æ¨ç†æ•ˆç‡
   - è·¯ç”±å¼€é”€
   - æ˜¾å­˜ç¢ç‰‡åŒ–
   - æ‰¹å¤„ç†å›°éš¾

4. å·¥ç¨‹æŒ‘æˆ˜
   - å®ç°å¤æ‚
   - è°ƒè¯•å›°éš¾
   - éƒ¨ç½²å¤æ‚
```

---

## ğŸ¯ æ€»ç»“

### ğŸ“Š MoE vs Dense

```python
é€‰æ‹©Denseæ¨¡å‹çš„åœºæ™¯ï¼š
  âœ… å°è§„æ¨¡æ¨¡å‹ï¼ˆ<1Bå‚æ•°ï¼‰
  âœ… éœ€è¦ç®€å•éƒ¨ç½²
  âœ… æ¨ç†å»¶è¿Ÿæ•æ„Ÿ
  âœ… å•æœºè®­ç»ƒ

é€‰æ‹©MoEæ¨¡å‹çš„åœºæ™¯ï¼š
  âœ… å¤§è§„æ¨¡æ¨¡å‹ï¼ˆ>10Bå‚æ•°ï¼‰
  âœ… è®­ç»ƒé¢„ç®—æœ‰é™
  âœ… è¿½æ±‚æœ€ä½³æ€§èƒ½
  âœ… æœ‰åˆ†å¸ƒå¼è®­ç»ƒèµ„æº
```

### ğŸš€ MoEçš„æœªæ¥

```python
å‘å±•æ–¹å‘ï¼š

1. æ›´é«˜æ•ˆçš„è·¯ç”±
   - Expert Choice
   - åŠ¨æ€ä¸“å®¶æ•°é‡
   - å±‚æ¬¡åŒ–è·¯ç”±

2. æ›´å¥½çš„è´Ÿè½½å‡è¡¡
   - è‡ªé€‚åº”å®¹é‡
   - è½¯çº¦æŸ
   - åœ¨çº¿è°ƒæ•´

3. æ¨ç†ä¼˜åŒ–
   - ä¸“å®¶ç¼“å­˜
   - æ‰¹å¤„ç†ä¼˜åŒ–
   - é‡åŒ–å‹ç¼©

4. æ–°åº”ç”¨
   - å¤šä»»åŠ¡å­¦ä¹ 
   - å¤šæ¨¡æ€æ¨¡å‹
   - æŒç»­å­¦ä¹ 
```

### ğŸ’¡ å®æˆ˜å»ºè®®

```python
å¼€å§‹ä½¿ç”¨MoEï¼š

1. ä»å°è§„æ¨¡å¼€å§‹
   - 4-8ä¸ªä¸“å®¶
   - Top-K=2
   - å•æœºè®­ç»ƒ

2. ç›‘æ§å…³é”®æŒ‡æ ‡
   - ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
   - è¾…åŠ©æŸå¤±
   - è·¯ç”±ç†µ

3. é€æ­¥æ‰©å±•
   - å¢åŠ ä¸“å®¶æ•°é‡
   - å°è¯•ä¸åŒè·¯ç”±ç­–ç•¥
   - ä¼˜åŒ–é€šä¿¡

4. å‚è€ƒå¼€æºå®ç°
   - Fairseq MoE
   - DeepSpeed MoE
   - Mesh TensorFlow
```

---

## ğŸ“š æ¨èèµ„æº

### è®ºæ–‡
- [Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer](https://arxiv.org/abs/1701.06538) - åŸå§‹MoEè®ºæ–‡
- [Switch Transformers](https://arxiv.org/abs/2101.03961) - Googleçš„Switch Transformer
- [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905) - Googleçš„GLaM
- [ST-MoE: Designing Stable and Transferable MoE Models](https://arxiv.org/abs/2202.08906) - ç¨³å®šè®­ç»ƒæŠ€å·§

### å¼€æºå®ç°
- [Fairseq MoE](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm)
- [DeepSpeed MoE](https://www.deepspeed.ai/tutorials/mixture-of-experts/)
- [Mesh TensorFlow](https://github.com/tensorflow/mesh)

### åšå®¢
- [Mixture of Experts Explained](https://huggingface.co/blog/moe)
- [Switch Transformers: Scaling to Trillion Parameter Models](https://ai.googleblog.com/2022/01/switch-transformers-scaling-to-trillion.html)

---

**ä¸‹ä¸€æ­¥ï¼š** å›åˆ°READMEæŸ¥çœ‹å®Œæ•´å­¦ä¹ è·¯çº¿

