# ç¬¬12ç« ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å®Œå…¨æŒ‡å— - ä»é›¶å¼€å§‹

> **å­¦ä¹ ç›®æ ‡**ï¼šç†è§£å¦‚ä½•ç”¨ç¨€ç–æ¿€æ´»å®ç°é«˜æ•ˆçš„è¶…å¤§æ¨¡å‹  
> **éš¾åº¦ç­‰çº§**ï¼šğŸŒ³ è¿›é˜¶ï¼ˆå‰æ²¿æŠ€æœ¯ï¼Œä½†æˆ‘ä»¬ä¼šä»åŸºç¡€è®²èµ·ï¼‰  
> **é¢„è®¡æ—¶é—´**ï¼š2-3å°æ—¶ï¼ˆåˆ†æ­¥å­¦ä¹ ï¼Œå¾ªåºæ¸è¿›ï¼‰  
> **å‰ç½®çŸ¥è¯†**ï¼š05_æ¨¡å‹æ¶æ„æ·±å…¥ç†è§£ï¼ˆå¿…é¡»ï¼‰ã€08_åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå»ºè®®ï¼‰

---

## ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£MoEçš„æ ¸å¿ƒæ€æƒ³ï¼šä¸ºä»€ä¹ˆ"ä¸“å®¶åˆ†å·¥"æ¯”"å…¨æ‰"æ›´é«˜æ•ˆ
- âœ… æŒæ¡ç¨€ç–æ¿€æ´»çš„åŸç†ï¼šå¦‚ä½•ç”¨20%çš„è®¡ç®—è·å¾—100%çš„æ•ˆæœ
- âœ… ç†è§£è·¯ç”±æœºåˆ¶ï¼šæ¨¡å‹å¦‚ä½•è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ä¸“å®¶
- âœ… æŒæ¡è´Ÿè½½å‡è¡¡æŠ€å·§ï¼šé¿å…"ä¸“å®¶é—²ç½®"é—®é¢˜
- âœ… äº†è§£ç»å…¸MoEæ¨¡å‹ï¼šSwitch Transformerã€Mixtralã€GPT-4
- âœ… èƒ½å¤Ÿå®ç°å’Œè®­ç»ƒç®€å•çš„MoEæ¨¡å‹
- âœ… ç†è§£MoEçš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼ŒçŸ¥é“ä½•æ—¶ä½¿ç”¨

---

## ğŸ’­ å¼€å§‹ä¹‹å‰ï¼šä¸ºä»€ä¹ˆè¦å­¦æ··åˆä¸“å®¶æ¨¡å‹ï¼Ÿ

### ğŸ¤” ä¸€ä¸ªçœŸå®çš„å›°å¢ƒ

æƒ³è±¡ä½ æ­£åœ¨è®­ç»ƒä¸€ä¸ªAIæ¨¡å‹ï¼š

```
ç°çŠ¶ï¼šä¼ ç»Ÿå¯†é›†æ¨¡å‹
  GPT-3 (175Bå‚æ•°):
    è®­ç»ƒæˆæœ¬: $4,600,000 ğŸ’¸
    è®­ç»ƒæ—¶é—´: 1ä¸ªæœˆ
    æ¨ç†é€Ÿåº¦: 100 tokens/ç§’
    æ˜¾å­˜éœ€æ±‚: 350GBï¼ˆéœ€è¦8å¼ A100ï¼‰
    
  é—®é¢˜ï¼šèƒ½ä¸èƒ½æ›´ä¾¿å®œã€æ›´å¿«ï¼Ÿ
```

### ğŸ’¡ MoEçš„çªç ´æ€§æƒ³æ³•

**æ ¸å¿ƒæ´å¯Ÿï¼šä¸æ˜¯æ‰€æœ‰ä»»åŠ¡éƒ½éœ€è¦æ‰€æœ‰çŸ¥è¯†ï¼**

å°±åƒç°å®ä¸–ç•Œä¸­ï¼š
```
çœ‹ç—… â†’ æ‰¾åŒ»ç”Ÿ ğŸ‘¨â€âš•ï¸ï¼ˆä¸éœ€è¦å¨å¸ˆï¼‰
ä¿®è½¦ â†’ æ‰¾å·¥ç¨‹å¸ˆ ğŸ‘¨â€ğŸ”§ï¼ˆä¸éœ€è¦åŒ»ç”Ÿï¼‰
åšé¥­ â†’ æ‰¾å¨å¸ˆ ğŸ‘¨â€ğŸ³ï¼ˆä¸éœ€è¦å·¥ç¨‹å¸ˆï¼‰

æ¯ä¸ªä»»åŠ¡åªéœ€è¦ç‰¹å®šé¢†åŸŸçš„ä¸“å®¶ï¼
```

**åº”ç”¨åˆ°AIï¼š**
```
å†™ä»£ç  â†’ æ¿€æ´»"ç¼–ç¨‹ä¸“å®¶"
ç¿»è¯‘æ–‡æœ¬ â†’ æ¿€æ´»"è¯­è¨€ä¸“å®¶"
æ•°å­¦è®¡ç®— â†’ æ¿€æ´»"æ•°å­¦ä¸“å®¶"

æ¯ä¸ªtokenåªç”¨ç›¸å…³çš„ä¸“å®¶ç½‘ç»œï¼
```

### ğŸ¯ MoEçš„ç¥å¥‡æ•ˆæœ

```python
Switch Transformer (1.6Tå‚æ•°):
  âœ… å‚æ•°é‡ï¼š10å€äºGPT-3ï¼ˆ1600B vs 175Bï¼‰
  âœ… è®­ç»ƒæˆæœ¬ï¼š1/7ï¼ˆ$650K vs $4.6Mï¼‰
  âœ… è®­ç»ƒé€Ÿåº¦ï¼š4å€å¿«
  âœ… æ€§èƒ½ï¼šç›¸å½“æˆ–æ›´å¥½
  
ç§˜å¯†æ­¦å™¨ï¼š
  è™½ç„¶æœ‰1.6Tå‚æ•°
  ä½†æ¯ä¸ªtokenåªç”¨çº¦13Bå‚æ•°
  = ç”¨å°æ¨¡å‹çš„æˆæœ¬ï¼Œè·å¾—å¤§æ¨¡å‹çš„èƒ½åŠ›ï¼
```

### ğŸŒŸ å­¦å®Œä¹‹åä½ èƒ½åšä»€ä¹ˆ

- âœ… ç†è§£GPT-4ã€Claude 3ã€Mixtralçš„æ ¸å¿ƒæŠ€æœ¯
- âœ… ç”¨æœ‰é™èµ„æºè®­ç»ƒè¶…å¤§è§„æ¨¡æ¨¡å‹
- âœ… ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡
- âœ… è®¾è®¡é€‚åˆè‡ªå·±é¡¹ç›®çš„MoEæ¶æ„

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šMoEåŸºç¡€æ¦‚å¿µï¼ˆä»é›¶å¼€å§‹ï¼‰

### ğŸŒ± 1.1 ä»€ä¹ˆæ˜¯MoEï¼Ÿç”¨æœ€ç®€å•çš„æ–¹å¼ç†è§£

#### ğŸ’¡ ç”Ÿæ´»ä¸­çš„ç±»æ¯”

**åœºæ™¯1ï¼šåŒ»é™¢çš„ä¸“ç§‘é—¨è¯Š**

```
ä¼ ç»ŸåŒ»é™¢ï¼ˆå¯†é›†æ¨¡å‹ï¼‰:
  æ‚£è€… â†’ å…¨ç§‘åŒ»ç”Ÿçœ‹æ‰€æœ‰ç—…
  
  é—®é¢˜ï¼š
    âŒ å…¨ç§‘åŒ»ç”Ÿä»€ä¹ˆéƒ½æ‡‚ä¸€ç‚¹ï¼Œä½†ä¸ç²¾é€š
    âŒ æ¯ä¸ªç—…äººéƒ½å ç”¨å…¨ç§‘åŒ»ç”Ÿçš„æ—¶é—´
    âŒ æ•ˆç‡ä½ï¼Œè´¨é‡ä¸€èˆ¬

ä¸“ç§‘åŒ»é™¢ï¼ˆMoEæ¨¡å‹ï¼‰:
  æ‚£è€… â†’ åˆ†è¯Šå°åˆ¤æ–­ â†’ ä¸“ç§‘åŒ»ç”Ÿ
  
  å¿ƒè„ç—… â†’ å¿ƒè„ç§‘ä¸“å®¶ â¤ï¸
  éª¨æŠ˜ â†’ éª¨ç§‘ä¸“å®¶ ğŸ¦´
  æ„Ÿå†’ â†’ å‘¼å¸ç§‘ä¸“å®¶ ğŸ«
  
  ä¼˜åŠ¿ï¼š
    âœ… æ¯ä¸ªä¸“å®¶åªç²¾é€šä¸€ä¸ªé¢†åŸŸ
    âœ… æ‚£è€…åªå ç”¨å¯¹åº”ä¸“å®¶çš„æ—¶é—´
    âœ… æ•ˆç‡é«˜ï¼Œè´¨é‡å¥½
```

**è¿™å°±æ˜¯MoEçš„æ ¸å¿ƒæ€æƒ³ï¼**

#### ğŸ“Š æŠ€æœ¯ä¸Šçš„MoE

```python
ä¼ ç»ŸTransformerï¼ˆå¯†é›†æ¨¡å‹ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾“å…¥ï¼š"å†™ä¸€ä¸ªæ’åºç®—æ³•"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FFNå±‚ï¼š175Bå‚æ•°å…¨éƒ¨å‚ä¸è®¡ç®—         â”‚
â”‚  [åŒ»å­¦çŸ¥è¯†][ç¼–ç¨‹çŸ¥è¯†][å†å²çŸ¥è¯†]...   â”‚
â”‚  [éŸ³ä¹çŸ¥è¯†][æ•°å­¦çŸ¥è¯†][æ–‡å­¦çŸ¥è¯†]...   â”‚
â”‚         å…¨éƒ¨éƒ½è¦è®¡ç®—ä¸€é             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
      è®¡ç®—é‡ï¼š175Bæ¬¡ä¹˜æ³•
      æ—¶é—´ï¼šæ…¢ ğŸŒ
      æµªè´¹ï¼šå¤§é‡æ— å…³çŸ¥è¯†å‚ä¸è®¡ç®—

MoE Transformerï¼ˆç¨€ç–æ¨¡å‹ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾“å…¥ï¼š"å†™ä¸€ä¸ªæ’åºç®—æ³•"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è·¯ç”±å™¨ï¼šåˆ†æè¾“å…¥ï¼Œé€‰æ‹©ä¸“å®¶          â”‚
â”‚  "è¿™æ˜¯ç¼–ç¨‹é—®é¢˜ â†’ é€‰æ‹©ç¼–ç¨‹ä¸“å®¶"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚åŒ»å­¦ä¸“å®¶â”‚ç¼–ç¨‹ä¸“å®¶â”‚å†å²ä¸“å®¶â”‚éŸ³ä¹ä¸“å®¶â”‚... (8ä¸ªä¸“å®¶)
â”‚ é—²ç½®   â”‚âœ… æ¿€æ´» â”‚ é—²ç½®   â”‚ é—²ç½®   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    åªæœ‰ç¼–ç¨‹ä¸“å®¶å‚ä¸è®¡ç®—
    è®¡ç®—é‡ï¼š175B/8 â‰ˆ 22Bæ¬¡ä¹˜æ³•
    æ—¶é—´ï¼šå¿« ğŸš€
    é«˜æ•ˆï¼šåªç”¨ç›¸å…³çŸ¥è¯†
```

#### ğŸ”¢ æ•°å­¦ä¸Šçš„å®šä¹‰

```python
# å¯†é›†FFNï¼ˆä¼ ç»Ÿï¼‰
def dense_ffn(x):
    """æ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®¡ç®—"""
    h = W1 @ x  # W1: [d_model, d_ff]
    h = activation(h)
    output = W2 @ h  # W2: [d_ff, d_model]
    return output

# å‚æ•°é‡ï¼šd_model Ã— d_ff Ã— 2
# è®¡ç®—é‡ï¼šæ‰€æœ‰å‚æ•°éƒ½ç”¨ä¸Š

# MoE FFNï¼ˆç¨€ç–ï¼‰
def moe_ffn(x, num_experts=8, top_k=2):
    """åªæœ‰é€‰ä¸­çš„ä¸“å®¶å‚ä¸è®¡ç®—"""
    
    # 1. è·¯ç”±ï¼šé€‰æ‹©ä¸“å®¶
    router_logits = router(x)  # [num_experts]
    top_k_experts = topk(router_logits, k=top_k)  # é€‰2ä¸ª
    
    # 2. åªè®¡ç®—é€‰ä¸­çš„ä¸“å®¶
    outputs = []
    for expert_id in top_k_experts:
        expert_out = experts[expert_id](x)
        outputs.append(expert_out)
    
    # 3. åŠ æƒç»„åˆ
    output = weighted_sum(outputs)
    return output

# å‚æ•°é‡ï¼šd_model Ã— d_ff Ã— 2 Ã— num_expertsï¼ˆ8å€ï¼ï¼‰
# è®¡ç®—é‡ï¼šåªç”¨top_kä¸ªä¸“å®¶ï¼ˆ1/4ï¼‰
# ç»“æœï¼šå‚æ•°å¤šï¼Œä½†è®¡ç®—å°‘ï¼
```

---

### ğŸŒ± 1.2 MoEçš„ä¸‰å¤§æ ¸å¿ƒç»„ä»¶

#### ğŸ¯ ç»„ä»¶1ï¼šä¸“å®¶ç½‘ç»œï¼ˆExpertsï¼‰

**æ˜¯ä»€ä¹ˆï¼Ÿ**  
æ¯ä¸ªä¸“å®¶å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œæœ‰è‡ªå·±çš„å‚æ•°ã€‚

```python
# ä¼ ç»ŸTransformerçš„FFN
class FFN:
    W1: [768, 3072]   # 2.4Må‚æ•°
    W2: [3072, 768]   # 2.4Må‚æ•°
    æ€»å‚æ•°ï¼š4.8M

# MoEçš„8ä¸ªä¸“å®¶
class MoE:
    Expert_0: FFN()   # 4.8Må‚æ•°
    Expert_1: FFN()   # 4.8Må‚æ•°
    Expert_2: FFN()   # 4.8Må‚æ•°
    Expert_3: FFN()   # 4.8Må‚æ•°
    Expert_4: FFN()   # 4.8Må‚æ•°
    Expert_5: FFN()   # 4.8Må‚æ•°
    Expert_6: FFN()   # 4.8Må‚æ•°
    Expert_7: FFN()   # 4.8Må‚æ•°
    æ€»å‚æ•°ï¼š38.4Mï¼ˆ8å€ï¼ï¼‰
```

**å…³é”®é—®é¢˜ï¼šæ¯ä¸ªä¸“å®¶å­¦ä»€ä¹ˆï¼Ÿ**

```python
è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸“å®¶ä¼šè‡ªåŠ¨ä¸“ç²¾åŒ–ï¼š

å¯èƒ½çš„åˆ†å·¥ï¼ˆæ¨¡å‹è‡ªå·±å­¦å‡ºæ¥çš„ï¼‰ï¼š
  Expert_0: æ“…é•¿ä»£ç å’ŒæŠ€æœ¯å†…å®¹
  Expert_1: æ“…é•¿æ—¥å¸¸å¯¹è¯å’Œé—²èŠ
  Expert_2: æ“…é•¿æ•°å­¦å’Œé€»è¾‘æ¨ç†
  Expert_3: æ“…é•¿åˆ›æ„å†™ä½œå’Œæ–‡å­¦
  Expert_4: æ“…é•¿å¤šè¯­è¨€ç¿»è¯‘
  Expert_5: æ“…é•¿ç§‘å­¦å’Œå­¦æœ¯å†…å®¹
  Expert_6: æ“…é•¿å†å²å’Œæ–‡åŒ–çŸ¥è¯†
  Expert_7: æ“…é•¿å¸¸è¯†å’Œäº‹å®æ€§é—®ç­”

æ³¨æ„ï¼šè¿™ä¸ªåˆ†å·¥æ˜¯æ¨¡å‹è‡ªåŠ¨å­¦å‡ºæ¥çš„ï¼Œ
ä¸æ˜¯äººå·¥è®¾è®¡çš„ï¼
```

#### ğŸ¯ ç»„ä»¶2ï¼šè·¯ç”±å™¨ï¼ˆRouter/Gateï¼‰

**æ˜¯ä»€ä¹ˆï¼Ÿ**  
è·¯ç”±å™¨å†³å®šå¯¹äºæ¯ä¸ªè¾“å…¥tokenï¼Œåº”è¯¥ç”¨å“ªä¸ªï¼ˆæˆ–å“ªäº›ï¼‰ä¸“å®¶ã€‚

**ç”Ÿæ´»ç±»æ¯”ï¼š**
```
è·¯ç”±å™¨ = åŒ»é™¢çš„åˆ†è¯Šå°
  
  æ‚£è€…è¯´ï¼š"æˆ‘å¤´ç–¼"
  åˆ†è¯Šå°åˆ†æï¼š
    - å¯èƒ½æ˜¯ç¥ç»ç§‘é—®é¢˜ï¼ˆ80%ï¼‰
    - å¯èƒ½æ˜¯äº”å®˜ç§‘é—®é¢˜ï¼ˆ20%ï¼‰
  
  å†³å®šï¼šæŒ‚ç¥ç»ç§‘ + äº”å®˜ç§‘
```

**æŠ€æœ¯å®ç°ï¼š**
```python
class Router(nn.Module):
    def __init__(self, d_model, num_experts):
        self.gate = nn.Linear(d_model, num_experts)
    
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        
        # 1. è®¡ç®—æ¯ä¸ªä¸“å®¶çš„"åŒ¹é…åº¦"
        logits = self.gate(x)  # [batch, seq_len, num_experts]
        
        # 2. è½¬æ¢ä¸ºæ¦‚ç‡
        probs = softmax(logits, dim=-1)
        
        # 3. é€‰æ‹©Top-Kä¸ªä¸“å®¶
        top_k_probs, top_k_indices = topk(probs, k=2)
        
        # 4. é‡æ–°å½’ä¸€åŒ–
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1)
        
        return top_k_indices, top_k_probs

# ç¤ºä¾‹
router = Router(d_model=768, num_experts=8)
x = ... # è¾“å…¥tokençš„embedding

indices, probs = router(x)
# indices: [batch, seq_len, 2] - é€‰ä¸­çš„ä¸“å®¶ç¼–å·
# probs: [batch, seq_len, 2] - å¯¹åº”çš„æƒé‡

# ä¾‹å¦‚ï¼š
# indices[0][0] = [2, 5]  # é€‰ä¸­ä¸“å®¶2å’Œ5
# probs[0][0] = [0.7, 0.3]  # æƒé‡70%å’Œ30%
```

#### ğŸ¯ ç»„ä»¶3ï¼šè´Ÿè½½å‡è¡¡ï¼ˆLoad Balancingï¼‰

**é—®é¢˜ï¼šä¸ºä»€ä¹ˆéœ€è¦è´Ÿè½½å‡è¡¡ï¼Ÿ**

```python
æ²¡æœ‰è´Ÿè½½å‡è¡¡çš„æƒ…å†µï¼š

è®­ç»ƒåˆæœŸï¼Œå¯èƒ½å‡ºç°ï¼š
  Expert_0: å¤„ç†äº†80%çš„token ğŸ˜“ï¼ˆè¿‡è½½ï¼ï¼‰
  Expert_1: å¤„ç†äº†10%çš„token
  Expert_2: å¤„ç†äº†5%çš„token
  Expert_3-7: å‡ ä¹æ²¡ç”¨ ğŸ˜´ï¼ˆæµªè´¹ï¼ï¼‰

é—®é¢˜ï¼š
  âŒ Expert_0è¿‡è½½ï¼Œæˆä¸ºç“¶é¢ˆ
  âŒ å…¶ä»–ä¸“å®¶é—²ç½®ï¼Œå‚æ•°æµªè´¹
  âŒ å¤±å»äº†MoEçš„ä¼˜åŠ¿
```

**è§£å†³æ–¹æ¡ˆï¼šè¾…åŠ©æŸå¤±å‡½æ•°**

```python
# ä¸»æŸå¤±ï¼šè¯­è¨€æ¨¡å‹çš„äº¤å‰ç†µ
main_loss = cross_entropy(predictions, targets)

# è¾…åŠ©æŸå¤±ï¼šé¼“åŠ±ä¸“å®¶ä½¿ç”¨å‡åŒ€
def load_balance_loss(router_probs):
    # router_probs: æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¦‚ç‡
    
    # 1. è®¡ç®—æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨é¢‘ç‡
    expert_usage = router_probs.mean(dim=[0, 1])
    # expert_usage: [num_experts]
    # ä¾‹å¦‚ï¼š[0.4, 0.3, 0.1, 0.05, 0.05, 0.05, 0.03, 0.02]
    
    # 2. ç†æƒ³æƒ…å†µï¼šæ¯ä¸ªä¸“å®¶ä½¿ç”¨1/8 = 0.125
    target = 1.0 / num_experts  # 0.125
    
    # 3. æƒ©ç½šåç¦»
    loss = ((expert_usage - target) ** 2).sum()
    
    return loss

# æ€»æŸå¤±
total_loss = main_loss + 0.01 * load_balance_loss
```

---

### ğŸŒ± 1.3 å®Œæ•´çš„MoEå·¥ä½œæµç¨‹

#### ğŸ¬ é€æ­¥æ¼”ç¤º

```python
è¾“å…¥å¥å­ï¼š"å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åº"

æ­¥éª¤1ï¼šTokenization
  tokens = ["å†™", "ä¸€ä¸ª", "Python", "å¿«é€Ÿ", "æ’åº"]

æ­¥éª¤2ï¼šEmbedding
  embeddings = [e1, e2, e3, e4, e5]
  æ¯ä¸ªembedding: [768ç»´å‘é‡]

æ­¥éª¤3ï¼šç»è¿‡Attentionå±‚ï¼ˆæ­£å¸¸ï¼‰
  contextualized = self_attention(embeddings)

æ­¥éª¤4ï¼šè¿›å…¥MoEå±‚ï¼ˆå…³é”®ï¼ï¼‰

  å¯¹äºæ¯ä¸ªtokenï¼š
  
  token "Python"çš„å¤„ç†:
  
  4.1 è·¯ç”±å™¨åˆ†æ
      router_input = contextualized[2]  # "Python"çš„å‘é‡
      logits = router(router_input)
      # logits = [0.2, 3.5, 0.1, -0.5, 0.8, 0.3, -0.2, 0.1]
      #         â†‘    â†‘                   â†‘
      #        ä¸“å®¶0 ä¸“å®¶1               ä¸“å®¶4
      
  4.2 é€‰æ‹©Top-2ä¸“å®¶
      probs = softmax(logits)
      # probs = [0.05, 0.75, 0.03, 0.01, 0.08, 0.04, 0.02, 0.03]
      
      top_2_experts = [1, 4]  # ä¸“å®¶1å’Œ4
      top_2_probs = [0.90, 0.10]  # å½’ä¸€åŒ–åçš„æƒé‡
      
      è§£é‡Šï¼š
        è·¯ç”±å™¨è®¤ä¸º"Python"åº”è¯¥ä¸»è¦ç”¨ä¸“å®¶1ï¼ˆ90%ï¼‰
        å°‘é‡ç”¨ä¸“å®¶4ï¼ˆ10%ï¼‰
      
  4.3 ä¸“å®¶è®¡ç®—
      expert_1_output = Expert_1(router_input)
      expert_4_output = Expert_4(router_input)
      
  4.4 åŠ æƒç»„åˆ
      output = 0.90 * expert_1_output + 0.10 * expert_4_output

æ­¥éª¤5ï¼šç»§ç»­åç»­å±‚
  æœ€ç»ˆè¾“å‡ºï¼šç”Ÿæˆçš„ä»£ç 
```

#### ğŸ“Š å¯è§†åŒ–æ•´ä¸ªè¿‡ç¨‹

```
Tokenåºåˆ—ï¼š["å†™", "ä¸€ä¸ª", "Python", "å¿«é€Ÿ", "æ’åº"]
            â†“      â†“       â†“        â†“      â†“

è·¯ç”±å†³ç­–ï¼š
  "å†™"    â†’ Expert_1 (60%) + Expert_3 (40%)
  "ä¸€ä¸ª"  â†’ Expert_1 (80%) + Expert_5 (20%)
  "Python"â†’ Expert_0 (90%) + Expert_2 (10%)  â† ç¼–ç¨‹ç›¸å…³
  "å¿«é€Ÿ"  â†’ Expert_2 (70%) + Expert_6 (30%)
  "æ’åº"  â†’ Expert_0 (85%) + Expert_2 (15%)  â† ç¼–ç¨‹ç›¸å…³

è§‚å¯Ÿï¼š
  âœ… "Python"å’Œ"æ’åº"ä¸»è¦ç”¨Expert_0ï¼ˆç¼–ç¨‹ä¸“å®¶ï¼‰
  âœ… ä¸åŒtokenç”¨ä¸åŒä¸“å®¶
  âœ… æ¯ä¸ªtokenåªç”¨2ä¸ªä¸“å®¶ï¼ˆä¸æ˜¯å…¨éƒ¨8ä¸ªï¼‰
```

---

### ğŸŒ± 1.4 MoE vs å¯†é›†æ¨¡å‹ï¼šç›´è§‚å¯¹æ¯”

#### ğŸ“Š å‚æ•°é‡å¯¹æ¯”

```python
åœºæ™¯ï¼š12å±‚Transformerï¼Œd_model=768

å¯†é›†æ¨¡å‹ï¼š
  æ¯å±‚FFN: 768 Ã— 3072 Ã— 2 â‰ˆ 4.7Må‚æ•°
  12å±‚FFN: 4.7M Ã— 12 â‰ˆ 56Må‚æ•°
  æ€»å‚æ•°ï¼ˆåŒ…æ‹¬Attentionç­‰ï¼‰: â‰ˆ 125M

MoEæ¨¡å‹ï¼ˆ8ä¸“å®¶ï¼Œæ¯å±‚éƒ½æ˜¯MoEï¼‰ï¼š
  æ¯å±‚MoE: 4.7M Ã— 8 â‰ˆ 38Må‚æ•°
  12å±‚MoE: 38M Ã— 12 â‰ˆ 456Må‚æ•°
  æ€»å‚æ•°ï¼ˆåŒ…æ‹¬Attentionç­‰ï¼‰: â‰ˆ 525M

å‚æ•°æ¯”ï¼š525M / 125M â‰ˆ 4.2å€
```

#### âš¡ è®¡ç®—é‡å¯¹æ¯”

```python
å¤„ç†1ä¸ªtokenï¼š

å¯†é›†æ¨¡å‹ï¼š
  FFNè®¡ç®—: 768 Ã— 3072 Ã— 2 = 4.7Mæ¬¡ä¹˜æ³•
  12å±‚: 4.7M Ã— 12 = 56Mæ¬¡ä¹˜æ³•

MoEæ¨¡å‹ï¼ˆTop-2è·¯ç”±ï¼‰ï¼š
  æ¯å±‚ç”¨2ä¸ªä¸“å®¶: 4.7M Ã— 2 = 9.4Mæ¬¡ä¹˜æ³•
  12å±‚: 9.4M Ã— 12 = 113Mæ¬¡ä¹˜æ³•
  è·¯ç”±å¼€é”€: 768 Ã— 8 = 6Kæ¬¡ä¹˜æ³•ï¼ˆå¯å¿½ç•¥ï¼‰

è®¡ç®—æ¯”ï¼š113M / 56M â‰ˆ 2å€

ç»“æœï¼š
  å‚æ•°é‡ï¼š4.2å€ â†‘
  è®¡ç®—é‡ï¼š2å€ â†‘
  
  æ€§èƒ½æå‡ï¼šé€šå¸¸ > 2å€ âœ…
  â†’ æŠ•å…¥äº§å‡ºæ¯”å¾ˆå¥½ï¼
```

#### ğŸ’¾ æ˜¾å­˜ä½¿ç”¨å¯¹æ¯”

```python
è®­ç»ƒæ—¶ï¼ˆbatch_size=32, seq_len=512ï¼‰ï¼š

å¯†é›†æ¨¡å‹ï¼ˆ125Må‚æ•°ï¼‰ï¼š
  æ¨¡å‹å‚æ•°: 125M Ã— 4å­—èŠ‚ = 500MB
  æ¢¯åº¦: 500MB
  ä¼˜åŒ–å™¨çŠ¶æ€: 1000MBï¼ˆAdamWï¼‰
  æ¿€æ´»å€¼: ~2000MB
  æ€»è®¡: ~4GB âœ…

MoEæ¨¡å‹ï¼ˆ525Må‚æ•°ï¼Œä½†åªæ¿€æ´»éƒ¨åˆ†ï¼‰ï¼š
  æ¨¡å‹å‚æ•°: 525M Ã— 4å­—èŠ‚ = 2100MB
  æ¢¯åº¦: 2100MBï¼ˆæ‰€æœ‰ä¸“å®¶éƒ½è¦å­˜ï¼‰
  ä¼˜åŒ–å™¨çŠ¶æ€: 4200MB
  æ¿€æ´»å€¼: ~2500MBï¼ˆåªæœ‰æ¿€æ´»çš„ä¸“å®¶ï¼‰
  æ€»è®¡: ~11GB âš ï¸

å…³é”®ï¼š
  è™½ç„¶è®¡ç®—å°‘ï¼Œä½†æ‰€æœ‰ä¸“å®¶çš„å‚æ•°éƒ½è¦å­˜åœ¨æ˜¾å­˜ä¸­
  â†’ æ˜¾å­˜éœ€æ±‚æ›´é«˜
```

#### ğŸ¯ æ€§èƒ½å¯¹æ¯”è¡¨

| æŒ‡æ ‡ | å¯†é›†æ¨¡å‹ | MoEæ¨¡å‹ | è¯´æ˜ |
|------|---------|---------|------|
| å‚æ•°é‡ | 125M | 525M (4.2Ã—) | MoEå¤šå¾ˆå¤šå‚æ•° |
| æ¿€æ´»å‚æ•° | 125M | ~180M (1.4Ã—) | ä½†æ¯æ¬¡åªç”¨éƒ¨åˆ† |
| è®¡ç®—é‡ | 100% | ~200% | ç•¥é«˜ï¼Œä½†å¯æ¥å— |
| æ˜¾å­˜éœ€æ±‚ | 4GB | 11GB (2.8Ã—) | ä¸»è¦æŒ‘æˆ˜ |
| è®­ç»ƒé€Ÿåº¦ | 100% | ~80% | è·¯ç”±æœ‰å¼€é”€ |
| æ¨ç†é€Ÿåº¦ | 100% | ~90% | ç•¥æ…¢ä½†ä¸å¤š |
| æ¨¡å‹è´¨é‡ | åŸºå‡† | +20~40% | æ˜¾è‘—æå‡ âœ… |

#### ğŸ¤” ä»€ä¹ˆæ—¶å€™å€¼å¾—ç”¨MoEï¼Ÿ

```python
âœ… é€‚åˆMoEï¼š
  - å¤§è§„æ¨¡æ¨¡å‹ï¼ˆ>1Bå‚æ•°ï¼‰
  - å¤šæ ·åŒ–æ•°æ®ï¼ˆå¤šè¯­è¨€ã€å¤šé¢†åŸŸï¼‰
  - æœ‰è¶³å¤Ÿæ˜¾å­˜ï¼ˆè‡³å°‘2Ã—A100ï¼‰
  - è¿½æ±‚æ€§èƒ½/æˆæœ¬æ¯”
  
  ä¾‹å­ï¼šGPT-4, Claude 3, Mixtral

âŒ ä¸é€‚åˆMoEï¼š
  - å°æ¨¡å‹ï¼ˆ<500Må‚æ•°ï¼‰
  - å•ä¸€ä»»åŠ¡
  - æ˜¾å­˜å—é™ï¼ˆå•GPUï¼‰
  - è¿½æ±‚æœ€ç®€å•éƒ¨ç½²
  
  ä¾‹å­ï¼šBERT, DistilBERT, å°å‹å¯¹è¯æ¨¡å‹
```

---

## ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šMoEæ•°å­¦åŸç†ï¼ˆä»ç®€å•åˆ°å¤æ‚ï¼‰

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šç†è§£MoEçš„æ•°å­¦æœºåˆ¶ï¼ŒçŸ¥é“"ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡"

### ğŸŒ± 2.1 æœ€ç®€å•çš„MoEå…¬å¼

#### ğŸ’¡ ä»ç›´è§‰å¼€å§‹

**é—®é¢˜**ï¼šå¦‚ä½•ç»„åˆå¤šä¸ªä¸“å®¶çš„è¾“å‡ºï¼Ÿ

**æœ€ç®€å•çš„æƒ³æ³•ï¼šåŠ æƒå¹³å‡**

```python
ç”Ÿæ´»ä¾‹å­ï¼šä¹°æ‰‹æœºæ—¶å’¨è¯¢å¤šä¸ªæœ‹å‹

æœ‹å‹Aï¼ˆæŠ€æœ¯ä¸“å®¶ï¼‰è¯´ï¼šä¹°è¿™æ¬¾ï¼Œè¯„åˆ†9åˆ†
æœ‹å‹Bï¼ˆçœé’±ä¸“å®¶ï¼‰è¯´ï¼šä¹°é‚£æ¬¾ï¼Œè¯„åˆ†7åˆ†
æœ‹å‹Cï¼ˆå¤–è§‚ä¸“å®¶ï¼‰è¯´ï¼šä¹°å¦ä¸€æ¬¾ï¼Œè¯„åˆ†8åˆ†

ä½ çš„å†³ç­–ï¼š
  - æŠ€æœ¯æœ€é‡è¦ï¼Œæƒé‡50%
  - ä»·æ ¼å…¶æ¬¡ï¼Œæƒé‡30%
  - å¤–è§‚ç¬¬ä¸‰ï¼Œæƒé‡20%
  
  æœ€ç»ˆåˆ†æ•° = 9Ã—0.5 + 7Ã—0.3 + 8Ã—0.2
           = 4.5 + 2.1 + 1.6
           = 8.2åˆ†
```

**åº”ç”¨åˆ°MoEï¼š**

```python
# åŸºç¡€å…¬å¼ï¼ˆæœ€ç®€å•å½¢å¼ï¼‰
è¾“å‡º = Î£ (æƒé‡_i Ã— ä¸“å®¶_içš„è¾“å‡º)
      i=1..N

ç”¨æ•°å­¦ç¬¦å·ï¼š
y = Î£ G(x)_i Â· E_i(x)
    i=1..N

å…¶ä¸­ï¼š
  x = è¾“å…¥ï¼ˆæŸä¸ªtokençš„embeddingï¼‰
  N = ä¸“å®¶æ€»æ•°ï¼ˆæ¯”å¦‚8ä¸ªï¼‰
  G(x)_i = è·¯ç”±å™¨ç»™ä¸“å®¶içš„æƒé‡ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰
  E_i(x) = ä¸“å®¶iå¤„ç†è¾“å…¥xåçš„è¾“å‡º
  y = æœ€ç»ˆè¾“å‡ºï¼ˆèåˆäº†æ‰€æœ‰ä¸“å®¶çš„æ™ºæ…§ï¼‰

çº¦æŸæ¡ä»¶ï¼š
  Î£ G(x)_i = 1  ï¼ˆæ‰€æœ‰æƒé‡åŠ èµ·æ¥=1ï¼‰
  G(x)_i â‰¥ 0    ï¼ˆæƒé‡ä¸èƒ½ä¸ºè´Ÿï¼‰
```

#### ğŸ“Š å…·ä½“æ•°å€¼ä¾‹å­

```python
å‡è®¾ï¼š8ä¸ªä¸“å®¶ï¼Œè¾“å…¥token "Python"

æ­¥éª¤1ï¼šè·¯ç”±å™¨è®¡ç®—æƒé‡
  G(x) = [0.05, 0.60, 0.15, 0.02, 0.08, 0.05, 0.03, 0.02]
         â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘
        ä¸“å®¶0 ä¸“å®¶1 ä¸“å®¶2 ä¸“å®¶3 ä¸“å®¶4 ä¸“å®¶5 ä¸“å®¶6 ä¸“å®¶7
  
  éªŒè¯ï¼š0.05+0.60+0.15+...+0.02 = 1.00 âœ…

æ­¥éª¤2ï¼šæ¯ä¸ªä¸“å®¶è®¡ç®—è¾“å‡ºï¼ˆ768ç»´å‘é‡ï¼‰
  E_0(x) = [0.1, 0.2, -0.3, ..., 0.5]  # 768ç»´
  E_1(x) = [0.5, 0.8, 0.2, ..., 0.9]
  E_2(x) = [0.3, 0.1, 0.4, ..., 0.7]
  ... ï¼ˆ8ä¸ªä¸“å®¶éƒ½è®¡ç®—ï¼‰

æ­¥éª¤3ï¼šåŠ æƒç»„åˆ
  y = 0.05Ã—E_0(x) + 0.60Ã—E_1(x) + 0.15Ã—E_2(x) + ...
  
  æ¯ä¸ªç»´åº¦éƒ½è¿™æ ·è®¡ç®—ï¼š
    y[0] = 0.05Ã—0.1 + 0.60Ã—0.5 + 0.15Ã—0.3 + ...
         = 0.005 + 0.300 + 0.045 + ...
         = 0.XXX

ç»“æœï¼šèåˆäº†æ‰€æœ‰ä¸“å®¶çš„è¾“å‡ºï¼Œä½†ä¸“å®¶1çš„è´¡çŒ®æœ€å¤§ï¼ˆ60%ï¼‰
```

---

### ğŸŒ± 2.2 è·¯ç”±å™¨ï¼šå¦‚ä½•è®¡ç®—æƒé‡ï¼Ÿ

#### ğŸ’¡ è·¯ç”±å™¨çš„å·¥ä½œåŸç†

**æ ¸å¿ƒé—®é¢˜**ï¼šç»™å®šè¾“å…¥xï¼Œå¦‚ä½•å†³å®šæ¯ä¸ªä¸“å®¶çš„æƒé‡ï¼Ÿ

**æ–¹æ³•ï¼šå¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ + Softmax**

```python
# æ­¥éª¤1ï¼šçº¿æ€§å˜æ¢
logits = x @ W_g + b_g

å…¶ä¸­ï¼š
  x: [d_model] - è¾“å…¥å‘é‡ï¼ˆæ¯”å¦‚768ç»´ï¼‰
  W_g: [d_model, num_experts] - è·¯ç”±å™¨æƒé‡çŸ©é˜µ
  b_g: [num_experts] - åç½®ï¼ˆé€šå¸¸ä¸ç”¨ï¼‰
  logits: [num_experts] - åŸå§‹åˆ†æ•°

# æ­¥éª¤2ï¼šSoftmaxå½’ä¸€åŒ–
G(x) = Softmax(logits)
     = exp(logits_i) / Î£ exp(logits_j)

ä½œç”¨ï¼š
  âœ… ä¿è¯æƒé‡å’Œä¸º1
  âœ… ä¿è¯æƒé‡éƒ½æ˜¯æ­£æ•°
  âœ… å¯å¾®åˆ†ï¼Œå¯ä»¥è®­ç»ƒ
```

#### ğŸ“Š å®Œæ•´è®¡ç®—æµç¨‹

```python
è¾“å…¥ï¼štoken "Python"çš„embedding

x = [0.23, -0.15, 0.87, ..., 0.34]  # 768ç»´

æ­¥éª¤1ï¼šçŸ©é˜µä¹˜æ³•
  W_g çš„å½¢çŠ¶: [768, 8]
  logits = x @ W_g
         = [0.23, -0.15, ...] @ W_g
         = [2.1, 5.3, 3.2, 0.8, 2.5, 1.9, 1.2, 0.5]
           â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘
          ä¸“å®¶0 ä¸“å®¶1 ä¸“å®¶2 ...

  è§£é‡Šï¼š
    ä¸“å®¶1çš„logit=5.3æœ€é«˜ â†’ å¯èƒ½æœ€åˆé€‚
    ä¸“å®¶2çš„logit=3.2æ¬¡é«˜ â†’ ä¹Ÿæ¯”è¾ƒåˆé€‚

æ­¥éª¤2ï¼šSoftmaxå½’ä¸€åŒ–
  exp(logits) = [8.2, 200.3, 24.5, 2.2, 12.2, 6.7, 3.3, 1.6]
  
  sum = 8.2 + 200.3 + ... + 1.6 = 258.9
  
  G(x) = exp(logits) / sum
       = [0.03, 0.77, 0.09, 0.01, 0.05, 0.03, 0.01, 0.01]
         â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘
        3%   77%   9%    1%    5%    3%    1%    1%

  ç»“è®ºï¼š
    ä¸“å®¶1è·å¾—77%çš„æƒé‡ â†’ ä¸»è¦ç”±å®ƒå¤„ç†
    ä¸“å®¶2è·å¾—9%çš„æƒé‡ â†’ è¾…åŠ©å¤„ç†
    å…¶ä»–ä¸“å®¶æƒé‡å¾ˆå° â†’ å‡ ä¹ä¸å‚ä¸
```

#### ğŸ§ª PyTorchå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleRouter(nn.Module):
    """æœ€ç®€å•çš„è·¯ç”±å™¨å®ç°"""
    def __init__(self, d_model, num_experts):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        
        # å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µ
        self.gate = nn.Linear(d_model, num_experts, bias=False)
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        è¿”å›: gates [batch_size, seq_len, num_experts]
        """
        # 1. çº¿æ€§å˜æ¢
        logits = self.gate(x)  # [batch, seq_len, num_experts]
        
        # 2. Softmaxå½’ä¸€åŒ–
        gates = F.softmax(logits, dim=-1)
        
        return gates

# ä½¿ç”¨ç¤ºä¾‹
router = SimpleRouter(d_model=768, num_experts=8)
x = torch.randn(2, 10, 768)  # batch=2, seq_len=10

gates = router(x)
print(gates.shape)  # [2, 10, 8]
print(gates[0, 0])  # ç¬¬1ä¸ªbatchï¼Œç¬¬1ä¸ªtokençš„æƒé‡
# tensor([0.05, 0.60, 0.15, 0.02, 0.08, 0.05, 0.03, 0.02])
print(gates[0, 0].sum())  # åº”è¯¥ç­‰äº1.0
# tensor(1.0000)
```

---

### ğŸŒ³ 2.3 Top-Kç¨€ç–è·¯ç”±ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰

#### ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦Top-Kï¼Ÿ

**é—®é¢˜ï¼šå¯†é›†è·¯ç”±å¤ªæ…¢**

```python
å¯†é›†è·¯ç”±ï¼ˆç”¨æ‰€æœ‰ä¸“å®¶ï¼‰:
  æƒé‡ = [0.05, 0.60, 0.15, 0.02, 0.08, 0.05, 0.03, 0.02]
  
  éœ€è¦è®¡ç®—ï¼š
    0.05Ã—Expert_0(x) +
    0.60Ã—Expert_1(x) +
    0.15Ã—Expert_2(x) +
    ... ï¼ˆ8ä¸ªéƒ½è¦è®¡ç®—ï¼‰
  
  é—®é¢˜ï¼š
    âŒ æƒé‡å¾ˆå°çš„ä¸“å®¶ï¼ˆå¦‚0.02ï¼‰è´¡çŒ®å¾ˆå°
    âŒ ä½†è¿˜æ˜¯è¦è®¡ç®—ï¼Œæµªè´¹ç®—åŠ›
    âŒ 8ä¸ªä¸“å®¶éƒ½è¦è®¡ç®— â†’ ä¸ç¨€ç–ï¼
```

**è§£å†³ï¼šTop-Kç¨€ç–è·¯ç”±**

```python
Top-Kè·¯ç”±ï¼ˆåªç”¨Kä¸ªæœ€å¥½çš„ä¸“å®¶ï¼‰:
  åŸå§‹æƒé‡ = [0.05, 0.60, 0.15, 0.02, 0.08, 0.05, 0.03, 0.02]
  
  Top-2: é€‰æ‹©æœ€å¤§çš„2ä¸ª
    é€‰ä¸­: ä¸“å®¶1 (0.60), ä¸“å®¶2 (0.15)
    
  é‡æ–°å½’ä¸€åŒ–:
    æ–°æƒé‡1 = 0.60 / (0.60 + 0.15) = 0.80
    æ–°æƒé‡2 = 0.15 / (0.60 + 0.15) = 0.20
  
  åªè®¡ç®—:
    0.80Ã—Expert_1(x) + 0.20Ã—Expert_2(x)
  
  ä¼˜åŠ¿ï¼š
    âœ… åªè®¡ç®—2ä¸ªä¸“å®¶ï¼ˆä¸æ˜¯8ä¸ªï¼‰
    âœ… è®¡ç®—é‡é™ä½75%
    âœ… ä¿ç•™äº†ä¸»è¦ä¿¡æ¯ï¼ˆ0.60+0.15=0.75ï¼Œ75%çš„æƒé‡ï¼‰
```

#### ğŸ“ Top-Kå…¬å¼

```python
# å¯†é›†MoEï¼ˆæ‰€æœ‰ä¸“å®¶ï¼‰
y_dense = Î£ G(x)_i Â· E_i(x)
          i=1..N

# Top-K MoEï¼ˆåªé€‰Kä¸ªï¼‰
y_topk = Î£ G'(x)_i Â· E_i(x)
         iâˆˆTopK(G(x))

å…¶ä¸­ï¼š
  TopK(G(x)) = é€‰æ‹©G(x)ä¸­æœ€å¤§çš„Kä¸ªä¸“å®¶
  G'(x) = é‡æ–°å½’ä¸€åŒ–åçš„æƒé‡

# é‡æ–°å½’ä¸€åŒ–
è®¾ TopK = {i1, i2, ..., iK}
G'(x)_ij = G(x)_ij / Î£ G(x)_ik
                     kâˆˆTopK
```

#### ğŸ”§ Top-Kå®ç°

```python
class TopKRouter(nn.Module):
    """Top-Kç¨€ç–è·¯ç”±å™¨"""
    def __init__(self, d_model, num_experts, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.gate = nn.Linear(d_model, num_experts, bias=False)
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        è¿”å›: 
          - top_k_gates: [batch, seq_len, top_k] é€‰ä¸­çš„ä¸“å®¶æƒé‡
          - top_k_indices: [batch, seq_len, top_k] é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
        """
        # 1. è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„logits
        logits = self.gate(x)  # [batch, seq_len, num_experts]
        
        # 2. Softmaxï¼ˆå¾—åˆ°åŸå§‹æƒé‡ï¼‰
        gates = F.softmax(logits, dim=-1)
        
        # 3. é€‰æ‹©Top-K
        top_k_gates, top_k_indices = torch.topk(
            gates, self.top_k, dim=-1
        )  # éƒ½æ˜¯ [batch, seq_len, top_k]
        
        # 4. é‡æ–°å½’ä¸€åŒ–
        top_k_gates = top_k_gates / top_k_gates.sum(dim=-1, keepdim=True)
        
        return top_k_gates, top_k_indices

# ä½¿ç”¨ç¤ºä¾‹
router = TopKRouter(d_model=768, num_experts=8, top_k=2)
x = torch.randn(2, 10, 768)

gates, indices = router(x)
print(gates.shape)    # [2, 10, 2] - åªæœ‰2ä¸ªä¸“å®¶çš„æƒé‡
print(indices.shape)  # [2, 10, 2] - è¿™2ä¸ªä¸“å®¶çš„ç´¢å¼•

# ç¬¬1ä¸ªbatchï¼Œç¬¬1ä¸ªtoken
print("é€‰ä¸­çš„ä¸“å®¶:", indices[0, 0])  # tensor([1, 2])
print("å¯¹åº”æƒé‡:", gates[0, 0])      # tensor([0.80, 0.20])
print("æƒé‡å’Œ:", gates[0, 0].sum())  # tensor(1.0000)
```

#### ğŸ“Š Top-1 vs Top-2 å¯¹æ¯”

```python
å¯¹äºè¾“å…¥ "Python":
  æ‰€æœ‰ä¸“å®¶çš„æƒé‡ = [0.05, 0.60, 0.15, 0.02, 0.08, 0.05, 0.03, 0.02]

Top-1 (Switch Transformer):
  é€‰ä¸­: [ä¸“å®¶1]
  æƒé‡: [1.00]
  è®¡ç®—: 1Ã—Expert_1(x)
  
  ä¼˜ç‚¹ï¼š
    âœ… æœ€å¿«ï¼ˆåªç”¨1ä¸ªä¸“å®¶ï¼‰
    âœ… æœ€ç¨€ç–
    âœ… æ˜¾å­˜å ç”¨æœ€å°
  
  ç¼ºç‚¹ï¼š
    âŒ ä¸¢å¤±äº†å…¶ä»–ä¸“å®¶çš„ä¿¡æ¯
    âŒ ä¸“å®¶1æ•…éšœå½±å“å¤§
    âŒ å¯èƒ½ä¸å¦‚Top-2å‡†ç¡®

Top-2 (å¸¸ç”¨):
  é€‰ä¸­: [ä¸“å®¶1, ä¸“å®¶2]
  æƒé‡: [0.80, 0.20]
  è®¡ç®—: 0.80Ã—Expert_1(x) + 0.20Ã—Expert_2(x)
  
  ä¼˜ç‚¹ï¼š
    âœ… é€Ÿåº¦å¿«ï¼ˆåªç”¨2ä¸ªï¼‰
    âœ… ä¿ç•™ä¸»è¦ä¿¡æ¯ï¼ˆ75%çš„æƒé‡ï¼‰
    âœ… å®¹é”™æ€§å¥½
  
  ç¼ºç‚¹ï¼š
    âŒ æ¯”Top-1ç•¥æ…¢
    âŒ æ˜¾å­˜ç•¥å¤š

Top-4:
  é€‰ä¸­: [ä¸“å®¶1, ä¸“å®¶2, ä¸“å®¶4, ä¸“å®¶0]
  æƒé‡: [0.68, 0.17, 0.09, 0.06]
  è®¡ç®—: å¤æ‚...
  
  ä¼˜ç‚¹ï¼š
    âœ… ä¿¡æ¯æœ€å…¨ï¼ˆ95%çš„æƒé‡ï¼‰
  
  ç¼ºç‚¹ï¼š
    âŒ è¾ƒæ…¢
    âŒ å¤±å»äº†ç¨€ç–æ€§çš„ä¼˜åŠ¿

å®é™…é€‰æ‹©ï¼š
  å¤§å¤šæ•°MoEä½¿ç”¨ Top-2
  å¹³è¡¡äº†æ€§èƒ½å’Œæ•ˆç‡
```

---

### ğŸŒ³ 2.4 è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆå…³é”®æŠ€å·§ï¼‰

#### ğŸ’¡ é—®é¢˜ï¼šè·¯ç”±åå¡Œ

**ç°è±¡**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰tokenéƒ½è·¯ç”±åˆ°å°‘æ•°å‡ ä¸ªä¸“å®¶

```python
ç†æƒ³æƒ…å†µï¼š
  Expert_0: 12.5% tokens  âœ…
  Expert_1: 12.5% tokens  âœ…
  Expert_2: 12.5% tokens  âœ…
  Expert_3: 12.5% tokens  âœ…
  Expert_4: 12.5% tokens  âœ…
  Expert_5: 12.5% tokens  âœ…
  Expert_6: 12.5% tokens  âœ…
  Expert_7: 12.5% tokens  âœ…
  ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰

å®é™…æƒ…å†µï¼ˆæ²¡æœ‰è´Ÿè½½å‡è¡¡ï¼‰ï¼š
  Expert_0: 5% tokens
  Expert_1: 65% tokens   âŒ è¿‡è½½ï¼
  Expert_2: 20% tokens
  Expert_3: 5% tokens
  Expert_4: 2% tokens    âŒ æµªè´¹ï¼
  Expert_5: 1% tokens    âŒ æµªè´¹ï¼
  Expert_6: 1% tokens    âŒ æµªè´¹ï¼
  Expert_7: 1% tokens    âŒ æµªè´¹ï¼
  ï¼ˆä¸¥é‡ä¸å‡ï¼‰

é—®é¢˜ï¼š
  âŒ Expert_1è¿‡è½½ â†’ æˆä¸ºç“¶é¢ˆ
  âŒ å¤§éƒ¨åˆ†ä¸“å®¶é—²ç½® â†’ å‚æ•°æµªè´¹
  âŒ å¤±å»äº†MoEçš„å¹¶è¡Œä¼˜åŠ¿
```

#### ğŸ“ è´Ÿè½½å‡è¡¡æŸå¤±å…¬å¼

**ç›®æ ‡**ï¼šé¼“åŠ±è·¯ç”±å™¨å‡åŒ€ä½¿ç”¨æ‰€æœ‰ä¸“å®¶

```python
# è¾…åŠ©æŸå¤±å‡½æ•°ï¼ˆLoad Balance Lossï¼‰
L_balance = Î± Â· N Â· Î£ f_i Â· P_i
                    i=1..N

å…¶ä¸­ï¼š
  N = ä¸“å®¶æ•°é‡
  f_i = ä¸“å®¶iè¢«é€‰ä¸­çš„é¢‘ç‡ï¼ˆfractionï¼‰
  P_i = è·¯ç”±åˆ°ä¸“å®¶içš„å¹³å‡æ¦‚ç‡ï¼ˆprobabilityï¼‰
  Î± = å¹³è¡¡ç³»æ•°ï¼ˆé€šå¸¸0.01ï¼‰

# ç†æƒ³æƒ…å†µ
å¦‚æœå®Œå…¨å‡åŒ€ï¼š
  f_i = 1/N ï¼ˆæ¯ä¸ªä¸“å®¶å¤„ç†1/Nçš„tokenï¼‰
  P_i = 1/N ï¼ˆæ¯ä¸ªä¸“å®¶å¹³å‡æƒé‡1/Nï¼‰
  
  L_balance = Î± Â· N Â· Î£ (1/N) Â· (1/N)
            = Î± Â· N Â· N Â· (1/NÂ²)
            = Î± Â· 1/N

# ä¸å‡åŒ€çš„æƒ…å†µ
å¦‚æœä¸“å®¶1å¤„ç†äº†65%çš„tokenï¼š
  f_1 = 0.65, P_1 = 0.65
  
  L_balance ä¼šæ›´å¤§
  
  æ¢¯åº¦ä¼šæ¨åŠ¨è·¯ç”±å™¨ï¼š
    - å‡å°‘å¯¹ä¸“å®¶1çš„ä½¿ç”¨
    - å¢åŠ å¯¹å…¶ä»–ä¸“å®¶çš„ä½¿ç”¨
```

#### ğŸ”¢ å…·ä½“è®¡ç®—ç¤ºä¾‹

```python
å‡è®¾ï¼š8ä¸ªä¸“å®¶ï¼Œå¤„ç†1000ä¸ªtoken

æ­¥éª¤1ï¼šç»Ÿè®¡ä½¿ç”¨é¢‘ç‡ f_i
  Expert_0å¤„ç†äº†50ä¸ªtoken  â†’ f_0 = 50/1000 = 0.05
  Expert_1å¤„ç†äº†650ä¸ªtoken â†’ f_1 = 650/1000 = 0.65
  Expert_2å¤„ç†äº†200ä¸ªtoken â†’ f_2 = 200/1000 = 0.20
  Expert_3å¤„ç†äº†50ä¸ªtoken  â†’ f_3 = 0.05
  Expert_4å¤„ç†äº†20ä¸ªtoken  â†’ f_4 = 0.02
  Expert_5å¤„ç†äº†10ä¸ªtoken  â†’ f_5 = 0.01
  Expert_6å¤„ç†äº†10ä¸ªtoken  â†’ f_6 = 0.01
  Expert_7å¤„ç†äº†10ä¸ªtoken  â†’ f_7 = 0.01
  
æ­¥éª¤2ï¼šè®¡ç®—å¹³å‡æ¦‚ç‡ P_i
  P_i = æ‰€æœ‰tokenç»™ä¸“å®¶içš„å¹³å‡æƒé‡
  
  æ¯”å¦‚Expert_1:
    1000ä¸ªtokençš„æƒé‡å¹³å‡å€¼ = 0.65
  
  ï¼ˆé€šå¸¸ P_i â‰ˆ f_iï¼‰

æ­¥éª¤3ï¼šè®¡ç®—æŸå¤±
  L_balance = 0.01 Ã— 8 Ã— (
    0.05Ã—0.05 + 0.65Ã—0.65 + 0.20Ã—0.20 + ... + 0.01Ã—0.01
  )
  = 0.08 Ã— (0.0025 + 0.4225 + 0.04 + 0.0025 + 0.0004 + ...)
  = 0.08 Ã— 0.48
  = 0.0384

  å¯¹æ¯”ç†æƒ³æƒ…å†µï¼ˆå®Œå…¨å‡åŒ€ï¼‰:
  L_ideal = 0.01 Ã— 8 Ã— (0.125Ã—0.125) Ã— 8
          = 0.08 Ã— 0.125
          = 0.01

  ç°åœ¨çš„æŸå¤± (0.0384) >> ç†æƒ³æŸå¤± (0.01)
  â†’ æ¢¯åº¦ä¼šæ¨åŠ¨æ”¹å–„å‡è¡¡æ€§
```

#### ğŸ”§ PyTorchå®ç°

```python
def load_balance_loss(gates, top_k_indices, num_experts, alpha=0.01):
    """
    è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
    
    Args:
        gates: [batch, seq_len, num_experts] - æ‰€æœ‰ä¸“å®¶çš„æƒé‡
        top_k_indices: [batch, seq_len, top_k] - è¢«é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
        num_experts: ä¸“å®¶æ€»æ•°
        alpha: æŸå¤±ç³»æ•°
    """
    batch_size, seq_len, _ = gates.shape
    num_tokens = batch_size * seq_len
    
    # 1. è®¡ç®— P_iï¼šæ¯ä¸ªä¸“å®¶çš„å¹³å‡è·¯ç”±æ¦‚ç‡
    P = gates.mean(dim=[0, 1])  # [num_experts]
    # P[i] = ä¸“å®¶içš„å¹³å‡æƒé‡
    
    # 2. è®¡ç®— f_iï¼šæ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
    # åˆ›å»ºone-hot mask
    expert_mask = F.one_hot(top_k_indices, num_experts).float()
    # expert_mask: [batch, seq_len, top_k, num_experts]
    
    # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
    f = expert_mask.sum(dim=[0, 1, 2])  # [num_experts]
    f = f / num_tokens  # å½’ä¸€åŒ–ä¸ºé¢‘ç‡
    
    # 3. è®¡ç®—æŸå¤±
    loss = (f * P).sum() * num_experts
    
    return alpha * loss

# ä½¿ç”¨ç¤ºä¾‹
batch_size, seq_len = 32, 512
num_experts = 8
top_k = 2

# æ¨¡æ‹Ÿè·¯ç”±å™¨è¾“å‡º
gates = torch.randn(batch_size, seq_len, num_experts)
gates = F.softmax(gates, dim=-1)

top_k_gates, top_k_indices = torch.topk(gates, top_k, dim=-1)

# è®¡ç®—æŸå¤±
loss_balance = load_balance_loss(gates, top_k_indices, num_experts)
print(f"Load balance loss: {loss_balance.item():.4f}")

# æ€»æŸå¤±
loss_total = loss_lm + loss_balance
# loss_lm: è¯­è¨€æ¨¡å‹çš„äº¤å‰ç†µæŸå¤±
# loss_balance: è¾…åŠ©æŸå¤±
```

#### ğŸ¯ æ•ˆæœå¯¹æ¯”

```python
è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–ï¼š

æ²¡æœ‰è´Ÿè½½å‡è¡¡æŸå¤±ï¼š
  Iter 0:    [12%, 15%, 10%, 13%, 11%, 14%, 12%, 13%]  â† å¼€å§‹è¿˜å‡åŒ€
  Iter 1000: [8%, 35%, 15%, 10%, 8%, 12%, 7%, 5%]      â† å¼€å§‹åç¦»
  Iter 5000: [5%, 65%, 20%, 5%, 2%, 1%, 1%, 1%]        â† ä¸¥é‡å¤±è¡¡
  Iter 10000:[3%, 80%, 15%, 1%, 0.5%, 0.3%, 0.1%, 0.1%] â† åå¡Œï¼

æœ‰è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆÎ±=0.01ï¼‰ï¼š
  Iter 0:    [12%, 15%, 10%, 13%, 11%, 14%, 12%, 13%]  â† å¼€å§‹
  Iter 1000: [10%, 18%, 14%, 12%, 11%, 13%, 11%, 11%]  â† è½»å¾®ä¸å‡
  Iter 5000: [11%, 16%, 13%, 12%, 12%, 13%, 12%, 11%]  â† åŸºæœ¬å‡è¡¡
  Iter 10000:[12%, 14%, 13%, 12%, 13%, 12%, 12%, 12%]  â† å¾ˆå‡åŒ€ âœ…

ç»“æœï¼š
  âœ… æ‰€æœ‰ä¸“å®¶éƒ½è¢«å……åˆ†åˆ©ç”¨
  âœ… æ²¡æœ‰ç“¶é¢ˆä¸“å®¶
  âœ… å‚æ•°æ•ˆç‡é«˜
```

---

### ğŸŒ³ 2.5 å®Œæ•´çš„MoEæ•°å­¦æµç¨‹æ€»ç»“

#### ğŸ“Š ç«¯åˆ°ç«¯å…¬å¼

```python
å®Œæ•´çš„MoEå‰å‘ä¼ æ’­ï¼š

è¾“å…¥ï¼šx âˆˆ R^d_model

# 1. è®¡ç®—è·¯ç”±æƒé‡
logits = x Â· W_g               # [num_experts]
gates = Softmax(logits)         # [num_experts]

# 2. Top-Ké€‰æ‹©
top_k_gates, top_k_indices = TopK(gates, k=2)
top_k_gates = top_k_gates / Î£ top_k_gates  # é‡å½’ä¸€åŒ–

# 3. ä¸“å®¶è®¡ç®—
outputs = []
for i in top_k_indices:
    expert_out = Expert_i(x)    # FFN forward
    outputs.append(expert_out)

# 4. åŠ æƒç»„åˆ
y = Î£ top_k_gates[j] Â· outputs[j]
    j=0..k-1

# 5. æŸå¤±å‡½æ•°
L_task = CrossEntropy(predictions, targets)
L_balance = Î± Â· N Â· Î£ f_i Â· P_i
L_total = L_task + L_balance

è¾“å‡ºï¼šy âˆˆ R^d_model
```

#### ğŸ¯ å…³é”®ç‚¹æ€»ç»“

```python
MoEæ•°å­¦çš„ä¸‰å¤§å…³é”®ï¼š

1. ç¨€ç–æ¿€æ´»ï¼ˆTop-Kï¼‰
   ç›®çš„ï¼šå‡å°‘è®¡ç®—é‡
   æ–¹æ³•ï¼šåªç”¨æœ€ç›¸å…³çš„Kä¸ªä¸“å®¶
   æ•ˆæœï¼šå‚æ•°å¤šä½†è®¡ç®—å°‘

2. å¯å­¦ä¹ è·¯ç”±ï¼ˆRouterï¼‰
   ç›®çš„ï¼šè‡ªåŠ¨å­¦ä¹ ä¸“å®¶åˆ†å·¥
   æ–¹æ³•ï¼šçº¿æ€§å˜æ¢ + Softmax + Top-K
   æ•ˆæœï¼šä¸åŒè¾“å…¥è‡ªåŠ¨é€‰æ‹©ä¸åŒä¸“å®¶

3. è´Ÿè½½å‡è¡¡ï¼ˆAuxiliary Lossï¼‰
   ç›®çš„ï¼šé˜²æ­¢è·¯ç”±åå¡Œ
   æ–¹æ³•ï¼šè¾…åŠ©æŸå¤±å‡½æ•°
   æ•ˆæœï¼šä¸“å®¶ä½¿ç”¨å‡åŒ€ï¼Œå‚æ•°åˆ©ç”¨ç‡é«˜
```

---

## ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»é›¶å®ç°MoEå±‚ï¼ˆå¾ªåºæ¸è¿›ï¼‰

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šä»æœ€ç®€å•çš„ç‰ˆæœ¬å¼€å§‹ï¼Œé€æ­¥å®ç°å®Œæ•´çš„MoEå±‚

### ğŸŒ± 3.1 ç‰ˆæœ¬1ï¼šæœ€ç®€å•çš„MoEï¼ˆç†è§£æ ¸å¿ƒé€»è¾‘ï¼‰

#### ğŸ’¡ å…ˆä»æœ€åŸºç¡€å¼€å§‹

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ª**è¶…çº§ç®€å•**çš„MoEï¼Œåªä¿ç•™æ ¸å¿ƒé€»è¾‘ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleMoE(nn.Module):
    """
    ç‰ˆæœ¬1ï¼šæœ€ç®€å•çš„MoEå®ç°
    ç›®çš„ï¼šç†è§£æ ¸å¿ƒæµç¨‹ï¼Œä¸è€ƒè™‘æ€§èƒ½ä¼˜åŒ–
    """
    def __init__(self, d_model=768, num_experts=4):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        
        # 1. è·¯ç”±å™¨ï¼šä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚
        self.router = nn.Linear(d_model, num_experts)
        
        # 2. ä¸“å®¶ä»¬ï¼šæ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªç®€å•çš„ä¸¤å±‚ç½‘ç»œ
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.ReLU(),
                nn.Linear(d_model * 4, d_model)
            )
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        ä¾‹å¦‚: [2, 10, 768] - 2ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª10ä¸ªtokenï¼Œæ¯ä¸ªtoken 768ç»´
        """
        batch_size, seq_len, d_model = x.shape
        
        # æ­¥éª¤1ï¼šè·¯ç”±å™¨å†³å®šæƒé‡
        router_logits = self.router(x)  # [batch, seq_len, num_experts]
        router_weights = F.softmax(router_logits, dim=-1)
        
        print(f"è·¯ç”±æƒé‡ç¤ºä¾‹ï¼ˆç¬¬1ä¸ªtokenï¼‰: {router_weights[0, 0]}")
        # è¾“å‡ºå¯èƒ½æ˜¯: [0.4, 0.3, 0.2, 0.1]
        
        # æ­¥éª¤2ï¼šæ¯ä¸ªä¸“å®¶å¤„ç†æ‰€æœ‰token
        expert_outputs = []
        for expert in self.experts:
            expert_out = expert(x)  # [batch, seq_len, d_model]
            expert_outputs.append(expert_out)
        
        # æ­¥éª¤3ï¼šåŠ æƒç»„åˆ
        # expert_outputs: list of 4ä¸ª [batch, seq_len, d_model]
        # router_weights: [batch, seq_len, 4]
        
        # å †å ä¸“å®¶è¾“å‡º: [batch, seq_len, num_experts, d_model]
        expert_outputs = torch.stack(expert_outputs, dim=2)
        
        # æ‰©å±•æƒé‡: [batch, seq_len, num_experts, 1]
        router_weights = router_weights.unsqueeze(-1)
        
        # åŠ æƒæ±‚å’Œ: [batch, seq_len, d_model]
        output = (expert_outputs * router_weights).sum(dim=2)
        
        return output

# ä½¿ç”¨ç¤ºä¾‹
moe = SimpleMoE(d_model=768, num_experts=4)
x = torch.randn(2, 10, 768)  # 2ä¸ªæ ·æœ¬ï¼Œ10ä¸ªtoken

output = moe(x)
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # åº”è¯¥ç›¸åŒ
```

#### ğŸ“Š è¿™ä¸ªç‰ˆæœ¬çš„é—®é¢˜

```python
é—®é¢˜ï¼š
  âŒ æ‰€æœ‰ä¸“å®¶éƒ½è¦è®¡ç®— â†’ ä¸ç¨€ç–ï¼
  âŒ è®¡ç®—é‡ = num_expertså€ â†’ å¤ªæ…¢ï¼
  âŒ æ²¡æœ‰è´Ÿè½½å‡è¡¡ â†’ å¯èƒ½å´©æºƒ
  âŒ ä½†é€»è¾‘æ¸…æ™°ï¼Œé€‚åˆç†è§£ âœ…
```

---

### ğŸŒ± 3.2 ç‰ˆæœ¬2ï¼šåŠ å…¥Top-Kç¨€ç–æ¿€æ´»

#### ğŸ’¡ æ ¸å¿ƒæ”¹è¿›ï¼šåªç”¨æœ€å¥½çš„Kä¸ªä¸“å®¶

```python
class TopKMoE(nn.Module):
    """
    ç‰ˆæœ¬2ï¼šåŠ å…¥Top-Kç¨€ç–è·¯ç”±
    å…³é”®æ”¹è¿›ï¼šæ¯ä¸ªtokenåªç”¨top_kä¸ªä¸“å®¶
    """
    def __init__(self, d_model=768, num_experts=8, top_k=2):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        self.top_k = top_k
        
        # è·¯ç”±å™¨
        self.router = nn.Linear(d_model, num_experts, bias=False)
        
        # ä¸“å®¶ä»¬
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.GELU(),  # ç”¨GELUä»£æ›¿ReLU
                nn.Linear(d_model * 4, d_model)
            )
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, d_model = x.shape
        
        # æ­¥éª¤1ï¼šè·¯ç”±ï¼ˆå’Œä¹‹å‰ä¸€æ ·ï¼‰
        router_logits = self.router(x)  # [batch, seq_len, num_experts]
        router_probs = F.softmax(router_logits, dim=-1)
        
        # æ­¥éª¤2ï¼šTop-Ké€‰æ‹©ï¼ˆå…³é”®ï¼ï¼‰
        top_k_probs, top_k_indices = torch.topk(
            router_probs, self.top_k, dim=-1
        )
        # top_k_probs: [batch, seq_len, top_k] - æœ€é«˜çš„Kä¸ªæƒé‡
        # top_k_indices: [batch, seq_len, top_k] - å¯¹åº”çš„ä¸“å®¶ç´¢å¼•
        
        # æ­¥éª¤3ï¼šé‡æ–°å½’ä¸€åŒ–æƒé‡
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)
        
        # æ­¥éª¤4ï¼šåªè®¡ç®—é€‰ä¸­çš„ä¸“å®¶ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
        output = torch.zeros_like(x)  # [batch, seq_len, d_model]
        
        # å¯¹æ¯ä¸ªTop-Kä½ç½®
        for k in range(self.top_k):
            # å½“å‰ä½ç½®é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
            expert_indices = top_k_indices[:, :, k]  # [batch, seq_len]
            # å¯¹åº”çš„æƒé‡
            weights = top_k_probs[:, :, k]  # [batch, seq_len]
            
            # å¯¹æ¯ä¸ªä¸“å®¶
            for expert_id in range(self.num_experts):
                # æ‰¾åˆ°é€‰æ‹©äº†è¿™ä¸ªä¸“å®¶çš„token
                mask = (expert_indices == expert_id)  # [batch, seq_len]
                
                if mask.any():
                    # æå–è¿™äº›token
                    selected_x = x[mask]  # [num_selected, d_model]
                    
                    # é€šè¿‡ä¸“å®¶
                    expert_out = self.experts[expert_id](selected_x)
                    
                    # åŠ æƒå†™å›
                    expert_weights = weights[mask].unsqueeze(-1)  # [num_selected, 1]
                    output[mask] += expert_weights * expert_out
        
        return output

# ä½¿ç”¨ç¤ºä¾‹
moe = TopKMoE(d_model=768, num_experts=8, top_k=2)
x = torch.randn(2, 10, 768)

output = moe(x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")

# è®¡ç®—é‡å¯¹æ¯”
print("\nè®¡ç®—é‡å¯¹æ¯”ï¼š")
print(f"ç‰ˆæœ¬1ï¼ˆå¯†é›†ï¼‰: éœ€è¦è®¡ç®— {8} ä¸ªä¸“å®¶")
print(f"ç‰ˆæœ¬2ï¼ˆTop-2ï¼‰: åªéœ€è®¡ç®— {2} ä¸ªä¸“å®¶")
print(f"è®¡ç®—é‡å‡å°‘: {(8-2)/8*100:.0f}%")
```

#### ğŸ¯ å…³é”®æ”¹è¿›ç‚¹

```python
æ”¹è¿›ï¼š
  âœ… ç¨€ç–æ¿€æ´»ï¼šåªç”¨top_kä¸ªä¸“å®¶
  âœ… è®¡ç®—é‡å¤§å¹…é™ä½
  âœ… æ•ˆæœå‡ ä¹ä¸å˜
  
è¿˜ç¼ºï¼š
  âŒ è´Ÿè½½å‡è¡¡
  âŒ æ•ˆç‡ä¼˜åŒ–ï¼ˆå¾ªç¯å¤ªå¤šï¼‰
  âŒ ä¸“å®¶å®¹é‡æ§åˆ¶
```

---

### ğŸŒ³ 3.3 ç‰ˆæœ¬3ï¼šå®Œæ•´çš„ç”Ÿäº§çº§MoE

#### ğŸ’¡ åŠ å…¥æ‰€æœ‰ä¼˜åŒ–æŠ€å·§

```python
class ProductionMoE(nn.Module):
    """
    ç‰ˆæœ¬3ï¼šç”Ÿäº§çº§MoEå®ç°
    åŒ…å«ï¼šTop-Kè·¯ç”± + è´Ÿè½½å‡è¡¡ + ä¸“å®¶å®¹é‡æ§åˆ¶
    """
    def __init__(
        self,
        d_model=768,
        num_experts=8,
        top_k=2,
        capacity_factor=1.25,
        dropout=0.1,
        aux_loss_weight=0.01
    ):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        self.top_k = top_k
        self.capacity_factor = capacity_factor
        self.aux_loss_weight = aux_loss_weight
        
        # è·¯ç”±å™¨
        self.router = nn.Linear(d_model, num_experts, bias=False)
        
        # ä¸“å®¶ç½‘ç»œï¼ˆå¸¦Dropoutï¼‰
        self.experts = nn.ModuleList([
            ExpertFFN(d_model, dropout) 
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        è¿”å›: (output, aux_loss)
        """
        batch_size, seq_len, d_model = x.shape
        num_tokens = batch_size * seq_len
        
        # å±•å¹³: [batch*seq_len, d_model]
        x_flat = x.view(-1, d_model)
        
        # === æ­¥éª¤1ï¼šè·¯ç”± ===
        router_logits = self.router(x_flat)  # [num_tokens, num_experts]
        router_probs = F.softmax(router_logits, dim=-1)
        
        # === æ­¥éª¤2ï¼šTop-Ké€‰æ‹© ===
        top_k_probs, top_k_indices = torch.topk(
            router_probs, self.top_k, dim=-1
        )
        
        # é‡æ–°å½’ä¸€åŒ–
        top_k_gates = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)
        
        # === æ­¥éª¤3ï¼šä¸“å®¶å®¹é‡æ§åˆ¶ ===
        # æ¯ä¸ªä¸“å®¶æœ€å¤šå¤„ç† capacity ä¸ªtoken
        capacity = int(self.capacity_factor * num_tokens / self.num_experts)
        
        # === æ­¥éª¤4ï¼šåˆ†å‘tokenåˆ°ä¸“å®¶ ===
        output = torch.zeros_like(x_flat)
        
        for k_idx in range(self.top_k):
            # å½“å‰Top-Kä½ç½®é€‰ä¸­çš„ä¸“å®¶
            expert_ids = top_k_indices[:, k_idx]  # [num_tokens]
            gates = top_k_gates[:, k_idx]  # [num_tokens]
            
            # å¯¹æ¯ä¸ªä¸“å®¶
            for expert_id in range(self.num_experts):
                # æ‰¾åˆ°è·¯ç”±åˆ°è¿™ä¸ªä¸“å®¶çš„token
                mask = (expert_ids == expert_id)
                
                if mask.any():
                    token_indices = torch.where(mask)[0]
                    
                    # å®¹é‡æ§åˆ¶ï¼šå¦‚æœè¶…è¿‡å®¹é‡ï¼Œåªå–å‰capacityä¸ª
                    if len(token_indices) > capacity:
                        token_indices = token_indices[:capacity]
                    
                    # ä¸“å®¶å¤„ç†
                    expert_input = x_flat[token_indices]
                    expert_output = self.experts[expert_id](expert_input)
                    
                    # åŠ æƒå†™å›
                    expert_gates = gates[token_indices].unsqueeze(-1)
                    output[token_indices] += expert_gates * expert_output
        
        # === æ­¥éª¤5ï¼šè®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤± ===
        aux_loss = self._compute_load_balance_loss(
            router_probs, top_k_indices
        )
        
        # æ¢å¤å½¢çŠ¶
        output = output.view(batch_size, seq_len, d_model)
        
        return output, aux_loss
    
    def _compute_load_balance_loss(self, router_probs, top_k_indices):
        """
        è®¡ç®—è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±
        é¼“åŠ±ä¸“å®¶ä½¿ç”¨å‡åŒ€
        """
        # ä¸“å®¶ä½¿ç”¨é¢‘ç‡
        expert_mask = F.one_hot(top_k_indices, self.num_experts).float()
        # expert_mask: [num_tokens, top_k, num_experts]
        
        f = expert_mask.sum(dim=[0, 1])  # [num_experts]
        f = f / f.sum()  # å½’ä¸€åŒ–
        
        # è·¯ç”±æ¦‚ç‡
        P = router_probs.mean(dim=0)  # [num_experts]
        
        # è´Ÿè½½å‡è¡¡æŸå¤±
        loss = (f * P).sum() * self.num_experts
        
        return self.aux_loss_weight * loss


class ExpertFFN(nn.Module):
    """
    å•ä¸ªä¸“å®¶çš„å‰é¦ˆç½‘ç»œ
    æ ‡å‡†çš„Transformer FFNç»“æ„
    """
    def __init__(self, d_model, dropout=0.1):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_model * 4)
        self.w2 = nn.Linear(d_model * 4, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x: [num_tokens, d_model]
        x = self.w1(x)
        x = F.gelu(x)
        x = self.dropout(x)
        x = self.w2(x)
        x = self.dropout(x)
        return x


# ä½¿ç”¨ç¤ºä¾‹
model = ProductionMoE(
    d_model=768,
    num_experts=8,
    top_k=2,
    capacity_factor=1.25,
    dropout=0.1
)

# å‰å‘ä¼ æ’­
x = torch.randn(4, 128, 768)  # batch=4, seq_len=128
output, aux_loss = model(x)

print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"è¾…åŠ©æŸå¤±: {aux_loss.item():.4f}")

# è®­ç»ƒæ—¶çš„ä½¿ç”¨
loss_lm = ...  # è¯­è¨€æ¨¡å‹æŸå¤±
loss_total = loss_lm + aux_loss  # æ€»æŸå¤±
```

#### ğŸ¯ ä¸‰ä¸ªç‰ˆæœ¬å¯¹æ¯”

| ç‰¹æ€§ | ç‰ˆæœ¬1ï¼ˆç®€å•ï¼‰ | ç‰ˆæœ¬2ï¼ˆTop-Kï¼‰ | ç‰ˆæœ¬3ï¼ˆç”Ÿäº§çº§ï¼‰ |
|------|-------------|--------------|----------------|
| å®ç°å¤æ‚åº¦ | â­ ç®€å• | â­â­ ä¸­ç­‰ | â­â­â­ å¤æ‚ |
| è®¡ç®—æ•ˆç‡ | âŒ æ…¢ï¼ˆç”¨æ‰€æœ‰ä¸“å®¶ï¼‰ | âœ… å¿«ï¼ˆTop-Kï¼‰ | âœ… æœ€å¿«ï¼ˆä¼˜åŒ–ï¼‰ |
| è´Ÿè½½å‡è¡¡ | âŒ æ—  | âŒ æ—  | âœ… æœ‰ |
| å®¹é‡æ§åˆ¶ | âŒ æ—  | âŒ æ—  | âœ… æœ‰ |
| è®­ç»ƒç¨³å®šæ€§ | âš ï¸ ä¸€èˆ¬ | âš ï¸ ä¸€èˆ¬ | âœ… ç¨³å®š |
| æ¨èåœºæ™¯ | å­¦ä¹ ç†è§£ | å¿«é€ŸåŸå‹ | å®é™…åº”ç”¨ |

---

### ğŸŒ³ 3.4 å®æˆ˜æŠ€å·§ï¼šå¦‚ä½•é€‰æ‹©å‚æ•°

#### ğŸ¯ ä¸“å®¶æ•°é‡ï¼ˆnum_expertsï¼‰

```python
# å†³ç­–æ ‘
if ä½ æ˜¯åˆå­¦è€… or å°æ¨¡å‹(<500M):
    num_experts = 4  # ä»å°å¼€å§‹
    ç†ç”±ï¼š
      - å®¹æ˜“è°ƒè¯•
      - è®­ç»ƒå¿«
      - å®¹æ˜“è§‚å¯Ÿä¸“å®¶åˆ†å·¥

elif ä¸­ç­‰æ¨¡å‹(500M-10B):
    num_experts = 8  # å¹³è¡¡é€‰æ‹©
    ç†ç”±ï¼š
      - è¶³å¤Ÿçš„ä¸“ç²¾åº¦
      - ä¸ä¼šè¿‡äºå¤æ‚
      - å¤§å¤šæ•°è®ºæ–‡ç”¨è¿™ä¸ª

elif å¤§æ¨¡å‹(>10B):
    num_experts = 16~64  # æ›´å¤šä¸“å®¶
    ç†ç”±ï¼š
      - å……åˆ†åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®
      - æ›´ç»†ç²’åº¦çš„ä¸“ç²¾
      - ä½†éœ€è¦æ³¨æ„è´Ÿè½½å‡è¡¡

# ç»éªŒï¼šä¸“å®¶æ•° â‰ˆ GPUæ•°é‡ï¼ˆä¾¿äºå¹¶è¡Œï¼‰
```

#### ğŸ¯ Top-Kå€¼

```python
# 99%çš„æƒ…å†µé€‰Top-2ï¼

top_k = 1  # Switch Transformer
  ä¼˜ç‚¹ï¼šâœ… æœ€å¿«ï¼Œæœ€ç¨€ç–
  ç¼ºç‚¹ï¼šâŒ å®¹é”™æ€§å·®
  é€‚ç”¨ï¼šè¿½æ±‚æè‡´é€Ÿåº¦

top_k = 2  # æ¨è â­â­â­â­â­
  ä¼˜ç‚¹ï¼šâœ… æ€§èƒ½å¥½ï¼Œé€Ÿåº¦å¿«
  ç¼ºç‚¹ï¼šåŸºæœ¬æ²¡æœ‰
  é€‚ç”¨ï¼š99%çš„åœºæ™¯

top_k = 4
  ä¼˜ç‚¹ï¼šâœ… ä¿¡æ¯æœ€å…¨
  ç¼ºç‚¹ï¼šâŒ å¤±å»ç¨€ç–ä¼˜åŠ¿
  é€‚ç”¨ï¼šæå°‘æ•°åœºæ™¯
```

#### ğŸ¯ å®¹é‡å› å­ï¼ˆcapacity_factorï¼‰

```python
capacity_factor = å®¹é‡ / ç†æƒ³å®¹é‡

ç†æƒ³å®¹é‡ = num_tokens / num_experts

å®é™…é€‰æ‹©ï¼š

capacity_factor = 1.0  # ä¸¥æ ¼å®¹é‡
  - ä¸“å®¶æ°å¥½å¤„ç†åˆ†é…ç»™å®ƒçš„token
  - é—®é¢˜ï¼šå¯èƒ½ä¸¢å¼ƒä¸€äº›token
  - é€‚ç”¨ï¼šæ˜¾å­˜å—é™

capacity_factor = 1.25  # æ¨è â­â­â­â­â­
  - ç•™25%çš„ç¼“å†²
  - å¹³è¡¡äº†æ•ˆç‡å’Œå®Œæ•´æ€§
  - é€‚ç”¨ï¼šå¤§å¤šæ•°åœºæ™¯

capacity_factor = 2.0  # å®½æ¾å®¹é‡
  - ä¸ä¸¢å¼ƒtoken
  - é—®é¢˜ï¼šå¯èƒ½è´Ÿè½½ä¸å‡
  - é€‚ç”¨ï¼šå°è§„æ¨¡å®éªŒ
```

#### ğŸ¯ è¾…åŠ©æŸå¤±æƒé‡ï¼ˆaux_loss_weightï¼‰

```python
aux_loss_weight = Î± = 0.01  # æ¨è

è¿‡å°ï¼ˆ0.001ï¼‰:
  âŒ è´Ÿè½½å‡è¡¡æ•ˆæœå·®
  âŒ ä¸“å®¶ä½¿ç”¨ä¸å‡

åˆšå¥½ï¼ˆ0.01ï¼‰:
  âœ… å‡è¡¡æ€§å¥½
  âœ… ä¸å½±å“ä¸»ä»»åŠ¡

è¿‡å¤§ï¼ˆ0.1ï¼‰:
  âŒ è¿‡åº¦çº¦æŸ
  âŒ å¯èƒ½å½±å“æ€§èƒ½

# è°ƒè¯•å»ºè®®
print(f"ä¸»æŸå¤±: {loss_lm.item():.4f}")
print(f"è¾…åŠ©æŸå¤±: {aux_loss.item():.4f}")
print(f"æ¯”ä¾‹: {(aux_loss/loss_lm).item():.2%}")

# ç†æƒ³æ¯”ä¾‹ï¼š1-5%
if (aux_loss/loss_lm) > 0.1:
    print("âš ï¸ è¾…åŠ©æŸå¤±å¤ªå¤§ï¼Œè€ƒè™‘å‡å°Î±")
```

---

### ğŸŒ³ 3.5 å®Œæ•´é…ç½®ç¤ºä¾‹

#### ğŸ“ å°è§„æ¨¡å®éªŒé…ç½®

```python
# é€‚åˆï¼šå•GPUï¼Œå¿«é€ŸéªŒè¯æƒ³æ³•
config_small = {
    'd_model': 512,
    'num_experts': 4,
    'top_k': 2,
    'capacity_factor': 1.5,  # å®½æ¾ä¸€ç‚¹
    'dropout': 0.1,
    'aux_loss_weight': 0.01,
}

model = ProductionMoE(**config_small)
# å‚æ•°é‡ï¼šçº¦ ~50Mï¼ˆå«4ä¸ªä¸“å®¶ï¼‰
# æ˜¾å­˜éœ€æ±‚ï¼šçº¦ 4GB
# è®­ç»ƒé€Ÿåº¦ï¼šå¿«
```

#### ğŸ“ æ ‡å‡†é…ç½®

```python
# é€‚åˆï¼š2-4Ã—GPUï¼Œæ­£å¼è®­ç»ƒ
config_standard = {
    'd_model': 768,
    'num_experts': 8,
    'top_k': 2,
    'capacity_factor': 1.25,
    'dropout': 0.1,
    'aux_loss_weight': 0.01,
}

model = ProductionMoE(**config_standard)
# å‚æ•°é‡ï¼šçº¦ ~400Mï¼ˆå«8ä¸ªä¸“å®¶ï¼‰
# æ˜¾å­˜éœ€æ±‚ï¼šçº¦ 16GB
# è®­ç»ƒé€Ÿåº¦ï¼šä¸­ç­‰
```

#### ğŸ“ å¤§è§„æ¨¡é…ç½®

```python
# é€‚åˆï¼š8+GPUï¼Œå¤§æ¨¡å‹è®­ç»ƒ
config_large = {
    'd_model': 1024,
    'num_experts': 16,
    'top_k': 2,
    'capacity_factor': 1.0,  # ä¸¥æ ¼å®¹é‡
    'dropout': 0.0,  # å¤§æ¨¡å‹é€šå¸¸ä¸ç”¨dropout
    'aux_loss_weight': 0.01,
}

model = ProductionMoE(**config_large)
# å‚æ•°é‡ï¼šçº¦ ~2Bï¼ˆå«16ä¸ªä¸“å®¶ï¼‰
# æ˜¾å­˜éœ€æ±‚ï¼šçº¦ 80GB
# è®­ç»ƒé€Ÿåº¦ï¼šéœ€è¦å¤šGPU
```

---

## ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šå°†MoEé›†æˆåˆ°Transformerï¼ˆå®æˆ˜ï¼‰

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šç†è§£å¦‚ä½•æŠŠMoEå±‚æ”¾å…¥å®Œæ•´çš„Transformeræ¨¡å‹

### ğŸŒ± 4.1 æ ‡å‡†Transformer vs MoE Transformer

#### ğŸ’¡ æ ¸å¿ƒåŒºåˆ«ï¼šåªæ›¿æ¢FFNå±‚

```python
æ ‡å‡†Transformer Blockï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LayerNorm                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-Head Attention       â”‚  â† ä¿æŒä¸å˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
         Add & Norm
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feed Forward Network (FFN) â”‚  â† æ›¿æ¢è¿™é‡Œï¼
â”‚  - Linear(d_model, 4*d)     â”‚
â”‚  - GELU                     â”‚
â”‚  - Linear(4*d, d_model)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
         Add & Norm
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MoE Transformer Blockï¼š
  ... (å‰é¢ä¸€æ ·) ...
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MoE Layer                  â”‚  â† ç”¨MoEæ›¿æ¢FFN
â”‚  - Router                   â”‚
â”‚  - 8ä¸ªExpert FFN            â”‚
â”‚  - Top-K Selection          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
  ... (åé¢ä¸€æ ·) ...
```

#### ğŸ“Š å¯¹æ¯”ï¼šä»€ä¹ˆå˜äº†ï¼Œä»€ä¹ˆæ²¡å˜

```python
ä¿æŒä¸å˜çš„éƒ¨åˆ†ï¼š
  âœ… Embeddingå±‚
  âœ… Positional Encoding
  âœ… Multi-Head Attention
  âœ… LayerNorm
  âœ… æ®‹å·®è¿æ¥
  âœ… è¾“å‡ºå±‚

åªæ”¹å˜çš„éƒ¨åˆ†ï¼š
  ğŸ”„ FFN â†’ MoE Layer

ç»“æœï¼š
  - ç»“æ„å…¼å®¹ï¼šå¯ä»¥ç›´æ¥æ›¿æ¢
  - è®­ç»ƒå…¼å®¹ï¼šæ¢¯åº¦æ­£å¸¸åå‘ä¼ æ’­
  - æ¨ç†å…¼å®¹ï¼šè¾“å…¥è¾“å‡ºç»´åº¦ä¸å˜
```

---

### ğŸŒ± 4.2 å®ç°MoE Transformer Block

#### ğŸ’¡ é€æ­¥æ„å»º

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# === æ­¥éª¤1ï¼šæ ‡å‡†çš„Attentionï¼ˆä¸å˜ï¼‰===
class MultiHeadAttention(nn.Module):
    """æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›ï¼ˆå’Œæ™®é€šTransformerä¸€æ ·ï¼‰"""
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.qkv = nn.Linear(d_model, 3 * d_model)
        self.out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        B, T, C = x.shape
        
        # QKV
        qkv = self.qkv(x)
        q, k, v = qkv.split(self.d_model, dim=-1)
        
        # Reshape
        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Attention
        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        # Output
        out = attn @ v
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        out = self.out(out)
        
        return out


# === æ­¥éª¤2ï¼šMoE Transformer Block ===
class MoETransformerBlock(nn.Module):
    """
    å°†MoEé›†æˆåˆ°Transformer Block
    """
    def __init__(
        self,
        d_model=768,
        num_heads=12,
        num_experts=8,
        top_k=2,
        dropout=0.1
    ):
        super().__init__()
        
        # === Attentionéƒ¨åˆ†ï¼ˆæ ‡å‡†ï¼‰===
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # === MoEéƒ¨åˆ†ï¼ˆæ›¿æ¢FFNï¼‰===
        self.ln2 = nn.LayerNorm(d_model)
        self.moe = ProductionMoE(  # ä½¿ç”¨ä¹‹å‰å®ç°çš„MoE
            d_model=d_model,
            num_experts=num_experts,
            top_k=top_k,
            dropout=dropout
        )
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        è¿”å›: (output, aux_loss)
        """
        # 1. Self-Attentionï¼ˆå’Œæ ‡å‡†Transformerä¸€æ ·ï¼‰
        x = x + self.attn(self.ln1(x))
        
        # 2. MoE FFNï¼ˆæ›¿æ¢äº†æ ‡å‡†FFNï¼‰
        moe_out, aux_loss = self.moe(self.ln2(x))
        x = x + moe_out
        
        return x, aux_loss

# ä½¿ç”¨ç¤ºä¾‹
block = MoETransformerBlock(
    d_model=768,
    num_heads=12,
    num_experts=8,
    top_k=2
)

x = torch.randn(4, 128, 768)  # batch=4, seq=128
output, aux_loss = block(x)

print(f"è¾“å…¥: {x.shape}")
print(f"è¾“å‡º: {output.shape}")  # ç›¸åŒï¼
print(f"è¾…åŠ©æŸå¤±: {aux_loss.item():.4f}")
```

---

### ğŸŒ³ 4.3 å®Œæ•´çš„MoE GPTæ¨¡å‹

#### ğŸ’¡ å †å å¤šä¸ªMoE Block

```python
class MoEGPT(nn.Module):
    """
    å®Œæ•´çš„MoE GPTæ¨¡å‹
    å°±æ˜¯æŠŠå¤šä¸ªMoE Blockå †å èµ·æ¥
    """
    def __init__(
        self,
        vocab_size=50257,
        block_size=1024,
        n_layer=12,
        n_head=12,
        n_embd=768,
        num_experts=8,
        top_k=2,
        dropout=0.1
    ):
        super().__init__()
        self.block_size = block_size
        
        # === Embeddingå±‚ï¼ˆæ ‡å‡†ï¼‰===
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        self.drop = nn.Dropout(dropout)
        
        # === MoE Transformer Blocksï¼ˆæ ¸å¿ƒï¼‰===
        self.blocks = nn.ModuleList([
            MoETransformerBlock(
                d_model=n_embd,
                num_heads=n_head,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout
            )
            for _ in range(n_layer)
        ])
        
        # === è¾“å‡ºå±‚ï¼ˆæ ‡å‡†ï¼‰===
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
        
        # æƒé‡å…±äº«ï¼ˆæ ‡å‡†æŠ€å·§ï¼‰
        self.token_embedding.weight = self.lm_head.weight
        
        # å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, idx, targets=None):
        """
        idx: [batch_size, seq_len] - è¾“å…¥tokenç´¢å¼•
        targets: [batch_size, seq_len] - ç›®æ ‡tokenï¼ˆè®­ç»ƒæ—¶ï¼‰
        """
        B, T = idx.shape
        assert T <= self.block_size, f"åºåˆ—é•¿åº¦{T}è¶…è¿‡æœ€å¤§é•¿åº¦{self.block_size}"
        
        # === 1. Embeddings ===
        tok_emb = self.token_embedding(idx)  # [B, T, n_embd]
        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # [T, n_embd]
        x = self.drop(tok_emb + pos_emb)  # [B, T, n_embd]
        
        # === 2. Transformer Blocks + æ”¶é›†è¾…åŠ©æŸå¤± ===
        aux_losses = []
        for block in self.blocks:
            x, aux_loss = block(x)
            aux_losses.append(aux_loss)
        
        # === 3. è¾“å‡º ===
        x = self.ln_f(x)  # [B, T, n_embd]
        logits = self.lm_head(x)  # [B, T, vocab_size]
        
        # === 4. è®¡ç®—æŸå¤±ï¼ˆè®­ç»ƒæ—¶ï¼‰===
        loss = None
        if targets is not None:
            # ä¸»æŸå¤±ï¼šè¯­è¨€æ¨¡å‹çš„äº¤å‰ç†µ
            loss_lm = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1
            )
            
            # è¾…åŠ©æŸå¤±ï¼šè´Ÿè½½å‡è¡¡
            loss_aux = sum(aux_losses) / len(aux_losses)
            
            # æ€»æŸå¤±
            loss = loss_lm + loss_aux
            
            # æ‰“å°æŸå¤±æ¯”ä¾‹ï¼ˆè°ƒè¯•ç”¨ï¼‰
            if torch.rand(1).item() < 0.01:  # 1%æ¦‚ç‡æ‰“å°
                print(f"Loss LM: {loss_lm.item():.4f}, "
                      f"Loss Aux: {loss_aux.item():.6f}, "
                      f"Ratio: {(loss_aux/loss_lm).item():.2%}")
        
        return logits, loss
    
    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆå’Œæ ‡å‡†GPTä¸€æ ·ï¼‰
        """
        for _ in range(max_new_tokens):
            # æˆªå–æœ€åblock_sizeä¸ªtoken
            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]
            
            # å‰å‘ä¼ æ’­
            logits, _ = self(idx_cond)
            
            # åªå–æœ€åä¸€ä¸ªtokençš„logits
            logits = logits[:, -1, :] / temperature
            
            # Top-Ké‡‡æ ·
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            
            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            
            # æ‹¼æ¥
            idx = torch.cat((idx, idx_next), dim=1)
        
        return idx


# === ä½¿ç”¨ç¤ºä¾‹ ===
# åˆ›å»ºæ¨¡å‹
model = MoEGPT(
    vocab_size=50257,
    block_size=1024,
    n_layer=12,
    n_head=12,
    n_embd=768,
    num_experts=8,
    top_k=2,
    dropout=0.1
)

# æŸ¥çœ‹å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ€»å‚æ•°é‡: {total_params/1e6:.1f}M")

# å‰å‘ä¼ æ’­
idx = torch.randint(0, 50257, (2, 128))  # batch=2, seq=128
targets = torch.randint(0, 50257, (2, 128))

logits, loss = model(idx, targets)
print(f"Logits: {logits.shape}")  # [2, 128, 50257]
print(f"Loss: {loss.item():.4f}")

# ç”Ÿæˆæ–‡æœ¬
gen_idx = model.generate(
    idx=torch.zeros((1, 1), dtype=torch.long),
    max_new_tokens=100
)
print(f"ç”Ÿæˆçš„token: {gen_idx.shape}")  # [1, 101]
```

---

### ğŸŒ³ 4.4 æ··åˆæ¶æ„ï¼šéƒ¨åˆ†å±‚ç”¨MoE

#### ğŸ’¡ ä¸æ˜¯æ‰€æœ‰å±‚éƒ½è¦ç”¨MoE

**ç»éªŒ**ï¼šåªåœ¨éƒ¨åˆ†å±‚ä½¿ç”¨MoEæ•ˆæœæ›´å¥½ï¼

```python
class HybridMoEGPT(nn.Module):
    """
    æ··åˆæ¶æ„ï¼šåªåœ¨æŸäº›å±‚ä½¿ç”¨MoE
    
    ç­–ç•¥ï¼š
      - æµ…å±‚ï¼šæ ‡å‡†FFNï¼ˆå­¦ä¹ åŸºç¡€ç‰¹å¾ï¼‰
      - æ·±å±‚ï¼šMoEï¼ˆå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰
    """
    def __init__(
        self,
        vocab_size=50257,
        block_size=1024,
        n_layer=12,
        n_head=12,
        n_embd=768,
        num_experts=8,
        top_k=2,
        moe_start_layer=4,  # ä»ç¬¬4å±‚å¼€å§‹ç”¨MoE
        dropout=0.1
    ):
        super().__init__()
        self.block_size = block_size
        
        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        
        # æ··åˆBlocks
        self.blocks = nn.ModuleList()
        for i in range(n_layer):
            if i < moe_start_layer:
                # å‰å‡ å±‚ï¼šæ ‡å‡†Transformer
                block = StandardTransformerBlock(n_embd, n_head, dropout)
            else:
                # åå‡ å±‚ï¼šMoE Transformer
                block = MoETransformerBlock(
                    n_embd, n_head, num_experts, top_k, dropout
                )
            self.blocks.append(block)
        
        # Output
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
    
    def forward(self, idx, targets=None):
        B, T = idx.shape
        
        # Embeddings
        tok_emb = self.token_embedding(idx)
        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))
        x = tok_emb + pos_emb
        
        # Blocksï¼ˆæ”¶é›†MoEå±‚çš„è¾…åŠ©æŸå¤±ï¼‰
        aux_losses = []
        for i, block in enumerate(self.blocks):
            if isinstance(block, MoETransformerBlock):
                x, aux_loss = block(x)
                aux_losses.append(aux_loss)
            else:
                x = block(x)  # æ ‡å‡†Blockæ²¡æœ‰aux_loss
        
        # Output
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        # Loss
        loss = None
        if targets is not None:
            loss_lm = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1)
            )
            
            if aux_losses:
                loss_aux = sum(aux_losses) / len(aux_losses)
                loss = loss_lm + loss_aux
            else:
                loss = loss_lm
        
        return logits, loss


# ä½¿ç”¨ç¤ºä¾‹
model = HybridMoEGPT(
    n_layer=12,
    moe_start_layer=6,  # å‰6å±‚æ ‡å‡†ï¼Œå6å±‚MoE
    num_experts=8
)

print("å±‚é…ç½®:")
for i, block in enumerate(model.blocks):
    block_type = "MoE" if isinstance(block, MoETransformerBlock) else "Standard"
    print(f"  Layer {i}: {block_type}")

# è¾“å‡º:
# Layer 0: Standard
# Layer 1: Standard
# ...
# Layer 6: MoE
# Layer 7: MoE
# ...
```

#### ğŸ¯ ä½•æ—¶ç”¨æ··åˆæ¶æ„ï¼Ÿ

```python
å…¨éƒ¨MoEï¼ˆæ‰€æœ‰å±‚ï¼‰:
  ä¼˜ç‚¹ï¼šâœ… æœ€å¤§å®¹é‡
  ç¼ºç‚¹ï¼šâŒ å¯èƒ½è¿‡æ‹Ÿåˆï¼Œè®­ç»ƒéš¾
  é€‚ç”¨ï¼šè¶…å¤§æ•°æ®é›†

æ··åˆæ¶æ„ï¼ˆéƒ¨åˆ†å±‚ï¼‰:
  ä¼˜ç‚¹ï¼šâœ… å¹³è¡¡ï¼Œè®­ç»ƒç¨³å®š
  ç¼ºç‚¹ï¼šå®¹é‡ç•¥å°
  é€‚ç”¨ï¼šâœ… æ¨èï¼Œå¤§å¤šæ•°åœºæ™¯

ç­–ç•¥é€‰æ‹©ï¼š
  å°æ¨¡å‹(<1B): å…¨æ ‡å‡†æˆ–å1/3ç”¨MoE
  ä¸­æ¨¡å‹(1-10B): å1/2ç”¨MoE
  å¤§æ¨¡å‹(>10B): å2/3ç”¨MoE
```

---

## ğŸ“š ç¬¬äº”éƒ¨åˆ†ï¼šè®­ç»ƒMoEæ¨¡å‹ï¼ˆå®æˆ˜æŒ‡å—ï¼‰

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šæŒæ¡è®­ç»ƒMoEæ¨¡å‹çš„å®Œæ•´æµç¨‹å’Œè°ƒè¯•æŠ€å·§

### ğŸŒ± 5.1 åŸºç¡€è®­ç»ƒè„šæœ¬ï¼ˆä»ç®€å•å¼€å§‹ï¼‰

#### ğŸ’¡ æœ€å°å¯ç”¨ç‰ˆæœ¬

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

def train_moe_simple():
    """
    æœ€ç®€å•çš„MoEè®­ç»ƒè„šæœ¬
    é€‚åˆï¼šå•GPUï¼Œå¿«é€ŸéªŒè¯
    """
    
    # === 1. å‡†å¤‡æ•°æ® ===
    # å‡è®¾ä½ å·²ç»æœ‰äº†æ•°æ®åŠ è½½å™¨
    train_loader = ...  # DataLoader
    
    # === 2. åˆ›å»ºæ¨¡å‹ ===
    model = MoEGPT(
        vocab_size=50257,
        block_size=512,  # è¾ƒçŸ­çš„åºåˆ—
        n_layer=6,       # è¾ƒå°‘çš„å±‚
        n_head=6,
        n_embd=384,
        num_experts=4,   # è¾ƒå°‘çš„ä¸“å®¶
        top_k=2,
        dropout=0.1
    ).cuda()
    
    print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.1f}M")
    
    # === 3. ä¼˜åŒ–å™¨ ===
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=3e-4,
        betas=(0.9, 0.95),
        weight_decay=0.1
    )
    
    # === 4. è®­ç»ƒå¾ªç¯ ===
    model.train()
    step = 0
    
    for epoch in range(10):
        for batch_idx, (x, y) in enumerate(train_loader):
            x, y = x.cuda(), y.cuda()
            
            # å‰å‘
            logits, loss = model(x, targets=y)
            
            # åå‘
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé‡è¦ï¼ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # æ›´æ–°
            optimizer.step()
            
            # æ—¥å¿—
            if step % 100 == 0:
                print(f"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}")
            
            step += 1
    
    return model


# è¿è¡Œ
model = train_moe_simple()
```

---

### ğŸŒ± 5.2 å®Œæ•´è®­ç»ƒè„šæœ¬ï¼ˆç”Ÿäº§çº§ï¼‰

#### ğŸ’¡ åŒ…å«æ‰€æœ‰å¿…è¦åŠŸèƒ½

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler

# å¯é€‰çš„ä¾èµ–
try:
    import wandb
    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False
    
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    tqdm = lambda x, **kwargs: x  # å›é€€åˆ°æ™®é€šè¿­ä»£å™¨

# ===== MoE æ¨¡å‹å®ç° =====

class SimpleMoE(nn.Module):
    """ç®€å•çš„MoEå±‚ï¼ˆTop-Kè·¯ç”±ï¼‰"""
    def __init__(self, d_model=768, num_experts=8, top_k=2, dropout=0.1):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.d_model = d_model
        
        # è·¯ç”±å™¨
        self.router = nn.Linear(d_model, num_experts, bias=False)
        
        # ä¸“å®¶ä»¬ï¼ˆæ¯ä¸ªæ˜¯æ ‡å‡†FFNï¼‰
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model * 4, d_model),
                nn.Dropout(dropout)
            )
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        è¿”å›: (output, aux_loss)
        """
        batch_size, seq_len, d_model = x.shape
        
        # Flatten for easier processing
        x_flat = x.view(-1, d_model)  # [batch*seq, d_model]
        
        # æ­¥éª¤1ï¼šè·¯ç”±
        router_logits = self.router(x_flat)  # [batch*seq, num_experts]
        router_probs = F.softmax(router_logits, dim=-1)
        
        # æ­¥éª¤2ï¼šTop-Ké€‰æ‹©
        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)
        
        # æ­¥éª¤3ï¼šä¸“å®¶è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸åšå®¹é‡æ§åˆ¶ï¼‰
        output = torch.zeros_like(x_flat)
        
        for k in range(self.top_k):
            expert_ids = top_k_indices[:, k]  # [batch*seq]
            gates = top_k_probs[:, k]  # [batch*seq]
            
            for expert_id in range(self.num_experts):
                mask = (expert_ids == expert_id)
                if mask.any():
                    expert_input = x_flat[mask]
                    expert_output = self.experts[expert_id](expert_input)
                    output[mask] += gates[mask].unsqueeze(-1) * expert_output
        
        # æ­¥éª¤4ï¼šè®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆç®€åŒ–ç‰ˆï¼‰
        aux_loss = self._compute_load_balance_loss(router_probs, top_k_indices)
        
        # æ¢å¤å½¢çŠ¶
        output = output.view(batch_size, seq_len, d_model)
        
        return output, aux_loss
    
    def _compute_load_balance_loss(self, router_probs, top_k_indices):
        """ç®€åŒ–çš„è´Ÿè½½å‡è¡¡æŸå¤±"""
        # ä¸“å®¶ä½¿ç”¨é¢‘ç‡
        expert_mask = F.one_hot(top_k_indices, self.num_experts).float()
        f = expert_mask.sum(dim=[0, 1]) / expert_mask.sum()
        
        # è·¯ç”±æ¦‚ç‡
        P = router_probs.mean(dim=0)
        
        # è´Ÿè½½å‡è¡¡æŸå¤±
        loss = (f * P).sum() * self.num_experts
        return 0.01 * loss  # æƒé‡0.01


class MultiHeadAttention(nn.Module):
    """æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›"""
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.qkv = nn.Linear(d_model, 3 * d_model)
        self.out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        B, T, C = x.shape
        
        # QKV
        qkv = self.qkv(x)
        q, k, v = qkv.split(self.d_model, dim=-1)
        
        # Reshape for multi-head
        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Attention
        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        # Output
        out = attn @ v
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        out = self.out(out)
        
        return out


class StandardTransformerBlock(nn.Module):
    """æ ‡å‡†Transformer Blockï¼ˆä½¿ç”¨æ™®é€šFFNï¼‰"""
    def __init__(self, d_model=768, num_heads=12, dropout=0.1):
        super().__init__()
        
        # Attentionéƒ¨åˆ†
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # æ ‡å‡†FFNï¼ˆä¸æ˜¯MoEï¼‰
        self.ln2 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        è¿”å›: outputï¼ˆæ²¡æœ‰aux_lossï¼‰
        """
        # Self-Attention
        x = x + self.attn(self.ln1(x))
        
        # æ ‡å‡†FFN
        x = x + self.ffn(self.ln2(x))
        
        return x


class MoETransformerBlock(nn.Module):
    """MoE Transformer Block"""
    def __init__(self, d_model=768, num_heads=12, num_experts=8, top_k=2, dropout=0.1):
        super().__init__()
        
        # Attentionéƒ¨åˆ†
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # MoEéƒ¨åˆ†
        self.ln2 = nn.LayerNorm(d_model)
        self.moe = SimpleMoE(d_model, num_experts, top_k, dropout)
    
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        è¿”å›: (output, aux_loss)
        """
        # Self-Attention
        x = x + self.attn(self.ln1(x))
        
        # MoE FFN
        moe_out, aux_loss = self.moe(self.ln2(x))
        x = x + moe_out
        
        return x, aux_loss


class MoEGPT(nn.Module):
    """
    æ··åˆæ¶æ„çš„MoE GPTæ¨¡å‹
    
    ç­–ç•¥ï¼š
      - æµ…å±‚ï¼šæ ‡å‡†FFNï¼ˆå­¦ä¹ åŸºç¡€ç‰¹å¾ï¼‰
      - æ·±å±‚ï¼šMoEï¼ˆå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰
    """
    def __init__(
        self,
        vocab_size=50257,
        block_size=1024,
        n_layer=12,
        n_head=12,
        n_embd=768,
        num_experts=8,
        top_k=2,
        moe_start_layer=None,  # ä»å“ªä¸€å±‚å¼€å§‹ä½¿ç”¨MoEï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å±‚éƒ½ç”¨MoE
        dropout=0.1
    ):
        super().__init__()
        self.block_size = block_size
        self.n_layer = n_layer
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šmoe_start_layerï¼Œæ ¹æ®æ¨¡å‹å¤§å°è‡ªåŠ¨å†³å®š
        if moe_start_layer is None:
            # é»˜è®¤ï¼šå2/3çš„å±‚ä½¿ç”¨MoE
            moe_start_layer = max(0, n_layer // 3)
        self.moe_start_layer = moe_start_layer
        
        # Embeddingå±‚
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        self.drop = nn.Dropout(dropout)
        
        # æ··åˆTransformer Blocks
        self.blocks = nn.ModuleList()
        for i in range(n_layer):
            if i < moe_start_layer:
                # å‰å‡ å±‚ï¼šæ ‡å‡†Transformerï¼ˆå­¦ä¹ åŸºç¡€ç‰¹å¾ï¼‰
                block = StandardTransformerBlock(
                    d_model=n_embd,
                    num_heads=n_head,
                    dropout=dropout
                )
            else:
                # åå‡ å±‚ï¼šMoE Transformerï¼ˆå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰
                block = MoETransformerBlock(
                    d_model=n_embd,
                    num_heads=n_head,
                    num_experts=num_experts,
                    top_k=top_k,
                    dropout=dropout
                )
            self.blocks.append(block)
        
        # è¾“å‡ºå±‚
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
        
        # æƒé‡å…±äº«
        self.token_embedding.weight = self.lm_head.weight
        
        # åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, idx, targets=None):
        """
        idx: [batch_size, seq_len] - è¾“å…¥tokenç´¢å¼•
        targets: [batch_size, seq_len] - ç›®æ ‡tokenï¼ˆè®­ç»ƒæ—¶ï¼‰
        """
        B, T = idx.shape
        assert T <= self.block_size, f"åºåˆ—é•¿åº¦{T}è¶…è¿‡æœ€å¤§é•¿åº¦{self.block_size}"
        
        # Embeddings
        tok_emb = self.token_embedding(idx)  # [B, T, n_embd]
        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # [T, n_embd]
        x = self.drop(tok_emb + pos_emb)  # [B, T, n_embd]
        
        # Transformer Blocks + æ”¶é›†è¾…åŠ©æŸå¤±
        aux_losses = []
        for i, block in enumerate(self.blocks):
            if isinstance(block, MoETransformerBlock):
                # MoEå—ï¼šè¿”å›outputå’Œaux_loss
                x, aux_loss = block(x)
                aux_losses.append(aux_loss)
            else:
                # æ ‡å‡†å—ï¼šåªè¿”å›output
                x = block(x)
        
        # è¾“å‡º
        x = self.ln_f(x)  # [B, T, n_embd]
        logits = self.lm_head(x)  # [B, T, vocab_size]
        
        # è®¡ç®—æŸå¤±ï¼ˆè®­ç»ƒæ—¶ï¼‰
        loss = None
        if targets is not None:
            # ä¸»æŸå¤±ï¼šè¯­è¨€æ¨¡å‹çš„äº¤å‰ç†µ
            loss_lm = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1
            )
            
            # è¾…åŠ©æŸå¤±ï¼šè´Ÿè½½å‡è¡¡ï¼ˆåªæœ‰MoEå±‚æ‰æœ‰ï¼‰
            if aux_losses:
                loss_aux = sum(aux_losses) / len(aux_losses)
                loss = loss_lm + loss_aux
            else:
                loss = loss_lm
        
        return logits, loss
    
    def print_architecture(self):
        """æ‰“å°æ¨¡å‹æ¶æ„ä¿¡æ¯"""
        print("\n" + "="*60)
        print("æ¨¡å‹æ¶æ„:")
        print("="*60)
        print(f"æ€»å±‚æ•°: {self.n_layer}")
        print(f"MoEèµ·å§‹å±‚: {self.moe_start_layer}")
        print(f"æ ‡å‡†FFNå±‚: 0-{self.moe_start_layer-1} ({self.moe_start_layer}å±‚)")
        print(f"MoEå±‚: {self.moe_start_layer}-{self.n_layer-1} ({self.n_layer - self.moe_start_layer}å±‚)")
        print("\nå„å±‚è¯¦æƒ…:")
        for i, block in enumerate(self.blocks):
            block_type = "MoE" if isinstance(block, MoETransformerBlock) else "Standard"
            print(f"  Layer {i:2d}: {block_type}")
        print("="*60 + "\n")


# ===== è®­ç»ƒå™¨ =====

class MoETrainer:
    """
    å®Œæ•´çš„MoEè®­ç»ƒå™¨
    åŠŸèƒ½ï¼š
      - æ··åˆç²¾åº¦è®­ç»ƒ
      - å­¦ä¹ ç‡è°ƒåº¦
      - æ¢¯åº¦ç´¯ç§¯
      - æ£€æŸ¥ç‚¹ä¿å­˜
      - ä¸“å®¶ä½¿ç”¨ç›‘æ§
      - WandBæ—¥å¿—
    """
    def __init__(
        self,
        model,
        train_loader,
        val_loader=None,
        config=None
    ):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config or self.default_config()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.config['learning_rate'],
            betas=self.config['betas'],
            weight_decay=self.config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['max_steps'],
            eta_min=self.config['min_lr']
        )
        
        # æ··åˆç²¾åº¦
        self.scaler = GradScaler() if self.config['use_amp'] else None
        
        # ç»Ÿè®¡
        self.step = 0
        self.epoch = 0
        
        # WandB
        if self.config['use_wandb'] and HAS_WANDB:
            wandb.init(project="moe-training", config=self.config)
        elif self.config['use_wandb'] and not HAS_WANDB:
            print("âš ï¸  wandbæœªå®‰è£…ï¼Œè·³è¿‡wandbæ—¥å¿—è®°å½•")
    
    @staticmethod
    def default_config():
        return {
            'learning_rate': 3e-4,
            'min_lr': 3e-5,
            'betas': (0.9, 0.95),
            'weight_decay': 0.1,
            'grad_clip': 1.0,
            'max_steps': 100000,
            'warmup_steps': 2000,
            'eval_interval': 500,
            'save_interval': 1000,
            'log_interval': 100,
            'gradient_accumulation_steps': 4,
            'use_amp': True,
            'use_wandb': True,
        }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.model.train()
        
        pbar = tqdm(total=self.config['max_steps'], desc="Training")
        
        while self.step < self.config['max_steps']:
            for batch_idx, (x, y) in enumerate(self.train_loader):
                x, y = x.cuda(), y.cuda()
                
                # === å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰===
                with autocast(enabled=self.config['use_amp']):
                    logits, loss = self.model(x, targets=y)
                    loss = loss / self.config['gradient_accumulation_steps']
                
                # === åå‘ä¼ æ’­ ===
                if self.scaler:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                # === æ¢¯åº¦ç´¯ç§¯ ===
                if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:
                    # æ¢¯åº¦è£å‰ª
                    if self.scaler:
                        self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config['grad_clip']
                    )
                    
                    # æ›´æ–°å‚æ•°
                    if self.scaler:
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()
                    
                    self.optimizer.zero_grad()
                    self.scheduler.step()
                    
                    # æ›´æ–°æ­¥æ•°
                    self.step += 1
                    pbar.update(1)
                    
                    # === æ—¥å¿—è®°å½• ===
                    if self.step % self.config['log_interval'] == 0:
                        self.log_metrics(loss.item())
                    
                    # === è¯„ä¼° ===
                    if self.step % self.config['eval_interval'] == 0:
                        self.evaluate()
                    
                    # === ä¿å­˜æ£€æŸ¥ç‚¹ ===
                    if self.step % self.config['save_interval'] == 0:
                        self.save_checkpoint()
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if self.step >= self.config['max_steps']:
                        break
            
            self.epoch += 1
        
        pbar.close()
        print("è®­ç»ƒå®Œæˆï¼")
    
    def log_metrics(self, loss):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        lr = self.scheduler.get_last_lr()[0]
        
        metrics = {
            'train/loss': loss,
            'train/lr': lr,
            'train/step': self.step,
            'train/epoch': self.epoch,
        }
        
        # ç›‘æ§ä¸“å®¶ä½¿ç”¨æƒ…å†µ
        expert_stats = self.monitor_expert_usage()
        metrics.update(expert_stats)
        
        # æ‰“å°
        print(f"Step {self.step}: Loss={loss:.4f}, LR={lr:.6f}")
        
        # WandB
        if self.config['use_wandb'] and HAS_WANDB:
            wandb.log(metrics, step=self.step)
    
    def monitor_expert_usage(self):
        """
        ç›‘æ§ä¸“å®¶ä½¿ç”¨æƒ…å†µ
        è¿™æ˜¯MoEè®­ç»ƒçš„å…³é”®ï¼
        """
        # ç®€åŒ–ç‰ˆï¼šä»æ¨¡å‹ä¸­æå–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        # å®é™…å®ç°éœ€è¦åœ¨MoEå±‚ä¸­è®°å½•
        stats = {}
        
        # TODO: å®ç°ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        # ä¾‹å¦‚ï¼š
        # - æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
        # - è·¯ç”±å™¨çš„ç†µ
        # - è´Ÿè½½å‡è¡¡åˆ†æ•°
        
        return stats
    
    @torch.no_grad()
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        if self.val_loader is None:
            return
        
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        for x, y in self.val_loader:
            x, y = x.cuda(), y.cuda()
            logits, loss = self.model(x, targets=y)
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / num_batches
        
        print(f"  Validation Loss: {avg_loss:.4f}")
        
        if self.config['use_wandb'] and HAS_WANDB:
            wandb.log({'val/loss': avg_loss}, step=self.step)
        
        self.model.train()
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'model': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'step': self.step,
            'epoch': self.epoch,
            'config': self.config,
        }
        
        path = f"checkpoints/moe_step_{self.step}.pt"
        torch.save(checkpoint, path)
        print(f"  ä¿å­˜æ£€æŸ¥ç‚¹: {path}")


# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    print("="*60)
    print("MoE GPT æ¼”ç¤ºç¨‹åº")
    print("="*60)
    
    # æ£€æŸ¥CUDA
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nè®¾å¤‡: {device}")
    if device == 'cpu':
        print("âš ï¸  è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUä¼šå¾ˆæ…¢")
    
    # åˆ›å»ºä¸€ä¸ªè¾ƒå°çš„æ··åˆæ¶æ„æ¨¡å‹ç”¨äºæ¼”ç¤º
    print("\nåˆ›å»ºæ··åˆæ¶æ„MoEæ¨¡å‹...")
    model = MoEGPT(
        vocab_size=1000,      # è¾ƒå°çš„è¯æ±‡è¡¨
        block_size=128,       # è¾ƒçŸ­çš„åºåˆ—
        n_layer=6,            # 6å±‚
        n_head=4,
        n_embd=256,           # è¾ƒå°çš„ç»´åº¦
        num_experts=4,        # 4ä¸ªä¸“å®¶
        top_k=2,
        moe_start_layer=2,    # å‰2å±‚æ ‡å‡†FFNï¼Œå4å±‚MoE
        dropout=0.1
    )
    
    if device == 'cuda':
        model = model.cuda()
    
    # æ‰“å°æ¶æ„ä¿¡æ¯
    model.print_architecture()
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°é‡: {total_params/1e6:.2f}M")
    
    # åˆ›å»ºä¸€äº›è™šæ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
    print("\nç”Ÿæˆæ¼”ç¤ºæ•°æ®...")
    num_batches = 20
    dummy_data = []
    for _ in range(num_batches):
        x = torch.randint(0, 1000, (2, 64))  # batch=2, seq=64
        y = torch.randint(0, 1000, (2, 64))
        dummy_data.append((x, y))
    
    # ç®€å•çš„è®­ç»ƒå¾ªç¯ï¼ˆä¸ä½¿ç”¨å®Œæ•´çš„Trainerï¼‰
    print("\nå¼€å§‹è®­ç»ƒæ¼”ç¤ºï¼ˆ20æ­¥ï¼‰...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    model.train()
    
    for step, (x, y) in enumerate(dummy_data):
        x, y = x.to(device), y.to(device)
        
        # å‰å‘
        logits, loss = model(x, targets=y)
        
        # åå‘
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        # æ‰“å°
        if step % 5 == 0:
            print(f"  Step {step:2d}: Loss = {loss.item():.4f}")
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\n" + "="*60)
    print("è¯´æ˜:")
    print("  - è¿™æ˜¯ä¸€ä¸ªæ··åˆæ¶æ„çš„MoEå®ç°ï¼ŒåŒ…å«äº†æ ¸å¿ƒç»„ä»¶ï¼š")
    print("    âœ“ StandardTransformerBlock: æ ‡å‡†FFNå±‚ï¼ˆå­¦ä¹ åŸºç¡€ç‰¹å¾ï¼‰")
    print("    âœ“ MoETransformerBlock: MoEå±‚ï¼ˆå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰")
    print("    âœ“ SimpleMoE: Top-Kè·¯ç”±çš„MoEå±‚")
    print("    âœ“ è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆè¾…åŠ©æŸå¤±ï¼‰")
    print("\n  - æ··åˆæ¶æ„çš„ä¼˜åŠ¿ï¼ˆæ ¹æ®æ–‡æ¡£12ç« ï¼‰ï¼š")
    print("    âœ“ è®­ç»ƒæ›´ç¨³å®šï¼ˆæµ…å±‚ç”¨æ ‡å‡†FFNï¼‰")
    print("    âœ“ æ€§èƒ½æ›´å¥½ï¼ˆæ·±å±‚ç”¨MoEå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰")
    print("    âœ“ å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœ")
    print("\n  - é…ç½®ç­–ç•¥ï¼š")
    print("    â€¢ å°æ¨¡å‹(<1B): å1/3å±‚ç”¨MoE")
    print("    â€¢ ä¸­æ¨¡å‹(1-10B): å1/2å±‚ç”¨MoE")
    print("    â€¢ å¤§æ¨¡å‹(>10B): å2/3å±‚ç”¨MoE")
    print("\n  - è¦ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒï¼Œè¯·ï¼š")
    print("    1. å‡†å¤‡æ•°æ®åŠ è½½å™¨ï¼ˆDataLoaderï¼‰")
    print("    2. ä½¿ç”¨ MoETrainer ç±»è¿›è¡Œå®Œæ•´è®­ç»ƒ")
    print("    3. è°ƒæ•´æ¨¡å‹å¤§å°å’Œè¶…å‚æ•°")
    print("    4. æ ¹æ®æ¨¡å‹è§„æ¨¡è°ƒæ•´ moe_start_layer")
    print("="*60)
```

è¾“å‡ºï¼š
```
============================================================
MoE GPT æ¼”ç¤ºç¨‹åº
============================================================

è®¾å¤‡: cuda

åˆ›å»ºæ··åˆæ¶æ„MoEæ¨¡å‹...

============================================================
æ¨¡å‹æ¶æ„:
============================================================
æ€»å±‚æ•°: 6
MoEèµ·å§‹å±‚: 2
æ ‡å‡†FFNå±‚: 0-1 (2å±‚)
MoEå±‚: 2-5 (4å±‚)

å„å±‚è¯¦æƒ…:
  Layer  0: Standard
  Layer  1: Standard
  Layer  2: MoE
  Layer  3: MoE
  Layer  4: MoE
  Layer  5: MoE
============================================================

æ€»å‚æ•°é‡: 11.34M

ç”Ÿæˆæ¼”ç¤ºæ•°æ®...

å¼€å§‹è®­ç»ƒæ¼”ç¤ºï¼ˆ20æ­¥ï¼‰...
  Step  0: Loss = 6.9391
  Step  5: Loss = 6.9628
  Step 10: Loss = 7.0374
  Step 15: Loss = 7.1128

âœ… æ¼”ç¤ºå®Œæˆï¼

============================================================
è¯´æ˜:
  - è¿™æ˜¯ä¸€ä¸ªæ··åˆæ¶æ„çš„MoEå®ç°ï¼ŒåŒ…å«äº†æ ¸å¿ƒç»„ä»¶ï¼š
    âœ“ StandardTransformerBlock: æ ‡å‡†FFNå±‚ï¼ˆå­¦ä¹ åŸºç¡€ç‰¹å¾ï¼‰
    âœ“ MoETransformerBlock: MoEå±‚ï¼ˆå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰
    âœ“ SimpleMoE: Top-Kè·¯ç”±çš„MoEå±‚
    âœ“ è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆè¾…åŠ©æŸå¤±ï¼‰

  - æ··åˆæ¶æ„çš„ä¼˜åŠ¿ï¼ˆæ ¹æ®æ–‡æ¡£12ç« ï¼‰ï¼š
    âœ“ è®­ç»ƒæ›´ç¨³å®šï¼ˆæµ…å±‚ç”¨æ ‡å‡†FFNï¼‰
    âœ“ æ€§èƒ½æ›´å¥½ï¼ˆæ·±å±‚ç”¨MoEå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰
    âœ“ å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœ

  - é…ç½®ç­–ç•¥ï¼š
    â€¢ å°æ¨¡å‹(<1B): å1/3å±‚ç”¨MoE
    â€¢ ä¸­æ¨¡å‹(1-10B): å1/2å±‚ç”¨MoE
    â€¢ å¤§æ¨¡å‹(>10B): å2/3å±‚ç”¨MoE

  - è¦ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒï¼Œè¯·ï¼š
    1. å‡†å¤‡æ•°æ®åŠ è½½å™¨ï¼ˆDataLoaderï¼‰
    2. ä½¿ç”¨ MoETrainer ç±»è¿›è¡Œå®Œæ•´è®­ç»ƒ
    3. è°ƒæ•´æ¨¡å‹å¤§å°å’Œè¶…å‚æ•°
    4. æ ¹æ®æ¨¡å‹è§„æ¨¡è°ƒæ•´ moe_start_layer
============================================================
```

---

### ğŸŒ³ 5.3 MoEç‰¹æ®Šè°ƒè¯•æŠ€å·§

#### ğŸ¯ æŠ€å·§1ï¼šç›‘æ§ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ

```python
class ExpertUsageMonitor:
    """
    ä¸“å®¶ä½¿ç”¨ç›‘æ§å™¨
    å¸®åŠ©è¯Šæ–­è´Ÿè½½ä¸å‡è¡¡é—®é¢˜
    """
    def __init__(self, num_experts):
        self.num_experts = num_experts
        self.usage_counts = torch.zeros(num_experts)
        self.total_tokens = 0
    
    def update(self, expert_indices):
        """
        æ›´æ–°ç»Ÿè®¡
        expert_indices: [batch, seq_len, top_k] - è¢«é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
        """
        for expert_id in range(self.num_experts):
            count = (expert_indices == expert_id).sum().item()
            self.usage_counts[expert_id] += count
        
        self.total_tokens += expert_indices.numel()
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        usage_percent = (self.usage_counts / self.total_tokens) * 100
        
        return {
            f'expert_{i}_usage': usage_percent[i].item()
            for i in range(self.num_experts)
        }
    
    def print_report(self):
        """æ‰“å°æŠ¥å‘Š"""
        usage_percent = (self.usage_counts / self.total_tokens) * 100
        
        print("\n" + "="*50)
        print("ä¸“å®¶ä½¿ç”¨æŠ¥å‘Š")
        print("="*50)
        
        for i in range(self.num_experts):
            bar = 'â–ˆ' * int(usage_percent[i] / 2)
            print(f"ä¸“å®¶{i}: {usage_percent[i]:5.2f}% {bar}")
        
        # è®¡ç®—æ ‡å‡†å·®ï¼ˆè¡¡é‡å‡è¡¡ç¨‹åº¦ï¼‰
        std = usage_percent.std().item()
        ideal = 100 / self.num_experts
        
        print(f"\nç†æƒ³ä½¿ç”¨ç‡: {ideal:.2f}%")
        print(f"æ ‡å‡†å·®: {std:.2f}%")
        
        if std < 5:
            print("âœ… è´Ÿè½½éå¸¸å‡è¡¡ï¼")
        elif std < 10:
            print("âœ… è´Ÿè½½åŸºæœ¬å‡è¡¡")
        else:
            print("âš ï¸ è´Ÿè½½ä¸å‡è¡¡ï¼Œè€ƒè™‘å¢å¤§aux_loss_weight")
        
        print("="*50 + "\n")
    
    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.usage_counts = torch.zeros(self.num_experts)
        self.total_tokens = 0


# ä½¿ç”¨ç¤ºä¾‹
monitor = ExpertUsageMonitor(num_experts=8)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for step in range(1000):
    # ... è®­ç»ƒä»£ç  ...
    
    # æ›´æ–°ç›‘æ§ï¼ˆéœ€è¦åœ¨MoEå±‚ä¸­è®°å½•expert_indicesï¼‰
    # monitor.update(expert_indices)
    
    # æ¯1000æ­¥æ‰“å°ä¸€æ¬¡æŠ¥å‘Š
    if step % 1000 == 0:
        monitor.print_report()
        monitor.reset()
```

#### ğŸ¯ æŠ€å·§2ï¼šè·¯ç”±ç†µç›‘æ§

```python
def compute_routing_entropy(router_probs):
    """
    è®¡ç®—è·¯ç”±ç†µ
    é«˜ç†µ = è·¯ç”±åˆ†æ•£ï¼ˆå¥½ï¼‰
    ä½ç†µ = è·¯ç”±é›†ä¸­ï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰
    
    router_probs: [batch, seq_len, num_experts]
    """
    # è®¡ç®—æ¯ä¸ªtokençš„ç†µ
    entropy = -(router_probs * torch.log(router_probs + 1e-10)).sum(dim=-1)
    
    # å¹³å‡ç†µ
    avg_entropy = entropy.mean().item()
    
    # æœ€å¤§å¯èƒ½ç†µï¼ˆå®Œå…¨å‡åŒ€åˆ†å¸ƒï¼‰
    num_experts = router_probs.size(-1)
    max_entropy = -torch.log(torch.tensor(1.0 / num_experts))
    
    # å½’ä¸€åŒ–ç†µï¼ˆ0-1ï¼‰
    normalized_entropy = avg_entropy / max_entropy
    
    return {
        'routing_entropy': avg_entropy,
        'routing_entropy_normalized': normalized_entropy,
    }

# åœ¨è®­ç»ƒä¸­ä½¿ç”¨
# entropy_stats = compute_routing_entropy(router_probs)
# print(f"è·¯ç”±ç†µ: {entropy_stats['routing_entropy']:.4f}")
```

#### ğŸ¯ æŠ€å·§3ï¼šå¯è§†åŒ–ä¸“å®¶ä¸“é•¿

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_expert_specialization(model, dataloader, num_samples=1000):
    """
    å¯è§†åŒ–ä¸“å®¶ä¸“é•¿
    åˆ†ææ¯ä¸ªä¸“å®¶å€¾å‘äºå¤„ç†ä»€ä¹ˆç±»å‹çš„token
    """
    model.eval()
    
    # æ”¶é›†æ•°æ®
    expert_token_counts = {}  # {expert_id: {token_id: count}}
    
    with torch.no_grad():
        for batch_idx, (x, y) in enumerate(dataloader):
            if batch_idx >= num_samples // x.size(0):
                break
            
            x = x.cuda()
            
            # å‰å‘ä¼ æ’­ï¼ˆéœ€è¦è®°å½•è·¯ç”±å†³ç­–ï¼‰
            # è¿™éœ€è¦ä¿®æ”¹MoEå±‚ä»¥è¿”å›è·¯ç”±ä¿¡æ¯
            # logits, loss, routing_info = model(x, return_routing=True)
            
            # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶å¤„ç†çš„token
            # ...
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    for expert_id in range(8):
        ax = axes[expert_id]
        
        # è·å–è¿™ä¸ªä¸“å®¶æœ€å¸¸å¤„ç†çš„token
        # top_tokens = sorted(expert_token_counts[expert_id].items(),
        #                     key=lambda x: x[1], reverse=True)[:20]
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        # ...
        
        ax.set_title(f'ä¸“å®¶{expert_id}')
        ax.set_xlabel('Token ID')
        ax.set_ylabel('å¤„ç†æ¬¡æ•°')
    
    plt.tight_layout()
    plt.savefig('expert_specialization.png')
    print("ä¸“å®¶ä¸“é•¿å¯è§†åŒ–å·²ä¿å­˜ï¼")
```

---

### ğŸŒ³ 5.4 å¸¸è§è®­ç»ƒé—®é¢˜åŠè§£å†³

#### âš ï¸ é—®é¢˜1ï¼šæŸäº›ä¸“å®¶ä»ä¸è¢«ä½¿ç”¨

```python
ç°è±¡ï¼š
  ä¸“å®¶0: 30% ä½¿ç”¨ç‡
  ä¸“å®¶1: 25% ä½¿ç”¨ç‡
  ä¸“å®¶2: 20% ä½¿ç”¨ç‡
  ä¸“å®¶3: 15% ä½¿ç”¨ç‡
  ä¸“å®¶4: 10% ä½¿ç”¨ç‡
  ä¸“å®¶5: 0%  ä½¿ç”¨ç‡ âŒ
  ä¸“å®¶6: 0%  ä½¿ç”¨ç‡ âŒ
  ä¸“å®¶7: 0%  ä½¿ç”¨ç‡ âŒ

åŸå› ï¼š
  - è¾…åŠ©æŸå¤±æƒé‡å¤ªå°
  - åˆå§‹åŒ–ä¸å¥½
  - æ•°æ®ä¸å¤Ÿå¤šæ ·

è§£å†³æ–¹æ¡ˆï¼š

# æ–¹æ¡ˆ1ï¼šå¢å¤§è¾…åŠ©æŸå¤±
config = {
    'aux_loss_weight': 0.05,  # ä»0.01å¢åˆ°0.05
}

# æ–¹æ¡ˆ2ï¼šä¸“å®¶Dropout
class MoEWithExpertDropout(nn.Module):
    def forward(self, x):
        # è®­ç»ƒæ—¶éšæœºç¦ç”¨ä¸€äº›ä¸“å®¶
        # å¼ºåˆ¶ä½¿ç”¨å…¶ä»–ä¸“å®¶
        if self.training:
            # éšæœºmaskæ‰1-2ä¸ªæœ€å¸¸ç”¨çš„ä¸“å®¶
            pass
        
        # æ­£å¸¸MoEå‰å‘
        return moe_forward(x)

# æ–¹æ¡ˆ3ï¼šå¼ºåˆ¶å‡åŒ€åˆå§‹åŒ–è·¯ç”±å™¨
def init_router_uniform(router):
    # è®©è·¯ç”±å™¨åˆå§‹åŒ–ä¸ºæ¥è¿‘å‡åŒ€åˆ†å¸ƒ
    nn.init.zeros_(router.weight)
```

#### âš ï¸ é—®é¢˜2ï¼šè®­ç»ƒä¸ç¨³å®šï¼ŒLosséœ‡è¡

```python
ç°è±¡ï¼š
  Step 1000: Loss=2.5
  Step 1100: Loss=2.3
  Step 1200: Loss=5.8 âŒ çªç„¶å¢å¤§
  Step 1300: Loss=2.4
  Step 1400: Loss=8.2 âŒ åˆå¢å¤§

åŸå› ï¼š
  - æŸäº›ä¸“å®¶æ¢¯åº¦çˆ†ç‚¸
  - è¾…åŠ©æŸå¤±å¹²æ‰°ä¸»ä»»åŠ¡
  - å­¦ä¹ ç‡å¤ªå¤§

è§£å†³æ–¹æ¡ˆï¼š

# æ–¹æ¡ˆ1ï¼šæ›´æ¿€è¿›çš„æ¢¯åº¦è£å‰ª
config = {
    'grad_clip': 0.5,  # ä»1.0é™åˆ°0.5
}

# æ–¹æ¡ˆ2ï¼šwarmupæ›´é•¿
config = {
    'warmup_steps': 5000,  # ä»2000å¢åˆ°5000
}

# æ–¹æ¡ˆ3ï¼šå‡å°è¾…åŠ©æŸå¤±
config = {
    'aux_loss_weight': 0.005,  # ä»0.01é™åˆ°0.005
}

# æ–¹æ¡ˆ4ï¼šç›‘æ§æ¯ä¸ªä¸“å®¶çš„æ¢¯åº¦
for name, param in model.named_parameters():
    if 'expert' in name:
        grad_norm = param.grad.norm().item()
        if grad_norm > 10:  # é˜ˆå€¼
            print(f"âš ï¸ {name} æ¢¯åº¦è¿‡å¤§: {grad_norm:.2f}")
```

#### âš ï¸ é—®é¢˜3ï¼šæ˜¾å­˜ä¸è¶³

```python
é”™è¯¯ï¼šCUDA out of memory

è§£å†³æ–¹æ¡ˆï¼š

# æ–¹æ¡ˆ1ï¼šå‡å°‘ä¸“å®¶æ•°é‡
config = {
    'num_experts': 4,  # ä»8é™åˆ°4
}

# æ–¹æ¡ˆ2ï¼šæ¢¯åº¦ç´¯ç§¯
config = {
    'batch_size': 4,  # ä»16é™åˆ°4
    'gradient_accumulation_steps': 4,  # ç­‰æ•ˆbatch=16
}

# æ–¹æ¡ˆ3ï¼šæ··åˆç²¾åº¦è®­ç»ƒ
config = {
    'use_amp': True,  # ä½¿ç”¨FP16
}

# æ–¹æ¡ˆ4ï¼šä¸“å®¶å‚æ•°offloadï¼ˆé«˜çº§ï¼‰
# è®­ç»ƒæ—¶æŠŠä¸ç”¨çš„ä¸“å®¶ç§»åˆ°CPU
# åªåœ¨éœ€è¦æ—¶åŠ è½½åˆ°GPU
```

---

## ğŸ“š ç¬¬å…­éƒ¨åˆ†ï¼šMoEå˜ä½“å¤§å…¨ï¼ˆäº†è§£å‰æ²¿ï¼‰

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šäº†è§£MoEçš„å„ç§å˜ä½“åŠå…¶åˆ›æ–°ç‚¹

### ğŸŒ± 6.1 Switch Transformerï¼ˆæœ€æˆåŠŸçš„å˜ä½“ï¼‰

#### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ï¼šTop-1è·¯ç”±

**Google 2021å¹´æå‡ºï¼Œ1.6Tå‚æ•°**

```python
ä¼ ç»ŸMoE vs Switch Transformerï¼š

ä¼ ç»ŸMoEï¼ˆTop-2ï¼‰:
  æ¯ä¸ªtoken â†’ é€‰2ä¸ªä¸“å®¶ â†’ åŠ æƒç»„åˆ
  
  ä¼˜ç‚¹ï¼šå®¹é”™æ€§å¥½
  ç¼ºç‚¹ï¼šè®¡ç®—é‡2å€

Switch Transformerï¼ˆTop-1ï¼‰:
  æ¯ä¸ªtoken â†’ åªé€‰1ä¸ªä¸“å®¶ â†’ ç›´æ¥ä½¿ç”¨
  
  ä¼˜ç‚¹ï¼š
    âœ… è®¡ç®—é‡æœ€å°ï¼ˆç¨€ç–åº¦æœ€é«˜ï¼‰
    âœ… è®­ç»ƒæ›´å¿«ï¼ˆ4-7å€ï¼‰
    âœ… å®ç°æ›´ç®€å•
  
  ç¼ºç‚¹ï¼š
    âŒ å®¹é”™æ€§ç•¥å·®
    âŒ æŸä¸ªä¸“å®¶æ•…éšœå½±å“å¤§
```

#### ğŸ“Š Switch Transformeré…ç½®

```python
# Switch-Base (7Bå‚æ•°ï¼Œ128ä¸“å®¶)
config = {
    'n_layer': 12,
    'n_embd': 768,
    'num_experts': 128,  # å¾ˆå¤šä¸“å®¶
    'top_k': 1,          # åªé€‰1ä¸ª
    'capacity_factor': 1.25,
    'aux_loss_weight': 0.01,
}

# Switch-XXL (395Bå‚æ•°ï¼Œ2048ä¸“å®¶)
config = {
    'n_layer': 48,
    'n_embd': 2048,
    'num_experts': 2048,  # è¶…å¤šä¸“å®¶ï¼
    'top_k': 1,
    'capacity_factor': 1.0,  # ä¸¥æ ¼å®¹é‡
    'aux_loss_weight': 0.01,
}

# è®­ç»ƒæ•ˆæœ
Switch vs T5ï¼ˆåŒç­‰è´¨é‡ï¼‰:
  è®­ç»ƒæ—¶é—´ï¼š1/4
  è®­ç»ƒæˆæœ¬ï¼š1/7
  æ€§èƒ½ï¼šç›¸å½“æˆ–æ›´å¥½
```

#### ğŸ”§ Switchç‰¹æœ‰æŠ€å·§

```python
# æŠ€å·§1ï¼šæ›´å¤§çš„ä¸“å®¶æ•°é‡
# Switchè¯æ˜ï¼š128-2048ä¸ªä¸“å®¶éƒ½æœ‰æ•ˆ
# åªè¦è´Ÿè½½å‡è¡¡åšå¥½

# æŠ€å·§2ï¼šé€‰æ‹©æ€§ç²¾åº¦
# è·¯ç”±å™¨ç”¨FP32ï¼ˆç²¾ç¡®ï¼‰
# ä¸“å®¶ç”¨FP16ï¼ˆå¿«é€Ÿï¼‰

class SwitchLayer(nn.Module):
    def forward(self, x):
        # è·¯ç”±å™¨ï¼šFP32
        with torch.autocast(enabled=False):
            router_logits = self.router(x.float())
            top_1_idx = router_logits.argmax(dim=-1)
        
        # ä¸“å®¶ï¼šFP16
        expert_out = self.experts[top_1_idx](x)
        
        return expert_out
```

---

### ğŸŒ± 6.2 Expert Choice Routingï¼ˆåå‘è·¯ç”±ï¼‰

#### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ï¼šä¸“å®¶é€‰tokenï¼Œè€Œä¸æ˜¯tokené€‰ä¸“å®¶

**Google 2022å¹´æå‡º**

```python
ä¼ ç»Ÿè·¯ç”±ï¼ˆToken Choiceï¼‰:
  é—®é¢˜ï¼š"æ¯ä¸ªtokené€‰Kä¸ªä¸“å®¶"
  
  Token_A: æˆ‘é€‰ Expert_1, Expert_2
  Token_B: æˆ‘é€‰ Expert_1, Expert_3
  Token_C: æˆ‘é€‰ Expert_1, Expert_2
  ...
  
  ç»“æœï¼š
    Expert_1: è¿‡è½½ï¼ˆè¢«å¾ˆå¤štokené€‰ä¸­ï¼‰âŒ
    Expert_4: é—²ç½®ï¼ˆæ²¡äººé€‰ï¼‰âŒ
    â†’ è´Ÿè½½ä¸å‡è¡¡ï¼

Expert Choiceï¼ˆä¸“å®¶é€‰tokenï¼‰:
  åˆ›æ–°ï¼š"æ¯ä¸ªä¸“å®¶é€‰Kä¸ªtoken"
  
  Expert_1: æˆ‘é€‰ Token_A, Token_D
  Expert_2: æˆ‘é€‰ Token_B, Token_E
  Expert_3: æˆ‘é€‰ Token_C, Token_F
  Expert_4: æˆ‘é€‰ Token_G, Token_H
  
  ç»“æœï¼š
    æ¯ä¸ªä¸“å®¶å¤„ç†ç›¸åŒæ•°é‡çš„token âœ…
    â†’ å®Œç¾è´Ÿè½½å‡è¡¡ï¼
```

#### ğŸ“ å®ç°åŸç†

```python
def expert_choice_routing(x, router, capacity_per_expert):
    """
    ä¸“å®¶é€‰æ‹©è·¯ç”±
    
    x: [num_tokens, d_model]
    router: [d_model, num_experts]
    capacity_per_expert: æ¯ä¸ªä¸“å®¶å¤„ç†å¤šå°‘token
    """
    num_tokens, d_model = x.shape
    num_experts = router.size(1)
    
    # 1. è®¡ç®—äº²å’Œåº¦çŸ©é˜µ
    # æ¯ä¸ªtokenä¸æ¯ä¸ªä¸“å®¶çš„åŒ¹é…åº¦
    affinity = x @ router  # [num_tokens, num_experts]
    
    # 2. æ¯ä¸ªä¸“å®¶é€‰æ‹©Top-Kä¸ªtokenï¼ˆè½¬ç½®åé€‰æ‹©ï¼‰
    # è¿™æ˜¯å…³é”®åˆ›æ–°ï¼
    top_k_per_expert, top_indices_per_expert = torch.topk(
        affinity.T,  # [num_experts, num_tokens]
        k=capacity_per_expert,
        dim=-1
    )
    # top_indices_per_expert: [num_experts, capacity]
    
    # 3. æ„å»ºåˆ†é…çŸ©é˜µ
    assignment = torch.zeros_like(affinity)
    for expert_id in range(num_experts):
        selected_tokens = top_indices_per_expert[expert_id]
        assignment[selected_tokens, expert_id] = 1.0
    
    # 4. å½’ä¸€åŒ–æƒé‡
    gates = F.softmax(affinity, dim=-1) * assignment
    gates = gates / (gates.sum(dim=-1, keepdim=True) + 1e-9)
    
    # 5. ä¸“å®¶è®¡ç®—
    output = torch.zeros_like(x)
    for expert_id in range(num_experts):
        selected_tokens = top_indices_per_expert[expert_id]
        expert_input = x[selected_tokens]
        expert_output = experts[expert_id](expert_input)
        
        # åŠ æƒå†™å›
        weights = gates[selected_tokens, expert_id].unsqueeze(-1)
        output[selected_tokens] += weights * expert_output
    
    return output


# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾æœ‰1000ä¸ªtokenï¼Œ8ä¸ªä¸“å®¶
# æ¯ä¸ªä¸“å®¶å¤„ç† 1000/8 = 125ä¸ªtoken
capacity = 125

output = expert_choice_routing(x, router, capacity)
# ä¿è¯ï¼šæ¯ä¸ªä¸“å®¶æ°å¥½å¤„ç†125ä¸ªtokenï¼
```

#### ğŸ¯ ä¼˜ç¼ºç‚¹

```python
ä¼˜ç‚¹ï¼š
  âœ… å®Œç¾è´Ÿè½½å‡è¡¡ï¼ˆä¸éœ€è¦è¾…åŠ©æŸå¤±ï¼‰
  âœ… è®­ç»ƒæ›´ç¨³å®š
  âœ… ä¸æµªè´¹è®¡ç®—èµ„æº
  
ç¼ºç‚¹ï¼š
  âŒ å®ç°ç¨å¤æ‚
  âŒ æŸäº›tokenå¯èƒ½è¢«å¤šä¸ªä¸“å®¶å¿½ç•¥
  âŒ éœ€è¦ä»”ç»†è®¾ç½®å®¹é‡
```

---

### ğŸŒ± 6.3 Mixtral 8x7Bï¼ˆå¼€æºä¹‹å…‰ï¼‰

#### ğŸ’¡ Mistral AIçš„å®ç”¨è®¾è®¡

**2024å¹´ï¼Œç¬¬ä¸€ä¸ªå¼€æºçš„é«˜è´¨é‡MoE**

```python
Mixtral 8x7B ç‰¹ç‚¹ï¼š

æ¶æ„ï¼š
  - 8ä¸ªä¸“å®¶ï¼Œæ¯ä¸ª7Bå‚æ•°
  - Top-2è·¯ç”±
  - 32å±‚ï¼Œæ¯å±‚éƒ½æ˜¯MoE
  - 32Kä¸Šä¸‹æ–‡é•¿åº¦

å‚æ•°é‡ï¼š
  æ€»å‚æ•°ï¼š47B
  æ¿€æ´»å‚æ•°ï¼š13Bï¼ˆæ¯æ¬¡åªç”¨2ä¸ªä¸“å®¶ï¼‰
  
æ€§èƒ½ï¼š
  åª²ç¾æˆ–è¶…è¿‡ Llama 2 70B
  ä½†æ¨ç†é€Ÿåº¦å¿«6å€ï¼

å¼€æºï¼š
  âœ… æƒé‡å¼€æº
  âœ… å¯å•†ç”¨
  âœ… ç¤¾åŒºæ´»è·ƒ
```

#### ğŸ“Š Mixtral vs å¯†é›†æ¨¡å‹

```python
å¯¹æ¯”ï¼šMixtral 8x7B vs Llama 2 70B

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŒ‡æ ‡           â”‚ Mixtral   â”‚ Llama 70Bâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€»å‚æ•°         â”‚ 47B       â”‚ 70B      â”‚
â”‚ æ¿€æ´»å‚æ•°       â”‚ 13B       â”‚ 70B      â”‚
â”‚ æ˜¾å­˜éœ€æ±‚       â”‚ 90GB      â”‚ 140GB    â”‚
â”‚ æ¨ç†é€Ÿåº¦       â”‚ å¿« âœ…     â”‚ æ…¢       â”‚
â”‚ MMLUå¾—åˆ†       â”‚ 70.6      â”‚ 69.8     â”‚
â”‚ HumanEval      â”‚ 40.2      â”‚ 29.9     â”‚
â”‚ å¼€æº           â”‚ âœ…        â”‚ âœ…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç»“è®ºï¼š
  Mixtralç”¨æ›´å°‘å‚æ•°ï¼Œè·å¾—æ›´å¥½æ€§èƒ½
  è¿™å°±æ˜¯MoEçš„å¨åŠ›ï¼
```

---

### ğŸŒ± 6.4 å…¶ä»–é‡è¦å˜ä½“

#### ğŸ”§ Soft MoEï¼ˆGoogle 2023ï¼‰

```python
åˆ›æ–°ï¼šä¸æ˜¯ç¡¬é€‰æ‹©ï¼Œè€Œæ˜¯è½¯æ··åˆ

ä¼ ç»ŸMoEï¼ˆç¡¬é€‰æ‹©ï¼‰:
  Token â†’ é€‰Top-2ä¸“å®¶ â†’ åªè®¡ç®—è¿™2ä¸ª
  
Soft MoEï¼ˆè½¯æ··åˆï¼‰:
  Token â†’ è®¡ç®—æ‰€æœ‰ä¸“å®¶ â†’ åŠ æƒå¹³å‡

å®ç°ï¼š
output = Î£ softmax(logits)_i Â· expert_i(x)
        i=1..N

ä¼˜ç‚¹ï¼š
  âœ… å®Œå…¨å¯å¾®ï¼ˆæœ‰åˆ©äºè®­ç»ƒï¼‰
  âœ… ä¸éœ€è¦è´Ÿè½½å‡è¡¡
  
ç¼ºç‚¹ï¼š
  âŒ å¤±å»ç¨€ç–æ€§ï¼ˆè®¡ç®—é‡å¤§ï¼‰
  âŒ ä¸é€‚åˆå¤§è§„æ¨¡

é€‚ç”¨ï¼š
  - å°æ¨¡å‹å®éªŒ
  - ç ”ç©¶ç¨€ç–æ€§çš„å½±å“
```

#### ğŸ”§ MoE-LoRAï¼ˆ2023ï¼‰

```python
åˆ›æ–°ï¼šæŠŠMoEå’ŒLoRAç»“åˆ

æ ‡å‡†MoEï¼š
  æ¯ä¸ªä¸“å®¶æ˜¯å®Œæ•´çš„FFNï¼ˆå‚æ•°å¤šï¼‰

MoE-LoRAï¼š
  å…±äº«åŸºç¡€FFN + å¤šä¸ªLoRAä¸“å®¶ï¼ˆå‚æ•°å°‘ï¼‰
  
  Base_FFN: [d, 4d, d]  # å…±äº«
  Expert_1: LoRA(rank=16)  # åªæœ‰å‡ Må‚æ•°
  Expert_2: LoRA(rank=16)
  ...
  
ä¼˜ç‚¹ï¼š
  âœ… å‚æ•°æ•ˆç‡æ›´é«˜
  âœ… æ˜¾å­˜éœ€æ±‚æ›´å°
  âœ… é€‚åˆå¾®è°ƒ
  
é€‚ç”¨ï¼š
  - èµ„æºå—é™åœºæ™¯
  - æ¨¡å‹å¾®è°ƒ
  - å¤šä»»åŠ¡å­¦ä¹ 
```

#### ğŸ”§ Hierarchical MoEï¼ˆå±‚æ¬¡åŒ–MoEï¼‰

```python
åˆ›æ–°ï¼šå¤šçº§è·¯ç”±

ä¼ ç»ŸMoEï¼š
  Token â†’ ç›´æ¥é€‰ä¸“å®¶
  
Hierarchical MoEï¼š
  Token â†’ é€‰ä¸“å®¶ç»„ â†’ å†é€‰ç»„å†…ä¸“å®¶
  
ç¤ºä¾‹ï¼ˆ64ä¸ªä¸“å®¶ï¼‰:
  Level 1: é€‰æ‹©8ä¸ªç»„ä¹‹ä¸€
  Level 2: é€‰æ‹©ç»„å†…8ä¸ªä¸“å®¶ä¹‹ä¸€
  
  æ€»å…±ï¼š8 Ã— 8 = 64ä¸ªä¸“å®¶
  
ä¼˜ç‚¹ï¼š
  âœ… è·¯ç”±å†³ç­–æ›´ç®€å•
  âœ… å¯ä»¥å½¢æˆä¸“å®¶å±‚æ¬¡
  âœ… æ‰©å±•æ€§æ›´å¥½
  
ç¼ºç‚¹ï¼š
  âŒ å®ç°å¤æ‚
  âŒ ä¸¤çº§è·¯ç”±éƒ½å¯èƒ½å‡ºé”™
```

---

### ğŸŒ± 6.5 MoEå˜ä½“å¯¹æ¯”æ€»ç»“

#### ğŸ“Š æ¨ªå‘å¯¹æ¯”

| å˜ä½“ | è·¯ç”±æ–¹å¼ | ç¨€ç–åº¦ | è´Ÿè½½å‡è¡¡ | å®ç°éš¾åº¦ | æ¨èåº¦ |
|------|---------|--------|---------|---------|--------|
| **ä¼ ç»ŸMoE** | Tokené€‰ä¸“å®¶ï¼ŒTop-2 | é«˜ | éœ€è¦è¾…åŠ©æŸå¤± | ä¸­ | â­â­â­â­ |
| **Switch** | Tokené€‰ä¸“å®¶ï¼ŒTop-1 | æœ€é«˜ | éœ€è¦è¾…åŠ©æŸå¤± | ä½ | â­â­â­â­â­ |
| **Expert Choice** | ä¸“å®¶é€‰Token | é«˜ | å¤©ç„¶å‡è¡¡ âœ… | é«˜ | â­â­â­â­ |
| **Mixtral** | Tokené€‰ä¸“å®¶ï¼ŒTop-2 | é«˜ | éœ€è¦è¾…åŠ©æŸå¤± | ä¸­ | â­â­â­â­â­ |
| **Soft MoE** | åŠ æƒæ‰€æœ‰ä¸“å®¶ | ä½ âŒ | ä¸éœ€è¦ | ä½ | â­â­ |
| **MoE-LoRA** | Tokené€‰LoRA | é«˜ | éœ€è¦è¾…åŠ©æŸå¤± | ä¸­ | â­â­â­ |

#### ğŸ¯ é€‰æ‹©æŒ‡å—

```python
if ä½ æ˜¯åˆå­¦è€…:
    æ¨èï¼šSwitch Transformer
    ç†ç”±ï¼šæœ€ç®€å•ï¼ŒTop-1è·¯ç”±ï¼Œå¤§é‡è®ºæ–‡å’Œæ•™ç¨‹

elif è¿½æ±‚æœ€ä½³æ€§èƒ½:
    æ¨èï¼šMixtralé£æ ¼ï¼ˆTop-2ï¼‰
    ç†ç”±ï¼šå¼€æºï¼Œæ€§èƒ½éªŒè¯ï¼Œç¤¾åŒºæ”¯æŒ

elif è´Ÿè½½å‡è¡¡æ˜¯å¤§é—®é¢˜:
    æ¨èï¼šExpert Choice
    ç†ç”±ï¼šå¤©ç„¶å‡è¡¡ï¼Œä¸éœ€è¦è°ƒå‚

elif æ˜¾å­˜/å‚æ•°å—é™:
    æ¨èï¼šMoE-LoRA
    ç†ç”±ï¼šå‚æ•°æ•ˆç‡é«˜ï¼Œé€‚åˆå¾®è°ƒ

elif åªæ˜¯å®éªŒç¨€ç–æ€§:
    æ¨èï¼šSoft MoE
    ç†ç”±ï¼šå®ç°ç®€å•ï¼Œä¾¿äºç†è§£
```

---

## ğŸ“š ç¬¬ä¸ƒéƒ¨åˆ†ï¼šMoEå®Œæ•´å®æˆ˜æ¡ˆä¾‹

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šé€šè¿‡å®Œæ•´æ¡ˆä¾‹æŒæ¡MoEè®­ç»ƒçš„å…¨æµç¨‹

### ğŸŒ± 7.1 æ¡ˆä¾‹èƒŒæ™¯ï¼šä»å¯†é›†æ¨¡å‹åˆ°MoE

#### ğŸ’¡ åˆå§‹çŠ¶æ€

```python
ç°æœ‰æ¨¡å‹ï¼š
  æ¶æ„ï¼šæ ‡å‡†GPT
  å‚æ•°é‡ï¼š768M
  æ€§èƒ½ï¼šè¿˜ä¸é”™ï¼Œä½†æƒ³è¦æ›´å¥½
  é—®é¢˜ï¼š
    - å¢å¤§æ¨¡å‹ â†’ æˆæœ¬ç¿»å€
    - è®­ç»ƒæ›´ä¹… â†’ æ—¶é—´å¤ªé•¿
  
ç›®æ ‡ï¼š
  ç”¨ç›¸ä¼¼æˆæœ¬ï¼Œè·å¾—æ›´å¥½æ€§èƒ½
  
æ–¹æ¡ˆï¼š
  æ”¹é€ ä¸ºMoEæ¨¡å‹
```

---

### ğŸŒ± 7.2 å®Œæ•´å®æ–½æ­¥éª¤

#### æ­¥éª¤1ï¼šæ¨¡å‹è®¾è®¡

```python
# === åŸå§‹å¯†é›†æ¨¡å‹é…ç½® ===
dense_config = {
    'vocab_size': 50257,
    'block_size': 1024,
    'n_layer': 12,
    'n_head': 12,
    'n_embd': 768,
    'dropout': 0.1,
}
# å‚æ•°é‡ï¼š~768M

# === MoEæ¨¡å‹é…ç½® ===
moe_config = {
    'vocab_size': 50257,
    'block_size': 1024,
    'n_layer': 12,
    'n_head': 12,
    'n_embd': 768,
    'dropout': 0.1,
    
    # MoEç‰¹æœ‰
    'num_experts': 8,      # 8ä¸ªä¸“å®¶
    'top_k': 2,            # Top-2è·¯ç”±
    'capacity_factor': 1.25,
    'aux_loss_weight': 0.01,
    'moe_start_layer': 4,  # ä»ç¬¬4å±‚å¼€å§‹ç”¨MoE
}
# å‚æ•°é‡ï¼š~2.4Bï¼ˆ3å€ï¼‰
# æ¿€æ´»å‚æ•°ï¼š~900Mï¼ˆç•¥å¤šäºåŸå§‹æ¨¡å‹ï¼‰
```

#### æ­¥éª¤2ï¼šæ•°æ®å‡†å¤‡

```python
# æ•°æ®é›†ï¼šOpenWebTextï¼ˆ40GBæ–‡æœ¬ï¼‰
# å¤„ç†æ–¹å¼ï¼šå’Œæ ‡å‡†GPTå®Œå…¨ç›¸åŒ

from torch.utils.data import DataLoader
from datasets import load_dataset

# åŠ è½½æ•°æ®
dataset = load_dataset("openwebtext")

# Tokenization
def tokenize(examples):
    return tokenizer(examples['text'], truncation=True, max_length=1024)

tokenized_dataset = dataset.map(tokenize, batched=True)

# DataLoader
train_loader = DataLoader(
    tokenized_dataset,
    batch_size=4,  # å°batch
    shuffle=True
)
```

#### æ­¥éª¤3ï¼šæ¨¡å‹åˆ›å»º

```python
# åˆ›å»ºMoEæ¨¡å‹
model = MoEGPT(**moe_config).cuda()

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ€»å‚æ•°é‡: {total_params/1e9:.2f}B")

# æ˜¾å­˜ä¼°ç®—
# è®­ç»ƒæ—¶æ˜¾å­˜ â‰ˆ æ¿€æ´»å‚æ•° Ã— 16å­—èŠ‚ï¼ˆFP16ï¼‰
estimated_memory = 0.9e9 * 16 / 1024**3
print(f"é¢„è®¡æ˜¾å­˜: {estimated_memory:.1f}GB")

# ä¸“å®¶åˆ†å¸ƒ
print("\næ¨¡å‹ç»“æ„:")
for i, block in enumerate(model.blocks):
    is_moe = isinstance(block, MoETransformerBlock)
    print(f"  Layer {i}: {'MoE' if is_moe else 'Dense'}")
```

#### æ­¥éª¤4ï¼šè®­ç»ƒé…ç½®

```python
# å®Œæ•´è®­ç»ƒé…ç½®
config = {
    # æ¨¡å‹
    'model_config': moe_config,
    
    # æ•°æ®
    'batch_size': 4,
    'gradient_accumulation_steps': 16,  # ç­‰æ•ˆbatch=64
    'block_size': 1024,
    
    # ä¼˜åŒ–å™¨
    'learning_rate': 3e-4,
    'min_lr': 3e-5,
    'weight_decay': 0.1,
    'betas': (0.9, 0.95),
    'grad_clip': 1.0,
    
    # è®­ç»ƒ
    'max_iters': 100000,
    'warmup_iters': 2000,
    'eval_interval': 500,
    'save_interval': 5000,
    
    # MoEç‰¹æœ‰
    'aux_loss_weight': 0.01,
    'monitor_expert_usage': True,
    
    # æ··åˆç²¾åº¦
    'use_amp': True,
    'dtype': 'float16',
}

# åˆ›å»ºè®­ç»ƒå™¨
trainer = MoETrainer(model, train_loader, val_loader, config)
```

#### æ­¥éª¤5ï¼šè®­ç»ƒç›‘æ§

```python
# å…³é”®æŒ‡æ ‡ç›‘æ§

import wandb

wandb.init(project="moe-gpt-training", config=config)

# ç›‘æ§å†…å®¹
monitor_metrics = {
    # æ ‡å‡†æŒ‡æ ‡
    'train/loss': None,
    'train/lr': None,
    'val/loss': None,
    'val/perplexity': None,
    
    # MoEç‰¹æœ‰æŒ‡æ ‡
    'moe/expert_0_usage': None,
    'moe/expert_1_usage': None,
    # ... å…¶ä»–ä¸“å®¶
    'moe/routing_entropy': None,
    'moe/load_balance_loss': None,
    'moe/expert_usage_std': None,  # æ ‡å‡†å·®
    
    # æ€§èƒ½æŒ‡æ ‡
    'perf/tokens_per_sec': None,
    'perf/memory_used_gb': None,
}

# æ¯100æ­¥è®°å½•ä¸€æ¬¡
if step % 100 == 0:
    wandb.log(monitor_metrics, step=step)
```

#### æ­¥éª¤6ï¼šè®­ç»ƒæ‰§è¡Œ

```python
# å¼€å§‹è®­ç»ƒ
print("å¼€å§‹è®­ç»ƒMoEæ¨¡å‹...")
print(f"æ€»æ­¥æ•°: {config['max_iters']}")
print(f"ç­‰æ•ˆbatchå¤§å°: {config['batch_size'] * config['gradient_accumulation_steps']}")

trainer.train()

# è®­ç»ƒè¿‡ç¨‹è¾“å‡ºç¤ºä¾‹ï¼š
"""
Step 0: Loss=10.5234, LR=0.000000
  Expert Usage: [12.1%, 13.2%, 11.8%, 12.9%, 12.5%, 12.1%, 13.1%, 12.3%]
  âœ… è´Ÿè½½å‡è¡¡è‰¯å¥½ï¼ˆæ ‡å‡†å·®: 0.5%ï¼‰

Step 100: Loss=5.2341, LR=0.000015
  Expert Usage: [11.9%, 13.5%, 11.2%, 12.8%, 12.1%, 12.5%, 13.3%, 12.7%]
  âœ… è´Ÿè½½å‡è¡¡è‰¯å¥½ï¼ˆæ ‡å‡†å·®: 0.8%ï¼‰

Step 500: Loss=3.1234, LR=0.000150
  Validation Loss: 3.2567
  Expert Usage: [12.3%, 12.1%, 12.8%, 11.9%, 12.5%, 12.2%, 12.9%, 13.3%]
  âœ… è´Ÿè½½å‡è¡¡è‰¯å¥½ï¼ˆæ ‡å‡†å·®: 0.5%ï¼‰
  
Step 1000: Loss=2.8234, LR=0.000300
  Validation Loss: 2.9123
  Routing Entropy: 2.05 (max: 2.08)
  âœ… è·¯ç”±åˆ†æ•£åº¦é«˜
  
  ä¿å­˜æ£€æŸ¥ç‚¹: checkpoints/moe_step_1000.pt
...
"""
```

---

### ğŸŒ± 7.3 è®­ç»ƒç»“æœåˆ†æ

#### ğŸ“Š æ€§èƒ½å¯¹æ¯”

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŒ‡æ ‡             â”‚ å¯†é›†æ¨¡å‹   â”‚ MoEæ¨¡å‹      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å‚æ•°é‡           â”‚ 768M       â”‚ 2.4B (3.1Ã—)  â”‚
â”‚ æ¿€æ´»å‚æ•°         â”‚ 768M       â”‚ 900M (1.2Ã—)  â”‚
â”‚                  â”‚            â”‚              â”‚
â”‚ è®­ç»ƒæ—¶é—´         â”‚ 100å°æ—¶    â”‚ 120å°æ—¶(+20%)â”‚
â”‚ è®­ç»ƒæˆæœ¬(A100)   â”‚ $1,000     â”‚ $1,200(+20%) â”‚
â”‚ æ˜¾å­˜éœ€æ±‚         â”‚ 32GB       â”‚ 40GB (+25%)  â”‚
â”‚                  â”‚            â”‚              â”‚
â”‚ éªŒè¯Loss(æœ€ç»ˆ)   â”‚ 2.50       â”‚ 2.35 âœ…      â”‚
â”‚ éªŒè¯Perplexity   â”‚ 12.18      â”‚ 10.49 âœ…     â”‚
â”‚                  â”‚            â”‚              â”‚
â”‚ æ¨ç†é€Ÿåº¦(batch=1)â”‚ 100tok/s   â”‚ 90tok/s(-10%)â”‚
â”‚ æ¨ç†é€Ÿåº¦(batch=8)â”‚ 650tok/s   â”‚ 600tok/s(-8%)â”‚
â”‚ æ¨ç†æ˜¾å­˜         â”‚ 2GB        â”‚ 5GB (+150%)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®ç»“è®ºï¼š
  âœ… ç”¨20%é¢å¤–æˆæœ¬ï¼Œè·å¾—15%æ€§èƒ½æå‡
  âœ… å‚æ•°å¤š3å€ï¼Œä½†è®­ç»ƒæˆæœ¬åªå¢åŠ 20%
  âš ï¸ æ¨ç†ç¨æ…¢ï¼Œä½†å¯æ¥å—
  âš ï¸ æ¨ç†æ˜¾å­˜éœ€æ±‚å¢åŠ ï¼ˆæ‰€æœ‰ä¸“å®¶éƒ½è¦åŠ è½½ï¼‰
```

#### ğŸ“ˆ è®­ç»ƒæ›²çº¿

```python
# è®­ç»ƒLossæ›²çº¿å¯¹æ¯”

å¯†é›†æ¨¡å‹ Loss:
  Step 0:     10.5
  Step 10k:   4.2
  Step 30k:   3.1
  Step 60k:   2.7
  Step 100k:  2.5

MoEæ¨¡å‹ Loss:
  Step 0:     10.5
  Step 10k:   3.9  (-0.3 vs Dense)
  Step 30k:   2.8  (-0.3)
  Step 60k:   2.5  (-0.2)
  Step 100k:  2.35 (-0.15) âœ…

è§‚å¯Ÿï¼š
  - MoEåœ¨å„ä¸ªé˜¶æ®µéƒ½ç•¥ä¼˜äºå¯†é›†æ¨¡å‹
  - å·®è·åœ¨è®­ç»ƒåæœŸç¨³å®šåœ¨0.15å·¦å³
  - è¯´æ˜MoEç¡®å®æœ‰æ•ˆ
```

#### ğŸ” ä¸“å®¶ä¸“ç²¾åŒ–åˆ†æ

```python
# è®­ç»ƒå®Œæˆåï¼Œåˆ†ææ¯ä¸ªä¸“å®¶çš„ä¸“é•¿

ä¸“å®¶ä½¿ç”¨æƒ…å†µï¼ˆæŒ‰tokenç±»å‹ç»Ÿè®¡ï¼‰:

Expert_0:
  ä»£ç token: 45%  â† ä¸“ç²¾ç¼–ç¨‹
  æ•°å­¦ç¬¦å·: 25%
  å…¶ä»–: 30%

Expert_1:
  æ—¥å¸¸å¯¹è¯: 60%  â† ä¸“ç²¾å¯¹è¯
  ä¿šè¯­: 20%
  å…¶ä»–: 20%

Expert_2:
  ç§‘å­¦æœ¯è¯­: 50%  â† ä¸“ç²¾å­¦æœ¯
  ä¸“ä¸šåè¯: 30%
  å…¶ä»–: 20%

Expert_3:
  æ–‡å­¦æ€§è¯æ±‡: 40%  â† ä¸“ç²¾æ–‡å­¦
  ä¿®è¾æ‰‹æ³•: 35%
  å…¶ä»–: 25%

... (å…¶ä»–ä¸“å®¶ç±»ä¼¼)

å‘ç°ï¼š
  âœ… æ¯ä¸ªä¸“å®¶ç¡®å®å­¦åˆ°äº†ä¸“é•¿
  âœ… ä¸“é•¿æ˜¯è‡ªåŠ¨å­¦å‡ºæ¥çš„ï¼ˆæ— äººå·¥è®¾è®¡ï¼‰
  âœ… ä¸“é•¿ä¹‹é—´æœ‰ä¸€å®šé‡å ï¼ˆåˆç†ï¼‰
```

---

### ğŸŒ± 7.4 å®æˆ˜ç»éªŒæ€»ç»“

#### âœ… æˆåŠŸçš„å…³é”®

```python
1. è´Ÿè½½å‡è¡¡åšå¥½
   - ä½¿ç”¨åˆé€‚çš„aux_loss_weight (0.01)
   - ç›‘æ§ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
   - åŠæ—¶è°ƒæ•´å‚æ•°

2. è®­ç»ƒç¨³å®šæ€§
   - æ›´é•¿çš„warmup (2000 steps)
   - åˆé€‚çš„æ¢¯åº¦è£å‰ª (1.0)
   - æ··åˆç²¾åº¦è®­ç»ƒ

3. èµ„æºç®¡ç†
   - æ¢¯åº¦ç´¯ç§¯èŠ‚çœæ˜¾å­˜
   - æ··åˆç²¾åº¦å‡å°‘æ˜¾å­˜
   - å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹

4. ç›‘æ§åˆ°ä½
   - å®æ—¶ç›‘æ§ä¸“å®¶ä½¿ç”¨
   - è¿½è¸ªè·¯ç”±ç†µ
   - å¯¹æ¯”å¯†é›†æ¨¡å‹åŸºçº¿
```

#### âš ï¸ é‡åˆ°çš„é—®é¢˜åŠè§£å†³

```python
é—®é¢˜1ï¼šè®­ç»ƒåˆæœŸä¸“å®¶ä¸¥é‡ä¸å‡è¡¡
  ç°è±¡ï¼šExpert_0ä½¿ç”¨80%ï¼Œå…¶ä»–é—²ç½®
  åŸå› ï¼šaux_loss_weightå¤ªå°
  è§£å†³ï¼šä»0.001å¢åˆ°0.01 âœ…

é—®é¢˜2ï¼šè®­ç»ƒä¸­æœŸLossçªç„¶ä¸Šå‡
  ç°è±¡ï¼šStep 15000æ—¶Lossä»2.8è·³åˆ°4.2
  åŸå› ï¼šæŸä¸ªä¸“å®¶æ¢¯åº¦çˆ†ç‚¸
  è§£å†³ï¼š
    - é™ä½learning_rate
    - åŠ å¼ºæ¢¯åº¦è£å‰ªï¼ˆ1.0 â†’ 0.5ï¼‰
    - å¢åŠ warmup âœ…

é—®é¢˜3ï¼šéªŒè¯Lossä¸ä¸‹é™
  ç°è±¡ï¼šè®­ç»ƒLossç»§ç»­é™ï¼ŒéªŒè¯Losså¡åœ¨2.9
  åŸå› ï¼šè¿‡æ‹Ÿåˆ
  è§£å†³ï¼š
    - å¢å¤§dropout (0.1 â†’ 0.15)
    - å¢å¤§weight_decay (0.1 â†’ 0.15)
    - ä½¿ç”¨æ›´å¤šéªŒè¯æ•°æ® âœ…

é—®é¢˜4ï¼šæ¨ç†æ˜¾å­˜å ç”¨å¤ªå¤§
  ç°è±¡ï¼šæ¨ç†éœ€è¦5GBæ˜¾å­˜
  åŸå› ï¼šæ‰€æœ‰ä¸“å®¶éƒ½è¦åŠ è½½
  è§£å†³ï¼š
    - é‡åŒ–ä¸“å®¶å‚æ•°ï¼ˆFP16 â†’ INT8ï¼‰
    - æ¨ç†æ—¶åªåŠ è½½å¸¸ç”¨ä¸“å®¶
    - ä½¿ç”¨ä¸“å®¶å¸è½½æŠ€æœ¯ âœ…
```

---

## ğŸ“š ç¬¬å…«éƒ¨åˆ†ï¼šMoEçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜ï¼ˆå…¨é¢åˆ†æï¼‰

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šå®¢è§‚è¯„ä»·MoEï¼ŒçŸ¥é“ä½•æ—¶ä½¿ç”¨

### ğŸŒ± 8.1 MoEçš„æ ¸å¿ƒä¼˜åŠ¿

#### âœ… ä¼˜åŠ¿1ï¼šå‚æ•°æ•ˆç‡

```python
æ ¸å¿ƒï¼šå‚æ•°å¤šï¼Œè®¡ç®—å°‘

å…·ä½“ä¾‹å­ï¼š
  Mixtral 8x7B:
    æ€»å‚æ•°ï¼š47B
    æ¿€æ´»å‚æ•°ï¼š13Bï¼ˆåªç”¨27%ï¼‰
    
  å¯¹æ¯” Llama 2 70B:
    æ€»å‚æ•°ï¼š70B
    æ¿€æ´»å‚æ•°ï¼š70Bï¼ˆå…¨ç”¨ï¼‰
    
  ç»“æœï¼š
    Mixtralæ€§èƒ½ â‰ˆ Llama 70B
    ä½†è®¡ç®—é‡åªæœ‰1/5 âœ…
    
ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
  ä¸åŒçŸ¥è¯†åŸŸåˆ†ç¦»å­˜å‚¨
  â†’ æ¨¡å‹å®¹é‡å¤§
  
  æ¯æ¬¡åªç”¨ç›¸å…³çŸ¥è¯†
  â†’ è®¡ç®—é‡å°
```

#### âœ… ä¼˜åŠ¿2ï¼šè®­ç»ƒæ•ˆç‡

```python
è®­ç»ƒé€Ÿåº¦å¯¹æ¯”ï¼ˆåŒç­‰è´¨é‡ï¼‰:

Switch Transformer vs T5-XXL:
  è¾¾åˆ°ç›¸åŒè´¨é‡ï¼š
    Switch: 25ä¸‡æ­¥
    T5-XXL: 100ä¸‡æ­¥
  
  è®­ç»ƒæ—¶é—´ï¼š
    Switch: 1å‘¨
    T5-XXL: 4å‘¨
  
  æˆæœ¬ï¼š
    Switch: $650K
    T5-XXL: $4.6M
  
  â†’ Switchå¿«4å€ï¼Œçœ7å€æˆæœ¬ âœ…
```

#### âœ… ä¼˜åŠ¿3ï¼šå¯æ‰©å±•æ€§

```python
æ‰©å±•æ–¹ä¾¿ï¼š

å¯†é›†æ¨¡å‹æ‰©å±•ï¼š
  1B â†’ 10B â†’ 100B
  â†“     â†“      â†“
  éœ€è¦é‡æ–°è®­ç»ƒæ¯ä¸€æ­¥
  æˆæœ¬çº¿æ€§å¢é•¿

MoEæ‰©å±•ï¼š
  8ä¸“å®¶ â†’ 16ä¸“å®¶ â†’ 64ä¸“å®¶
  â†“       â†“        â†“
  åªéœ€å¢åŠ ä¸“å®¶æ•°é‡
  å¯ä»¥ä»å°æ¨¡å‹çƒ­å¯åŠ¨
  
ä¼˜åŠ¿ï¼š
  âœ… å¢é‡æ‰©å±•
  âœ… å¯ä»¥é‡ç”¨å·²è®­ç»ƒä¸“å®¶
  âœ… æ‰©å±•æˆæœ¬ä½
```

#### âœ… ä¼˜åŠ¿4ï¼šä¸“å®¶ä¸“ç²¾åŒ–

```python
è‡ªåŠ¨ä»»åŠ¡åˆ†è§£ï¼š

è®­ç»ƒåå‘ç°ï¼š
  Expert_0: æ“…é•¿ä»£ç 
  Expert_1: æ“…é•¿å¯¹è¯
  Expert_2: æ“…é•¿ç§‘å­¦
  ...

å¥½å¤„ï¼š
  âœ… æ¯ä¸ªä¸“å®¶ç²¾é€šä¸€ä¸ªé¢†åŸŸ
  âœ… æ•´ä½“èƒ½åŠ›å¼º
  âœ… å¯è§£é‡Šæ€§å¥½ï¼ˆçŸ¥é“å“ªä¸ªä¸“å®¶å¤„ç†ï¼‰
  
åº”ç”¨ï¼š
  - å¤šè¯­è¨€æ¨¡å‹ï¼ˆæ¯ä¸ªä¸“å®¶è´Ÿè´£ä¸€ç§è¯­è¨€ï¼‰
  - å¤šä»»åŠ¡å­¦ä¹ ï¼ˆæ¯ä¸ªä¸“å®¶è´Ÿè´£ä¸€ä¸ªä»»åŠ¡ï¼‰
  - æŒç»­å­¦ä¹ ï¼ˆæ–°ä¸“å®¶å­¦æ–°çŸ¥è¯†ï¼‰
```

---

### ğŸŒ± 8.2 MoEçš„ä¸»è¦æŒ‘æˆ˜

#### âŒ æŒ‘æˆ˜1ï¼šè®­ç»ƒå¤æ‚åº¦

```python
é—®é¢˜ï¼š

1. è´Ÿè½½å‡è¡¡éš¾
   - ä¸“å®¶ä½¿ç”¨ä¸å‡
   - éœ€è¦è¾…åŠ©æŸå¤±
   - è°ƒå‚å›°éš¾

2. è®­ç»ƒä¸ç¨³å®š
   - æŸäº›ä¸“å®¶æ¢¯åº¦çˆ†ç‚¸
   - è·¯ç”±åå¡Œ
   - éœ€è¦ç‰¹æ®ŠæŠ€å·§

3. è¶…å‚æ•°å¤š
   - num_experts
   - top_k
   - capacity_factor
   - aux_loss_weight
   - ...

è§£å†³æ–¹æ¡ˆï¼š
  âœ… ä½¿ç”¨æˆç†Ÿé…ç½®ï¼ˆå¦‚Mixtralï¼‰
  âœ… å……åˆ†ç›‘æ§
  âœ… å‚è€ƒè®ºæ–‡ç»éªŒ
```

#### âŒ æŒ‘æˆ˜2ï¼šé€šä¿¡å¼€é”€

```python
åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜ï¼š

ä¸“å®¶å¹¶è¡Œï¼ˆä¸åŒGPUè´Ÿè´£ä¸åŒä¸“å®¶ï¼‰:
  
  æ­¥éª¤1ï¼šTokenåˆ†å‘åˆ°ä¸“å®¶
    GPU_0 â†’ GPU_2 (Expert_2)
    GPU_1 â†’ GPU_3 (Expert_3)
    ... (All-to-Allé€šä¿¡)
  
  æ­¥éª¤2ï¼šä¸“å®¶è®¡ç®—
  
  æ­¥éª¤3ï¼šç»“æœæ±‡æ€»
    GPU_2 â†’ GPU_0
    GPU_3 â†’ GPU_1
    ... (All-to-Allé€šä¿¡)
  
é—®é¢˜ï¼š
  âŒ All-to-Allé€šä¿¡æ…¢
  âŒ éœ€è¦é«˜é€Ÿäº’è”ï¼ˆInfiniBandï¼‰
  âŒ ç½‘ç»œæˆä¸ºç“¶é¢ˆ

è§£å†³æ–¹æ¡ˆï¼š
  âœ… ä½¿ç”¨é«˜é€Ÿç½‘ç»œ
  âœ… ä¸“å®¶æ”¾åŒä¸€èŠ‚ç‚¹
  âœ… ä¼˜åŒ–é€šä¿¡æ¨¡å¼
```

#### âŒ æŒ‘æˆ˜3ï¼šæ¨ç†æ•ˆç‡

```python
æ¨ç†æ—¶çš„é—®é¢˜ï¼š

1. æ˜¾å­˜å ç”¨å¤§
   - æ‰€æœ‰ä¸“å®¶éƒ½è¦åŠ è½½
   - Mixtral 47Béœ€è¦90GBæ˜¾å­˜
   
2. è·¯ç”±å¼€é”€
   - æ¯ä¸ªtokenéƒ½è¦è·¯ç”±
   - å¢åŠ å»¶è¿Ÿ
   
3. æ‰¹å¤„ç†éš¾
   - ä¸åŒtokenç”¨ä¸åŒä¸“å®¶
   - éš¾ä»¥æ‰¹é‡è®¡ç®—
   - GPUåˆ©ç”¨ç‡ä½

è§£å†³æ–¹æ¡ˆï¼š
  âœ… é‡åŒ–ï¼ˆINT8/INT4ï¼‰
  âœ… ä¸“å®¶å¸è½½ï¼ˆåªåŠ è½½å¸¸ç”¨ä¸“å®¶ï¼‰
  âœ… ä¸“å®¶ç¼“å­˜
  âœ… æ‰¹å¤„ç†ä¼˜åŒ–
```

#### âŒ æŒ‘æˆ˜4ï¼šå·¥ç¨‹å¤æ‚åº¦

```python
å®ç°å’Œéƒ¨ç½²æŒ‘æˆ˜ï¼š

1. ä»£ç å¤æ‚
   - è·¯ç”±é€»è¾‘
   - ä¸“å®¶åˆ†å‘
   - è´Ÿè½½å‡è¡¡
   - è°ƒè¯•å›°éš¾

2. éƒ¨ç½²å¤æ‚
   - æ¨¡å‹æ–‡ä»¶å¤§
   - éœ€è¦ç‰¹æ®ŠåŠ è½½
   - æ¨ç†æ¡†æ¶æ”¯æŒå°‘

3. ç»´æŠ¤æˆæœ¬é«˜
   - ç›‘æ§æ›´å¤šæŒ‡æ ‡
   - é—®é¢˜å®šä½å›°éš¾
   - éœ€è¦ä¸“ä¸šçŸ¥è¯†

å»ºè®®ï¼š
  âœ… ä½¿ç”¨æˆç†Ÿæ¡†æ¶ï¼ˆDeepSpeed MoEï¼‰
  âœ… å‚è€ƒå¼€æºå®ç°ï¼ˆMixtralï¼‰
  âœ… ä»å°è§„æ¨¡å¼€å§‹
```

---

### ğŸŒ± 8.3 MoE vs å¯†é›†æ¨¡å‹ï¼šä½•æ—¶é€‰æ‹©ï¼Ÿ

#### ğŸ¯ å†³ç­–æ ‘

```python
ä½ åº”è¯¥ä½¿ç”¨MoEå¦‚æœï¼š
  âœ… æ¨¡å‹è§„æ¨¡å¤§ï¼ˆ>10Bå‚æ•°ï¼‰
  âœ… æ•°æ®å¤šæ ·åŒ–ï¼ˆå¤šè¯­è¨€ã€å¤šé¢†åŸŸï¼‰
  âœ… è®­ç»ƒé¢„ç®—æœ‰é™ä½†è¿½æ±‚å¤§æ¨¡å‹
  âœ… æœ‰åˆ†å¸ƒå¼è®­ç»ƒèµ„æº
  âœ… å¯ä»¥æ¥å—æ¨ç†å¤æ‚åº¦
  
  ä¾‹å­ï¼š
    - GPT-4ï¼ˆå¤§è§„æ¨¡ã€å¤šä»»åŠ¡ï¼‰
    - Mixtralï¼ˆå¼€æºæ›¿ä»£ï¼‰
    - å¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹

ä½ åº”è¯¥ä½¿ç”¨å¯†é›†æ¨¡å‹å¦‚æœï¼š
  âœ… æ¨¡å‹è§„æ¨¡å°ï¼ˆ<1Bå‚æ•°ï¼‰
  âœ… å•ä¸€ä»»åŠ¡/é¢†åŸŸ
  âœ… è¿½æ±‚ç®€å•éƒ¨ç½²
  âœ… æ¨ç†å»¶è¿Ÿæ•æ„Ÿ
  âœ… å•æœºè®­ç»ƒ
  
  ä¾‹å­ï¼š
    - BERTï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
    - å°å‹å¯¹è¯æ¨¡å‹
    - è¾¹ç¼˜è®¾å¤‡æ¨¡å‹
```

#### ğŸ“Š ç»¼åˆå¯¹æ¯”è¡¨

| ç»´åº¦ | å¯†é›†æ¨¡å‹ | MoEæ¨¡å‹ | æ¨èåœºæ™¯ |
|------|---------|---------|---------|
| **è®­ç»ƒæˆæœ¬** | é«˜ | ä¸­ï¼ˆåŒç­‰è´¨é‡ä¸‹ä½ï¼‰ | MoE â­â­â­ |
| **è®­ç»ƒé€Ÿåº¦** | æ…¢ | å¿«ï¼ˆ4-7å€ï¼‰ | MoE â­â­â­ |
| **æ¨ç†é€Ÿåº¦** | å¿« | ä¸­ï¼ˆæ…¢10-20%ï¼‰ | å¯†é›† â­â­â­ |
| **æ¨ç†æ˜¾å­˜** | å° | å¤§ï¼ˆ2-3å€ï¼‰ | å¯†é›† â­â­â­ |
| **æ¨¡å‹è´¨é‡** | å¥½ | å¾ˆå¥½ï¼ˆåŒæˆæœ¬ï¼‰ | MoE â­â­â­ |
| **å®ç°éš¾åº¦** | ç®€å• | å¤æ‚ | å¯†é›† â­â­â­ |
| **å¯æ‰©å±•æ€§** | ä¸€èˆ¬ | å¾ˆå¥½ | MoE â­â­â­ |
| **å¯è§£é‡Šæ€§** | ä¸€èˆ¬ | å¥½ï¼ˆä¸“å®¶ä¸“é•¿ï¼‰ | MoE â­â­â­ |
| **éƒ¨ç½²éš¾åº¦** | ç®€å• | å¤æ‚ | å¯†é›† â­â­â­ |
| **é€‚ç”¨è§„æ¨¡** | å°ä¸­å¤§ | ä¸­å¤§ | çœ‹åœºæ™¯ |

---

## ğŸ“š ç¬¬ä¹éƒ¨åˆ†ï¼šå¸¸è§é—®é¢˜FAQå’Œå®æˆ˜æŒ‡å—

> **æœ¬éƒ¨åˆ†ç›®æ ‡**ï¼šè§£ç­”æœ€å¸¸è§çš„10ä¸ªé—®é¢˜ï¼Œæä¾›å®æˆ˜å»ºè®®

### ğŸŒ± 9.1 åå¤§å¸¸è§é—®é¢˜

#### â“ Q1: MoEå’Œå¯†é›†æ¨¡å‹æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ

**A**: æ ¸å¿ƒæ˜¯**ç¨€ç–æ¿€æ´»**ã€‚

```python
å¯†é›†æ¨¡å‹ï¼ˆå¦‚GPT-3ï¼‰:
  æ‰€æœ‰å‚æ•°: 175B
  æ¿€æ´»å‚æ•°: 175Bï¼ˆå…¨éƒ¨ï¼‰ â† æ¯ä¸ªtokenéƒ½ç”¨
  è®¡ç®—é‡: å¤§
  æ¨ç†é€Ÿåº¦: æ…¢

MoEæ¨¡å‹ï¼ˆå¦‚Switch-XXLï¼‰:
  æ‰€æœ‰å‚æ•°: 395B
  æ¿€æ´»å‚æ•°: 13Bï¼ˆåªç”¨3.3%ï¼‰ â† æ¯ä¸ªtokenåªç”¨éƒ¨åˆ†
  è®¡ç®—é‡: å°
  æ¨ç†é€Ÿåº¦: å¿«

å…³é”®å·®å¼‚ï¼š
  å¯†é›†: æ¯ä¸ªtokenä½¿ç”¨æ‰€æœ‰å‚æ•°
  MoE: æ¯ä¸ªtokenåªä½¿ç”¨ç›¸å…³ä¸“å®¶

æ¯”å–»ï¼š
  å¯†é›†æ¨¡å‹ = å…¨ç§‘åŒ»ç”Ÿï¼ˆä»€ä¹ˆéƒ½æ‡‚ä¸€ç‚¹ï¼Œä½†ä¸ç²¾æ·±ï¼‰
  MoEæ¨¡å‹ = ä¸“ç§‘åŒ»é™¢ï¼ˆæ¯ä¸ªä¸“å®¶ç²¾é€šä¸€ä¸ªé¢†åŸŸï¼‰

å®é™…æ•ˆæœï¼š
  Switch-XXL (395B, æ¿€æ´»13B) â‰ˆ GPT-3 (175B)
  ä½†è®­ç»ƒå’Œæ¨ç†æ›´å¿«ï¼
```

---

#### â“ Q2: ä¸ºä»€ä¹ˆMoEèƒ½æå‡æ•ˆç‡ï¼Ÿ

**A**: **å‚æ•°å¤šã€è®¡ç®—å°‘ã€ä¸“å®¶ä¸“ç²¾**ã€‚

```python
# æ•ˆç‡æ¥æº1ï¼šç¨€ç–æ¿€æ´»
å¯†é›†æ¨¡å‹ï¼ˆ7Bå‚æ•°ï¼‰:
  æ¯ä¸ªtoken: 7Bæ¬¡ä¹˜æ³•
  100ä¸ªtoken: 700Bæ¬¡ä¹˜æ³•

MoEæ¨¡å‹ï¼ˆ8Ã—7B=56Bå‚æ•°ï¼ŒTop-2ï¼‰:
  æ¯ä¸ªtoken: 2Ã—7B = 14Bæ¬¡ä¹˜æ³•
  100ä¸ªtoken: 1400Bæ¬¡ä¹˜æ³•
  
ç­‰ç­‰ï¼Œè®¡ç®—é‡ä¸æ˜¯æ›´å¤šå—ï¼Ÿ

å…³é”®ï¼šå‚æ•°é‡ vs è®¡ç®—é‡
  MoEå‚æ•°é‡: 56Bï¼ˆ8å€ï¼‰ â† æ¨¡å‹å®¹é‡å¤§
  MoEè®¡ç®—é‡: 14Bï¼ˆ2å€ï¼‰ â† å®é™…è®¡ç®—å°‘
  
  ç»“æœï¼š
  - æ¨¡å‹å®¹é‡æå‡8å€ï¼ˆå¯ä»¥å­¦æ›´å¤šçŸ¥è¯†ï¼‰
  - è®¡ç®—é‡åªå¢åŠ 2å€ï¼ˆè®­ç»ƒ/æ¨ç†å¿«ï¼‰
  - æ€§èƒ½æå‡ > 2å€ âœ…

# æ•ˆç‡æ¥æº2ï¼šä¸“å®¶ä¸“ç²¾
ä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæ¨¡å¼:
  Expert_0: æ“…é•¿ä»£ç 
  Expert_1: æ“…é•¿å¯¹è¯
  Expert_2: æ“…é•¿æ•°å­¦
  
æ¯ä¸ªtokenåªéœ€è¦ç›¸å…³ä¸“å®¶:
  å†™ä»£ç  â†’ Expert_0 ï¼ˆä¸éœ€è¦å…¶ä»–7ä¸ªï¼‰
  ç¿»è¯‘ â†’ Expert_1 ï¼ˆä¸éœ€è¦å…¶ä»–7ä¸ªï¼‰
  
â†’ æ¯ä¸ªä¸“å®¶å¯ä»¥æ›´æ·±å…¥å­¦ä¹ ç‰¹å®šçŸ¥è¯†
â†’ æ•´ä½“æ€§èƒ½æ›´å¥½

# å®æµ‹æ•°æ®ï¼ˆSwitch vs T5ï¼‰:
  å‚æ•°é‡: Switchæ˜¯T5çš„7å€
  è®­ç»ƒé€Ÿåº¦: Switchå¿«4å€ âœ…
  è®­ç»ƒæˆæœ¬: Switchæ˜¯T5çš„1/7 âœ…
  æ€§èƒ½: ç›¸å½“æˆ–æ›´å¥½ âœ…
```

---

#### â“ Q3: å¦‚ä½•è§£å†³è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼Ÿ

**A**: **è¾…åŠ©æŸå¤± + ä¸“å®¶å®¹é‡ + ç›‘æ§**ã€‚

```python
# é—®é¢˜ï¼šæŸäº›ä¸“å®¶è¿‡è½½
ä¸“å®¶ä½¿ç”¨æƒ…å†µï¼ˆæœªå¹³è¡¡ï¼‰:
  ä¸“å®¶0: 80% tokens  # è¿‡è½½ï¼
  ä¸“å®¶1: 15% tokens
  ä¸“å®¶2: 5% tokens   # æµªè´¹
  ä¸“å®¶3: 0% tokens   # å®Œå…¨æœªç”¨

# è§£å†³æ–¹æ¡ˆ1ï¼šè¾…åŠ©æŸå¤±ï¼ˆLoad Balance Lossï¼‰
def load_balance_loss(router_probs, expert_mask):
    """
    é¼“åŠ±å‡åŒ€ä½¿ç”¨ä¸“å®¶
    """
    # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„è´Ÿè½½
    expert_load = expert_mask.float().mean(dim=0)  # [num_experts]
    
    # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„è·¯ç”±æ¦‚ç‡
    router_prob_per_expert = router_probs.mean(dim=0)  # [num_experts]
    
    # è¾…åŠ©æŸå¤±ï¼šP_i Ã— f_i ä¹‹å’Œ
    loss = (expert_load * router_prob_per_expert).sum() * num_experts
    return loss

# æ·»åŠ åˆ°æ€»æŸå¤±
total_loss = lm_loss + 0.01 * load_balance_loss
# 0.01æ˜¯æƒé‡ï¼Œé€šå¸¸å–0.001-0.1

# è§£å†³æ–¹æ¡ˆ2ï¼šä¸“å®¶å®¹é‡ï¼ˆCapacityï¼‰
capacity = (num_tokens / num_experts) * capacity_factor
# capacity_factoré€šå¸¸æ˜¯1.25

if expert_tokens > capacity:
    # ä¸¢å¼ƒå¤šä½™çš„tokenæˆ–ä½¿ç”¨æº¢å‡ºæœºåˆ¶
    expert_tokens = expert_tokens[:capacity]

# è§£å†³æ–¹æ¡ˆ3ï¼šç›‘æ§å’Œè°ƒæ•´
# å®æ—¶ç›‘æ§ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
expert_usage = count_expert_usage(top_k_indices)
print(f"Expert usage: {expert_usage}")

# å¦‚æœä¸å‡è¡¡ï¼Œè°ƒæ•´å‚æ•°:
if max(expert_usage) / min(expert_usage) > 3:
    # ä¸“å®¶ä½¿ç”¨å·®è·>3å€ï¼Œéœ€è¦è°ƒæ•´
    aux_loss_weight *= 1.5  # å¢å¤§è¾…åŠ©æŸå¤±æƒé‡

# æ•ˆæœ
ä½¿ç”¨è¾…åŠ©æŸå¤±å:
  ä¸“å®¶0: 28% tokens  # å¹³è¡¡äº† âœ…
  ä¸“å®¶1: 26% tokens
  ä¸“å®¶2: 24% tokens
  ä¸“å®¶3: 22% tokens
```

---

#### â“ Q4: MoEéœ€è¦å¤šå°‘æ˜¾å­˜ï¼Ÿ

**A**: å–å†³äº**æ¿€æ´»å‚æ•°**ï¼Œä¸æ˜¯æ€»å‚æ•°ã€‚

```python
# æ˜¾å­˜ä¼°ç®—å…¬å¼

# è®­ç»ƒæ˜¾å­˜ï¼ˆFP16 + AdamWï¼‰
æ¿€æ´»å‚æ•° = å…±äº«å‚æ•° + Top_K Ã— æ¯ä¸ªä¸“å®¶å‚æ•°

è®­ç»ƒæ˜¾å­˜ = æ¿€æ´»å‚æ•° Ã— 2å­—èŠ‚ Ã— 4
         = æ¿€æ´»å‚æ•° Ã— 8å­—èŠ‚
# 4å€ = æ¨¡å‹(1Ã—) + æ¢¯åº¦(1Ã—) + ä¼˜åŒ–å™¨(2Ã—)

# ä¾‹å­ï¼šMixtral 8x7B
æ€»å‚æ•°: 47B
æ¿€æ´»å‚æ•°: 13Bï¼ˆå…±äº«12B + Top-2 Ã— 0.5Bï¼‰

FP16è®­ç»ƒæ˜¾å­˜:
  æ¨¡å‹å‚æ•°: 13B Ã— 2å­—èŠ‚ = 26GB
  æ¢¯åº¦: 26GB
  ä¼˜åŒ–å™¨çŠ¶æ€: 52GBï¼ˆAdamWï¼‰
  æ¿€æ´»å€¼: ~20GBï¼ˆå–å†³äºbatch sizeï¼‰
  
  æ€»è®¡: ~124GB
  éœ€è¦: 2Ã—A100 (80GB) âœ…

# æ¨ç†æ˜¾å­˜ï¼ˆåªéœ€è¦æ¨¡å‹å‚æ•°ï¼‰
FP16æ¨ç†: 26GB â†’ 1Ã—A100 âœ…
INT8æ¨ç†: 13GB â†’ 1Ã—A10 âœ…
INT4æ¨ç†: 6.5GB â†’ 1Ã—T4 âœ…

# å¯¹æ¯”ï¼šå¯†é›†æ¨¡å‹ï¼ˆ47Bå‚æ•°ï¼‰
FP16è®­ç»ƒ: 47B Ã— 2 Ã— 4 = 376GB
éœ€è¦: 5Ã—A100 (80GB) âŒ

# å…³é”®ç»“è®º
âœ… MoEæ˜¾å­˜éœ€æ±‚åŸºäºæ¿€æ´»å‚æ•°ï¼Œä¸æ˜¯æ€»å‚æ•°
âœ… æ¨ç†æ—¶å¯ä»¥ç”¨é‡åŒ–è¿›ä¸€æ­¥é™ä½
âœ… è®­ç»ƒæ—¶æ˜¾å­˜æ˜¯æ¨ç†çš„4-5å€
```

---

#### â“ Q5: å¦‚ä½•é€‰æ‹©ä¸“å®¶æ•°é‡ï¼Ÿ

**A**: **å¹³è¡¡æ€§èƒ½å’Œå¤æ‚åº¦**ï¼Œæ¨è8-16ä¸ªã€‚

```
ä¸“å®¶æ•°é‡çš„å½±å“ï¼š

ã€å¤ªå°‘ã€‘2-4ä¸ªä¸“å®¶:
  âœ… è®­ç»ƒç®€å•
  âœ… é€šä¿¡å¼€é”€å°
  âœ… è´Ÿè½½å‡è¡¡å®¹æ˜“
  âŒ ä¸“ç²¾åº¦ä¸å¤Ÿ
  âŒ æ€§èƒ½æå‡æœ‰é™
  é€‚åˆï¼šåˆå­¦è€…å®éªŒ

ã€é€‚ä¸­ã€‘8-16ä¸ªä¸“å®¶:
  âœ… æ€§èƒ½æå‡æ˜æ˜¾
  âœ… è´Ÿè½½å‡è¡¡å¯æ§
  âœ… é€šä¿¡å¼€é”€å¯æ¥å—
  âœ… å·¥ç¨‹å¤æ‚åº¦é€‚ä¸­
  æ¨èï¼šç”Ÿäº§åº”ç”¨ â­â­â­â­â­

ã€å¾ˆå¤šã€‘64-128ä¸ªä¸“å®¶:
  âœ… æ€§èƒ½æœ€å¥½
  âŒ è´Ÿè½½å‡è¡¡å›°éš¾
  âŒ é€šä¿¡å¼€é”€å¤§
  âŒ è®­ç»ƒä¸ç¨³å®š
  é€‚åˆï¼šå¤§è§„æ¨¡è®­ç»ƒ

ã€è¶…å¤šã€‘>1000ä¸ªä¸“å®¶:
  âœ… ç†è®ºå®¹é‡æœ€å¤§
  âŒ å®é™…éš¾ä»¥è®­ç»ƒ
  âŒ å·¥ç¨‹å¤æ‚åº¦æé«˜
  âŒ ä¸æ¨è

# å®é™…é€‰æ‹©å»ºè®®
ç ”ç©¶/å­¦ä¹ : 4-8ä¸ªä¸“å®¶
ç”Ÿäº§åº”ç”¨: 8-16ä¸ªä¸“å®¶ï¼ˆMixtralç”¨8ä¸ªï¼‰
å¤§è§„æ¨¡è®­ç»ƒ: 64-128ä¸ªä¸“å®¶ï¼ˆSwitchç”¨128-2048ä¸ªï¼‰

# ç»éªŒæ³•åˆ™
ä¸“å®¶æ•° â‰ˆ GPUæ•°é‡ Ã— 2
ï¼ˆä¾¿äºä¸“å®¶å¹¶è¡Œï¼Œæ¯ä¸ªGPUè´Ÿè´£2ä¸ªä¸“å®¶ï¼‰
```

---

#### â“ Q6: Top-1è¿˜æ˜¯Top-2è·¯ç”±ï¼Ÿ

**A**: **Top-1æ›´å¿«ï¼ŒTop-2æ›´ç¨³å®š**ã€‚

```python
# Top-1ï¼ˆSwitch Transformeré£æ ¼ï¼‰
ä¼˜ç‚¹:
  âœ… è®¡ç®—é‡æœ€å°ï¼ˆç¨€ç–åº¦æœ€é«˜ï¼‰
  âœ… è·¯ç”±ç®€å•ï¼ˆæ¯ä¸ªtokené€‰1ä¸ªä¸“å®¶ï¼‰
  âœ… è®­ç»ƒå¿«ï¼ˆ4-7å€ï¼‰
  âœ… æ¨ç†å¿«
  âœ… æ˜¾å­˜éœ€æ±‚å°

ç¼ºç‚¹:
  âŒ å®¹é”™æ€§å·®ï¼ˆä¸“å®¶æ•…éšœå½±å“å¤§ï¼‰
  âŒ è´Ÿè½½å‡è¡¡æ›´éš¾
  âŒ æŸäº›tokenå¯èƒ½é€‰ä¸åˆ°å¥½ä¸“å®¶

æ¨èåœºæ™¯:
  - è¿½æ±‚æè‡´æ•ˆç‡
  - èµ„æºå—é™
  - ä¸“å®¶æ•°é‡å¤šï¼ˆ>64ä¸ªï¼‰

# Top-2ï¼ˆMixtralé£æ ¼ï¼‰
ä¼˜ç‚¹:
  âœ… å®¹é”™æ€§å¥½ï¼ˆé€‰2ä¸ªä¸“å®¶ï¼Œäº’ç›¸backupï¼‰
  âœ… è´Ÿè½½å‡è¡¡å®¹æ˜“
  âœ… æ€§èƒ½é€šå¸¸æ›´å¥½ï¼ˆ5-10%ï¼‰
  âœ… ä¸šç•Œä¸»æµ

ç¼ºç‚¹:
  âŒ è®¡ç®—é‡2å€ï¼ˆé€‰2ä¸ªä¸“å®¶ï¼‰
  âŒ è·¯ç”±ç¨å¤æ‚
  âŒ æ˜¾å­˜éœ€æ±‚ç¨å¤§

æ¨èåœºæ™¯:
  - è¿½æ±‚æœ€ä½³æ€§èƒ½ â­â­â­â­â­
  - ç”Ÿäº§ç¯å¢ƒ
  - ä¸“å®¶æ•°é‡é€‚ä¸­ï¼ˆ8-32ä¸ªï¼‰

# å®æµ‹å¯¹æ¯”ï¼ˆSwitchè®ºæ–‡æ•°æ®ï¼‰
Top-1: 100% baselineé€Ÿåº¦
Top-2: 50%é€Ÿåº¦ï¼ˆæ…¢2å€ï¼‰ï¼Œä½†105%æ€§èƒ½

# é€‰æ‹©å»ºè®®
if è¿½æ±‚æè‡´æ•ˆç‡ and èµ„æºå—é™:
    use Top-1  # Switch Transformer
elif è¿½æ±‚æ€§èƒ½ and æœ‰è¶³å¤Ÿèµ„æº:
    use Top-2  # Mixtral, æ¨è â­â­â­â­â­
elif ä¸“å®¶æ•°é‡ > 64:
    use Top-1  # å¤ªå¤šä¸“å®¶ç”¨Top-2å¼€é”€å¤§

# æ–°è¶‹åŠ¿ï¼šåŠ¨æ€Top-K
ç®€å•tokenç”¨Top-1ï¼Œå¤æ‚tokenç”¨Top-2
ï¼ˆæ ¹æ®è·¯ç”±ç½®ä¿¡åº¦åŠ¨æ€é€‰æ‹©ï¼‰
```

---

#### â“ Q7: MoEè®­ç»ƒç¨³å®šå—ï¼Ÿ

**A**: éœ€è¦**ç‰¹æ®ŠæŠ€å·§**ï¼Œä½†å¯ä»¥ç¨³å®šè®­ç»ƒã€‚

```python
# å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

ã€é—®é¢˜1ã€‘è·¯ç”±åå¡Œï¼ˆRouter Collapseï¼‰
ç°è±¡: æ‰€æœ‰tokenéƒ½è·¯ç”±åˆ°å°‘æ•°ä¸“å®¶
åŸå› : æ¢¯åº¦ä¸å¹³è¡¡ï¼ŒæŸäº›ä¸“å®¶è¶Šç”¨è¶Šå¼º

è§£å†³:
  âœ… ä½¿ç”¨è¾…åŠ©æŸå¤±ï¼ˆload_balance_lossï¼‰
  âœ… ä¸“å®¶dropoutï¼ˆéšæœºå…³é—­éƒ¨åˆ†ä¸“å®¶ï¼‰
  âœ… è·¯ç”±å™ªå£°ï¼ˆæ·»åŠ éšæœºæ€§ï¼‰
  âœ… æ›´é•¿çš„warmupï¼ˆè®©è·¯ç”±æ…¢æ…¢å­¦ä¹ ï¼‰

ã€é—®é¢˜2ã€‘è®­ç»ƒå‘æ•£ï¼ˆTraining Divergenceï¼‰
ç°è±¡: Lossçªç„¶å˜æˆNaN
åŸå› : æŸäº›ä¸“å®¶æ¢¯åº¦çˆ†ç‚¸

è§£å†³:
  âœ… æ¢¯åº¦è£å‰ªï¼ˆclip_grad_norm=1.0ï¼‰
  âœ… è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆæ¯”å¯†é›†æ¨¡å‹å°10å€ï¼‰
  âœ… ä¸“å®¶å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰
  âœ… æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16+æ¢¯åº¦ç¼©æ”¾ï¼‰

ã€é—®é¢˜3ã€‘ä¸“å®¶æœªä½¿ç”¨ï¼ˆDead Expertsï¼‰
ç°è±¡: æŸäº›ä¸“å®¶å®Œå…¨ä¸è¢«é€‰æ‹©
åŸå› : åˆå§‹åŒ–ä¸å¥½æˆ–è´Ÿè½½ä¸å‡

è§£å†³:
  âœ… ä¸“å®¶dropoutï¼ˆå¼ºåˆ¶ä½¿ç”¨å†·é—¨ä¸“å®¶ï¼‰
  âœ… å‡åŒ€åˆå§‹åŒ–è·¯ç”±å™¨ï¼ˆrouter.weightåˆå§‹åŒ–ä¸º0ï¼‰
  âœ… ä¸“å®¶é‡å¯ï¼ˆé‡æ–°åˆå§‹åŒ–æœªä½¿ç”¨çš„ä¸“å®¶ï¼‰
  âœ… å¢å¤§aux_loss_weight

# ç¨³å®šè®­ç»ƒé…ç½®ï¼ˆæ¨èï¼‰
config = {
    # æ ¸å¿ƒå‚æ•°
    "num_experts": 8,
    "top_k": 2,
    "capacity_factor": 1.25,
    
    # ç¨³å®šæ€§å‚æ•°
    "aux_loss_weight": 0.01,        # è´Ÿè½½å‡è¡¡
    "gradient_clip_norm": 1.0,      # é˜²æ­¢çˆ†ç‚¸
    "learning_rate": 1e-4,          # æ¯”å¯†é›†æ¨¡å‹å°
    "warmup_steps": 5000,           # æ›´é•¿warmup
    "expert_dropout": 0.1,          # ä¸“å®¶dropout
    "router_z_loss_weight": 0.001,  # è·¯ç”±æ­£åˆ™åŒ–
}

# ç›‘æ§æŒ‡æ ‡ï¼ˆå¿…é¡»ç›‘æ§ï¼‰
æ¯100æ­¥æ£€æŸ¥:
  âœ… ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒï¼ˆåº”è¯¥å‡åŒ€ï¼Œæ ‡å‡†å·®<2%ï¼‰
  âœ… è·¯ç”±ç†µï¼ˆåº”è¯¥é«˜ï¼Œæ¥è¿‘log(num_experts)ï¼‰
  âœ… è¾…åŠ©æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
  âœ… æ¯ä¸ªä¸“å®¶çš„æ¢¯åº¦èŒƒæ•°ï¼ˆä¸åº”è¯¥å·®å¤ªå¤šï¼‰

# å®é™…ç»éªŒ
Switch Transformerè®ºæ–‡:
  "MoEè®­ç»ƒæ¯”å¯†é›†æ¨¡å‹æ›´ä¸ç¨³å®šï¼Œä½†é€šè¿‡
   è¾…åŠ©æŸå¤±å’Œä»”ç»†è°ƒå‚å¯ä»¥ç¨³å®šè®­ç»ƒ"

Mixtralè®ºæ–‡:
  "æˆ‘ä»¬çš„8ä¸“å®¶MoEè®­ç»ƒéå¸¸ç¨³å®šï¼Œ
   å‡ ä¹å’Œå¯†é›†æ¨¡å‹ä¸€æ ·å®¹æ˜“è®­ç»ƒ"
```

---

#### â“ Q8: MoEå¦‚ä½•éƒ¨ç½²ï¼Ÿ

**A**: éœ€è¦**ç‰¹æ®Šä¼˜åŒ–**ï¼Œä½†å·²æœ‰æˆç†Ÿæ–¹æ¡ˆã€‚

```python
# æŒ‘æˆ˜1ï¼šæ¨¡å‹å¤ªå¤§
Mixtral 8x7B: 47Bå‚æ•°
å­˜å‚¨: 94GB (FP16)

è§£å†³æ–¹æ¡ˆ:
  âœ… æ¨¡å‹å¹¶è¡Œï¼ˆåˆ†å¸ƒåˆ°å¤šGPUï¼‰
  âœ… é‡åŒ–ï¼ˆINT8/INT4ï¼Œå‡å°‘2-4å€ï¼‰
  âœ… ä¸“å®¶å¸è½½ï¼ˆä¸å¸¸ç”¨çš„ä¸“å®¶æ”¾CPUï¼‰

# æŒ‘æˆ˜2ï¼šåŠ¨æ€è®¡ç®—å›¾
æ¯ä¸ªtokenä½¿ç”¨ä¸åŒä¸“å®¶ â†’ éš¾ä»¥æ‰¹å¤„ç†

è§£å†³æ–¹æ¡ˆ:
  âœ… æ‰¹å¤„ç†ç›¸åŒä¸“å®¶çš„token
  âœ… é¢„æµ‹ä¸“å®¶ä½¿ç”¨æ¨¡å¼ï¼ˆæå‰åŠ è½½ï¼‰
  âœ… ä¸“å®¶ç¼“å­˜ï¼ˆç¼“å­˜æœ€è¿‘ä½¿ç”¨çš„ä¸“å®¶ï¼‰

# å®é™…éƒ¨ç½²æ–¹æ¡ˆ

## æ–¹æ¡ˆ1ï¼švLLMï¼ˆæ¨è â­â­â­â­â­ï¼‰
from vllm import LLM

model = LLM(
    "mistralai/Mixtral-8x7B-v0.1", 
    tensor_parallel_size=2,  # 2Ã—GPUå¹¶è¡Œ
    dtype="float16"
)
output = model.generate(prompts)

ä¼˜ç‚¹ï¼šç®€å•ã€å¿«é€Ÿã€ç¤¾åŒºæ”¯æŒå¥½
ç¼ºç‚¹ï¼šéœ€è¦å¤šGPU

## æ–¹æ¡ˆ2ï¼šDeepSpeed Inference
import deepspeed

model = deepspeed.init_inference(
    model,
    mp_size=2,        # æ¨¡å‹å¹¶è¡Œ
    dtype=torch.float16,
    replace_with_kernel_inject=True  # ä½¿ç”¨ä¼˜åŒ–kernel
)

ä¼˜ç‚¹ï¼šæœ€æˆç†Ÿã€ä¼˜åŒ–æœ€å¥½
ç¼ºç‚¹ï¼šé…ç½®å¤æ‚

## æ–¹æ¡ˆ3ï¼šTensorRT-LLM
# æœ€å¿«ï¼Œä½†éœ€è¦è½¬æ¢æ¨¡å‹
# é€‚åˆç”Ÿäº§ç¯å¢ƒï¼Œéœ€è¦å·¥ç¨‹æŠ•å…¥

## æ–¹æ¡ˆ4ï¼šé‡åŒ–æ¨ç†ï¼ˆçœæ˜¾å­˜ï¼‰
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mixtral-8x7B-v0.1",
    load_in_8bit=True,  # INT8é‡åŒ–
    device_map="auto"   # è‡ªåŠ¨åˆ†é…GPU
)

# æ€§èƒ½å¯¹æ¯”ï¼ˆMixtral 8x7Bï¼‰
å•GPU (A100 80GB):
  - FP16: æ— æ³•åŠ è½½ âŒ
  - INT8: å¯ä»¥ï¼Œä½†æ…¢ (~10 tok/s)
  - INT4: å¯ä»¥ (~15 tok/s)

2Ã—GPU (A100 80GB):
  - FP16: 20-25 tok/s âœ…
  - INT8: 35-40 tok/s âœ…

4Ã—GPU (A100 80GB):
  - FP16: 35-45 tok/s âœ…
  - INT8: 60-80 tok/s âœ…
```

---

#### â“ Q9: MoEé€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ

**A**: **å¤§è§„æ¨¡ã€å¤šæ ·åŒ–ä»»åŠ¡**ã€‚

```
âœ… é€‚åˆçš„åœºæ™¯ï¼š

1. å¤§è§„æ¨¡é¢„è®­ç»ƒ
   æ¡ä»¶ï¼šæ•°æ®å¤šæ ·ï¼ˆå¤šè¯­è¨€ã€å¤šé¢†åŸŸï¼‰
         éœ€è¦å¤§å®¹é‡æ¨¡å‹
         æœ‰åˆ†å¸ƒå¼è®­ç»ƒèµ„æº
   ä¾‹å­ï¼šGPT-4, Mixtral

2. å¤šä»»åŠ¡å­¦ä¹ 
   æ¡ä»¶ï¼šä¸åŒä»»åŠ¡éœ€è¦ä¸åŒèƒ½åŠ›
         ä¸“å®¶å¯ä»¥ä¸“ç²¾ä¸åŒä»»åŠ¡
   ä¾‹å­ï¼šå¤šè¯­è¨€ç¿»è¯‘ã€å¤šé¢†åŸŸé—®ç­”

3. é•¿å°¾åˆ†å¸ƒæ•°æ®
   æ¡ä»¶ï¼šå¸¸è§æ¨¡å¼ç”¨å¸¸ç”¨ä¸“å®¶
         ç½•è§æ¨¡å¼ç”¨ä¸“é—¨ä¸“å®¶
   ä¾‹å­ï¼šä»£ç +æ–‡æœ¬æ··åˆæ¨¡å‹

4. è¿½æ±‚è®­ç»ƒæ•ˆç‡
   æ¡ä»¶ï¼šé¢„ç®—æœ‰é™ä½†æƒ³è¦å¤§æ¨¡å‹
         å¯ä»¥æ¥å—æ¨ç†å¤æ‚åº¦
   ä¾‹å­ï¼šå­¦æœ¯ç ”ç©¶ã€åˆ›ä¸šå…¬å¸

âŒ ä¸é€‚åˆçš„åœºæ™¯ï¼š

1. å°è§„æ¨¡æ¨¡å‹ï¼ˆ<1Bå‚æ•°ï¼‰
   åŸå› ï¼šMoEå¼€é”€å¤§äºæ”¶ç›Š
   å»ºè®®ï¼šç”¨å¯†é›†æ¨¡å‹

2. å•ä¸€ç®€å•ä»»åŠ¡
   åŸå› ï¼šä¸éœ€è¦ä¸“å®¶ä¸“ç²¾
   ä¾‹å­ï¼šç®€å•åˆ†ç±»ä»»åŠ¡

3. èµ„æºæåº¦å—é™
   åŸå› ï¼šå•GPUè®­ç»ƒï¼Œæ˜¾å­˜ä¸è¶³
   å»ºè®®ï¼šç”¨å°å¯†é›†æ¨¡å‹

4. è¾¹ç¼˜éƒ¨ç½²
   åŸå› ï¼šæ¨¡å‹å¤ªå¤§ï¼Œæ— æ³•éƒ¨ç½²
   ä¾‹å­ï¼šæ‰‹æœºã€IoTè®¾å¤‡

# å†³ç­–æ ‘
if æ¨¡å‹è§„æ¨¡ < 1B:
    ç”¨å¯†é›†æ¨¡å‹  # MoEä¸åˆ’ç®—

elif æ¨¡å‹è§„æ¨¡ 1B-10B:
    if å¤šä»»åŠ¡ or å¤šè¯­è¨€:
        å¯ä»¥å°è¯•MoEï¼ˆ4-8ä¸“å®¶ï¼‰
    else:
        ç”¨å¯†é›†æ¨¡å‹  # ç®€å•åœºæ™¯ä¸éœ€è¦

elif æ¨¡å‹è§„æ¨¡ > 10B:
    å¼ºçƒˆæ¨èMoEï¼ˆ8-64ä¸“å®¶ï¼‰ â­â­â­â­â­
    # è®­ç»ƒæˆæœ¬é™ä½50-70%

# å®é™…æ¡ˆä¾‹
âœ… GPT-4: å¤šè¯­è¨€ã€å¤šä»»åŠ¡ â†’ ç”¨MoE
âœ… Mixtral: å¼€æºã€é«˜æ€§èƒ½ â†’ ç”¨MoE
âœ… Switch: è¶…å¤§è§„æ¨¡ â†’ ç”¨MoE
âŒ BERT: å•ä»»åŠ¡åˆ†ç±» â†’ ç”¨å¯†é›†
âŒ MobileNet: ç§»åŠ¨ç«¯ â†’ ç”¨å¯†é›†
```

---

#### â“ Q10: MoEçš„æœªæ¥æ–¹å‘ï¼Ÿ

**A**: **æ›´é«˜æ•ˆã€æ›´æ˜“ç”¨ã€æ›´å¹¿æ³›**ã€‚

```
ã€è¶‹åŠ¿1ã€‘æ›´é«˜æ•ˆçš„è·¯ç”±
  ç°åœ¨ï¼šTop-Kç¡¬è·¯ç”±ï¼ˆè¦ä¹ˆé€‰ï¼Œè¦ä¹ˆä¸é€‰ï¼‰
  æœªæ¥ï¼šè½¯è·¯ç”±ã€åŠ¨æ€è·¯ç”±ã€å¯å­¦ä¹ è·¯ç”±
  ä¾‹å­ï¼š
    - Soft MoEï¼ˆGoogle, 2023ï¼‰- è½¯æ··åˆ
    - Expert Choiceï¼ˆä¸“å®¶é€‰tokenï¼‰
    - Dynamic MoEï¼ˆæ ¹æ®éš¾åº¦è°ƒæ•´Kï¼‰

ã€è¶‹åŠ¿2ã€‘è‡ªåŠ¨åŒ–MoE
  ç°åœ¨ï¼šæ‰‹åŠ¨è®¾è®¡ä¸“å®¶æ•°é‡å’Œä½ç½®
  æœªæ¥ï¼šè‡ªåŠ¨æœç´¢æœ€ä¼˜é…ç½®
  ä¾‹å­ï¼š
    - AutoMoE - NASæœç´¢MoEæ¶æ„
    - Adaptive MoE - è‡ªåŠ¨è°ƒæ•´ä¸“å®¶æ•°
    - Per-layer MoE - æ¯å±‚ä¸åŒé…ç½®

ã€è¶‹åŠ¿3ã€‘ç»†ç²’åº¦MoE
  ç°åœ¨ï¼šå±‚çº§MoEï¼ˆæ•´ä¸ªFFNæ˜¯ä¸“å®¶ï¼‰
  æœªæ¥ï¼šæ›´ç»†ç²’åº¦çš„ä¸“å®¶
  ä¾‹å­ï¼š
    - MoE-LoRAï¼ˆä¸“å®¶æ˜¯LoRAè€Œéå®Œæ•´FFNï¼‰
    - Token-level MoEï¼ˆæ¯ä¸ªtokenç‹¬ç«‹ä¸“å®¶ï¼‰
    - Parameter-level MoEï¼ˆå‚æ•°çº§åˆ«ä¸“å®¶ï¼‰

ã€è¶‹åŠ¿4ã€‘å¤šæ¨¡æ€MoE
  ç°åœ¨ï¼šä¸»è¦ç”¨äºè¯­è¨€æ¨¡å‹
  æœªæ¥ï¼šè§†è§‰ã€éŸ³é¢‘ã€å¤šæ¨¡æ€
  ä¾‹å­ï¼š
    - Vision MoEï¼ˆå›¾åƒä¸“å®¶ï¼‰
    - Audio MoEï¼ˆéŸ³é¢‘ä¸“å®¶ï¼‰
    - Multimodal MoEï¼ˆè·¨æ¨¡æ€ä¸“å®¶ï¼‰

ã€è¶‹åŠ¿5ã€‘é«˜æ•ˆæ¨ç†
  ç°åœ¨ï¼šæ¨ç†å¼€é”€å¤§ã€æ˜¾å­˜å ç”¨é«˜
  æœªæ¥ï¼šæ¨ç†ä¼˜åŒ–ã€ä¸“å®¶å‹ç¼©
  ä¾‹å­ï¼š
    - Expert Pruningï¼ˆä¸“å®¶å‰ªæï¼‰
    - Expert Distillationï¼ˆä¸“å®¶è’¸é¦ï¼‰
    - Expert Cachingï¼ˆæ™ºèƒ½ç¼“å­˜ï¼‰
    - Speculative MoEï¼ˆæŠ•æœºæ‰§è¡Œï¼‰

ã€è¶‹åŠ¿6ã€‘å°å‹åŒ–MoE
  ç°åœ¨ï¼šä¸»è¦æ˜¯è¶…å¤§æ¨¡å‹ï¼ˆ>10Bï¼‰
  æœªæ¥ï¼šå°æ¨¡å‹ä¹Ÿç”¨MoE
  ä¾‹å­ï¼š
    - Tiny MoEï¼ˆ<1Bä½†æœ‰ä¸“å®¶ï¼‰
    - Edge MoEï¼ˆè¾¹ç¼˜è®¾å¤‡MoEï¼‰
    - Mobile MoEï¼ˆæ‰‹æœºç«¯MoEï¼‰

# ç ”ç©¶çƒ­ç‚¹ï¼ˆ2024-2025ï¼‰
  âœ… åŠ¨æ€ä¸“å®¶æ•°é‡ï¼ˆæ ¹æ®ä»»åŠ¡è‡ªåŠ¨è°ƒæ•´ï¼‰
  âœ… å±‚æ¬¡åŒ–ä¸“å®¶ï¼ˆä¸“å®¶ç»„+ç»„å†…ä¸“å®¶ï¼‰
  âœ… ä¸“å®¶çŸ¥è¯†å…±äº«ï¼ˆå‡å°‘å‚æ•°å†—ä½™ï¼‰
  âœ… MoE + LoRAï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰
  âœ… ç«¯ä¾§MoEï¼ˆåœ¨ç§»åŠ¨è®¾å¤‡è¿è¡Œï¼‰

# å•†ä¸šæœºä¼š
  ğŸ’¼ å‚ç›´é¢†åŸŸMoEï¼ˆåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰
  ğŸ’¼ å¤šè¯­è¨€MoEï¼ˆä¸“æ³¨ä½èµ„æºè¯­è¨€ï¼‰
  ğŸ’¼ ä¸ªæ€§åŒ–MoEï¼ˆæ¯ä¸ªç”¨æˆ·ä¸“å±ä¸“å®¶ï¼‰
  ğŸ’¼ è”é‚¦å­¦ä¹ MoEï¼ˆåˆ†å¸ƒå¼åä½œè®­ç»ƒï¼‰
```

---

### ğŸŒ± 9.2 å®æˆ˜æŒ‡å—

#### ğŸ’¡ ç«‹å³å¯åšï¼ˆ30åˆ†é’Ÿï¼‰

```python
# æ­¥éª¤1ï¼šå®ç°æœ€å°MoEï¼ˆç†è§£åŸç†ï¼‰

import torch
import torch.nn as nn
import torch.nn.functional as F

class MinimalMoE(nn.Module):
    """æœ€ç®€å•çš„MoEå®ç°"""
    def __init__(self, d_model=512, num_experts=4):
        super().__init__()
        self.router = nn.Linear(d_model, num_experts)
        self.experts = nn.ModuleList([
            nn.Linear(d_model, d_model) 
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        # è·¯ç”±
        logits = self.router(x)
        weights = F.softmax(logits, dim=-1)
        
        # ä¸“å®¶è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆï¼šåŠ æƒæ‰€æœ‰ä¸“å®¶ï¼‰
        output = torch.zeros_like(x)
        for i, expert in enumerate(self.experts):
            output += weights[:, :, i:i+1] * expert(x)
        
        return output

# æµ‹è¯•
moe = MinimalMoE()
x = torch.randn(2, 10, 512)  # [batch, seq, dim]
y = moe(x)
print(f"è¾“å…¥: {x.shape}, è¾“å‡º: {y.shape}")
# è¾“å‡º: è¾“å…¥: torch.Size([2, 10, 512]), è¾“å‡º: torch.Size([2, 10, 512])
```

#### ğŸš€ ä¸€å‘¨é¡¹ç›®ï¼ˆæ·±å…¥ç†è§£ï¼‰

```python
ç›®æ ‡ï¼šè®­ç»ƒä¸€ä¸ª4ä¸“å®¶çš„å°å‹MoEï¼Œå¯¹æ¯”å¯†é›†æ¨¡å‹

Day 1-2: å®ç°
  - å®ç°TopKMoEï¼ˆå¸¦Top-2è·¯ç”±ï¼‰
  - å®ç°è´Ÿè½½å‡è¡¡æŸå¤±
  - é›†æˆåˆ°å°å‹GPTï¼ˆ6å±‚ï¼Œ512dimï¼‰
  
  ä»£ç ç»“æ„:
    moe_layer.py  # MoEå±‚å®ç°
    moe_gpt.py    # é›†æˆåˆ°GPT
    train.py      # è®­ç»ƒè„šæœ¬

Day 3-4: è®­ç»ƒ
  - å‡†å¤‡å°æ•°æ®é›†ï¼ˆå¦‚TinyStories, 50MBï¼‰
  - è®­ç»ƒå¯†é›†åŸºçº¿ï¼ˆ6å±‚ï¼Œ512dimï¼Œ8å°æ—¶ï¼‰
  - è®­ç»ƒMoEç‰ˆæœ¬ï¼ˆ6å±‚ï¼Œ512dimï¼Œ4ä¸“å®¶ï¼Œ10å°æ—¶ï¼‰
  
  å‘½ä»¤:
    python train.py --model dense --data tinystories
    python train.py --model moe --num_experts 4 --data tinystories

Day 5-6: åˆ†æ
  - å¯¹æ¯”Lossæ›²çº¿ï¼ˆç”¨wandbæˆ–tensorboardï¼‰
  - åˆ†æä¸“å®¶ä½¿ç”¨åˆ†å¸ƒï¼ˆæ˜¯å¦å‡è¡¡ï¼Ÿï¼‰
  - è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼ˆå“ªä¸ªæ›´å¥½ï¼Ÿï¼‰
  - å¯¹æ¯”è®­ç»ƒé€Ÿåº¦ï¼ˆMoEæ˜¯å¦æ›´å¿«ï¼Ÿï¼‰
  
  åˆ†æå·¥å…·:
    analyze_expert_usage.py  # ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
    compare_models.py         # æ¨¡å‹å¯¹æ¯”

Day 7: ä¼˜åŒ–å’Œæ€»ç»“
  - è°ƒæ•´aux_loss_weightï¼ˆå°è¯•0.001, 0.01, 0.1ï¼‰
  - å°è¯•ä¸åŒnum_expertsï¼ˆ2, 4, 8ï¼‰
  - è®°å½•æœ€ä½³é…ç½®
  - å†™æ€»ç»“æŠ¥å‘Š

é¢„æœŸç»“æœï¼š
  âœ… MoEåº”è¯¥æ¯”å¯†é›†æ¨¡å‹å¿«10-20%
  âœ… æœ€ç»ˆLossä½5-10%ï¼ˆå¦‚æœè°ƒå‚å¾—å½“ï¼‰
  âœ… ä¸“å®¶ä½¿ç”¨åŸºæœ¬å‡è¡¡ï¼ˆæ ‡å‡†å·®<5%ï¼‰
  âœ… ç”Ÿæˆè´¨é‡ç•¥å¥½
```

#### ğŸ“š æ·±å…¥å­¦ä¹ è·¯å¾„ï¼ˆ1-2ä¸ªæœˆï¼‰

```python
ã€é˜¶æ®µ1ã€‘ç†è®ºåŸºç¡€ï¼ˆ1-2å‘¨ï¼‰
  å¿…è¯»è®ºæ–‡:
    1. Shazeer et al., 2017
       "Outrageously Large Neural Networks"
       â†’ MoEçš„å¥ åŸºä¹‹ä½œï¼Œå¿…è¯» â­â­â­â­â­
    
    2. Lepikhin et al., 2020
       "GShard: Scaling Giant Models"
       â†’ Googleçš„å¤§è§„æ¨¡MoE
    
    3. Fedus et al., 2021
       "Switch Transformers"
       â†’ ç®€åŒ–çš„MoEï¼Œå¼ºçƒˆæ¨è â­â­â­â­â­
    
    4. Mistral AI, 2024
       "Mixtral of Experts"
       â†’ å¼€æºé«˜æ€§èƒ½MoE
  
  åšå®¢æ•™ç¨‹:
    - Hugging Face MoE Guide
    - Google Research Blog: Switch Transformers
    - Mistral AIæŠ€æœ¯æŠ¥å‘Š

ã€é˜¶æ®µ2ã€‘åŠ¨æ‰‹å®è·µï¼ˆ2-4å‘¨ï¼‰
  é¡¹ç›®è¿›åº¦:
    Week 1: å®ç°SimpleMoEï¼ˆä¸å¸¦è´Ÿè½½å‡è¡¡ï¼‰
    Week 2: æ·»åŠ è´Ÿè½½å‡è¡¡å’ŒTop-Kè·¯ç”±
    Week 3: é›†æˆåˆ°nanoGPTï¼Œè®­ç»ƒå°æ¨¡å‹
    Week 4: å¯¹æ¯”å®éªŒï¼Œåˆ†æç»“æœ
  
  ä½¿ç”¨å·¥å…·:
    - DeepSpeed MoEï¼ˆç”Ÿäº§çº§å®ç°ï¼‰
    - FairSeq MoEï¼ˆç ”ç©¶ç”¨ï¼‰
    - Mixtralæºç ï¼ˆHugging Faceï¼‰

ã€é˜¶æ®µ3ã€‘è¿›é˜¶ç ”ç©¶ï¼ˆ1-2ä¸ªæœˆï¼‰
  ç ”ç©¶æ–¹å‘:
    1. Expert Choice Routing
       - ç†è§£åå‘è·¯ç”±çš„ä¼˜åŠ¿
       - å®ç°å¹¶å¯¹æ¯”Top-K
    
    2. MoE + LoRA
       - ç»“åˆå‚æ•°é«˜æ•ˆå¾®è°ƒ
       - å‡å°‘ä¸“å®¶å‚æ•°é‡
    
    3. å¤šæ¨¡æ€MoE
       - è§†è§‰+è¯­è¨€ä¸“å®¶
       - è·¨æ¨¡æ€è·¯ç”±
    
    4. MoEé‡åŒ–å’Œè’¸é¦
       - ä¸“å®¶å‹ç¼©
       - æ¨ç†ä¼˜åŒ–

ã€é˜¶æ®µ4ã€‘ç”Ÿäº§å®æˆ˜ï¼ˆæŒç»­ï¼‰
  æŒ‘æˆ˜:
    - å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒï¼ˆ8+ GPUï¼‰
    - æ¨ç†ä¼˜åŒ–å’Œéƒ¨ç½²ï¼ˆvLLM, DeepSpeedï¼‰
    - æˆæœ¬æ§åˆ¶ï¼ˆäº‘ç«¯è®­ç»ƒæˆæœ¬ç®¡ç†ï¼‰
    - æŒç»­ç›‘æ§å’Œæ”¹è¿›
```

---

## ğŸ“ æ€»ç»“ä¸æ£€æŸ¥

### âœ… çŸ¥è¯†æ£€æŸ¥æ¸…å•

å®Œæˆå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€æ¦‚å¿µï¼ˆå¿…é¡»æŒæ¡ï¼‰**
- [ ] ç†è§£MoEçš„æ ¸å¿ƒæ€æƒ³ï¼ˆä¸“ä¸šçš„äº‹äº¤ç»™ä¸“ä¸šçš„äººåšï¼‰
- [ ] çŸ¥é“ä»€ä¹ˆæ˜¯ç¨€ç–æ¿€æ´»ï¼ˆæ¯æ¬¡åªç”¨éƒ¨åˆ†å‚æ•°ï¼‰
- [ ] ç†è§£è·¯ç”±æœºåˆ¶çš„ä½œç”¨ï¼ˆæ™ºèƒ½åˆ†é…tokenåˆ°ä¸“å®¶ï¼‰
- [ ] çŸ¥é“Top-Kè·¯ç”±çš„å·¥ä½œåŸç†
- [ ] ç†è§£ä¸ºä»€ä¹ˆMoEèƒ½æå‡æ•ˆç‡ï¼ˆå‚æ•°å¤šè®¡ç®—å°‘ï¼‰
- [ ] èƒ½å¤Ÿè§£é‡ŠMoE vs å¯†é›†æ¨¡å‹çš„åŒºåˆ«

**è¿›é˜¶ç†è§£ï¼ˆå»ºè®®æŒæ¡ï¼‰**
- [ ] ç†è§£è´Ÿè½½å‡è¡¡é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼ˆè¾…åŠ©æŸå¤±ã€å®¹é‡æ§åˆ¶ï¼‰
- [ ] çŸ¥é“è¾…åŠ©æŸå¤±çš„æ•°å­¦åŸç†
- [ ] ç†è§£ä¸“å®¶å®¹é‡çš„æ¦‚å¿µå’Œä½œç”¨
- [ ] èƒ½å¤Ÿåˆ†æMoEçš„é€šä¿¡å¼€é”€ï¼ˆAll-to-Allï¼‰
- [ ] çŸ¥é“Switch Transformerå’ŒMixtralçš„ç‰¹ç‚¹
- [ ] ç†è§£MoEçš„è®­ç»ƒç¨³å®šæ€§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

**å®æˆ˜èƒ½åŠ›ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰**
- [ ] èƒ½å¤Ÿå®ç°ç®€å•çš„MoEå±‚ï¼ˆRouter + Experts + Load Balanceï¼‰
- [ ] ä¼šå°†MoEé›†æˆåˆ°Transformeræ¶æ„
- [ ] èƒ½å¤Ÿé…ç½®å’Œè®­ç»ƒMoEæ¨¡å‹ï¼ˆé€‰æ‹©åˆé€‚çš„è¶…å‚æ•°ï¼‰
- [ ] ä¼šç›‘æ§ä¸“å®¶ä½¿ç”¨æƒ…å†µï¼ˆåˆ†å¸ƒã€ç†µã€æ ‡å‡†å·®ï¼‰
- [ ] èƒ½å¤Ÿè¯Šæ–­å’Œè§£å†³è®­ç»ƒé—®é¢˜ï¼ˆä¸å‡è¡¡ã€ä¸ç¨³å®šã€OOMï¼‰
- [ ] ä¼šä¼˜åŒ–MoEçš„æ€§èƒ½ï¼ˆæ··åˆç²¾åº¦ã€é€šä¿¡ä¼˜åŒ–ï¼‰
- [ ] èƒ½å¤Ÿè¯„ä¼°MoEæ˜¯å¦é€‚ç”¨äºä½ çš„åœºæ™¯
- [ ] ç†è§£MoEçš„éƒ¨ç½²å’Œæ¨ç†ä¼˜åŒ–

### ğŸ“Š MoEå‚æ•°é€ŸæŸ¥è¡¨

| å‚æ•° | å¢å¤§æ•ˆæœ | å‡å°æ•ˆæœ | æ¨èå€¼ |
|------|---------|---------|--------|
| **num_experts** | å®¹é‡â†‘, å‡è¡¡éš¾â†‘ | å®¹é‡â†“, å‡è¡¡æ˜“â†‘ | 8-16 |
| **top_k** | è®¡ç®—â†‘, è´¨é‡â†‘ | è®¡ç®—â†“, è´¨é‡â†“ | 1-2 |
| **capacity_factor** | æº¢å‡ºå°‘, æ˜¾å­˜â†‘ | æº¢å‡ºå¤š, æ˜¾å­˜â†“ | 1.25 |
| **aux_loss_weight** | å‡è¡¡å¼º, æ€§èƒ½â†“ | å‡è¡¡å¼±, æ€§èƒ½â†‘ | 0.01 |
| **ä¸“å®¶å±‚æ•°** | MoEæ•ˆæœâ†‘, æˆæœ¬â†‘ | MoEæ•ˆæœâ†“, æˆæœ¬â†“ | 50%-100%å±‚ |

**å…³é”®å†³ç­–æ ‘**ï¼š
```python
if æ¨¡å‹è§„æ¨¡ < 1B:
    å»ºè®®ç”¨å¯†é›†æ¨¡å‹  # MoEå¼€é”€å¤§äºæ”¶ç›Š
elif æ¨¡å‹è§„æ¨¡ 1B-10B:
    num_experts = 4-8
    top_k = 2
    # è°¨æ…è¯„ä¼°æ”¶ç›Š
elif æ¨¡å‹è§„æ¨¡ > 10B:
    num_experts = 8-64
    top_k = 1-2
    # å¼ºçƒˆæ¨èMoE âœ…
```

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

ç°åœ¨ä½ å·²ç»æŒæ¡äº†MoEæ¨¡å‹ï¼Œæ¥ä¸‹æ¥åº”è¯¥å­¦ä¹ ï¼š

1. **13_rlhf_and_alignment.md** - å­¦ä¹ RLHFä¸æ¨¡å‹å¯¹é½ï¼ˆæœ€åä¸€ç« ï¼ï¼‰
2. **å®è·µé¡¹ç›®**ï¼š
   - ä»SimpleMoEå¼€å§‹ï¼Œé€æ­¥å®ç°å®Œæ•´çš„MoE
   - è®­ç»ƒä¸€ä¸ªå°å‹MoEï¼Œå¯¹æ¯”å¯†é›†æ¨¡å‹
   - ç ”ç©¶Mixtralæºç ï¼Œå­¦ä¹ ç”Ÿäº§çº§å®ç°
3. **è¿›é˜¶ç ”ç©¶**ï¼š
   - æ¢ç´¢Expert Choice Routingç­‰æ–°å˜ä½“
   - ç ”ç©¶MoEåœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„åº”ç”¨
   - å­¦ä¹ MoEçš„æ¨ç†ä¼˜åŒ–æŠ€æœ¯

### ğŸ’¡ å®è·µå»ºè®®

1. **å¾ªåºæ¸è¿›**ï¼š
   - å…ˆå®ç°SimpleMoEï¼ˆä¸å¸¦è´Ÿè½½å‡è¡¡ï¼‰
   - å†æ·»åŠ Top-Kè·¯ç”±å’Œè´Ÿè½½å‡è¡¡
   - æœ€åå®ç°ç”Ÿäº§çº§ç‰¹æ€§ï¼ˆå®¹é‡æ§åˆ¶ã€ç›‘æ§ï¼‰

2. **ç³»ç»Ÿå¯¹æ¯”**ï¼š
   - å§‹ç»ˆä¸å¯†é›†æ¨¡å‹åŸºçº¿å¯¹æ¯”
   - è®°å½•è®­ç»ƒæ—¶é—´ã€Lossã€ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
   - åˆ†æMoEæ˜¯å¦çœŸçš„å¸¦æ¥æ”¶ç›Š

3. **å……åˆ†ç›‘æ§**ï¼š
   - å®æ—¶ç›‘æ§ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒï¼ˆåº”è¯¥å‡åŒ€ï¼‰
   - è¿½è¸ªè·¯ç”±ç†µï¼ˆåº”è¯¥é«˜ï¼‰
   - è§‚å¯Ÿè¾…åŠ©æŸå¤±çš„å˜åŒ–

4. **å‚è€ƒæœ€ä½³å®è·µ**ï¼š
   - ä½¿ç”¨æˆç†Ÿçš„é…ç½®ï¼ˆå¦‚Mixtralçš„8ä¸“å®¶ã€Top-2ï¼‰
   - å€Ÿé‰´DeepSpeed MoEçš„å®ç°
   - é˜…è¯»Switch Transformerè®ºæ–‡çš„è®­ç»ƒæŠ€å·§

---

## ğŸ“š æ¨èèµ„æº

### ğŸ“– å¿…è¯»è®ºæ–‡

**å¥ åŸºä¹‹ä½œ**ï¼š
- [Outrageously Large Neural Networks (Shazeer et al., 2017)](https://arxiv.org/abs/1701.06538) - MoEçš„å¼€å±±ä¹‹ä½œ
- [GShard (Lepikhin et al., 2020)](https://arxiv.org/abs/2006.16668) - Googleçš„å¤§è§„æ¨¡MoE

**æ ¸å¿ƒå¿…è¯»**ï¼š
- [Switch Transformers (Fedus et al., 2021)](https://arxiv.org/abs/2101.03961) - ç®€åŒ–çš„MoEï¼Œå¼ºçƒˆæ¨è â­â­â­â­â­
- [Mixtral of Experts (Mistral AI, 2024)](https://arxiv.org/abs/2401.04088) - å¼€æºé«˜æ€§èƒ½MoE

**è¿›é˜¶é˜…è¯»**ï¼š
- [GLaM (Du et al., 2021)](https://arxiv.org/abs/2112.06905) - é«˜æ•ˆçš„MoEè®¾è®¡
- [ST-MoE (Zoph et al., 2022)](https://arxiv.org/abs/2202.08906) - è®­ç»ƒç¨³å®šæ€§ç ”ç©¶

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- [Mixture of Experts Explained](https://www.youtube.com/results?search_query=mixture+of+experts+explained) - å…¥é—¨è®²è§£
- [Andrej Karpathy on MoE](https://www.youtube.com/c/AndrejKarpathy) - æ·±åº¦è§£æï¼ˆå¦‚æœæœ‰ï¼‰
- [Mixtral 8x7B æŠ€æœ¯è§£è¯»](https://www.youtube.com/results?search_query=mixtral+8x7b) - å®æˆ˜æ¡ˆä¾‹

### ğŸ”§ å®ç”¨å·¥å…·

**è®­ç»ƒæ¡†æ¶**ï¼š
- [DeepSpeed MoE](https://www.deepspeed.ai/tutorials/mixture-of-experts/) - æœ€æˆç†Ÿçš„MoEè®­ç»ƒæ¡†æ¶ â­â­â­â­â­
- [FairSeq MoE](https://github.com/facebookresearch/fairseq) - Facebookçš„å®ç°
- [Mesh TensorFlow](https://github.com/tensorflow/mesh) - Googleçš„åˆ†å¸ƒå¼æ¡†æ¶

**æ¨¡å‹åº“**ï¼š
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/model_doc/mixtral) - Mixtralç­‰MoEæ¨¡å‹
- [Mistral AI](https://mistral.ai/) - Mixtralå®˜æ–¹èµ„æº

**ç›‘æ§å·¥å…·**ï¼š
- [Weights & Biases](https://wandb.ai/) - å®æ—¶ç›‘æ§è®­ç»ƒ
- [TensorBoard](https://www.tensorflow.org/tensorboard) - å¯è§†åŒ–ä¸“å®¶ä½¿ç”¨

---

**æ­å–œä½ å®Œæˆç¬¬12ç« ï¼** ğŸ‰

ä½ ç°åœ¨å·²ç»æŒæ¡äº†MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä»ç¨€ç–æ¿€æ´»åˆ°è·¯ç”±æœºåˆ¶ï¼Œä»è´Ÿè½½å‡è¡¡åˆ°è®­ç»ƒä¼˜åŒ–ï¼Œä»ç»å…¸å˜ä½“åˆ°æœ€æ–°ç ”ç©¶ï¼Œä½ å·²ç»å…·å¤‡äº†ç†è§£å’Œä½¿ç”¨å¤§è§„æ¨¡ç¨€ç–æ¨¡å‹çš„èƒ½åŠ›ã€‚

MoEæ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ï¼ŒGPT-4ç­‰é¡¶çº§æ¨¡å‹éƒ½ä½¿ç”¨äº†MoEæ¶æ„ã€‚æŒæ¡MoEï¼Œä½ å°±æŒæ¡äº†é€šå‘è¶…å¤§è§„æ¨¡AIçš„é’¥åŒ™ã€‚

**æœ€åä¸€ç« äº†ï¼è®©æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼Œå­¦ä¹ å¦‚ä½•è®©æ¨¡å‹æ›´å®‰å…¨ã€æ›´æœ‰ç”¨ï¼** 

â†’ [13_rlhf_and_alignment.md](13_rlhf_and_alignment.md)
