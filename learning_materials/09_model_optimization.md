# æ¨¡å‹ä¼˜åŒ–å®Œå…¨æŒ‡å—ï¼šä»é‡åŒ–åˆ°éƒ¨ç½²

## ğŸ¯ æ¦‚è§ˆ

æœ¬æŒ‡å—æ¶µç›–æ¨¡å‹ä¼˜åŒ–çš„ä¸¤ä¸ªæ ¸å¿ƒæ–¹é¢ï¼š
1. **æ¨¡å‹é‡åŒ–**ï¼šå‹ç¼©æ¨¡å‹å¤§å°ï¼ŒåŠ é€Ÿæ¨ç†
2. **éƒ¨ç½²ä¼˜åŒ–**ï¼šé«˜æ•ˆæœåŠ¡åŒ–ï¼Œç”Ÿäº§çº§éƒ¨ç½²

```
ä¼˜åŒ–æµç¨‹ï¼š

è®­ç»ƒå¥½çš„æ¨¡å‹ (FP32, 500MB)
    â†“
ğŸ“¦ é‡åŒ–ä¼˜åŒ–
    â”œâ”€ INT8é‡åŒ– â†’ 125MB (4xå‹ç¼©)
    â”œâ”€ INT4é‡åŒ– â†’ 62MB (8xå‹ç¼©)
    â””â”€ æ¨ç†åŠ é€Ÿ 2-4x
    â†“
ğŸš€ éƒ¨ç½²ä¼˜åŒ–
    â”œâ”€ æ¨ç†å¼•æ“ (vLLM, TensorRT)
    â”œâ”€ æœåŠ¡åŒ– (FastAPI)
    â”œâ”€ è´Ÿè½½å‡è¡¡
    â””â”€ ç›‘æ§è¿ç»´
    â†“
ç”Ÿäº§çº§æœåŠ¡ (ä½å»¶è¿Ÿã€é«˜åå)
```

---

# Part 1: æ¨¡å‹é‡åŒ–

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**éƒ¨ç½²å¤§æ¨¡å‹çš„æŒ‘æˆ˜ï¼š**
- GPT-2 (124Må‚æ•°) = **500MB** (FP32)
- LLaMA-7B = **28GB** (FP32)
- æ¨ç†æ…¢ã€æ˜¾å­˜å ç”¨å¤§
- æ— æ³•åœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡è¿è¡Œ

**é‡åŒ–çš„è§£å†³æ–¹æ¡ˆï¼š**
- FP32 â†’ INT8: **4å€å‹ç¼©**
- FP32 â†’ INT4: **8å€å‹ç¼©**
- æ¨¡å‹ä»28GB â†’ **7GB** ç”šè‡³ **3.5GB**
- æ¨ç†é€Ÿåº¦**æå‡2-4å€**

---

## ğŸ“š 1.1 é‡åŒ–åŸºç¡€

### ğŸ” ä»€ä¹ˆæ˜¯é‡åŒ–ï¼Ÿ

```python
é‡åŒ– = ç”¨æ›´å°‘çš„æ¯”ç‰¹è¡¨ç¤ºæ•°å­—

FP32 (32ä½æµ®ç‚¹):
  èŒƒå›´: Â±3.4Ã—10Â³â¸
  ç²¾åº¦: 7ä½å°æ•°
  ä¾‹å­: 3.14159265...
  
INT8 (8ä½æ•´æ•°):
  èŒƒå›´: -128 åˆ° 127
  ç²¾åº¦: æ•´æ•°
  ä¾‹å­: 3
  
å‹ç¼©: 32ä½ â†’ 8ä½ = 4å€
```

### ğŸ“Š é‡åŒ–å¦‚ä½•å·¥ä½œï¼Ÿ

**çº¿æ€§é‡åŒ–ï¼ˆå¯¹ç§°ï¼‰ï¼š**

```python
# åŸå§‹æƒé‡ï¼ˆFP32ï¼‰
weights_fp32 = [-2.5, -1.0, 0.0, 1.2, 3.8]

# æ­¥éª¤1: æ‰¾åˆ°æœ€å¤§ç»å¯¹å€¼
max_val = max(abs(weights_fp32)) = 3.8

# æ­¥éª¤2: è®¡ç®—ç¼©æ”¾å› å­
scale = max_val / 127 = 3.8 / 127 = 0.0299

# æ­¥éª¤3: é‡åŒ–
weights_int8 = round(weights_fp32 / scale)
# ç»“æœ: [-84, -33, 0, 40, 127]

# æ­¥éª¤4: åé‡åŒ–ï¼ˆæ¨ç†æ—¶ï¼‰
weights_dequant = weights_int8 * scale
# ç»“æœ: [-2.51, -0.99, 0.0, 1.20, 3.80]

# è¯¯å·®å¾ˆå°ï¼
```

**é‡åŒ–å…¬å¼ï¼š**

```python
# é‡åŒ–
Q = round(R / S) + Z

å…¶ä¸­:
  R = å®æ•°å€¼ (FP32)
  Q = é‡åŒ–å€¼ (INT8)
  S = ç¼©æ”¾å› å­ (scale)
  Z = é›¶ç‚¹ (zero-point, éå¯¹ç§°é‡åŒ–)
  
# åé‡åŒ–
R = (Q - Z) * S
```

### ğŸ¯ é‡åŒ–çš„ç±»å‹

```python
1. æŒ‰ç²’åº¦åˆ†ç±»:
â”œâ”€â”€ Per-Tensoré‡åŒ–
â”‚   â””â”€â”€ æ•´ä¸ªå¼ é‡ç”¨ä¸€ä¸ªscale
â”‚   â””â”€â”€ ç®€å•ä½†ç²¾åº¦ç•¥ä½
â”‚
â”œâ”€â”€ Per-Channelé‡åŒ–
â”‚   â””â”€â”€ æ¯ä¸ªè¾“å‡ºé€šé“ä¸€ä¸ªscale
â”‚   â””â”€â”€ ç²¾åº¦æ›´é«˜ï¼ˆæ¨èï¼‰
â”‚
â””â”€â”€ Per-Groupé‡åŒ–
    â””â”€â”€ æ¯ç»„å‚æ•°ä¸€ä¸ªscale
    â””â”€â”€ æœ€é«˜ç²¾åº¦ï¼ˆGPTQä½¿ç”¨ï¼‰

2. æŒ‰æ—¶æœºåˆ†ç±»:
â”œâ”€â”€ è®­ç»ƒåé‡åŒ– (PTQ)
â”‚   â””â”€â”€ è®­ç»ƒå®Œæˆåé‡åŒ–
â”‚   â””â”€â”€ å¿«é€Ÿä½†ç²¾åº¦ç•¥ä½
â”‚
â””â”€â”€ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)
    â””â”€â”€ è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ–
    â””â”€â”€ ç²¾åº¦æœ€é«˜ä½†è€—æ—¶
```

---

## ğŸ“š 1.2 è®­ç»ƒåé‡åŒ– (PTQ)

### ğŸ”§ åŠ¨æ€é‡åŒ–ï¼ˆæœ€ç®€å•ï¼‰

```python
import torch
from model import GPT

# åŠ è½½æ¨¡å‹
model = GPT.from_pretrained('gpt2')
model.eval()

# åŠ¨æ€é‡åŒ–ï¼ˆåªé‡åŒ–æƒé‡ï¼‰
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # é‡åŒ–Linearå±‚
    dtype=torch.qint8   # INT8
)

# ä¿å­˜
torch.save(quantized_model.state_dict(), 'model_int8.pt')

# æ•ˆæœ
print(f"åŸå§‹æ¨¡å‹: {get_model_size(model):.2f} MB")
print(f"é‡åŒ–æ¨¡å‹: {get_model_size(quantized_model):.2f} MB")
# åŸå§‹æ¨¡å‹: 500.00 MB
# é‡åŒ–æ¨¡å‹: 125.00 MB (4xå‹ç¼©)
```

### ğŸ¯ é™æ€é‡åŒ–ï¼ˆæ›´é«˜æ€§èƒ½ï¼‰

```python
# é™æ€é‡åŒ–éœ€è¦æ ¡å‡†æ•°æ®
def calibrate(model, dataloader):
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            model(batch)

# å‡†å¤‡é‡åŒ–
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

# æ ¡å‡†
calibrate(model, calibration_dataloader)

# è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
quantized_model = torch.quantization.convert(model, inplace=False)
```

---

## ğŸ“š 1.3 é«˜çº§é‡åŒ–ç®—æ³•

### âš¡ GPTQ (GPT Quantization)

**æ ¸å¿ƒæ€æƒ³ï¼š** æœ€å°åŒ–é‡åŒ–è¯¯å·®

```python
# GPTQä¼ªä»£ç 
def gptq_quantize(weight, bits=4):
    """
    weight: [out_features, in_features]
    """
    # 1. è®¡ç®—HessiançŸ©é˜µï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
    H = compute_hessian(weight)
    
    # 2. é€åˆ—é‡åŒ–
    for i in range(weight.shape[1]):
        # é‡åŒ–ç¬¬iåˆ—
        w_q = quantize_column(weight[:, i], bits)
        
        # è®¡ç®—è¯¯å·®
        error = weight[:, i] - w_q
        
        # ç”¨Hessianæ›´æ–°åç»­åˆ—ï¼ˆè¡¥å¿è¯¯å·®ï¼‰
        weight[:, i+1:] -= error @ H[i, i+1:]
        
        weight[:, i] = w_q
    
    return weight

# ä½¿ç”¨GPTQ
from auto_gptq import AutoGPTQForCausalLM

model = AutoGPTQForCausalLM.from_pretrained(
    "gpt2",
    quantize_config={
        "bits": 4,  # 4-bité‡åŒ–
        "group_size": 128,
        "desc_act": False
    }
)

# é‡åŒ–
model.quantize(calibration_data)

# ä¿å­˜
model.save_quantized("gpt2-gptq-4bit")
```

**æ•ˆæœå¯¹æ¯”ï¼š**

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹æ³•     â”‚ å¤§å°   â”‚ é€Ÿåº¦   â”‚ å›°æƒ‘åº¦   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FP32     â”‚ 500MB  â”‚ 1.0x   â”‚ 25.3     â”‚
â”‚ INT8 PTQ â”‚ 125MB  â”‚ 2.5x   â”‚ 25.8 â†‘   â”‚
â”‚ GPTQ-4bitâ”‚ 62MB   â”‚ 3.5x   â”‚ 25.5 â†‘   â”‚
â”‚ GPTQ-3bitâ”‚ 47MB   â”‚ 4.0x   â”‚ 26.2 â†‘â†‘  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç»“è®º: GPTQ-4bitæ˜¯æœ€ä½³å¹³è¡¡ç‚¹
```

### ğŸ¯ AWQ (Activation-aware Weight Quantization)

**æ ¸å¿ƒæ€æƒ³ï¼š** ä¿æŠ¤é‡è¦æƒé‡

```python
# AWQçš„å…³é”®ï¼šä¸æ˜¯æ‰€æœ‰æƒé‡éƒ½åŒç­‰é‡è¦
def awq_quantize(weight, activation):
    """
    åŸºäºæ¿€æ´»å€¼ä¿æŠ¤é‡è¦æƒé‡
    """
    # 1. è®¡ç®—æ¯ä¸ªé€šé“çš„é‡è¦æ€§
    importance = activation.abs().mean(dim=0)
    
    # 2. å¯¹é‡è¦é€šé“ä½¿ç”¨æ›´é«˜ç²¾åº¦
    for i, imp in enumerate(importance):
        if imp > threshold:
            # é‡è¦é€šé“ï¼š8-bit
            weight[:, i] = quantize(weight[:, i], bits=8)
        else:
            # ä¸é‡è¦é€šé“ï¼š4-bit
            weight[:, i] = quantize(weight[:, i], bits=4)
    
    return weight

# ä½¿ç”¨AWQ
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained("gpt2")
model.quantize(
    tokenizer,
    quant_config={"bits": 4, "group_size": 128}
)
model.save_quantized("gpt2-awq-4bit")
```

---

## ğŸ“š 1.4 å®æˆ˜ï¼šé‡åŒ–GPT-2

### ğŸ”§ å®Œæ•´é‡åŒ–æµç¨‹

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# 1. åŠ è½½æ¨¡å‹
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

print(f"åŸå§‹æ¨¡å‹å¤§å°: {get_model_size(model):.2f} MB")

# 2. å‡†å¤‡æ ¡å‡†æ•°æ®
calibration_data = [
    "The quick brown fox jumps over the lazy dog.",
    "To be or not to be, that is the question.",
    # ... æ›´å¤šæ•°æ®
]

# 3. GPTQé‡åŒ–é…ç½®
quantize_config = BaseQuantizeConfig(
    bits=4,  # 4-bit
    group_size=128,  # åˆ†ç»„å¤§å°
    desc_act=False,  # ä¸ä½¿ç”¨é™åºæ¿€æ´»
)

# 4. é‡åŒ–
model_gptq = AutoGPTQForCausalLM.from_pretrained(
    model_name,
    quantize_config=quantize_config
)

model_gptq.quantize(calibration_data, use_triton=False)

# 5. ä¿å­˜
output_dir = "gpt2-gptq-4bit"
model_gptq.save_quantized(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"é‡åŒ–æ¨¡å‹å¤§å°: {get_model_size(model_gptq):.2f} MB")

# 6. æµ‹è¯•
def test_generation(model, prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=50)
    return tokenizer.decode(outputs[0])

print("åŸå§‹æ¨¡å‹:", test_generation(model, "Once upon a time"))
print("é‡åŒ–æ¨¡å‹:", test_generation(model_gptq, "Once upon a time"))
```

### ğŸ“Š é‡åŒ–æ•ˆæœè¯„ä¼°

```python
def evaluate_quantization(original_model, quantized_model, test_data):
    """
    è¯„ä¼°é‡åŒ–æ•ˆæœ
    """
    results = {
        'size': {},
        'speed': {},
        'quality': {}
    }
    
    # 1. æ¨¡å‹å¤§å°
    results['size']['original'] = get_model_size(original_model)
    results['size']['quantized'] = get_model_size(quantized_model)
    results['size']['compression'] = results['size']['original'] / results['size']['quantized']
    
    # 2. æ¨ç†é€Ÿåº¦
    import time
    
    start = time.time()
    for batch in test_data:
        original_model(batch)
    results['speed']['original'] = time.time() - start
    
    start = time.time()
    for batch in test_data:
        quantized_model(batch)
    results['speed']['quantized'] = time.time() - start
    results['speed']['speedup'] = results['speed']['original'] / results['speed']['quantized']
    
    # 3. è´¨é‡ï¼ˆå›°æƒ‘åº¦ï¼‰
    results['quality']['original'] = compute_perplexity(original_model, test_data)
    results['quality']['quantized'] = compute_perplexity(quantized_model, test_data)
    results['quality']['degradation'] = results['quality']['quantized'] - results['quality']['original']
    
    return results

# è¿è¡Œè¯„ä¼°
results = evaluate_quantization(model, model_gptq, test_dataloader)

print(f"""
é‡åŒ–æ•ˆæœæŠ¥å‘Šï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¦ æ¨¡å‹å¤§å°
  åŸå§‹: {results['size']['original']:.2f} MB
  é‡åŒ–: {results['size']['quantized']:.2f} MB
  å‹ç¼©æ¯”: {results['size']['compression']:.2f}x
  
âš¡ æ¨ç†é€Ÿåº¦
  åŸå§‹: {results['speed']['original']:.2f}s
  é‡åŒ–: {results['speed']['quantized']:.2f}s
  åŠ é€Ÿæ¯”: {results['speed']['speedup']:.2f}x
  
ğŸ“Š æ¨¡å‹è´¨é‡
  åŸå§‹å›°æƒ‘åº¦: {results['quality']['original']:.2f}
  é‡åŒ–å›°æƒ‘åº¦: {results['quality']['quantized']:.2f}
  è´¨é‡ä¸‹é™: {results['quality']['degradation']:.2f}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")
```

---

# Part 2: éƒ¨ç½²ä¼˜åŒ–

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**ä»ç ”ç©¶åˆ°ç”Ÿäº§çš„é¸¿æ²Ÿï¼š**
- è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æœ¬åœ°è·‘å¾—å¾ˆå¥½
- ä½†å¦‚ä½•è®©ç”¨æˆ·è®¿é—®ï¼Ÿ
- å¦‚ä½•å¤„ç†å¹¶å‘è¯·æ±‚ï¼Ÿ
- å¦‚ä½•ä¿è¯ä½å»¶è¿Ÿå’Œé«˜ååï¼Ÿ

**éƒ¨ç½²ç›®æ ‡ï¼š**
```python
æ€§èƒ½æŒ‡æ ‡:
â”œâ”€â”€ å»¶è¿Ÿ (Latency)
â”‚   â””â”€â”€ é¦–tokenå»¶è¿Ÿ < 100ms
â”‚   â””â”€â”€ å¹³å‡å»¶è¿Ÿ < 500ms
â”‚
â”œâ”€â”€ ååé‡ (Throughput)
â”‚   â””â”€â”€ > 1000 tokens/s
â”‚   â””â”€â”€ æ”¯æŒ100+å¹¶å‘ç”¨æˆ·
â”‚
â”œâ”€â”€ å¯ç”¨æ€§ (Availability)
â”‚   â””â”€â”€ 99.9% uptime
â”‚   â””â”€â”€ è‡ªåŠ¨æ•…éšœæ¢å¤
â”‚
â””â”€â”€ æˆæœ¬ (Cost)
    â””â”€â”€ < $0.001 per 1K tokens
    â””â”€â”€ GPUåˆ©ç”¨ç‡ > 80%
```

---

## ğŸ“š 2.1 æ¨ç†ä¼˜åŒ–æŠ€æœ¯

### âš¡ KV Cacheï¼ˆå…³é”®ä¼˜åŒ–ï¼‰

**é—®é¢˜ï¼š** è‡ªå›å½’ç”Ÿæˆæ—¶é‡å¤è®¡ç®—

```python
# æ²¡æœ‰KV Cacheï¼ˆä½æ•ˆï¼‰
def generate_without_cache(model, prompt):
    tokens = [prompt]
    for i in range(max_length):
        # æ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰tokençš„attention
        output = model(tokens)  # è®¡ç®—é‡éšé•¿åº¦çº¿æ€§å¢é•¿
        next_token = sample(output[-1])
        tokens.append(next_token)
    return tokens

# æ—¶é—´å¤æ‚åº¦: O(nÂ²)
# ç”Ÿæˆ100ä¸ªtokenéœ€è¦è®¡ç®—: 1+2+3+...+100 = 5050æ¬¡attention

# ä½¿ç”¨KV Cacheï¼ˆé«˜æ•ˆï¼‰
def generate_with_cache(model, prompt):
    tokens = [prompt]
    past_key_values = None  # ç¼“å­˜
    
    for i in range(max_length):
        # åªè®¡ç®—æ–°tokençš„attention
        output, past_key_values = model(
            tokens[-1:],  # åªä¼ å…¥æœ€åä¸€ä¸ªtoken
            past_key_values=past_key_values  # ä½¿ç”¨ç¼“å­˜
        )
        next_token = sample(output[-1])
        tokens.append(next_token)
    return tokens

# æ—¶é—´å¤æ‚åº¦: O(n)
# ç”Ÿæˆ100ä¸ªtokenåªéœ€è¦: 100æ¬¡attention
# åŠ é€Ÿ: 50å€ï¼
```

**å®ç°KV Cacheï¼š**

```python
class GPTWithCache(nn.Module):
    def forward(self, x, past_key_values=None):
        B, T = x.shape
        
        # Embedding
        tok_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(torch.arange(T))
        x = tok_emb + pos_emb
        
        # Transformer blocks
        present_key_values = []
        for i, block in enumerate(self.blocks):
            # ä½¿ç”¨ç¼“å­˜çš„KV
            past_kv = past_key_values[i] if past_key_values else None
            x, present_kv = block(x, past_kv)
            present_key_values.append(present_kv)
        
        # Output
        logits = self.lm_head(x)
        
        return logits, present_key_values

# ä½¿ç”¨
model = GPTWithCache()
past_kv = None

for _ in range(100):
    logits, past_kv = model(next_token, past_key_values=past_kv)
    next_token = sample(logits)
```

### ğŸš€ Continuous Batching

**é—®é¢˜ï¼š** ä¼ ç»Ÿbatchingæ•ˆç‡ä½

```python
# ä¼ ç»ŸBatchingï¼ˆä½æ•ˆï¼‰
batch = [req1, req2, req3, req4]  # 4ä¸ªè¯·æ±‚
# å¿…é¡»ç­‰æœ€é•¿çš„è¯·æ±‚å®Œæˆ
# req1: 10 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
# req2: 50 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
# req3: 20 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
# req4: 15 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
#                  â†‘ æµªè´¹çš„è®¡ç®—ï¼ˆreq1å·²å®Œæˆä½†è¦ç­‰å¾…ï¼‰

# Continuous Batchingï¼ˆé«˜æ•ˆï¼‰
# åŠ¨æ€æ·»åŠ /ç§»é™¤è¯·æ±‚
batch = [req1, req2, req3, req4]
# req1å®Œæˆ â†’ ç«‹å³æ·»åŠ req5
batch = [req2, req3, req4, req5]
# req4å®Œæˆ â†’ ç«‹å³æ·»åŠ req6
batch = [req2, req3, req5, req6]
# å§‹ç»ˆä¿æŒbatchæ»¡è½½ï¼ŒGPUåˆ©ç”¨ç‡æœ€å¤§åŒ–
```

### ğŸ“Š PagedAttentionï¼ˆvLLMæ ¸å¿ƒæŠ€æœ¯ï¼‰

```python
# é—®é¢˜ï¼šKV Cacheæ˜¾å­˜ç¢ç‰‡åŒ–
# ä¼ ç»Ÿæ–¹å¼ï¼šä¸ºæ¯ä¸ªè¯·æ±‚é¢„åˆ†é…è¿ç»­æ˜¾å­˜
req1_kv: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 32 tokens (é¢„åˆ†é…100)
req2_kv: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 32 tokens (é¢„åˆ†é…100)
# æµªè´¹: 68% æ˜¾å­˜

# PagedAttentionï¼šåˆ†é¡µç®¡ç†ï¼ˆç±»ä¼¼æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜ï¼‰
req1_kv: [page1][page2][page3]...  # æŒ‰éœ€åˆ†é…
req2_kv: [page5][page6]...         # ä¸è¿ç»­ä¹ŸOK
# åˆ©ç”¨ç‡: 95%+
```

---

## ğŸ“š 2.2 éƒ¨ç½²æ¡†æ¶é€‰æ‹©

### ğŸ”§ æ–¹æ¡ˆå¯¹æ¯”

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¡†æ¶        â”‚ æ˜“ç”¨æ€§   â”‚ æ€§èƒ½     â”‚ åŠŸèƒ½     â”‚ æ¨èåœºæ™¯ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FastAPI     â”‚ â­â­â­â­â­â”‚ â­â­     â”‚ â­â­     â”‚ åŸå‹/å°è§„æ¨¡â”‚
â”‚ vLLM        â”‚ â­â­â­â­ â”‚ â­â­â­â­â­â”‚ â­â­â­â­ â”‚ ç”Ÿäº§æ¨è  â”‚
â”‚ TensorRT-LLMâ”‚ â­â­     â”‚ â­â­â­â­â­â”‚ â­â­â­   â”‚ æè‡´æ€§èƒ½  â”‚
â”‚ Text Gen UI â”‚ â­â­â­â­â­â”‚ â­â­â­   â”‚ â­â­â­â­â­â”‚ å¼€ç®±å³ç”¨  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš¡ vLLMéƒ¨ç½²ï¼ˆæ¨èï¼‰

```python
# å®‰è£…
pip install vllm

# å¯åŠ¨æœåŠ¡
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="gpt2",
    tensor_parallel_size=1,  # å•GPU
    dtype="float16",
    max_model_len=2048,
)

# æ¨ç†
prompts = [
    "Once upon a time",
    "The future of AI is",
]

sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100
)

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)

# æ€§èƒ½å¯¹æ¯”
"""
HuggingFace:  100 tokens/s
vLLM:         2000+ tokens/s
åŠ é€Ÿ: 20xï¼
"""
```

### ğŸŒ APIæœåŠ¡åŒ–

```python
# ä½¿ç”¨FastAPI + vLLM
from fastapi import FastAPI
from pydantic import BaseModel
from vllm import LLM, SamplingParams

app = FastAPI()
llm = LLM(model="gpt2")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.8

@app.post("/generate")
async def generate(request: GenerateRequest):
    sampling_params = SamplingParams(
        temperature=request.temperature,
        max_tokens=request.max_tokens
    )
    
    outputs = llm.generate([request.prompt], sampling_params)
    
    return {
        "text": outputs[0].outputs[0].text,
        "tokens": len(outputs[0].outputs[0].token_ids)
    }

# å¯åŠ¨
# uvicorn app:app --host 0.0.0.0 --port 8000

# æµ‹è¯•
"""
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Once upon a time", "max_tokens": 50}'
"""
```

---

## ğŸ“š 2.3 ç”Ÿäº§çº§éƒ¨ç½²

### ğŸ³ Dockerå®¹å™¨åŒ–

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# å®‰è£…Python
RUN apt-get update && apt-get install -y python3.10 python3-pip

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . /app
WORKDIR /app

# ä¸‹è½½æ¨¡å‹
RUN python download_model.py

# å¯åŠ¨æœåŠ¡
CMD ["python", "-m", "vllm.entrypoints.api_server", \
     "--model", "gpt2", \
     "--host", "0.0.0.0", \
     "--port", "8000"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t gpt2-service:v1 .

# è¿è¡Œå®¹å™¨
docker run -d \
  --gpus all \
  -p 8000:8000 \
  --name gpt2-api \
  gpt2-service:v1

# æµ‹è¯•
curl http://localhost:8000/health
```

### â˜¸ï¸ Kuberneteséƒ¨ç½²

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt2-deployment
spec:
  replicas: 3  # 3ä¸ªå‰¯æœ¬
  selector:
    matchLabels:
      app: gpt2
  template:
    metadata:
      labels:
        app: gpt2
    spec:
      containers:
      - name: gpt2
        image: gpt2-service:v1
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1  # æ¯ä¸ªpodä¸€ä¸ªGPU
          requests:
            memory: "8Gi"
            cpu: "4"
---
apiVersion: v1
kind: Service
metadata:
  name: gpt2-service
spec:
  selector:
    app: gpt2
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

```bash
# éƒ¨ç½²
kubectl apply -f deployment.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods
kubectl get services

# æ‰©å®¹
kubectl scale deployment gpt2-deployment --replicas=10
```

---

## ğŸ“š 2.4 ç›‘æ§ä¸è¿ç»´

### ğŸ“Š æ€§èƒ½ç›‘æ§

```python
# ä½¿ç”¨Prometheus + Grafana
from prometheus_client import Counter, Histogram, Gauge
import time

# å®šä¹‰æŒ‡æ ‡
request_count = Counter('requests_total', 'Total requests')
request_duration = Histogram('request_duration_seconds', 'Request duration')
gpu_utilization = Gauge('gpu_utilization', 'GPU utilization')
active_requests = Gauge('active_requests', 'Active requests')

@app.post("/generate")
async def generate(request: GenerateRequest):
    request_count.inc()
    active_requests.inc()
    
    start = time.time()
    try:
        # ç”Ÿæˆ
        output = llm.generate(...)
        
        duration = time.time() - start
        request_duration.observe(duration)
        
        return output
    finally:
        active_requests.dec()

# Grafanaä»ªè¡¨æ¿
"""
é¢æ¿1: è¯·æ±‚QPSï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰
é¢æ¿2: P50/P95/P99å»¶è¿Ÿ
é¢æ¿3: GPUåˆ©ç”¨ç‡
é¢æ¿4: æ´»è·ƒè¯·æ±‚æ•°
é¢æ¿5: é”™è¯¯ç‡
"""
```

### ğŸš¨ å‘Šè­¦é…ç½®

```yaml
# alerting_rules.yaml
groups:
- name: gpt2_alerts
  rules:
  - alert: HighLatency
    expr: histogram_quantile(0.95, request_duration_seconds) > 1.0
    for: 5m
    annotations:
      summary: "P95 latency > 1s"
      
  - alert: HighErrorRate
    expr: rate(errors_total[5m]) > 0.05
    for: 5m
    annotations:
      summary: "Error rate > 5%"
      
  - alert: LowGPUUtilization
    expr: gpu_utilization < 0.5
    for: 10m
    annotations:
      summary: "GPU utilization < 50%"
```

---

## ğŸ“š 2.5 æˆæœ¬ä¼˜åŒ–

### ğŸ’° æˆæœ¬åˆ†æ

```python
# æˆæœ¬è®¡ç®—
def calculate_cost(
    gpu_type="A100",
    num_gpus=4,
    hours_per_month=730,
    requests_per_second=100
):
    # GPUæˆæœ¬
    gpu_costs = {
        "A100": 3.0,  # $/hour
        "A10G": 1.0,
        "T4": 0.35,
    }
    
    gpu_cost = gpu_costs[gpu_type] * num_gpus * hours_per_month
    
    # è¯·æ±‚é‡
    total_requests = requests_per_second * 3600 * hours_per_month
    
    # æ¯åƒæ¬¡è¯·æ±‚æˆæœ¬
    cost_per_1k = (gpu_cost / total_requests) * 1000
    
    print(f"""
æˆæœ¬åˆ†ææŠ¥å‘Šï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GPUé…ç½®: {num_gpus}x {gpu_type}
æœˆåº¦æˆæœ¬: ${gpu_cost:,.2f}
æœˆåº¦è¯·æ±‚: {total_requests:,.0f}
æ¯1Kè¯·æ±‚æˆæœ¬: ${cost_per_1k:.4f}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """)

# ç¤ºä¾‹
calculate_cost(gpu_type="A10G", num_gpus=2, requests_per_second=50)
```

### ğŸ¯ ä¼˜åŒ–ç­–ç•¥

```python
ä¼˜åŒ–ç­–ç•¥ï¼š

1. æ¨¡å‹ä¼˜åŒ–
â”œâ”€â”€ é‡åŒ–åˆ°INT4 â†’ æ˜¾å­˜å‡å°‘8x â†’ GPUæ•°é‡å‡åŠ
â”œâ”€â”€ ä½¿ç”¨å°æ¨¡å‹ â†’ GPT-2 (124M) vs GPT-2-XL (1.5B)
â””â”€â”€ æ¨¡å‹è’¸é¦ â†’ ä¿æŒè´¨é‡ï¼Œå‡å°‘å‚æ•°

2. æ¨ç†ä¼˜åŒ–
â”œâ”€â”€ ä½¿ç”¨vLLM â†’ ååé‡æå‡20x â†’ GPUæ•°é‡å‡å°‘
â”œâ”€â”€ Continuous Batching â†’ GPUåˆ©ç”¨ç‡ä»50% â†’ 90%
â””â”€â”€ KV Cache â†’ å»¶è¿Ÿé™ä½50x

3. åŸºç¡€è®¾æ–½ä¼˜åŒ–
â”œâ”€â”€ Spotå®ä¾‹ â†’ æˆæœ¬é™ä½70%
â”œâ”€â”€ è‡ªåŠ¨æ‰©ç¼©å®¹ â†’ æ ¹æ®è´Ÿè½½è°ƒæ•´GPUæ•°é‡
â””â”€â”€ å¤šåŒºåŸŸéƒ¨ç½² â†’ é™ä½ç½‘ç»œå»¶è¿Ÿ

4. ä¸šåŠ¡ä¼˜åŒ–
â”œâ”€â”€ ç¼“å­˜å¸¸è§è¯·æ±‚ â†’ å‡å°‘é‡å¤è®¡ç®—
â”œâ”€â”€ é™æµ â†’ é˜²æ­¢èµ„æºæµªè´¹
â””â”€â”€ å¼‚æ­¥å¤„ç† â†’ æé«˜ååé‡

ç»¼åˆä¼˜åŒ–åï¼šæˆæœ¬å¯é™ä½ 80-90%ï¼
```

---

## ğŸ¯ æ€»ç»“ï¼šç«¯åˆ°ç«¯ä¼˜åŒ–æµç¨‹

```python
å®Œæ•´ä¼˜åŒ–æµç¨‹ï¼š

Step 1: æ¨¡å‹é‡åŒ–
  â”œâ”€â”€ é€‰æ‹©é‡åŒ–æ–¹æ³•ï¼ˆGPTQ-4bitæ¨èï¼‰
  â”œâ”€â”€ å‡†å¤‡æ ¡å‡†æ•°æ®
  â”œâ”€â”€ æ‰§è¡Œé‡åŒ–
  â””â”€â”€ éªŒè¯è´¨é‡ï¼ˆå›°æƒ‘åº¦ < 5%ä¸‹é™ï¼‰
  
Step 2: æ¨ç†ä¼˜åŒ–
  â”œâ”€â”€ å®ç°KV Cache
  â”œâ”€â”€ é€‰æ‹©æ¨ç†æ¡†æ¶ï¼ˆvLLMæ¨èï¼‰
  â”œâ”€â”€ é…ç½®Continuous Batching
  â””â”€â”€ æ€§èƒ½æµ‹è¯•

Step 3: æœåŠ¡åŒ–
  â”œâ”€â”€ APIå°è£…ï¼ˆFastAPIï¼‰
  â”œâ”€â”€ Dockerå®¹å™¨åŒ–
  â”œâ”€â”€ Kuberneteséƒ¨ç½²
  â””â”€â”€ è´Ÿè½½å‡è¡¡

Step 4: ç›‘æ§è¿ç»´
  â”œâ”€â”€ æ·»åŠ ç›‘æ§æŒ‡æ ‡
  â”œâ”€â”€ é…ç½®å‘Šè­¦
  â”œâ”€â”€ æ—¥å¿—æ”¶é›†
  â””â”€â”€ æ€§èƒ½è°ƒä¼˜

Step 5: æˆæœ¬ä¼˜åŒ–
  â”œâ”€â”€ åˆ†ææˆæœ¬ç“¶é¢ˆ
  â”œâ”€â”€ åº”ç”¨ä¼˜åŒ–ç­–ç•¥
  â”œâ”€â”€ æŒç»­ç›‘æ§
  â””â”€â”€ è¿­ä»£æ”¹è¿›

æœ€ç»ˆæ•ˆæœï¼š
  âœ… æ¨¡å‹å¤§å°: 500MB â†’ 62MB (8xå‹ç¼©)
  âœ… æ¨ç†é€Ÿåº¦: 100 tokens/s â†’ 2000+ tokens/s (20xåŠ é€Ÿ)
  âœ… æˆæœ¬: $10/1K requests â†’ $0.001/1K requests (10000xé™ä½)
  âœ… å»¶è¿Ÿ: 5s â†’ 100ms (50xé™ä½)
```

---

## ğŸ“š æ¨èèµ„æº

### é‡åŒ–ç›¸å…³
- [GPTQè®ºæ–‡](https://arxiv.org/abs/2210.17323)
- [AWQè®ºæ–‡](https://arxiv.org/abs/2306.00978)
- [AutoGPTQåº“](https://github.com/PanQiWei/AutoGPTQ)

### éƒ¨ç½²ç›¸å…³
- [vLLMæ–‡æ¡£](https://docs.vllm.ai/)
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- [Text Generation Inference](https://github.com/huggingface/text-generation-inference)

### ç›‘æ§ç›¸å…³
- [Prometheus](https://prometheus.io/)
- [Grafana](https://grafana.com/)

---

**ä¸‹ä¸€æ­¥ï¼š** å­¦ä¹ ç”Ÿäº§çº§éƒ¨ç½²å®æˆ˜ï¼ˆ10_production_deployment.mdï¼‰

