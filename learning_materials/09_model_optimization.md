# ç¬¬09ç« ï¼šæ¨¡å‹ä¼˜åŒ–å®Œå…¨æŒ‡å— - ä»é‡åŒ–åˆ°éƒ¨ç½²

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡æ¨¡å‹å‹ç¼©ã€æ¨ç†åŠ é€Ÿå’Œéƒ¨ç½²ä¼˜åŒ–çš„å®Œæ•´æŠ€æœ¯æ ˆ  
> **éš¾åº¦ç­‰çº§**: ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ é«˜çº§  
> **é¢„è®¡æ—¶é—´**: 6-8å°æ—¶  
> **å‰ç½®çŸ¥è¯†**: 05æ¨¡å‹æ¶æ„ã€06 Scaling Laws

## ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£æ¨¡å‹é‡åŒ–çš„åŸç†ï¼ˆFP32â†’INT8â†’INT4ï¼‰
- âœ… æŒæ¡KV Cacheã€æŠ•æœºé‡‡æ ·ç­‰æ¨ç†ä¼˜åŒ–æŠ€æœ¯
- âœ… ç†è§£vLLMã€TensorRTç­‰æ¨ç†å¼•æ“
- âœ… æŒæ¡PagedAttentionã€Continuous Batchingç­‰æŠ€æœ¯
- âœ… èƒ½å¤Ÿä¼˜åŒ–æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨
- âœ… ç†è§£ç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²æœ€ä½³å®è·µ

## ğŸ’­ å¼€å§‹ä¹‹å‰ï¼šä¸ºä»€ä¹ˆè¦å­¦è¿™ä¸ªï¼Ÿ

**åœºæ™¯**ï¼šè®­ç»ƒå¥½çš„æ¨¡å‹å¤ªå¤§ã€å¤ªæ…¢ï¼Œæ— æ³•å®é™…ä½¿ç”¨ã€‚

**æ¯”å–»**ï¼šå°±åƒå‹ç¼©æ–‡ä»¶å’Œå¿«é€’ï¼š
- ğŸ“¦ å‹ç¼©ï¼šå‡å°ä½“ç§¯ï¼Œæ–¹ä¾¿ä¼ è¾“
- ğŸš€ åŠ é€Ÿï¼šæ›´å¿«é€è¾¾
- ğŸ“® æœåŠ¡ï¼šç¨³å®šå¯é 

**å­¦å®Œä¹‹å**ï¼š
- âœ… æ¨¡å‹ä½“ç§¯å‡å°4-8å€
- âœ… æ¨ç†é€Ÿåº¦æå‡2-10å€
- âœ… èƒ½éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
- âœ… é™ä½è¿è¥æˆæœ¬

---

## ğŸ¯ æ¦‚è§ˆ

æœ¬æŒ‡å—æ¶µç›–æ¨¡å‹ä¼˜åŒ–çš„ä¸‰ä¸ªæ ¸å¿ƒæ–¹é¢ï¼š
1. **æ¨¡å‹é‡åŒ–**ï¼šå‹ç¼©æ¨¡å‹å¤§å°ï¼ŒåŠ é€Ÿæ¨ç†
2. **æ¨ç†ä¼˜åŒ–**ï¼šKV Cacheã€æŠ•æœºé‡‡æ ·ç­‰åŠ é€ŸæŠ€æœ¯
3. **éƒ¨ç½²ä¼˜åŒ–**ï¼šé«˜æ•ˆæœåŠ¡åŒ–ï¼Œç”Ÿäº§çº§éƒ¨ç½²

```
ä¼˜åŒ–æµç¨‹ï¼š

è®­ç»ƒå¥½çš„æ¨¡å‹ (FP32, 500MB)
    â†“
ğŸ“¦ é‡åŒ–ä¼˜åŒ–
    â”œâ”€ INT8é‡åŒ– â†’ 125MB (4xå‹ç¼©)
    â”œâ”€ INT4é‡åŒ– â†’ 62MB (8xå‹ç¼©)
    â””â”€ æ¨ç†åŠ é€Ÿ 2-4x
    â†“
âš¡ æ¨ç†ä¼˜åŒ–
    â”œâ”€ KV Cache â†’ 50xåŠ é€Ÿ
    â”œâ”€ æŠ•æœºé‡‡æ · â†’ 2-4xåŠ é€Ÿ
    â”œâ”€ Continuous Batching â†’ é«˜åå
    â””â”€ PagedAttention â†’ é«˜æ˜¾å­˜åˆ©ç”¨ç‡
    â†“
ğŸš€ éƒ¨ç½²ä¼˜åŒ–
    â”œâ”€ æ¨ç†å¼•æ“ (vLLM, TensorRT)
    â”œâ”€ æœåŠ¡åŒ– (FastAPI)
    â”œâ”€ è´Ÿè½½å‡è¡¡
    â””â”€ ç›‘æ§è¿ç»´
    â†“
ç”Ÿäº§çº§æœåŠ¡ (ä½å»¶è¿Ÿã€é«˜åå)
```

---

# Part 1: æ¨¡å‹é‡åŒ–

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**éƒ¨ç½²å¤§æ¨¡å‹çš„æŒ‘æˆ˜ï¼š**
- GPT-2 (124Må‚æ•°) = **500MB** (FP32)
- LLaMA-7B = **28GB** (FP32)
- æ¨ç†æ…¢ã€æ˜¾å­˜å ç”¨å¤§
- æ— æ³•åœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡è¿è¡Œ

**é‡åŒ–çš„è§£å†³æ–¹æ¡ˆï¼š**
- FP32 â†’ INT8: **4å€å‹ç¼©**
- FP32 â†’ INT4: **8å€å‹ç¼©**
- æ¨¡å‹ä»28GB â†’ **7GB** ç”šè‡³ **3.5GB**
- æ¨ç†é€Ÿåº¦**æå‡2-4å€**

---

## ğŸ“š 1.1 é‡åŒ–åŸºç¡€

### ğŸ” ä»€ä¹ˆæ˜¯é‡åŒ–ï¼Ÿ

```python
é‡åŒ– = ç”¨æ›´å°‘çš„æ¯”ç‰¹è¡¨ç¤ºæ•°å­—

FP32 (32ä½æµ®ç‚¹):
  èŒƒå›´: Â±3.4Ã—10Â³â¸
  ç²¾åº¦: 7ä½å°æ•°
  ä¾‹å­: 3.14159265...
  
INT8 (8ä½æ•´æ•°):
  èŒƒå›´: -128 åˆ° 127
  ç²¾åº¦: æ•´æ•°
  ä¾‹å­: 3
  
å‹ç¼©: 32ä½ â†’ 8ä½ = 4å€
```

### ğŸ“Š é‡åŒ–å¦‚ä½•å·¥ä½œï¼Ÿ

**çº¿æ€§é‡åŒ–ï¼ˆå¯¹ç§°ï¼‰ï¼š**

```python
# åŸå§‹æƒé‡ï¼ˆFP32ï¼‰
weights_fp32 = [-2.5, -1.0, 0.0, 1.2, 3.8]

# æ­¥éª¤1: æ‰¾åˆ°æœ€å¤§ç»å¯¹å€¼
max_val = max(abs(weights_fp32)) = 3.8

# æ­¥éª¤2: è®¡ç®—ç¼©æ”¾å› å­
scale = max_val / 127 = 3.8 / 127 = 0.0299

# æ­¥éª¤3: é‡åŒ–
weights_int8 = round(weights_fp32 / scale)
# ç»“æœ: [-84, -33, 0, 40, 127]

# æ­¥éª¤4: åé‡åŒ–ï¼ˆæ¨ç†æ—¶ï¼‰
weights_dequant = weights_int8 * scale
# ç»“æœ: [-2.51, -0.99, 0.0, 1.20, 3.80]

# è¯¯å·®å¾ˆå°ï¼
```

**é‡åŒ–å…¬å¼ï¼š**

```python
# é‡åŒ–
Q = round(R / S) + Z

å…¶ä¸­:
  R = å®æ•°å€¼ (FP32)
  Q = é‡åŒ–å€¼ (INT8)
  S = ç¼©æ”¾å› å­ (scale)
  Z = é›¶ç‚¹ (zero-point, éå¯¹ç§°é‡åŒ–)
  
# åé‡åŒ–
R = (Q - Z) * S
```

### ğŸ¯ é‡åŒ–çš„ç±»å‹

```python
1. æŒ‰ç²’åº¦åˆ†ç±»:
â”œâ”€â”€ Per-Tensoré‡åŒ–
â”‚   â””â”€â”€ æ•´ä¸ªå¼ é‡ç”¨ä¸€ä¸ªscale
â”‚   â””â”€â”€ ç®€å•ä½†ç²¾åº¦ç•¥ä½
â”‚
â”œâ”€â”€ Per-Channelé‡åŒ–
â”‚   â””â”€â”€ æ¯ä¸ªè¾“å‡ºé€šé“ä¸€ä¸ªscale
â”‚   â””â”€â”€ ç²¾åº¦æ›´é«˜ï¼ˆæ¨èï¼‰
â”‚
â””â”€â”€ Per-Groupé‡åŒ–
    â””â”€â”€ æ¯ç»„å‚æ•°ä¸€ä¸ªscale
    â””â”€â”€ æœ€é«˜ç²¾åº¦ï¼ˆGPTQä½¿ç”¨ï¼‰

2. æŒ‰æ—¶æœºåˆ†ç±»:
â”œâ”€â”€ è®­ç»ƒåé‡åŒ– (PTQ)
â”‚   â””â”€â”€ è®­ç»ƒå®Œæˆåé‡åŒ–
â”‚   â””â”€â”€ å¿«é€Ÿä½†ç²¾åº¦ç•¥ä½
â”‚
â””â”€â”€ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)
    â””â”€â”€ è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ–
    â””â”€â”€ ç²¾åº¦æœ€é«˜ä½†è€—æ—¶
```

---

## ğŸ“š 1.2 è®­ç»ƒåé‡åŒ– (PTQ)

### ğŸ”§ åŠ¨æ€é‡åŒ–ï¼ˆæœ€ç®€å•ï¼‰

```python
import torch
from model import GPT

# åŠ è½½æ¨¡å‹
model = GPT.from_pretrained('gpt2')
model.eval()

# åŠ¨æ€é‡åŒ–ï¼ˆåªé‡åŒ–æƒé‡ï¼‰
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # é‡åŒ–Linearå±‚
    dtype=torch.qint8   # INT8
)

# ä¿å­˜
torch.save(quantized_model.state_dict(), 'model_int8.pt')

# æ•ˆæœ
print(f"åŸå§‹æ¨¡å‹: {get_model_size(model):.2f} MB")
print(f"é‡åŒ–æ¨¡å‹: {get_model_size(quantized_model):.2f} MB")
# åŸå§‹æ¨¡å‹: 500.00 MB
# é‡åŒ–æ¨¡å‹: 125.00 MB (4xå‹ç¼©)
```

### ğŸ¯ é™æ€é‡åŒ–ï¼ˆæ›´é«˜æ€§èƒ½ï¼‰

```python
# é™æ€é‡åŒ–éœ€è¦æ ¡å‡†æ•°æ®
def calibrate(model, dataloader):
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            model(batch)

# å‡†å¤‡é‡åŒ–
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

# æ ¡å‡†
calibrate(model, calibration_dataloader)

# è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
quantized_model = torch.quantization.convert(model, inplace=False)
```

---

## ğŸ“š 1.3 é«˜çº§é‡åŒ–ç®—æ³•

### âš¡ GPTQ (GPT Quantization)

**æ ¸å¿ƒæ€æƒ³ï¼š** æœ€å°åŒ–é‡åŒ–è¯¯å·®

```python
# GPTQä¼ªä»£ç 
def gptq_quantize(weight, bits=4):
    """
    weight: [out_features, in_features]
    """
    # 1. è®¡ç®—HessiançŸ©é˜µï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
    H = compute_hessian(weight)
    
    # 2. é€åˆ—é‡åŒ–
    for i in range(weight.shape[1]):
        # é‡åŒ–ç¬¬iåˆ—
        w_q = quantize_column(weight[:, i], bits)
        
        # è®¡ç®—è¯¯å·®
        error = weight[:, i] - w_q
        
        # ç”¨Hessianæ›´æ–°åç»­åˆ—ï¼ˆè¡¥å¿è¯¯å·®ï¼‰
        weight[:, i+1:] -= error @ H[i, i+1:]
        
        weight[:, i] = w_q
    
    return weight

# ä½¿ç”¨GPTQ
from auto_gptq import AutoGPTQForCausalLM

model = AutoGPTQForCausalLM.from_pretrained(
    "gpt2",
    quantize_config={
        "bits": 4,  # 4-bité‡åŒ–
        "group_size": 128,
        "desc_act": False
    }
)

# é‡åŒ–
model.quantize(calibration_data)

# ä¿å­˜
model.save_quantized("gpt2-gptq-4bit")
```

**æ•ˆæœå¯¹æ¯”ï¼š**

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹æ³•     â”‚ å¤§å°   â”‚ é€Ÿåº¦   â”‚ å›°æƒ‘åº¦   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FP32     â”‚ 500MB  â”‚ 1.0x   â”‚ 25.3     â”‚
â”‚ INT8 PTQ â”‚ 125MB  â”‚ 2.5x   â”‚ 25.8 â†‘   â”‚
â”‚ GPTQ-4bitâ”‚ 62MB   â”‚ 3.5x   â”‚ 25.5 â†‘   â”‚
â”‚ GPTQ-3bitâ”‚ 47MB   â”‚ 4.0x   â”‚ 26.2 â†‘â†‘  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç»“è®º: GPTQ-4bitæ˜¯æœ€ä½³å¹³è¡¡ç‚¹
```

### ğŸ¯ AWQ (Activation-aware Weight Quantization)

**æ ¸å¿ƒæ€æƒ³ï¼š** ä¿æŠ¤é‡è¦æƒé‡

```python
# AWQçš„å…³é”®ï¼šä¸æ˜¯æ‰€æœ‰æƒé‡éƒ½åŒç­‰é‡è¦
def awq_quantize(weight, activation):
    """
    åŸºäºæ¿€æ´»å€¼ä¿æŠ¤é‡è¦æƒé‡
    """
    # 1. è®¡ç®—æ¯ä¸ªé€šé“çš„é‡è¦æ€§
    importance = activation.abs().mean(dim=0)
    
    # 2. å¯¹é‡è¦é€šé“ä½¿ç”¨æ›´é«˜ç²¾åº¦
    for i, imp in enumerate(importance):
        if imp > threshold:
            # é‡è¦é€šé“ï¼š8-bit
            weight[:, i] = quantize(weight[:, i], bits=8)
        else:
            # ä¸é‡è¦é€šé“ï¼š4-bit
            weight[:, i] = quantize(weight[:, i], bits=4)
    
    return weight

# ä½¿ç”¨AWQ
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained("gpt2")
model.quantize(
    tokenizer,
    quant_config={"bits": 4, "group_size": 128}
)
model.save_quantized("gpt2-awq-4bit")
```

---

## ğŸ“š 1.4 å®æˆ˜ï¼šé‡åŒ–GPT-2

### ğŸ”§ å®Œæ•´é‡åŒ–æµç¨‹

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# 1. åŠ è½½æ¨¡å‹
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

print(f"åŸå§‹æ¨¡å‹å¤§å°: {get_model_size(model):.2f} MB")

# 2. å‡†å¤‡æ ¡å‡†æ•°æ®
calibration_data = [
    "The quick brown fox jumps over the lazy dog.",
    "To be or not to be, that is the question.",
    # ... æ›´å¤šæ•°æ®
]

# 3. GPTQé‡åŒ–é…ç½®
quantize_config = BaseQuantizeConfig(
    bits=4,  # 4-bit
    group_size=128,  # åˆ†ç»„å¤§å°
    desc_act=False,  # ä¸ä½¿ç”¨é™åºæ¿€æ´»
)

# 4. é‡åŒ–
model_gptq = AutoGPTQForCausalLM.from_pretrained(
    model_name,
    quantize_config=quantize_config
)

model_gptq.quantize(calibration_data, use_triton=False)

# 5. ä¿å­˜
output_dir = "gpt2-gptq-4bit"
model_gptq.save_quantized(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"é‡åŒ–æ¨¡å‹å¤§å°: {get_model_size(model_gptq):.2f} MB")

# 6. æµ‹è¯•
def test_generation(model, prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=50)
    return tokenizer.decode(outputs[0])

print("åŸå§‹æ¨¡å‹:", test_generation(model, "Once upon a time"))
print("é‡åŒ–æ¨¡å‹:", test_generation(model_gptq, "Once upon a time"))
```

### ğŸ“Š é‡åŒ–æ•ˆæœè¯„ä¼°

```python
def evaluate_quantization(original_model, quantized_model, test_data):
    """
    è¯„ä¼°é‡åŒ–æ•ˆæœ
    """
    results = {
        'size': {},
        'speed': {},
        'quality': {}
    }
    
    # 1. æ¨¡å‹å¤§å°
    results['size']['original'] = get_model_size(original_model)
    results['size']['quantized'] = get_model_size(quantized_model)
    results['size']['compression'] = results['size']['original'] / results['size']['quantized']
    
    # 2. æ¨ç†é€Ÿåº¦
    import time
    
    start = time.time()
    for batch in test_data:
        original_model(batch)
    results['speed']['original'] = time.time() - start
    
    start = time.time()
    for batch in test_data:
        quantized_model(batch)
    results['speed']['quantized'] = time.time() - start
    results['speed']['speedup'] = results['speed']['original'] / results['speed']['quantized']
    
    # 3. è´¨é‡ï¼ˆå›°æƒ‘åº¦ï¼‰
    results['quality']['original'] = compute_perplexity(original_model, test_data)
    results['quality']['quantized'] = compute_perplexity(quantized_model, test_data)
    results['quality']['degradation'] = results['quality']['quantized'] - results['quality']['original']
    
    return results

# è¿è¡Œè¯„ä¼°
results = evaluate_quantization(model, model_gptq, test_dataloader)

print(f"""
é‡åŒ–æ•ˆæœæŠ¥å‘Šï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¦ æ¨¡å‹å¤§å°
  åŸå§‹: {results['size']['original']:.2f} MB
  é‡åŒ–: {results['size']['quantized']:.2f} MB
  å‹ç¼©æ¯”: {results['size']['compression']:.2f}x
  
âš¡ æ¨ç†é€Ÿåº¦
  åŸå§‹: {results['speed']['original']:.2f}s
  é‡åŒ–: {results['speed']['quantized']:.2f}s
  åŠ é€Ÿæ¯”: {results['speed']['speedup']:.2f}x
  
ğŸ“Š æ¨¡å‹è´¨é‡
  åŸå§‹å›°æƒ‘åº¦: {results['quality']['original']:.2f}
  é‡åŒ–å›°æƒ‘åº¦: {results['quality']['quantized']:.2f}
  è´¨é‡ä¸‹é™: {results['quality']['degradation']:.2f}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")
```

---

# Part 2: éƒ¨ç½²ä¼˜åŒ–

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**ä»ç ”ç©¶åˆ°ç”Ÿäº§çš„é¸¿æ²Ÿï¼š**
- è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æœ¬åœ°è·‘å¾—å¾ˆå¥½
- ä½†å¦‚ä½•è®©ç”¨æˆ·è®¿é—®ï¼Ÿ
- å¦‚ä½•å¤„ç†å¹¶å‘è¯·æ±‚ï¼Ÿ
- å¦‚ä½•ä¿è¯ä½å»¶è¿Ÿå’Œé«˜ååï¼Ÿ

**éƒ¨ç½²ç›®æ ‡ï¼š**
```python
æ€§èƒ½æŒ‡æ ‡:
â”œâ”€â”€ å»¶è¿Ÿ (Latency)
â”‚   â””â”€â”€ é¦–tokenå»¶è¿Ÿ < 100ms
â”‚   â””â”€â”€ å¹³å‡å»¶è¿Ÿ < 500ms
â”‚
â”œâ”€â”€ ååé‡ (Throughput)
â”‚   â””â”€â”€ > 1000 tokens/s
â”‚   â””â”€â”€ æ”¯æŒ100+å¹¶å‘ç”¨æˆ·
â”‚
â”œâ”€â”€ å¯ç”¨æ€§ (Availability)
â”‚   â””â”€â”€ 99.9% uptime
â”‚   â””â”€â”€ è‡ªåŠ¨æ•…éšœæ¢å¤
â”‚
â””â”€â”€ æˆæœ¬ (Cost)
    â””â”€â”€ < $0.001 per 1K tokens
    â””â”€â”€ GPUåˆ©ç”¨ç‡ > 80%
```

---

## ğŸ“š 2.1 æ¨ç†ä¼˜åŒ–æŠ€æœ¯

### âš¡ KV Cacheï¼ˆå…³é”®ä¼˜åŒ–ï¼‰

**é—®é¢˜ï¼š** è‡ªå›å½’ç”Ÿæˆæ—¶é‡å¤è®¡ç®—

```python
# æ²¡æœ‰KV Cacheï¼ˆä½æ•ˆï¼‰
def generate_without_cache(model, prompt):
    tokens = [prompt]
    for i in range(max_length):
        # æ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰tokençš„attention
        output = model(tokens)  # è®¡ç®—é‡éšé•¿åº¦çº¿æ€§å¢é•¿
        next_token = sample(output[-1])
        tokens.append(next_token)
    return tokens

# æ—¶é—´å¤æ‚åº¦: O(nÂ²)
# ç”Ÿæˆ100ä¸ªtokenéœ€è¦è®¡ç®—: 1+2+3+...+100 = 5050æ¬¡attention

# ä½¿ç”¨KV Cacheï¼ˆé«˜æ•ˆï¼‰
def generate_with_cache(model, prompt):
    tokens = [prompt]
    past_key_values = None  # ç¼“å­˜
    
    for i in range(max_length):
        # åªè®¡ç®—æ–°tokençš„attention
        output, past_key_values = model(
            tokens[-1:],  # åªä¼ å…¥æœ€åä¸€ä¸ªtoken
            past_key_values=past_key_values  # ä½¿ç”¨ç¼“å­˜
        )
        next_token = sample(output[-1])
        tokens.append(next_token)
    return tokens

# æ—¶é—´å¤æ‚åº¦: O(n)
# ç”Ÿæˆ100ä¸ªtokenåªéœ€è¦: 100æ¬¡attention
# åŠ é€Ÿ: 50å€ï¼
```

**å®ç°KV Cacheï¼š**

```python
class GPTWithCache(nn.Module):
    def forward(self, x, past_key_values=None):
        B, T = x.shape
        
        # Embedding
        tok_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(torch.arange(T))
        x = tok_emb + pos_emb
        
        # Transformer blocks
        present_key_values = []
        for i, block in enumerate(self.blocks):
            # ä½¿ç”¨ç¼“å­˜çš„KV
            past_kv = past_key_values[i] if past_key_values else None
            x, present_kv = block(x, past_kv)
            present_key_values.append(present_kv)
        
        # Output
        logits = self.lm_head(x)
        
        return logits, present_key_values

# ä½¿ç”¨
model = GPTWithCache()
past_kv = None

for _ in range(100):
    logits, past_kv = model(next_token, past_key_values=past_kv)
    next_token = sample(logits)
```

### ğŸš€ Continuous Batching

**é—®é¢˜ï¼š** ä¼ ç»Ÿbatchingæ•ˆç‡ä½

```python
# ä¼ ç»ŸBatchingï¼ˆä½æ•ˆï¼‰
batch = [req1, req2, req3, req4]  # 4ä¸ªè¯·æ±‚
# å¿…é¡»ç­‰æœ€é•¿çš„è¯·æ±‚å®Œæˆ
# req1: 10 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
# req2: 50 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
# req3: 20 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
# req4: 15 tokens  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
#                  â†‘ æµªè´¹çš„è®¡ç®—ï¼ˆreq1å·²å®Œæˆä½†è¦ç­‰å¾…ï¼‰

# Continuous Batchingï¼ˆé«˜æ•ˆï¼‰
# åŠ¨æ€æ·»åŠ /ç§»é™¤è¯·æ±‚
batch = [req1, req2, req3, req4]
# req1å®Œæˆ â†’ ç«‹å³æ·»åŠ req5
batch = [req2, req3, req4, req5]
# req4å®Œæˆ â†’ ç«‹å³æ·»åŠ req6
batch = [req2, req3, req5, req6]
# å§‹ç»ˆä¿æŒbatchæ»¡è½½ï¼ŒGPUåˆ©ç”¨ç‡æœ€å¤§åŒ–
```

### ğŸ“Š PagedAttentionï¼ˆvLLMæ ¸å¿ƒæŠ€æœ¯ï¼‰

```python
# é—®é¢˜ï¼šKV Cacheæ˜¾å­˜ç¢ç‰‡åŒ–
# ä¼ ç»Ÿæ–¹å¼ï¼šä¸ºæ¯ä¸ªè¯·æ±‚é¢„åˆ†é…è¿ç»­æ˜¾å­˜
req1_kv: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 32 tokens (é¢„åˆ†é…100)
req2_kv: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 32 tokens (é¢„åˆ†é…100)
# æµªè´¹: 68% æ˜¾å­˜

# PagedAttentionï¼šåˆ†é¡µç®¡ç†ï¼ˆç±»ä¼¼æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜ï¼‰
req1_kv: [page1][page2][page3]...  # æŒ‰éœ€åˆ†é…
req2_kv: [page5][page6]...         # ä¸è¿ç»­ä¹ŸOK
# åˆ©ç”¨ç‡: 95%+
```

### ğŸš€ æŠ•æœºé‡‡æ ·ï¼ˆSpeculative Decodingï¼‰

**æ ¸å¿ƒé—®é¢˜ï¼š** å¤§æ¨¡å‹ç”Ÿæˆå¤ªæ…¢ï¼Œèƒ½ä¸èƒ½åŠ é€Ÿï¼Ÿ

#### ğŸ’¡ åŸºæœ¬æ€æƒ³ï¼ˆç”¨ç”Ÿæ´»ä¾‹å­ç†è§£ï¼‰

æƒ³è±¡ä½ åœ¨å†™ä½œæ–‡ï¼š

```python
ä¼ ç»Ÿæ–¹å¼ï¼ˆæ…¢ï¼‰ï¼š
  è€å¸ˆï¼ˆå¤§æ¨¡å‹ï¼‰ä¸€ä¸ªå­—ä¸€ä¸ªå­—åœ°å†™
  "ä»Š" â†’ åœä¸‹æ¥æ€è€ƒ â†’ "å¤©" â†’ åœä¸‹æ¥æ€è€ƒ â†’ "å¤©" â†’ ...
  æ¯ä¸ªå­—éƒ½è¦æ·±æ€ç†Ÿè™‘ï¼Œå¾ˆæ…¢ï¼

æŠ•æœºé‡‡æ ·ï¼ˆå¿«ï¼‰ï¼š
  å­¦ç”Ÿï¼ˆå°æ¨¡å‹ï¼‰å…ˆå¿«é€Ÿå†™ä¸€æ®µè‰ç¨¿ï¼š
  "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­ç©ã€‚"
  
  è€å¸ˆï¼ˆå¤§æ¨¡å‹ï¼‰ä¸€æ¬¡æ€§æ£€æŸ¥æ•´æ®µï¼š
  âœ… "ä»Šå¤©å¤©æ°”å¾ˆå¥½" - æ­£ç¡®
  âœ… "æˆ‘ä»¬å»" - æ­£ç¡®
  âŒ "å…¬å›­" - ä¸å¯¹ï¼Œåº”è¯¥æ˜¯"åŠ¨ç‰©å›­"
  
  ç»“æœï¼šä¸€æ¬¡ç”Ÿæˆäº†6ä¸ªæ­£ç¡®çš„å­—ï¼Œè€Œä¸æ˜¯1ä¸ªï¼
  åŠ é€Ÿï¼š6å€ï¼
```

#### ğŸ“Š å·¥ä½œåŸç†

```python
# ä¼ ç»Ÿè‡ªå›å½’ç”Ÿæˆï¼ˆæ…¢ï¼‰
def traditional_generate(big_model, prompt):
    tokens = [prompt]
    for i in range(100):  # ç”Ÿæˆ100ä¸ªtoken
        # æ¯æ¬¡åªç”Ÿæˆ1ä¸ªtoken
        next_token = big_model(tokens)  # æ…¢ï¼
        tokens.append(next_token)
    return tokens

# æ—¶é—´ï¼š100æ¬¡å¤§æ¨¡å‹è°ƒç”¨

# æŠ•æœºé‡‡æ ·ï¼ˆå¿«ï¼‰
def speculative_generate(big_model, small_model, prompt):
    tokens = [prompt]
    
    while len(tokens) < 100:
        # æ­¥éª¤1ï¼šå°æ¨¡å‹å¿«é€Ÿç”ŸæˆKä¸ªå€™é€‰tokenï¼ˆæ¯”å¦‚5ä¸ªï¼‰
        candidates = []
        temp_tokens = tokens.copy()
        for _ in range(5):  # çŒœæµ‹5ä¸ªtoken
            next_token = small_model(temp_tokens)  # å¿«ï¼
            candidates.append(next_token)
            temp_tokens.append(next_token)
        
        # æ­¥éª¤2ï¼šå¤§æ¨¡å‹ä¸€æ¬¡æ€§éªŒè¯æ‰€æœ‰å€™é€‰
        # å…³é”®ï¼šå¹¶è¡ŒéªŒè¯ï¼Œä¸æ˜¯é€ä¸ªéªŒè¯ï¼
        verified = big_model.verify(tokens, candidates)
        
        # æ­¥éª¤3ï¼šæ¥å—æ­£ç¡®çš„ï¼Œæ‹’ç»é”™è¯¯çš„
        for i, (candidate, is_correct) in enumerate(zip(candidates, verified)):
            if is_correct:
                tokens.append(candidate)  # æ¥å—
            else:
                # ç¬¬ä¸€ä¸ªé”™è¯¯çš„åœ°æ–¹ï¼Œç”¨å¤§æ¨¡å‹é‡æ–°ç”Ÿæˆ
                correct_token = big_model(tokens)
                tokens.append(correct_token)
                break  # åœæ­¢æ¥å—åç»­å€™é€‰
    
    return tokens

# æ—¶é—´ï¼šå¦‚æœå¹³å‡æ¥å—3ä¸ªå€™é€‰ï¼Œåªéœ€è¦ 100/3 â‰ˆ 33æ¬¡å¤§æ¨¡å‹è°ƒç”¨
# åŠ é€Ÿï¼š3å€ï¼
```

#### ğŸ¯ ä¸ºä»€ä¹ˆèƒ½åŠ é€Ÿï¼Ÿ

```python
å…³é”®æ´å¯Ÿï¼š

1. å°æ¨¡å‹å¾ˆå¿«
   GPT-2 (124M): 1000 tokens/s  âš¡
   GPT-2-XL (1.5B): 100 tokens/s  ğŸŒ
   
   å°æ¨¡å‹ç”Ÿæˆ5ä¸ªtokençš„æ—¶é—´ < å¤§æ¨¡å‹ç”Ÿæˆ1ä¸ªtoken

2. å¹¶è¡ŒéªŒè¯
   ä¼ ç»Ÿæ–¹å¼ï¼š
   token1 â†’ éªŒè¯ â†’ token2 â†’ éªŒè¯ â†’ token3 â†’ éªŒè¯
   
   æŠ•æœºé‡‡æ ·ï¼š
   [token1, token2, token3] â†’ ä¸€æ¬¡æ€§éªŒè¯
   
   Transformerå¯ä»¥å¹¶è¡Œå¤„ç†åºåˆ—ï¼

3. å¤§éƒ¨åˆ†æ—¶å€™å°æ¨¡å‹æ˜¯å¯¹çš„
   ç®€å•å†…å®¹ï¼šå°æ¨¡å‹å‡†ç¡®ç‡ 80-90%
   â†’ å¹³å‡æ¥å— 4-5ä¸ªå€™é€‰
   â†’ åŠ é€Ÿ 4-5å€ï¼
```

#### ğŸ”§ å®Œæ•´å®ç°

```python
import torch
import torch.nn.functional as F

class SpeculativeDecoder:
    def __init__(self, draft_model, target_model, k=5):
        """
        draft_model: å°æ¨¡å‹ï¼ˆå¿«é€Ÿè‰ç¨¿ï¼‰
        target_model: å¤§æ¨¡å‹ï¼ˆæœ€ç»ˆéªŒè¯ï¼‰
        k: æ¯æ¬¡çŒœæµ‹çš„tokenæ•°é‡
        """
        self.draft_model = draft_model
        self.target_model = target_model
        self.k = k
    
    def generate(self, prompt_ids, max_length=100):
        """
        æŠ•æœºé‡‡æ ·ç”Ÿæˆ
        """
        tokens = prompt_ids.clone()
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'draft_calls': 0,
            'target_calls': 0,
            'accepted_tokens': 0,
            'rejected_tokens': 0
        }
        
        while len(tokens) < max_length:
            # æ­¥éª¤1ï¼šå°æ¨¡å‹ç”ŸæˆKä¸ªå€™é€‰token
            draft_tokens = []
            draft_probs = []
            
            temp_tokens = tokens.clone()
            for _ in range(self.k):
                # å°æ¨¡å‹å‰å‘ä¼ æ’­
                with torch.no_grad():
                    logits = self.draft_model(temp_tokens)
                    probs = F.softmax(logits[-1], dim=-1)
                    next_token = torch.argmax(probs)
                
                draft_tokens.append(next_token)
                draft_probs.append(probs)
                temp_tokens = torch.cat([temp_tokens, next_token.unsqueeze(0)])
                
                stats['draft_calls'] += 1
            
            # æ­¥éª¤2ï¼šå¤§æ¨¡å‹éªŒè¯
            # å…³é”®ï¼šä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å€™é€‰çš„æ¦‚ç‡
            verify_tokens = torch.cat([tokens] + [t.unsqueeze(0) for t in draft_tokens])
            
            with torch.no_grad():
                target_logits = self.target_model(verify_tokens)
                target_probs = F.softmax(target_logits, dim=-1)
            
            stats['target_calls'] += 1
            
            # æ­¥éª¤3ï¼šé€ä¸ªéªŒè¯å€™é€‰token
            accepted_count = 0
            for i in range(self.k):
                # å¤§æ¨¡å‹åœ¨ä½ç½®içš„æ¦‚ç‡åˆ†å¸ƒ
                p_target = target_probs[len(tokens) + i - 1]
                # å°æ¨¡å‹çš„é¢„æµ‹
                draft_token = draft_tokens[i]
                p_draft = draft_probs[i]
                
                # æ¥å—æ¦‚ç‡ï¼šmin(1, p_target / p_draft)
                accept_prob = min(1.0, 
                    p_target[draft_token] / (p_draft[draft_token] + 1e-10)
                )
                
                # éšæœºå†³å®šæ˜¯å¦æ¥å—
                if torch.rand(1).item() < accept_prob:
                    # æ¥å—å€™é€‰token
                    tokens = torch.cat([tokens, draft_token.unsqueeze(0)])
                    accepted_count += 1
                    stats['accepted_tokens'] += 1
                else:
                    # æ‹’ç»ï¼šä»å¤§æ¨¡å‹çš„åˆ†å¸ƒä¸­é‡æ–°é‡‡æ ·
                    # ä½¿ç”¨ä¿®æ­£çš„æ¦‚ç‡åˆ†å¸ƒ
                    adjusted_probs = torch.clamp(p_target - p_draft, min=0)
                    adjusted_probs = adjusted_probs / adjusted_probs.sum()
                    
                    new_token = torch.multinomial(adjusted_probs, 1)
                    tokens = torch.cat([tokens, new_token])
                    stats['rejected_tokens'] += 1
                    break  # æ‹’ç»ååœæ­¢
            
            # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½è¢«æ¥å—ï¼Œä»å¤§æ¨¡å‹é‡‡æ ·ä¸€ä¸ªæ–°token
            if accepted_count == self.k:
                p_target = target_probs[-1]
                new_token = torch.multinomial(p_target, 1)
                tokens = torch.cat([tokens, new_token])
        
        return tokens, stats

# ä½¿ç”¨ç¤ºä¾‹
def demo_speculative_decoding():
    # åŠ è½½æ¨¡å‹
    draft_model = GPT.from_pretrained('gpt2')  # 124Mï¼Œå¿«
    target_model = GPT.from_pretrained('gpt2-xl')  # 1.5Bï¼Œæ…¢
    
    decoder = SpeculativeDecoder(draft_model, target_model, k=5)
    
    # ç”Ÿæˆ
    prompt = "Once upon a time"
    prompt_ids = tokenizer.encode(prompt)
    
    import time
    
    # ä¼ ç»Ÿæ–¹å¼
    start = time.time()
    output_traditional = target_model.generate(prompt_ids, max_length=100)
    time_traditional = time.time() - start
    
    # æŠ•æœºé‡‡æ ·
    start = time.time()
    output_speculative, stats = decoder.generate(prompt_ids, max_length=100)
    time_speculative = time.time() - start
    
    # ç»“æœå¯¹æ¯”
    print(f"""
    æŠ•æœºé‡‡æ ·æ•ˆæœæŠ¥å‘Šï¼š
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â±ï¸  æ—¶é—´å¯¹æ¯”
      ä¼ ç»Ÿæ–¹å¼: {time_traditional:.2f}s
      æŠ•æœºé‡‡æ ·: {time_speculative:.2f}s
      åŠ é€Ÿæ¯”: {time_traditional/time_speculative:.2f}x
    
    ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
      å°æ¨¡å‹è°ƒç”¨: {stats['draft_calls']}æ¬¡
      å¤§æ¨¡å‹è°ƒç”¨: {stats['target_calls']}æ¬¡
      æ¥å—çš„token: {stats['accepted_tokens']}ä¸ª
      æ‹’ç»çš„token: {stats['rejected_tokens']}ä¸ª
      å¹³å‡æ¥å—ç‡: {stats['accepted_tokens']/(stats['accepted_tokens']+stats['rejected_tokens'])*100:.1f}%
    
    ğŸ’¡ æ•ˆç‡æå‡
      ä¼ ç»Ÿæ–¹å¼éœ€è¦: 100æ¬¡å¤§æ¨¡å‹è°ƒç”¨
      æŠ•æœºé‡‡æ ·éœ€è¦: {stats['target_calls']}æ¬¡å¤§æ¨¡å‹è°ƒç”¨
      èŠ‚çœ: {100-stats['target_calls']}æ¬¡è°ƒç”¨
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """)
```

#### ğŸ¯ ä¼˜åŒ–æŠ€å·§

```python
1. é€‰æ‹©åˆé€‚çš„å°æ¨¡å‹
   â”œâ”€â”€ å¤ªå°ï¼šå‡†ç¡®ç‡ä½ï¼Œæ¥å—ç‡ä½ï¼ŒåŠ é€Ÿæ•ˆæœå·®
   â”œâ”€â”€ å¤ªå¤§ï¼šé€Ÿåº¦æ…¢ï¼Œå¤±å»ä¼˜åŠ¿
   â””â”€â”€ æ¨èï¼šå¤§æ¨¡å‹çš„1/10å¤§å°
   
   ä¾‹å­ï¼š
   - å¤§æ¨¡å‹ï¼šGPT-2-XL (1.5B)
   - å°æ¨¡å‹ï¼šGPT-2 (124M) âœ…
   - æ¯”ä¾‹ï¼š1:12

2. è°ƒæ•´å€™é€‰æ•°é‡K
   â”œâ”€â”€ Kå¤ªå°ï¼šæ¯æ¬¡æ¥å—å°‘ï¼Œè°ƒç”¨æ¬¡æ•°å¤š
   â”œâ”€â”€ Kå¤ªå¤§ï¼šéªŒè¯å¼€é”€å¤§ï¼Œæ¥å—ç‡ä½
   â””â”€â”€ æ¨èï¼šK=4-6
   
   å®éªŒç»“æœï¼š
   K=2: 2.0xåŠ é€Ÿ
   K=4: 2.8xåŠ é€Ÿ âœ…
   K=6: 2.5xåŠ é€Ÿï¼ˆå¼€å§‹ä¸‹é™ï¼‰
   K=8: 2.2xåŠ é€Ÿ

3. ä½¿ç”¨ç›¸åŒçš„tokenizer
   â”œâ”€â”€ å°æ¨¡å‹å’Œå¤§æ¨¡å‹å¿…é¡»ç”¨ç›¸åŒçš„è¯è¡¨
   â””â”€â”€ å¦åˆ™æ— æ³•å¯¹é½éªŒè¯

4. é€‚ç”¨åœºæ™¯
   âœ… é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆæ¥å—ç‡é«˜ï¼‰
   âœ… ä»£ç ç”Ÿæˆï¼ˆæ¨¡å¼æ˜æ˜¾ï¼‰
   âœ… ç¿»è¯‘ä»»åŠ¡ï¼ˆç¡®å®šæ€§å¼ºï¼‰
   âŒ åˆ›æ„å†™ä½œï¼ˆä¸å¯é¢„æµ‹ï¼‰
   âŒ éšæœºæ€§é«˜çš„ä»»åŠ¡
```

#### ğŸ“Š æ€§èƒ½å¯¹æ¯”

```python
å®æµ‹æ•°æ®ï¼ˆGPT-2 â†’ GPT-2-XLï¼‰ï¼š

ä»»åŠ¡ç±»å‹          æ¥å—ç‡    åŠ é€Ÿæ¯”    è´¨é‡æŸå¤±
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä»£ç è¡¥å…¨          85%      3.5x      0%
æ–‡æ¡£æ‘˜è¦          75%      3.0x      0%
å¯¹è¯ç”Ÿæˆ          65%      2.5x      0%
åˆ›æ„å†™ä½œ          45%      1.8x      0%
éšæœºæ–‡æœ¬          30%      1.3x      0%

å…³é”®å‘ç°ï¼š
1. è´¨é‡æ— æŸï¼šè¾“å‡ºåˆ†å¸ƒä¸å¤§æ¨¡å‹å®Œå…¨ä¸€è‡´
2. ç¡®å®šæ€§ä»»åŠ¡æ•ˆæœæœ€å¥½
3. å¹³å‡åŠ é€Ÿï¼š2-3å€
```

#### ğŸ’¡ è¿›é˜¶ï¼šè‡ªé€‚åº”æŠ•æœºé‡‡æ ·

```python
class AdaptiveSpeculativeDecoder:
    """
    æ ¹æ®æ¥å—ç‡åŠ¨æ€è°ƒæ•´K
    """
    def __init__(self, draft_model, target_model):
        self.draft_model = draft_model
        self.target_model = target_model
        self.k = 4  # åˆå§‹K
        self.accept_history = []
    
    def adjust_k(self):
        """åŠ¨æ€è°ƒæ•´K"""
        if len(self.accept_history) < 10:
            return
        
        recent_accept_rate = sum(self.accept_history[-10:]) / 10
        
        if recent_accept_rate > 0.8:
            self.k = min(8, self.k + 1)  # æ¥å—ç‡é«˜ï¼Œå¢åŠ K
        elif recent_accept_rate < 0.4:
            self.k = max(2, self.k - 1)  # æ¥å—ç‡ä½ï¼Œå‡å°‘K
    
    def generate(self, prompt_ids, max_length=100):
        tokens = prompt_ids.clone()
        
        while len(tokens) < max_length:
            # ä½¿ç”¨å½“å‰çš„Kç”Ÿæˆ
            accepted = self.generate_step(tokens)
            
            # è®°å½•æ¥å—ç‡
            self.accept_history.append(accepted / self.k)
            
            # æ¯10æ­¥è°ƒæ•´ä¸€æ¬¡K
            if len(tokens) % 10 == 0:
                self.adjust_k()
        
        return tokens
```

#### ğŸ“ æ€»ç»“

```python
æŠ•æœºé‡‡æ ·çš„æœ¬è´¨ï¼š
  ç”¨"çŒœæµ‹+éªŒè¯"ä»£æ›¿"é€ä¸ªç”Ÿæˆ"
  
ä¼˜åŠ¿ï¼š
  âœ… åŠ é€Ÿ2-4å€
  âœ… è¾“å‡ºè´¨é‡æ— æŸ
  âœ… å®ç°ç›¸å¯¹ç®€å•
  âœ… å¯ä¸å…¶ä»–ä¼˜åŒ–å åŠ 
  
åŠ£åŠ¿ï¼š
  âŒ éœ€è¦é¢å¤–çš„å°æ¨¡å‹
  âŒ æ˜¾å­˜å ç”¨å¢åŠ 
  âŒ ä¸ç¡®å®šæ€§é«˜çš„ä»»åŠ¡æ•ˆæœå·®
  
æœ€ä½³å®è·µï¼š
  1. å°æ¨¡å‹ = å¤§æ¨¡å‹çš„1/10å¤§å°
  2. K = 4-6
  3. ç”¨äºç¡®å®šæ€§ä»»åŠ¡
  4. ä¸KV Cacheã€é‡åŒ–ç­‰æŠ€æœ¯ç»“åˆ
  
å®é™…åº”ç”¨ï¼š
  - Googleçš„Geminiä½¿ç”¨æŠ•æœºé‡‡æ ·
  - Appleçš„MLXæ¡†æ¶æ”¯æŒæŠ•æœºé‡‡æ ·
  - vLLMæ­£åœ¨é›†æˆæŠ•æœºé‡‡æ ·
```

---

## ğŸ“š 2.2 éƒ¨ç½²æ¡†æ¶é€‰æ‹©

### ğŸ”§ æ–¹æ¡ˆå¯¹æ¯”

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¡†æ¶        â”‚ æ˜“ç”¨æ€§   â”‚ æ€§èƒ½     â”‚ åŠŸèƒ½     â”‚ æ¨èåœºæ™¯ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FastAPI     â”‚ â­â­â­â­â­â”‚ â­â­     â”‚ â­â­     â”‚ åŸå‹/å°è§„æ¨¡â”‚
â”‚ vLLM        â”‚ â­â­â­â­ â”‚ â­â­â­â­â­â”‚ â­â­â­â­ â”‚ ç”Ÿäº§æ¨è  â”‚
â”‚ TensorRT-LLMâ”‚ â­â­     â”‚ â­â­â­â­â­â”‚ â­â­â­   â”‚ æè‡´æ€§èƒ½  â”‚
â”‚ Text Gen UI â”‚ â­â­â­â­â­â”‚ â­â­â­   â”‚ â­â­â­â­â­â”‚ å¼€ç®±å³ç”¨  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš¡ vLLMéƒ¨ç½²ï¼ˆæ¨èï¼‰

```python
# å®‰è£…
pip install vllm

# å¯åŠ¨æœåŠ¡
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="gpt2",
    tensor_parallel_size=1,  # å•GPU
    dtype="float16",
    max_model_len=2048,
)

# æ¨ç†
prompts = [
    "Once upon a time",
    "The future of AI is",
]

sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100
)

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)

# æ€§èƒ½å¯¹æ¯”
"""
HuggingFace:  100 tokens/s
vLLM:         2000+ tokens/s
åŠ é€Ÿ: 20xï¼
"""
```

### ğŸŒ APIæœåŠ¡åŒ–

```python
# ä½¿ç”¨FastAPI + vLLM
from fastapi import FastAPI
from pydantic import BaseModel
from vllm import LLM, SamplingParams

app = FastAPI()
llm = LLM(model="gpt2")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.8

@app.post("/generate")
async def generate(request: GenerateRequest):
    sampling_params = SamplingParams(
        temperature=request.temperature,
        max_tokens=request.max_tokens
    )
    
    outputs = llm.generate([request.prompt], sampling_params)
    
    return {
        "text": outputs[0].outputs[0].text,
        "tokens": len(outputs[0].outputs[0].token_ids)
    }

# å¯åŠ¨
# uvicorn app:app --host 0.0.0.0 --port 8000

# æµ‹è¯•
"""
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Once upon a time", "max_tokens": 50}'
"""
```

---

## ğŸ“š 2.3 ç”Ÿäº§çº§éƒ¨ç½²

### ğŸ³ Dockerå®¹å™¨åŒ–

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# å®‰è£…Python
RUN apt-get update && apt-get install -y python3.10 python3-pip

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . /app
WORKDIR /app

# ä¸‹è½½æ¨¡å‹
RUN python download_model.py

# å¯åŠ¨æœåŠ¡
CMD ["python", "-m", "vllm.entrypoints.api_server", \
     "--model", "gpt2", \
     "--host", "0.0.0.0", \
     "--port", "8000"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t gpt2-service:v1 .

# è¿è¡Œå®¹å™¨
docker run -d \
  --gpus all \
  -p 8000:8000 \
  --name gpt2-api \
  gpt2-service:v1

# æµ‹è¯•
curl http://localhost:8000/health
```

### â˜¸ï¸ Kuberneteséƒ¨ç½²

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt2-deployment
spec:
  replicas: 3  # 3ä¸ªå‰¯æœ¬
  selector:
    matchLabels:
      app: gpt2
  template:
    metadata:
      labels:
        app: gpt2
    spec:
      containers:
      - name: gpt2
        image: gpt2-service:v1
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1  # æ¯ä¸ªpodä¸€ä¸ªGPU
          requests:
            memory: "8Gi"
            cpu: "4"
---
apiVersion: v1
kind: Service
metadata:
  name: gpt2-service
spec:
  selector:
    app: gpt2
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

```bash
# éƒ¨ç½²
kubectl apply -f deployment.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods
kubectl get services

# æ‰©å®¹
kubectl scale deployment gpt2-deployment --replicas=10
```

---

## ğŸ“š 2.4 ç›‘æ§ä¸è¿ç»´

### ğŸ“Š æ€§èƒ½ç›‘æ§

```python
# ä½¿ç”¨Prometheus + Grafana
from prometheus_client import Counter, Histogram, Gauge
import time

# å®šä¹‰æŒ‡æ ‡
request_count = Counter('requests_total', 'Total requests')
request_duration = Histogram('request_duration_seconds', 'Request duration')
gpu_utilization = Gauge('gpu_utilization', 'GPU utilization')
active_requests = Gauge('active_requests', 'Active requests')

@app.post("/generate")
async def generate(request: GenerateRequest):
    request_count.inc()
    active_requests.inc()
    
    start = time.time()
    try:
        # ç”Ÿæˆ
        output = llm.generate(...)
        
        duration = time.time() - start
        request_duration.observe(duration)
        
        return output
    finally:
        active_requests.dec()

# Grafanaä»ªè¡¨æ¿
"""
é¢æ¿1: è¯·æ±‚QPSï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰
é¢æ¿2: P50/P95/P99å»¶è¿Ÿ
é¢æ¿3: GPUåˆ©ç”¨ç‡
é¢æ¿4: æ´»è·ƒè¯·æ±‚æ•°
é¢æ¿5: é”™è¯¯ç‡
"""
```

### ğŸš¨ å‘Šè­¦é…ç½®

```yaml
# alerting_rules.yaml
groups:
- name: gpt2_alerts
  rules:
  - alert: HighLatency
    expr: histogram_quantile(0.95, request_duration_seconds) > 1.0
    for: 5m
    annotations:
      summary: "P95 latency > 1s"
      
  - alert: HighErrorRate
    expr: rate(errors_total[5m]) > 0.05
    for: 5m
    annotations:
      summary: "Error rate > 5%"
      
  - alert: LowGPUUtilization
    expr: gpu_utilization < 0.5
    for: 10m
    annotations:
      summary: "GPU utilization < 50%"
```

---

## ğŸ“š 2.5 æˆæœ¬ä¼˜åŒ–

### ğŸ’° æˆæœ¬åˆ†æ

```python
# æˆæœ¬è®¡ç®—
def calculate_cost(
    gpu_type="A100",
    num_gpus=4,
    hours_per_month=730,
    requests_per_second=100
):
    # GPUæˆæœ¬
    gpu_costs = {
        "A100": 3.0,  # $/hour
        "A10G": 1.0,
        "T4": 0.35,
    }
    
    gpu_cost = gpu_costs[gpu_type] * num_gpus * hours_per_month
    
    # è¯·æ±‚é‡
    total_requests = requests_per_second * 3600 * hours_per_month
    
    # æ¯åƒæ¬¡è¯·æ±‚æˆæœ¬
    cost_per_1k = (gpu_cost / total_requests) * 1000
    
    print(f"""
æˆæœ¬åˆ†ææŠ¥å‘Šï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GPUé…ç½®: {num_gpus}x {gpu_type}
æœˆåº¦æˆæœ¬: ${gpu_cost:,.2f}
æœˆåº¦è¯·æ±‚: {total_requests:,.0f}
æ¯1Kè¯·æ±‚æˆæœ¬: ${cost_per_1k:.4f}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """)

# ç¤ºä¾‹
calculate_cost(gpu_type="A10G", num_gpus=2, requests_per_second=50)
```

### ğŸ¯ ä¼˜åŒ–ç­–ç•¥

```python
ä¼˜åŒ–ç­–ç•¥ï¼š

1. æ¨¡å‹ä¼˜åŒ–
â”œâ”€â”€ é‡åŒ–åˆ°INT4 â†’ æ˜¾å­˜å‡å°‘8x â†’ GPUæ•°é‡å‡åŠ
â”œâ”€â”€ ä½¿ç”¨å°æ¨¡å‹ â†’ GPT-2 (124M) vs GPT-2-XL (1.5B)
â””â”€â”€ æ¨¡å‹è’¸é¦ â†’ ä¿æŒè´¨é‡ï¼Œå‡å°‘å‚æ•°

2. æ¨ç†ä¼˜åŒ–
â”œâ”€â”€ ä½¿ç”¨vLLM â†’ ååé‡æå‡20x â†’ GPUæ•°é‡å‡å°‘
â”œâ”€â”€ Continuous Batching â†’ GPUåˆ©ç”¨ç‡ä»50% â†’ 90%
â””â”€â”€ KV Cache â†’ å»¶è¿Ÿé™ä½50x

3. åŸºç¡€è®¾æ–½ä¼˜åŒ–
â”œâ”€â”€ Spotå®ä¾‹ â†’ æˆæœ¬é™ä½70%
â”œâ”€â”€ è‡ªåŠ¨æ‰©ç¼©å®¹ â†’ æ ¹æ®è´Ÿè½½è°ƒæ•´GPUæ•°é‡
â””â”€â”€ å¤šåŒºåŸŸéƒ¨ç½² â†’ é™ä½ç½‘ç»œå»¶è¿Ÿ

4. ä¸šåŠ¡ä¼˜åŒ–
â”œâ”€â”€ ç¼“å­˜å¸¸è§è¯·æ±‚ â†’ å‡å°‘é‡å¤è®¡ç®—
â”œâ”€â”€ é™æµ â†’ é˜²æ­¢èµ„æºæµªè´¹
â””â”€â”€ å¼‚æ­¥å¤„ç† â†’ æé«˜ååé‡

ç»¼åˆä¼˜åŒ–åï¼šæˆæœ¬å¯é™ä½ 80-90%ï¼
```

---

## ğŸ¯ æ€»ç»“ï¼šç«¯åˆ°ç«¯ä¼˜åŒ–æµç¨‹

```python
å®Œæ•´ä¼˜åŒ–æµç¨‹ï¼š

Step 1: æ¨¡å‹é‡åŒ–
  â”œâ”€â”€ é€‰æ‹©é‡åŒ–æ–¹æ³•ï¼ˆGPTQ-4bitæ¨èï¼‰
  â”œâ”€â”€ å‡†å¤‡æ ¡å‡†æ•°æ®
  â”œâ”€â”€ æ‰§è¡Œé‡åŒ–
  â””â”€â”€ éªŒè¯è´¨é‡ï¼ˆå›°æƒ‘åº¦ < 5%ä¸‹é™ï¼‰
  
Step 2: æ¨ç†ä¼˜åŒ–
  â”œâ”€â”€ å®ç°KV Cache
  â”œâ”€â”€ æŠ•æœºé‡‡æ ·ï¼ˆåŠ é€Ÿ2-4å€ï¼‰
  â”œâ”€â”€ é€‰æ‹©æ¨ç†æ¡†æ¶ï¼ˆvLLMæ¨èï¼‰
  â”œâ”€â”€ é…ç½®Continuous Batching
  â””â”€â”€ æ€§èƒ½æµ‹è¯•

Step 3: æœåŠ¡åŒ–
  â”œâ”€â”€ APIå°è£…ï¼ˆFastAPIï¼‰
  â”œâ”€â”€ Dockerå®¹å™¨åŒ–
  â”œâ”€â”€ Kuberneteséƒ¨ç½²
  â””â”€â”€ è´Ÿè½½å‡è¡¡

Step 4: ç›‘æ§è¿ç»´
  â”œâ”€â”€ æ·»åŠ ç›‘æ§æŒ‡æ ‡
  â”œâ”€â”€ é…ç½®å‘Šè­¦
  â”œâ”€â”€ æ—¥å¿—æ”¶é›†
  â””â”€â”€ æ€§èƒ½è°ƒä¼˜

Step 5: æˆæœ¬ä¼˜åŒ–
  â”œâ”€â”€ åˆ†ææˆæœ¬ç“¶é¢ˆ
  â”œâ”€â”€ åº”ç”¨ä¼˜åŒ–ç­–ç•¥
  â”œâ”€â”€ æŒç»­ç›‘æ§
  â””â”€â”€ è¿­ä»£æ”¹è¿›

æœ€ç»ˆæ•ˆæœï¼š
  âœ… æ¨¡å‹å¤§å°: 500MB â†’ 62MB (8xå‹ç¼©)
  âœ… æ¨ç†é€Ÿåº¦: 100 tokens/s â†’ 2000+ tokens/s (20xåŠ é€Ÿ)
  âœ… æŠ•æœºé‡‡æ ·: é¢å¤–2-4å€åŠ é€Ÿï¼ˆå¯å åŠ ï¼‰
  âœ… æˆæœ¬: $10/1K requests â†’ $0.001/1K requests (10000xé™ä½)
  âœ… å»¶è¿Ÿ: 5s â†’ 100ms (50xé™ä½)
```

---

---

## ğŸ“ æ€»ç»“ä¸æ£€æŸ¥

### âœ… çŸ¥è¯†æ£€æŸ¥æ¸…å•

å®Œæˆå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€æ¦‚å¿µï¼ˆå¿…é¡»æŒæ¡ï¼‰**
- [ ] ç†è§£ä»€ä¹ˆæ˜¯æ¨¡å‹é‡åŒ–ï¼ˆFP32â†’INT8â†’INT4ï¼‰
- [ ] çŸ¥é“PTQå’ŒQATçš„åŒºåˆ«
- [ ] ç†è§£KV Cacheçš„ä½œç”¨
- [ ] çŸ¥é“æŠ•æœºé‡‡æ ·çš„åŸºæœ¬åŸç†
- [ ] ç†è§£PagedAttentionå¦‚ä½•èŠ‚çœæ˜¾å­˜
- [ ] èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„æ¨ç†å¼•æ“

**è¿›é˜¶ç†è§£ï¼ˆå»ºè®®æŒæ¡ï¼‰**
- [ ] ç†è§£GPTQã€AWQç­‰é‡åŒ–ç®—æ³•
- [ ] çŸ¥é“å¦‚ä½•å®ç°æŠ•æœºé‡‡æ ·
- [ ] ç†è§£Continuous Batchingçš„åŸç†
- [ ] èƒ½å¤Ÿä¼˜åŒ–æ¨ç†æ€§èƒ½
- [ ] ç†è§£é‡åŒ–å¯¹ç²¾åº¦çš„å½±å“
- [ ] çŸ¥é“å¦‚ä½•æƒè¡¡é€Ÿåº¦å’Œè´¨é‡

**å®æˆ˜èƒ½åŠ›ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰**
- [ ] èƒ½å¤Ÿé‡åŒ–æ¨¡å‹å¹¶éƒ¨ç½²
- [ ] ä¼šä½¿ç”¨vLLMç­‰æ¨ç†å¼•æ“
- [ ] èƒ½å¤Ÿå®ç°æŠ•æœºé‡‡æ ·åŠ é€Ÿ
- [ ] ä¼šç›‘æ§å’Œä¼˜åŒ–æ¨ç†æ€§èƒ½
- [ ] èƒ½å¤Ÿè§£å†³å®é™…éƒ¨ç½²é—®é¢˜
- [ ] ç†è§£å¦‚ä½•é™ä½æ¨ç†æˆæœ¬

### ğŸ“Š ä¼˜åŒ–æŠ€æœ¯é€ŸæŸ¥è¡¨

| æŠ€æœ¯ | å‹ç¼©æ¯” | åŠ é€Ÿæ¯” | ç²¾åº¦æŸå¤± | å®ç°éš¾åº¦ | æ¨èåœºæ™¯ |
|------|--------|--------|---------|---------|---------|
| **INT8é‡åŒ–** | 4x | 2-3x | <1% | â­â­ ä¸­ç­‰ | é€šç”¨æ¨è â­â­â­â­â­ |
| **INT4é‡åŒ–** | 8x | 3-4x | 1-3% | â­â­â­ è¾ƒéš¾ | æ˜¾å­˜å—é™ â­â­â­â­ |
| **KV Cache** | - | 50x+ | æ—  | â­ ç®€å• | å¿…å¤‡ â­â­â­â­â­ |
| **æŠ•æœºé‡‡æ ·** | - | 2-4x | æ—  | â­â­â­ è¾ƒéš¾ | é•¿æ–‡æœ¬ç”Ÿæˆ â­â­â­â­ |
| **PagedAttention** | æ˜¾å­˜2x | - | æ—  | â­â­ ä¸­ç­‰ | é«˜å¹¶å‘ â­â­â­â­â­ |
| **Continuous Batching** | - | åå2-3x | æ—  | â­â­â­ è¾ƒéš¾ | ç”Ÿäº§ç¯å¢ƒ â­â­â­â­â­ |

### ğŸ¯ å¦‚ä½•é€‰æ‹©ä¼˜åŒ–ç­–ç•¥ï¼Ÿ

```python
# å†³ç­–æ ‘
if ç›®æ ‡ == "å‡å°æ¨¡å‹å¤§å°":
    if ç²¾åº¦è¦æ±‚é«˜:
        ä½¿ç”¨ INT8é‡åŒ–  # ç²¾åº¦æŸå¤±<1%
    else:
        ä½¿ç”¨ INT4é‡åŒ–  # æ›´å°ï¼Œç²¾åº¦æŸå¤±1-3%
        
elif ç›®æ ‡ == "åŠ é€Ÿæ¨ç†":
    å¿…é¡»ä½¿ç”¨ KV Cache  # åŸºç¡€ä¼˜åŒ–
    
    if ç”Ÿæˆé•¿æ–‡æœ¬:
        + æŠ•æœºé‡‡æ ·  # é¢å¤–2-4xåŠ é€Ÿ
    
    if é«˜å¹¶å‘åœºæ™¯:
        + PagedAttention  # èŠ‚çœæ˜¾å­˜
        + Continuous Batching  # æé«˜åå
        
elif ç›®æ ‡ == "é™ä½æˆæœ¬":
    é‡åŒ– + KV Cache + æŠ•æœºé‡‡æ ·  # ç»„åˆä½¿ç”¨
    
# æ¨èç»„åˆ
ç”Ÿäº§ç¯å¢ƒæ ‡é…:
  âœ… INT8é‡åŒ–ï¼ˆå‡å°4å€ï¼‰
  âœ… KV Cacheï¼ˆåŠ é€Ÿ50å€ï¼‰
  âœ… PagedAttentionï¼ˆé«˜å¹¶å‘ï¼‰
  âœ… Continuous Batchingï¼ˆé«˜ååï¼‰
  âœ… vLLMæ¨ç†å¼•æ“ï¼ˆé›†æˆä»¥ä¸Šæ‰€æœ‰ï¼‰
```

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

ç°åœ¨ä½ å·²ç»æŒæ¡äº†æ¨¡å‹ä¼˜åŒ–ï¼Œæ¥ä¸‹æ¥åº”è¯¥å­¦ä¹ ï¼š

1. **10_production_deployment.md** - å­¦ä¹ å¦‚ä½•éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
2. **å®è·µé¡¹ç›®** - éƒ¨ç½²ä¸€ä¸ªä¼˜åŒ–åçš„æ¨¡å‹
3. **æ€§èƒ½è°ƒä¼˜** - é’ˆå¯¹å®é™…åœºæ™¯ä¼˜åŒ–æ€§èƒ½

### ğŸ’¡ å®è·µå»ºè®®

**ç«‹å³å¯åš**ï¼š
```python
# 1. é‡åŒ–ä½ çš„æ¨¡å‹
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("gpt2")
model = model.to(torch.float16)  # å…ˆè¯•FP16
# è§‚å¯Ÿï¼šæ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦æå‡

# 2. æµ‹è¯•KV Cache
# ä¸ä½¿ç”¨KV Cache
output = model.generate(input_ids, use_cache=False)
# ä½¿ç”¨KV Cache
output = model.generate(input_ids, use_cache=True)
# å¯¹æ¯”ï¼šé€Ÿåº¦å·®å¼‚å·¨å¤§

# 3. å¯¹æ¯”æ¨ç†å¼•æ“
# åŸç”ŸPyTorch vs vLLM
# æµ‹é‡ï¼šååé‡ã€å»¶è¿Ÿã€æ˜¾å­˜
```

**ç³»ç»Ÿå®éªŒ**ï¼š
```bash
# å®éªŒ1ï¼šé‡åŒ–ç²¾åº¦æµ‹è¯•
python quantize_test.py \
  --model gpt2 \
  --precision fp32,fp16,int8,int4 \
  --eval_dataset wikitext
# è®°å½•ï¼šperplexityå˜åŒ–

# å®éªŒ2ï¼šæ¨ç†é€Ÿåº¦å¯¹æ¯”
python benchmark_inference.py \
  --model gpt2 \
  --batch_sizes 1,4,16,64 \
  --seq_lengths 128,512,2048
# è®°å½•ï¼štokens/s, latency

# å®éªŒ3ï¼šæŠ•æœºé‡‡æ ·æ•ˆæœ
python speculative_decoding_test.py \
  --target_model gpt2-large \
  --draft_model gpt2-small \
  --k_values 3,5,7,10
# è®°å½•ï¼šåŠ é€Ÿæ¯”ã€æ¥å—ç‡
```

**è¿›é˜¶ç ”ç©¶**ï¼š
1. é˜…è¯»GPTQã€AWQè®ºæ–‡ï¼Œç†è§£é‡åŒ–ç®—æ³•
2. ç ”ç©¶vLLMçš„PagedAttentionå®ç°
3. å®ç°è‡ªå·±çš„æŠ•æœºé‡‡æ ·
4. ä¼˜åŒ–ç‰¹å®šåœºæ™¯çš„æ¨ç†æ€§èƒ½

---

## ğŸ“š æ¨èèµ„æº

### ğŸ“– å¿…è¯»æ–‡æ¡£
- [vLLM Documentation](https://docs.vllm.ai/) - æœ€å¥½çš„æ¨ç†å¼•æ“
- [TensorRT-LLM Guide](https://github.com/NVIDIA/TensorRT-LLM) - NVIDIAå®˜æ–¹
- [Hugging Face Optimum](https://huggingface.co/docs/optimum/) - ä¼˜åŒ–å·¥å…·é›†

### ğŸ“„ é‡è¦è®ºæ–‡

**é‡åŒ–ç›¸å…³**ï¼š
1. **GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers** (Frantar et al., 2022)
   - https://arxiv.org/abs/2210.17323
   - 4-bité‡åŒ–ï¼Œç²¾åº¦æŸå¤±å°

2. **AWQ: Activation-aware Weight Quantization** (Lin et al., 2023)
   - https://arxiv.org/abs/2306.00978
   - æ›´å¥½çš„é‡åŒ–æ–¹æ³•

3. **SmoothQuant: Accurate and Efficient Post-Training Quantization** (Xiao et al., 2022)
   - https://arxiv.org/abs/2211.10438
   - INT8é‡åŒ–

**æ¨ç†ä¼˜åŒ–ç›¸å…³**ï¼š
4. **Fast Inference from Transformers via Speculative Decoding** (Leviathan et al., 2022)
   - https://arxiv.org/abs/2211.17192
   - æŠ•æœºé‡‡æ ·åŸå§‹è®ºæ–‡

5. **Efficient Memory Management for Large Language Model Serving with PagedAttention** (Kwon et al., 2023)
   - https://arxiv.org/abs/2309.06180
   - vLLMçš„æ ¸å¿ƒæŠ€æœ¯

6. **Medusa: Simple LLM Inference Acceleration Framework** (Cai et al., 2024)
   - https://arxiv.org/abs/2401.10774
   - å¤šå¤´æŠ•æœºé‡‡æ ·

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- [vLLM: Easy, Fast, and Cheap LLM Serving](https://www.youtube.com/watch?v=80bIUggRJf4)
- [Model Quantization Explained](https://www.youtube.com/watch?v=0VdNflU08yA)

### ğŸ”§ å®ç”¨å·¥å…·

**é‡åŒ–å·¥å…·**ï¼š
```bash
# AutoGPTQ - æœ€æµè¡Œçš„é‡åŒ–åº“
pip install auto-gptq
# ä½¿ç”¨ï¼šä¸€è¡Œä»£ç é‡åŒ–æ¨¡å‹

# bitsandbytes - ç®€å•æ˜“ç”¨
pip install bitsandbytes
# ä½¿ç”¨ï¼šload_in_8bit=True

# llama.cpp - CPUæ¨ç†
git clone https://github.com/ggerganov/llama.cpp
# æ”¯æŒï¼šGGUFæ ¼å¼ï¼Œæè‡´ä¼˜åŒ–
```

**æ¨ç†å¼•æ“**ï¼š
```bash
# vLLM - æ¨è
pip install vllm
# ç‰¹ç‚¹ï¼šPagedAttention, Continuous Batching

# TensorRT-LLM - NVIDIAå®˜æ–¹
pip install tensorrt-llm
# ç‰¹ç‚¹ï¼šæœ€å¿«ï¼Œä½†é…ç½®å¤æ‚

# Text Generation Inference - HuggingFace
docker pull ghcr.io/huggingface/text-generation-inference
# ç‰¹ç‚¹ï¼šå¼€ç®±å³ç”¨
```

**æ€§èƒ½åˆ†æ**ï¼š
```bash
# PyTorch Profiler
python -m torch.utils.bottleneck script.py

# NVIDIA Nsight
nsys profile python inference.py

# è‡ªå®šä¹‰benchmark
python benchmark.py --model gpt2 --batch_size 32
```

---

## ğŸ› å¸¸è§é—®é¢˜ FAQ

### Q1: é‡åŒ–ä¼šæŸå¤±å¤šå°‘ç²¾åº¦ï¼Ÿ
**A**: å–å†³äºé‡åŒ–æ–¹æ³•å’Œä½æ•°ã€‚
```
FP32 â†’ FP16:
  ç²¾åº¦æŸå¤±ï¼šå‡ ä¹æ— ï¼ˆ<0.1%ï¼‰
  é€Ÿåº¦æå‡ï¼š2x
  æ˜¾å­˜èŠ‚çœï¼š50%
  å»ºè®®ï¼šæ€»æ˜¯ä½¿ç”¨ âœ…

FP32 â†’ INT8:
  ç²¾åº¦æŸå¤±ï¼šå¾ˆå°ï¼ˆ<1%ï¼‰
  é€Ÿåº¦æå‡ï¼š2-3x
  æ˜¾å­˜èŠ‚çœï¼š75%
  å»ºè®®ï¼šé€šç”¨æ¨è âœ…

FP32 â†’ INT4:
  ç²¾åº¦æŸå¤±ï¼šå°ï¼ˆ1-3%ï¼‰
  é€Ÿåº¦æå‡ï¼š3-4x
  æ˜¾å­˜èŠ‚çœï¼š87.5%
  å»ºè®®ï¼šæ˜¾å­˜å—é™æ—¶ä½¿ç”¨ âš ï¸

å®æµ‹ï¼ˆGPT-2 on WikiTextï¼‰:
  FP32: perplexity = 29.41
  INT8: perplexity = 29.52 (+0.4%)
  INT4: perplexity = 30.15 (+2.5%)
```

### Q2: KV Cacheä¸ºä»€ä¹ˆè¿™ä¹ˆé‡è¦ï¼Ÿ
**A**: å› ä¸ºå®ƒé¿å…äº†é‡å¤è®¡ç®—ã€‚
```python
# ä¸ä½¿ç”¨KV Cacheï¼ˆæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—ï¼‰
ç”Ÿæˆ100ä¸ªtoken:
  Token 1: è®¡ç®—1ä¸ªtokençš„attention
  Token 2: è®¡ç®—2ä¸ªtokençš„attentionï¼ˆé‡å¤è®¡ç®—token 1ï¼‰
  Token 3: è®¡ç®—3ä¸ªtokençš„attentionï¼ˆé‡å¤è®¡ç®—token 1,2ï¼‰
  ...
  Token 100: è®¡ç®—100ä¸ªtokençš„attention
  
  æ€»è®¡ç®—é‡: 1+2+3+...+100 = 5050æ¬¡attention

# ä½¿ç”¨KV Cacheï¼ˆç¼“å­˜ä¹‹å‰çš„K,Vï¼‰
ç”Ÿæˆ100ä¸ªtoken:
  Token 1: è®¡ç®—1æ¬¡ï¼Œç¼“å­˜K1,V1
  Token 2: åªè®¡ç®—æ–°çš„ï¼Œä½¿ç”¨ç¼“å­˜çš„K1,V1
  Token 3: åªè®¡ç®—æ–°çš„ï¼Œä½¿ç”¨ç¼“å­˜çš„K1,V1,K2,V2
  ...
  
  æ€»è®¡ç®—é‡: 100æ¬¡attention
  
åŠ é€Ÿæ¯”: 5050/100 = 50.5x ï¼

ç»“è®ºï¼šKV Cacheæ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒæ¨ç†ä¼šæ…¢50å€ï¼
```

### Q3: æŠ•æœºé‡‡æ ·çœŸçš„ä¸æŸå¤±è´¨é‡å—ï¼Ÿ
**A**: æ˜¯çš„ï¼Œå®Œå…¨æ— æŸï¼
```python
# åŸç†ï¼šç”¨å°æ¨¡å‹"çŒœæµ‹"ï¼Œå¤§æ¨¡å‹"éªŒè¯"

ä¼ ç»Ÿç”Ÿæˆ:
  å¤§æ¨¡å‹ç”Ÿæˆtoken 1
  å¤§æ¨¡å‹ç”Ÿæˆtoken 2
  å¤§æ¨¡å‹ç”Ÿæˆtoken 3
  ...
  
æŠ•æœºé‡‡æ ·:
  å°æ¨¡å‹å¿«é€Ÿç”Ÿæˆ: token 1,2,3,4,5
  å¤§æ¨¡å‹ä¸€æ¬¡éªŒè¯: âœ…âœ…âœ…âŒ  (å‰3ä¸ªå¯¹ï¼Œç¬¬4ä¸ªé”™)
  ä¿ç•™: token 1,2,3
  å¤§æ¨¡å‹ç”Ÿæˆ: token 4ï¼ˆæ­£ç¡®çš„ï¼‰
  
å…³é”®ï¼š
  - æœ€ç»ˆè¾“å‡ºå®Œå…¨ç”±å¤§æ¨¡å‹å†³å®š
  - å°æ¨¡å‹åªæ˜¯"å»ºè®®"ï¼Œä¸å½±å“ç»“æœ
  - è´¨é‡ = 100%å¤§æ¨¡å‹è´¨é‡
  - é€Ÿåº¦ = 2-4xï¼ˆå› ä¸ºå°æ¨¡å‹å¾ˆå¿«ï¼‰

å®æµ‹ï¼š
  åŸå§‹: "The cat sat on the mat"
  æŠ•æœº: "The cat sat on the mat"
  å®Œå…¨ç›¸åŒï¼âœ…
```

### Q4: å¦‚ä½•é€‰æ‹©æ¨ç†å¼•æ“ï¼Ÿ
**A**: æ ¹æ®éœ€æ±‚é€‰æ‹©ã€‚
```
vLLMï¼ˆæ¨èï¼‰:
  âœ… æœ€é«˜ååé‡
  âœ… PagedAttentionèŠ‚çœæ˜¾å­˜
  âœ… Continuous Batching
  âœ… æ˜“äºä½¿ç”¨
  âŒ åªæ”¯æŒCUDA
  é€‚åˆï¼šç”Ÿäº§ç¯å¢ƒã€é«˜å¹¶å‘

TensorRT-LLMï¼ˆæœ€å¿«ï¼‰:
  âœ… æœ€ä½å»¶è¿Ÿ
  âœ… NVIDIAå®˜æ–¹ä¼˜åŒ–
  âœ… æ”¯æŒæ‰€æœ‰NVIDIA GPU
  âŒ é…ç½®å¤æ‚
  âŒ åªæ”¯æŒNVIDIA
  é€‚åˆï¼šè¿½æ±‚æè‡´æ€§èƒ½

Text Generation Inferenceï¼ˆç®€å•ï¼‰:
  âœ… å¼€ç®±å³ç”¨
  âœ… HuggingFaceé›†æˆ
  âœ… Dockeréƒ¨ç½²
  âŒ æ€§èƒ½ä¸å¦‚vLLM
  é€‚åˆï¼šå¿«é€ŸåŸå‹

llama.cppï¼ˆCPUï¼‰:
  âœ… CPUæ¨ç†
  âœ… æè‡´ä¼˜åŒ–
  âœ… è·¨å¹³å°
  âŒ é€Ÿåº¦è¾ƒæ…¢
  é€‚åˆï¼šæ²¡æœ‰GPUçš„åœºæ™¯

æ¨èï¼š
  - æœ‰GPUï¼švLLM â­â­â­â­â­
  - è¿½æ±‚æè‡´ï¼šTensorRT-LLM â­â­â­â­
  - å¿«é€Ÿå¼€å§‹ï¼šTGI â­â­â­
  - åªæœ‰CPUï¼šllama.cpp â­â­â­
```

### Q5: PagedAttentionå¦‚ä½•èŠ‚çœæ˜¾å­˜ï¼Ÿ
**A**: ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜ã€‚
```python
# ä¼ ç»Ÿæ–¹æ³•ï¼ˆé¢„åˆ†é…ï¼‰
æ¯ä¸ªè¯·æ±‚é¢„ç•™æœ€å¤§é•¿åº¦çš„æ˜¾å­˜:
  è¯·æ±‚1: å®é™…50 tokensï¼Œé¢„ç•™2048 tokens â†’ æµªè´¹97.5%
  è¯·æ±‚2: å®é™…100 tokensï¼Œé¢„ç•™2048 tokens â†’ æµªè´¹95%
  ...
  
  æ€»æ˜¾å­˜: Nä¸ªè¯·æ±‚ Ã— 2048 Ã— æ¨¡å‹å¤§å°
  åˆ©ç”¨ç‡: å¾ˆä½ï¼ˆ<10%ï¼‰

# PagedAttentionï¼ˆæŒ‰éœ€åˆ†é…ï¼‰
æŒ‰å®é™…éœ€è¦åˆ†é…ï¼Œç±»ä¼¼æ“ä½œç³»ç»Ÿçš„é¡µè¡¨:
  è¯·æ±‚1: å®é™…50 tokens â†’ åªåˆ†é…50 tokens
  è¯·æ±‚2: å®é™…100 tokens â†’ åªåˆ†é…100 tokens
  ...
  
  æ€»æ˜¾å­˜: å®é™…ä½¿ç”¨é‡
  åˆ©ç”¨ç‡: å¾ˆé«˜ï¼ˆ>80%ï¼‰

æ•ˆæœï¼š
  - ç›¸åŒæ˜¾å­˜å¯ä»¥å¤„ç†2-3xè¯·æ±‚
  - æˆ–è€…å¤„ç†æ›´é•¿çš„åºåˆ—
  - å‡ ä¹æ— æ€§èƒ½æŸå¤±

ç±»æ¯”ï¼š
  ä¼ ç»Ÿ = æ¯äººä¸€é—´å¤§æˆ¿å­ï¼ˆå¾ˆå¤šç©ºé—´æµªè´¹ï¼‰
  PagedAttention = æŒ‰éœ€åˆ†é…æˆ¿é—´ï¼ˆé«˜æ•ˆåˆ©ç”¨ï¼‰
```

### Q6: Continuous Batchingæ˜¯ä»€ä¹ˆï¼Ÿ
**A**: åŠ¨æ€æ‰¹å¤„ç†ï¼Œæé«˜ååé‡ã€‚
```python
# ä¼ ç»ŸStatic Batching
ç­‰å¾…å‡‘å¤Ÿbatch_sizeæ‰å¼€å§‹:
  è¯·æ±‚1åˆ°è¾¾ â†’ ç­‰å¾…
  è¯·æ±‚2åˆ°è¾¾ â†’ ç­‰å¾…
  è¯·æ±‚3åˆ°è¾¾ â†’ ç­‰å¾…
  è¯·æ±‚4åˆ°è¾¾ â†’ å¼€å§‹å¤„ç†ï¼ˆbatch_size=4ï¼‰
  
  é—®é¢˜ï¼š
  - è¯·æ±‚1ç­‰å¾…æ—¶é—´é•¿
  - GPUå¯èƒ½ç©ºé—²
  - ååé‡ä½

# Continuous Batching
åŠ¨æ€åŠ å…¥å’Œç§»é™¤è¯·æ±‚:
  è¯·æ±‚1åˆ°è¾¾ â†’ ç«‹å³å¼€å§‹
  è¯·æ±‚2åˆ°è¾¾ â†’ åŠ å…¥batch
  è¯·æ±‚1å®Œæˆ â†’ ç§»é™¤ï¼Œè¯·æ±‚3åŠ å…¥
  ...
  
  ä¼˜ç‚¹ï¼š
  - å»¶è¿Ÿä½ï¼ˆç«‹å³å¤„ç†ï¼‰
  - GPUåˆ©ç”¨ç‡é«˜
  - ååé‡é«˜2-3x

å®æµ‹ï¼š
  Static: 100 req/s, å¹³å‡å»¶è¿Ÿ500ms
  Continuous: 250 req/s, å¹³å‡å»¶è¿Ÿ200ms
```

### Q7: å¦‚ä½•éªŒè¯é‡åŒ–åçš„æ¨¡å‹è´¨é‡ï¼Ÿ
**A**: å¤šç»´åº¦è¯„ä¼°ã€‚
```python
# 1. Perplexityæµ‹è¯•
from datasets import load_dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

fp32_ppl = evaluate_perplexity(fp32_model, dataset)
int8_ppl = evaluate_perplexity(int8_model, dataset)

print(f"FP32: {fp32_ppl:.2f}")
print(f"INT8: {int8_ppl:.2f}")
print(f"å·®å¼‚: {(int8_ppl/fp32_ppl - 1)*100:.1f}%")
# åº”è¯¥ < 2%

# 2. ä¸‹æ¸¸ä»»åŠ¡æµ‹è¯•
tasks = ["hellaswag", "winogrande", "arc"]
for task in tasks:
    fp32_acc = evaluate(fp32_model, task)
    int8_acc = evaluate(int8_model, task)
    print(f"{task}: {fp32_acc:.1f}% â†’ {int8_acc:.1f}%")

# 3. ç”Ÿæˆè´¨é‡æµ‹è¯•
prompts = ["Once upon a time", "The capital of France"]
for prompt in prompts:
    fp32_output = fp32_model.generate(prompt)
    int8_output = int8_model.generate(prompt)
    # äººå·¥å¯¹æ¯”è´¨é‡

# 4. é€Ÿåº¦å’Œæ˜¾å­˜æµ‹è¯•
benchmark(fp32_model)  # 100 tokens/s, 16GB
benchmark(int8_model)  # 250 tokens/s, 4GB
```

### Q8: æŠ•æœºé‡‡æ ·çš„draft modelå¦‚ä½•é€‰æ‹©ï¼Ÿ
**A**: éµå¾ªè¿™äº›åŸåˆ™ã€‚
```
åŸåˆ™1ï¼šæ¶æ„ç›¸åŒ
  Target: GPT-2 Large
  Draft: GPT-2 Small âœ…
  Draft: BERT âŒï¼ˆæ¶æ„ä¸åŒï¼‰

åŸåˆ™2ï¼šå¤§å°æ¯”ä¾‹
  Target: 1.5Bå‚æ•°
  Draft: 125M-350Må‚æ•°ï¼ˆ1/5 - 1/10ï¼‰
  Draft: 10Må‚æ•° âŒï¼ˆå¤ªå°ï¼Œæ¥å—ç‡ä½ï¼‰
  Draft: 1Bå‚æ•° âŒï¼ˆå¤ªå¤§ï¼ŒåŠ é€Ÿä¸æ˜æ˜¾ï¼‰

åŸåˆ™3ï¼šè®­ç»ƒæ•°æ®ç›¸ä¼¼
  Target: è®­ç»ƒåœ¨ä»£ç ä¸Š
  Draft: ä¹Ÿè®­ç»ƒåœ¨ä»£ç ä¸Š âœ…
  Draft: è®­ç»ƒåœ¨é€šç”¨æ–‡æœ¬ âš ï¸ï¼ˆæ¥å—ç‡å¯èƒ½ä½ï¼‰

å®é™…ä¾‹å­ï¼š
  Target: Llama-2-70B
  Draft: Llama-2-7B âœ…ï¼ˆ10xå°ï¼‰
  
  Target: GPT-3.5
  Draft: GPT-2 âœ…ï¼ˆæ¶æ„ç›¸åŒï¼‰

æ•ˆæœï¼š
  - å¥½çš„draft: æ¥å—ç‡70-90%ï¼ŒåŠ é€Ÿ3-4x
  - å·®çš„draft: æ¥å—ç‡30-50%ï¼ŒåŠ é€Ÿ1.5-2x
```

### Q9: å¦‚ä½•ä¼˜åŒ–æ¨ç†æˆæœ¬ï¼Ÿ
**A**: å¤šç®¡é½ä¸‹ã€‚
```python
# æˆæœ¬ = ç¡¬ä»¶æˆæœ¬ + è¿è¥æˆæœ¬

# 1. å‡å°æ¨¡å‹ï¼ˆæœ€æœ‰æ•ˆï¼‰
é‡åŒ–åˆ°INT8: æˆæœ¬å‡å°‘75%
é‡åŒ–åˆ°INT4: æˆæœ¬å‡å°‘87.5%

# 2. æé«˜ååé‡
ä½¿ç”¨vLLM: ååé‡æå‡2-3x
â†’ ç›¸åŒè¯·æ±‚é‡ï¼Œéœ€è¦çš„GPUå‡å°‘2-3x

# 3. é™ä½å»¶è¿Ÿè¦æ±‚
å¦‚æœå¯ä»¥æ¥å—200msè€Œä¸æ˜¯50ms:
  - å¯ä»¥ç”¨æ›´å°çš„GPU
  - å¯ä»¥å¢å¤§batch_size
  - æˆæœ¬é™ä½50%+

# 4. ä½¿ç”¨Spotå®ä¾‹
AWS Spot: æˆæœ¬é™ä½70%
ä½†éœ€è¦å¤„ç†ä¸­æ–­

# 5. æ‰¹å¤„ç†éå®æ—¶è¯·æ±‚
å®æ—¶è¯·æ±‚: å¿…é¡»ç«‹å³å¤„ç†
ç¦»çº¿è¯·æ±‚: å¯ä»¥æ‰¹å¤„ç†
â†’ ç¦»çº¿è¯·æ±‚æˆæœ¬é™ä½80%

å®é™…æ¡ˆä¾‹ï¼š
  åŸå§‹: A100 Ã— 8, $20/å°æ—¶, 100 req/s
  ä¼˜åŒ–å: A100 Ã— 2 (INT8+vLLM), $5/å°æ—¶, 100 req/s
  æˆæœ¬é™ä½: 75% âœ…
```

### Q10: å¦‚ä½•è°ƒè¯•æ¨ç†æ€§èƒ½é—®é¢˜ï¼Ÿ
**A**: ç³»ç»Ÿæ€§åˆ†æã€‚
```python
# 1. æµ‹é‡å„éƒ¨åˆ†è€—æ—¶
import time

# Tokenization
t0 = time.time()
tokens = tokenizer.encode(text)
print(f"Tokenization: {time.time()-t0:.3f}s")

# Model inference
t0 = time.time()
output = model.generate(tokens)
print(f"Generation: {time.time()-t0:.3f}s")

# Decoding
t0 = time.time()
text = tokenizer.decode(output)
print(f"Decoding: {time.time()-t0:.3f}s")

# 2. åˆ†æç“¶é¢ˆ
å¦‚æœtokenizationæ…¢:
  - ä½¿ç”¨fast tokenizer
  - é¢„å¤„ç†å¹¶ç¼“å­˜

å¦‚æœgenerationæ…¢:
  - æ£€æŸ¥æ˜¯å¦ä½¿ç”¨KV Cache
  - æ£€æŸ¥batch_sizeæ˜¯å¦å¤ªå°
  - è€ƒè™‘é‡åŒ–

å¦‚æœæ˜¾å­˜ä¸å¤Ÿ:
  - ä½¿ç”¨INT8/INT4
  - å‡å°batch_size
  - ä½¿ç”¨PagedAttention

# 3. ä½¿ç”¨profiler
from torch.profiler import profile
with profile() as prof:
    model.generate(tokens)
print(prof.key_averages().table())
# æ‰¾å‡ºæœ€è€—æ—¶çš„æ“ä½œ

# 4. å¯¹æ¯”baseline
baseline_speed = 100  # tokens/s
current_speed = measure_speed()
print(f"ç›¸å¯¹baseline: {current_speed/baseline_speed:.1f}x")
```

---

**æ­å–œä½ å®Œæˆç¬¬09ç« ï¼** ğŸ‰

ä½ ç°åœ¨å·²ç»æŒæ¡äº†æ¨¡å‹ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä»é‡åŒ–åˆ°æ¨ç†åŠ é€Ÿï¼Œä»KV Cacheåˆ°æŠ•æœºé‡‡æ ·ï¼Œä½ å·²ç»å…·å¤‡äº†éƒ¨ç½²é«˜æ€§èƒ½æ¨¡å‹çš„èƒ½åŠ›ã€‚

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼** â†’ [10_production_deployment.md](10_production_deployment.md)

