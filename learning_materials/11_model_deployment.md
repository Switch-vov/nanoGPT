# æ¨¡å‹éƒ¨ç½²å®Œå…¨æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**ä»ç ”ç©¶åˆ°ç”Ÿäº§çš„é¸¿æ²Ÿï¼š**
- è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æœ¬åœ°è·‘å¾—å¾ˆå¥½
- ä½†å¦‚ä½•è®©ç”¨æˆ·è®¿é—®ï¼Ÿ
- å¦‚ä½•å¤„ç†å¹¶å‘è¯·æ±‚ï¼Ÿ
- å¦‚ä½•ä¿è¯ä½å»¶è¿Ÿå’Œé«˜ååï¼Ÿ
- å¦‚ä½•ç›‘æ§å’Œç»´æŠ¤ï¼Ÿ

**éƒ¨ç½²çš„æŒ‘æˆ˜ï¼š**
```python
ç ”ç©¶ç¯å¢ƒ:
  python train.py  # ç®€å•
  python sample.py # ç”Ÿæˆæ–‡æœ¬

ç”Ÿäº§ç¯å¢ƒ:
  â“ å¦‚ä½•æä¾›APIæœåŠ¡ï¼Ÿ
  â“ å¦‚ä½•å¤„ç†1000ä¸ªå¹¶å‘ç”¨æˆ·ï¼Ÿ
  â“ å¦‚ä½•ä¿è¯99.9%å¯ç”¨æ€§ï¼Ÿ
  â“ å¦‚ä½•ç›‘æ§å’Œè°ƒè¯•ï¼Ÿ
  â“ å¦‚ä½•æ›´æ–°æ¨¡å‹ï¼Ÿ
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šéƒ¨ç½²æ¶æ„æ¦‚è§ˆ

### ğŸ—ï¸ éƒ¨ç½²å±‚çº§

```python
Level 1: æœ¬åœ°è„šæœ¬
  é€‚ç”¨: ä¸ªäººä½¿ç”¨ã€å®éªŒ
  æ–¹å¼: python sample.py
  
Level 2: APIæœåŠ¡
  é€‚ç”¨: å°å›¢é˜Ÿã€å†…éƒ¨å·¥å…·
  æ–¹å¼: Flask/FastAPI
  
Level 3: ç”Ÿäº§çº§æœåŠ¡
  é€‚ç”¨: å¯¹å¤–äº§å“ã€å¤§è§„æ¨¡åº”ç”¨
  æ–¹å¼: Docker + Kubernetes + è´Ÿè½½å‡è¡¡
  
Level 4: ä¸“ä¸šæ¨ç†å¹³å°
  é€‚ç”¨: ä¼ä¸šçº§ã€é«˜æ€§èƒ½éœ€æ±‚
  æ–¹å¼: TensorRT, Triton, vLLM
```

### ğŸ¯ éƒ¨ç½²ç›®æ ‡

```python
å…³é”®æŒ‡æ ‡:

å»¶è¿Ÿ (Latency):
  < 100ms: å®æ—¶å¯¹è¯ âœ…
  100-500ms: å¯æ¥å—
  > 1s: ç”¨æˆ·ä½“éªŒå·® âŒ
  
ååé‡ (Throughput):
  requests/second
  tokens/second
  
å¯ç”¨æ€§ (Availability):
  99.9%: æ¯æœˆåœæœº43åˆ†é’Ÿ
  99.99%: æ¯æœˆåœæœº4åˆ†é’Ÿ
  
æˆæœ¬ (Cost):
  GPUåˆ©ç”¨ç‡
  æ¯ä¸ªè¯·æ±‚çš„æˆæœ¬
```

---

## ğŸš€ ç¬¬äºŒéƒ¨åˆ†ï¼šFastAPIå¿«é€Ÿéƒ¨ç½²

### ğŸ“ åŸºç¡€APIæœåŠ¡

**åˆ›å»º serve.pyï¼š**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from model import GPT, GPTConfig
import pickle
from contextlib import asynccontextmanager

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
model_cache = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load('out/ckpt.pt', map_location='cuda')
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    model.to('cuda')
    
    # åŠ è½½tokenizer
    with open('data/shakespeare_char/meta.pkl', 'rb') as f:
        meta = pickle.load(f)
    
    # å­˜å‚¨åˆ°å…¨å±€
    model_cache['model'] = model
    model_cache['meta'] = meta
    
    print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    yield
    
    # æ¸…ç†
    model_cache.clear()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="NanoGPT API", lifespan=lifespan)

# è¯·æ±‚æ¨¡å‹
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.8
    top_k: int = 200

class GenerateResponse(BaseModel):
    text: str
    tokens_generated: int

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "message": "NanoGPT API is running"}

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """ç”Ÿæˆæ–‡æœ¬"""
    try:
        model = model_cache['model']
        meta = model_cache['meta']
        
        # ç¼–ç 
        stoi = meta['stoi']
        itos = meta['itos']
        encode = lambda s: [stoi[c] for c in s]
        decode = lambda l: ''.join([itos[i] for i in l])
        
        # ç”Ÿæˆ
        input_ids = torch.tensor([encode(request.prompt)], dtype=torch.long, device='cuda')
        
        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_k=request.top_k
            )
        
        # è§£ç 
        generated_text = decode(output[0].tolist())
        
        return GenerateResponse(
            text=generated_text,
            tokens_generated=len(output[0]) - len(input_ids[0])
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    if 'model' in model_cache:
        return {"status": "healthy", "model_loaded": True}
    else:
        return {"status": "unhealthy", "model_loaded": False}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**å¯åŠ¨æœåŠ¡ï¼š**

```bash
# å®‰è£…ä¾èµ–
pip install fastapi uvicorn pydantic

# å¯åŠ¨æœåŠ¡
python serve.py

# è¾“å‡º:
# åŠ è½½æ¨¡å‹...
# æ¨¡å‹åŠ è½½å®Œæˆ!
# INFO:     Started server process
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

**æµ‹è¯•APIï¼š**

```bash
# æ–¹æ³•1: curl
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ROMEO:",
    "max_tokens": 100,
    "temperature": 0.8
  }'

# è¾“å‡º:
# {
#   "text": "ROMEO:\nWhat lady is that, which doth enrich the hand\nOf yonder knight?",
#   "tokens_generated": 100
# }

# æ–¹æ³•2: Pythonå®¢æˆ·ç«¯
import requests

response = requests.post(
    "http://localhost:8000/generate",
    json={
        "prompt": "To be or not to be",
        "max_tokens": 50,
        "temperature": 0.8
    }
)

print(response.json()['text'])
```

---

## ğŸ³ ç¬¬ä¸‰éƒ¨åˆ†ï¼šDockerå®¹å™¨åŒ–

### ğŸ“¦ åˆ›å»ºDockerfile

```dockerfile
# Dockerfile

FROM nvidia/cuda:12.1.0-base-ubuntu22.04

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…Pythonå’Œä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶requirements
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY model.py .
COPY serve.py .
COPY out/ ./out/
COPY data/shakespeare_char/meta.pkl ./data/shakespeare_char/

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "serve.py"]
```

### ğŸ“ requirements.txt

```txt
torch==2.1.0
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
```

### ğŸš€ æ„å»ºå’Œè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t nanogpt-api:latest .

# è¿è¡Œå®¹å™¨
docker run -d \
  --name nanogpt \
  --gpus all \
  -p 8000:8000 \
  nanogpt-api:latest

# æŸ¥çœ‹æ—¥å¿—
docker logs -f nanogpt

# æµ‹è¯•
curl http://localhost:8000/health
```

### ğŸ“Š Docker Composeï¼ˆæ¨èï¼‰

```yaml
# docker-compose.yml

version: '3.8'

services:
  nanogpt-api:
    build: .
    container_name: nanogpt
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./out:/app/out:ro  # åªè¯»æŒ‚è½½æ¨¡å‹
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    container_name: nanogpt-nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - nanogpt-api
    restart: unless-stopped
```

**nginx.confï¼ˆè´Ÿè½½å‡è¡¡ï¼‰ï¼š**

```nginx
events {
    worker_connections 1024;
}

http {
    upstream nanogpt_backend {
        # å¤šä¸ªå®ä¾‹è´Ÿè½½å‡è¡¡
        server nanogpt-api:8000;
        # server nanogpt-api-2:8000;
        # server nanogpt-api-3:8000;
    }
    
    server {
        listen 80;
        
        location / {
            proxy_pass http://nanogpt_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            
            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
    }
}
```

**å¯åŠ¨å®Œæ•´æœåŠ¡ï¼š**

```bash
docker-compose up -d

# æ‰©å±•åˆ°3ä¸ªå®ä¾‹
docker-compose up -d --scale nanogpt-api=3
```

---

## âš¡ ç¬¬å››éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–

### 1ï¸âƒ£ **æ‰¹å¤„ç†æ¨ç†**

```python
# ä¼˜åŒ–ç‰ˆserve.py - æ”¯æŒæ‰¹å¤„ç†

from collections import deque
import asyncio
import time

class BatchProcessor:
    def __init__(self, model, max_batch_size=8, max_wait_time=0.01):
        self.model = model
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        
        self.queue = deque()
        self.processing = False
    
    async def add_request(self, request):
        """æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        future = asyncio.Future()
        self.queue.append((request, future))
        
        # å¦‚æœæ²¡åœ¨å¤„ç†ï¼Œå¯åŠ¨å¤„ç†
        if not self.processing:
            asyncio.create_task(self._process_batch())
        
        return await future
    
    async def _process_batch(self):
        """æ‰¹å¤„ç†è¯·æ±‚"""
        self.processing = True
        start_time = time.time()
        
        # ç­‰å¾…å‡‘å¤Ÿä¸€æ‰¹æˆ–è¶…æ—¶
        while len(self.queue) < self.max_batch_size:
            if time.time() - start_time > self.max_wait_time:
                break
            await asyncio.sleep(0.001)
        
        # å–å‡ºä¸€æ‰¹è¯·æ±‚
        batch = []
        futures = []
        for _ in range(min(self.max_batch_size, len(self.queue))):
            if self.queue:
                req, future = self.queue.popleft()
                batch.append(req)
                futures.append(future)
        
        if batch:
            # æ‰¹é‡æ¨ç†
            results = await self._batch_generate(batch)
            
            # è¿”å›ç»“æœ
            for future, result in zip(futures, results):
                future.set_result(result)
        
        self.processing = False
        
        # å¦‚æœè¿˜æœ‰è¯·æ±‚ï¼Œç»§ç»­å¤„ç†
        if self.queue:
            asyncio.create_task(self._process_batch())
    
    async def _batch_generate(self, batch):
        """æ‰¹é‡ç”Ÿæˆ"""
        # ç¼–ç æ‰€æœ‰prompt
        input_ids_list = []
        max_len = 0
        
        for req in batch:
            ids = encode(req.prompt)
            input_ids_list.append(ids)
            max_len = max(max_len, len(ids))
        
        # Paddingåˆ°ç›¸åŒé•¿åº¦
        padded_ids = []
        for ids in input_ids_list:
            padded = ids + [0] * (max_len - len(ids))
            padded_ids.append(padded)
        
        input_ids = torch.tensor(padded_ids, dtype=torch.long, device='cuda')
        
        # æ‰¹é‡ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_new_tokens=batch[0].max_tokens,
                temperature=batch[0].temperature
            )
        
        # è§£ç æ‰€æœ‰è¾“å‡º
        results = []
        for output in outputs:
            text = decode(output.tolist())
            results.append(GenerateResponse(
                text=text,
                tokens_generated=len(output) - max_len
            ))
        
        return results

# ä½¿ç”¨æ‰¹å¤„ç†å™¨
batch_processor = None

@app.on_event("startup")
async def startup():
    global batch_processor
    # ... åŠ è½½æ¨¡å‹ ...
    batch_processor = BatchProcessor(model)

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    return await batch_processor.add_request(request)
```

**æ•ˆæœï¼š**

```python
æ€§èƒ½å¯¹æ¯”:

å•è¯·æ±‚å¤„ç†:
  å»¶è¿Ÿ: 100ms
  åå: 10 req/s

æ‰¹å¤„ç† (batch_size=8):
  å»¶è¿Ÿ: 110ms (ç•¥å¢)
  åå: 70 req/s (7x!)
```

---

### 2ï¸âƒ£ **KV Cacheä¼˜åŒ–**

```python
# åœ¨model.pyä¸­å®ç°KV Cache

class GPT(nn.Module):
    def generate_with_cache(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """ä½¿ç”¨KV Cacheçš„ç”Ÿæˆï¼ˆæ›´å¿«ï¼‰"""
        # åˆå§‹åŒ–cache
        past_key_values = None
        
        for _ in range(max_new_tokens):
            # å¦‚æœæœ‰cacheï¼Œåªå¤„ç†æœ€åä¸€ä¸ªtoken
            if past_key_values is not None:
                idx_cond = idx[:, [-1]]
            else:
                idx_cond = idx
            
            # å‰å‘ä¼ æ’­ï¼ˆè¿”å›cacheï¼‰
            logits, past_key_values = self.forward_with_cache(
                idx_cond, 
                past_key_values
            )
            
            # é‡‡æ ·
            logits = logits[:, -1, :] / temperature
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        
        return idx
    
    def forward_with_cache(self, idx, past_key_values=None):
        """æ”¯æŒKV Cacheçš„å‰å‘ä¼ æ’­"""
        # å®ç°ç»†èŠ‚...
        # ä¿å­˜æ¯å±‚çš„K, V
        # ä¸‹æ¬¡åªè®¡ç®—æ–°tokençš„K, V
        pass

# åŠ é€Ÿæ•ˆæœ
æ ‡å‡†ç”Ÿæˆ: 100 tokens in 2.5s
KV Cache: 100 tokens in 0.8s (3x faster!)
```

---

### 3ï¸âƒ£ **æ¨¡å‹ä¼˜åŒ–æ±‡æ€»**

```python
ä¼˜åŒ–æ¸…å•:

â–¡ é‡åŒ– (INT8/INT4)
  å‹ç¼©4-8xï¼ŒåŠ é€Ÿ2-3x
  
â–¡ KV Cache
  ç”ŸæˆåŠ é€Ÿ2-3x
  
â–¡ æ‰¹å¤„ç†
  ååæå‡5-10x
  
â–¡ TorchScript/ONNX
  ç§»é™¤Pythonå¼€é”€
  
â–¡ TensorRT
  GPUæ¨ç†ä¼˜åŒ–
  
â–¡ Flash Attention
  å†…å­˜æ•ˆç‡æå‡

ç»„åˆæ•ˆæœ:
  åŸºçº¿: 10 req/s
  + é‡åŒ–: 20 req/s
  + KV Cache: 40 req/s
  + æ‰¹å¤„ç†: 200 req/s (20x!)
```

---

## ğŸ”§ ç¬¬äº”éƒ¨åˆ†ï¼šä¸“ä¸šæ¨ç†å¼•æ“

### ğŸš€ vLLMï¼ˆæ¨èï¼‰

**å®‰è£…ï¼š**

```bash
pip install vllm
```

**ä½¿ç”¨ä»£ç ï¼š**

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="gpt2",  # æˆ–ä½ çš„æ¨¡å‹è·¯å¾„
    tensor_parallel_size=1,  # GPUæ•°é‡
    dtype="float16",
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100,
)

# æ‰¹é‡ç”Ÿæˆ
prompts = [
    "Hello, my name is",
    "The capital of France is",
    "The future of AI is",
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
    print()
```

**åˆ›å»ºAPIæœåŠ¡å™¨ï¼š**

```python
# vllm_serve.py

from vllm import LLM, SamplingParams
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

# å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
llm = LLM(model="gpt2", dtype="float16")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.8

@app.post("/generate")
async def generate(request: GenerateRequest):
    sampling_params = SamplingParams(
        temperature=request.temperature,
        max_tokens=request.max_tokens,
    )
    
    outputs = llm.generate([request.prompt], sampling_params)
    return {"text": outputs[0].outputs[0].text}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**æ€§èƒ½ä¼˜åŠ¿ï¼š**

```python
å¯¹æ¯” (LLaMA-7B, batch_size=32):

æ–¹æ³•                | Throughput
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HuggingFace         | 150 tokens/s
Custom FastAPI      | 450 tokens/s
vLLM               | 2100 tokens/s (14x!)

vLLMä¼˜åŠ¿:
  âœ… PagedAttention (æ›´å¥½çš„å†…å­˜ç®¡ç†)
  âœ… Continuous batching (æŒç»­æ‰¹å¤„ç†)
  âœ… ä¼˜åŒ–çš„CUDA kernels
  âœ… å¼€ç®±å³ç”¨
```

---

### ğŸ”¥ TensorRTï¼ˆNVIDIAï¼‰

**è½¬æ¢ä¸ºTensorRTï¼š**

```bash
# å®‰è£…
pip install tensorrt onnx

# å¯¼å‡ºONNX
python export_onnx.py

# è½¬æ¢ä¸ºTensorRT
trtexec --onnx=model.onnx \
        --saveEngine=model.trt \
        --fp16  # ä½¿ç”¨FP16
```

**ä½¿ç”¨TensorRTï¼š**

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

class TensorRTInference:
    def __init__(self, engine_path):
        # åŠ è½½å¼•æ“
        with open(engine_path, 'rb') as f:
            runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
            self.engine = runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
    
    def infer(self, input_data):
        # æ¨ç†
        # ... TensorRTæ¨ç†ä»£ç  ...
        pass

# ä½¿ç”¨
model = TensorRTInference("model.trt")
output = model.infer(input_data)
```

**åŠ é€Ÿæ•ˆæœï¼š**

```
å¯¹æ¯” (GPT-2, T4 GPU):

PyTorch FP32: 45 ms/iter
PyTorch FP16: 28 ms/iter
TensorRT FP16: 12 ms/iter (3.8x!)
TensorRT INT8: 7 ms/iter (6.4x!)
```

---

## ğŸ“Š ç¬¬å…­éƒ¨åˆ†ï¼šç›‘æ§å’Œæ—¥å¿—

### ğŸ“ˆ æ·»åŠ ç›‘æ§

```python
# serve.py æ·»åŠ ç›‘æ§

from prometheus_client import Counter, Histogram, make_asgi_app
import time

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter(
    'nanogpt_requests_total',
    'Total requests',
    ['endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'nanogpt_request_latency_seconds',
    'Request latency',
    ['endpoint']
)

TOKENS_GENERATED = Counter(
    'nanogpt_tokens_generated_total',
    'Total tokens generated'
)

@app.post("/generate")
async def generate(request: GenerateRequest):
    start_time = time.time()
    
    try:
        # ... ç”Ÿæˆé€»è¾‘ ...
        
        # è®°å½•æŒ‡æ ‡
        REQUEST_COUNT.labels(endpoint='generate', status='success').inc()
        TOKENS_GENERATED.inc(len(output))
        
        return response
        
    except Exception as e:
        REQUEST_COUNT.labels(endpoint='generate', status='error').inc()
        raise
    
    finally:
        # è®°å½•å»¶è¿Ÿ
        latency = time.time() - start_time
        REQUEST_LATENCY.labels(endpoint='generate').observe(latency)

# æ·»åŠ metricsç«¯ç‚¹
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

### ğŸ“ ç»“æ„åŒ–æ—¥å¿—

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
        }
        if hasattr(record, 'request_id'):
            log_obj['request_id'] = record.request_id
        return json.dumps(log_obj)

# é…ç½®æ—¥å¿—
logger = logging.getLogger('nanogpt')
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# ä½¿ç”¨
@app.post("/generate")
async def generate(request: GenerateRequest):
    request_id = str(uuid.uuid4())
    logger.info(
        f"Received request: prompt='{request.prompt[:50]}...'",
        extra={'request_id': request_id}
    )
    
    # ... ç”Ÿæˆ ...
    
    logger.info(
        f"Generated {tokens} tokens in {latency:.2f}s",
        extra={'request_id': request_id}
    )
```

---

## ğŸ”’ ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå®‰å…¨å’Œè®¤è¯

### ğŸ” APIè®¤è¯

```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt

security = HTTPBearer()

SECRET_KEY = "your-secret-key"  # åº”è¯¥ä»ç¯å¢ƒå˜é‡è¯»å–

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """éªŒè¯JWT token"""
    try:
        token = credentials.credentials
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        return payload
    except jwt.ExpiredSignatureError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token expired"
        )
    except jwt.InvalidTokenError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token"
        )

@app.post("/generate")
async def generate(
    request: GenerateRequest,
    user = Depends(verify_token)  # éœ€è¦è®¤è¯
):
    # ... ç”Ÿæˆé€»è¾‘ ...
    pass
```

### ğŸ›¡ï¸ é€Ÿç‡é™åˆ¶

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/generate")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡
async def generate(request: Request, gen_request: GenerateRequest):
    # ... ç”Ÿæˆé€»è¾‘ ...
    pass
```

---

## ğŸŒ ç¬¬å…«éƒ¨åˆ†ï¼šäº‘å¹³å°éƒ¨ç½²

### â˜ï¸ AWSéƒ¨ç½²

**ä½¿ç”¨Amazon SageMakerï¼š**

```python
# deploy_sagemaker.py

from sagemaker.pytorch import PyTorchModel
from sagemaker import get_execution_role

role = get_execution_role()

# åˆ›å»ºæ¨¡å‹
pytorch_model = PyTorchModel(
    model_data='s3://my-bucket/model.tar.gz',
    role=role,
    entry_point='inference.py',
    framework_version='2.0',
    py_version='py310',
)

# éƒ¨ç½²
predictor = pytorch_model.deploy(
    instance_type='ml.g4dn.xlarge',  # GPUå®ä¾‹
    initial_instance_count=1,
)

# è°ƒç”¨
result = predictor.predict({
    'prompt': 'Hello, world',
    'max_tokens': 100
})
```

### ğŸ”· Azureéƒ¨ç½²

**ä½¿ç”¨Azure MLï¼š**

```python
# deploy_azure.py

from azureml.core import Workspace, Model
from azureml.core.webservice import AciWebservice, Webservice
from azureml.core.model import InferenceConfig

# è¿æ¥workspace
ws = Workspace.from_config()

# æ³¨å†Œæ¨¡å‹
model = Model.register(
    workspace=ws,
    model_path='out/ckpt.pt',
    model_name='nanogpt'
)

# éƒ¨ç½²é…ç½®
aci_config = AciWebservice.deploy_configuration(
    cpu_cores=2,
    memory_gb=8,
    gpu_cores=1
)

# éƒ¨ç½²
service = Model.deploy(
    workspace=ws,
    name='nanogpt-service',
    models=[model],
    inference_config=InferenceConfig(...),
    deployment_config=aci_config
)

service.wait_for_deployment(show_output=True)
print(service.scoring_uri)
```

---

## ğŸ’¡ ç¬¬ä¹éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ

### âœ… éƒ¨ç½²æ¸…å•

```python
â–¡ æ¨¡å‹ä¼˜åŒ–
  â–¡ é‡åŒ– (INT8)
  â–¡ KV Cache
  â–¡ æ‰¹å¤„ç†
  
â–¡ å®¹å™¨åŒ–
  â–¡ Dockerfile
  â–¡ Docker Compose
  â–¡ å¥åº·æ£€æŸ¥
  
â–¡ APIè®¾è®¡
  â–¡ æ¸…æ™°çš„ç«¯ç‚¹
  â–¡ è¯·æ±‚éªŒè¯
  â–¡ é”™è¯¯å¤„ç†
  â–¡ æ–‡æ¡£ (Swagger)
  
â–¡ æ€§èƒ½
  â–¡ å¼‚æ­¥å¤„ç†
  â–¡ è¿æ¥æ± 
  â–¡ ç¼“å­˜
  
â–¡ å®‰å…¨
  â–¡ è®¤è¯
  â–¡ é€Ÿç‡é™åˆ¶
  â–¡ è¾“å…¥éªŒè¯
  
â–¡ ç›‘æ§
  â–¡ æ—¥å¿—
  â–¡ æŒ‡æ ‡ (Prometheus)
  â–¡ å‘Šè­¦
  
â–¡ å¯é æ€§
  â–¡ è´Ÿè½½å‡è¡¡
  â–¡ è‡ªåŠ¨æ‰©å±•
  â–¡ æ•…éšœæ¢å¤
```

### ğŸ¯ æ€§èƒ½è°ƒä¼˜

```python
ä¼˜å…ˆçº§æ’åº:

1. æ¨¡å‹é‡åŒ– (INT8)
   å½±å“: 4xå‹ç¼©, 2xåŠ é€Ÿ
   éš¾åº¦: â­
   
2. KV Cache
   å½±å“: 2-3xåŠ é€Ÿ
   éš¾åº¦: â­â­
   
3. æ‰¹å¤„ç†
   å½±å“: 5-10xåå
   éš¾åº¦: â­â­â­
   
4. vLLM/TensorRT
   å½±å“: 10-20xåå
   éš¾åº¦: â­â­
   
5. å¤šGPUéƒ¨ç½²
   å½±å“: Nxåå
   éš¾åº¦: â­â­â­â­
```

---

## ğŸ› ç¬¬åéƒ¨åˆ†ï¼šæ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

```python
é—®é¢˜1: OOM (Out of Memory)
è§£å†³:
  - å‡å°batch_size
  - å¯ç”¨KV Cacheå¤ç”¨
  - ä½¿ç”¨é‡åŒ–æ¨¡å‹
  - å¢åŠ GPUå†…å­˜

é—®é¢˜2: æ¨ç†æ…¢
è§£å†³:
  - å¯ç”¨æ‰¹å¤„ç†
  - ä½¿ç”¨é‡åŒ–
  - æ£€æŸ¥CPUç“¶é¢ˆ
  - ä½¿ç”¨vLLM/TensorRT

é—®é¢˜3: å¹¶å‘è¯·æ±‚å¤±è´¥
è§£å†³:
  - å¢åŠ workeræ•°é‡
  - ä½¿ç”¨å¼‚æ­¥å¤„ç†
  - æ·»åŠ è¯·æ±‚é˜Ÿåˆ—
  - è´Ÿè½½å‡è¡¡

é—®é¢˜4: æ¨¡å‹é¢„æµ‹ä¸ä¸€è‡´
è§£å†³:
  - æ£€æŸ¥éšæœºç§å­
  - éªŒè¯æ¨¡å‹ç‰ˆæœ¬
  - ç¡®è®¤æ¸©åº¦å‚æ•°
  - æ£€æŸ¥tokenizer

é—®é¢˜5: å®¹å™¨å¯åŠ¨å¤±è´¥
è§£å†³:
  - æ£€æŸ¥GPUé©±åŠ¨
  - éªŒè¯é•œåƒ
  - æŸ¥çœ‹æ—¥å¿—
  - æ£€æŸ¥èµ„æºé™åˆ¶
```

---

## ğŸ“ æ€»ç»“

### âœ¨ æ ¸å¿ƒè¦ç‚¹

```python
1. éƒ¨ç½²å±‚çº§
   æœ¬åœ°è„šæœ¬ â†’ APIæœåŠ¡ â†’ ç”Ÿäº§çº§ â†’ ä¸“ä¸šå¹³å°
   
2. å…³é”®æŠ€æœ¯
   - FastAPI: ç®€å•æ˜“ç”¨
   - Docker: å®¹å™¨åŒ–
   - vLLM: é«˜æ€§èƒ½æ¨ç†
   - TensorRT: æè‡´ä¼˜åŒ–
   
3. æ€§èƒ½ä¼˜åŒ–
   é‡åŒ– + KV Cache + æ‰¹å¤„ç† = 20x+æå‡
   
4. ç›‘æ§å¿…å¤‡
   - æ—¥å¿—
   - æŒ‡æ ‡ (Prometheus)
   - å‘Šè­¦
   
5. å®‰å…¨æªæ–½
   - è®¤è¯
   - é€Ÿç‡é™åˆ¶
   - è¾“å…¥éªŒè¯
```

### ğŸ¯ æ¨èæ–¹æ¡ˆ

```python
ä½ çš„åœºæ™¯ â†’ æ¨èæ–¹æ¡ˆ

ä¸ªäººé¡¹ç›®/åŸå‹:
  â†’ FastAPI + Docker
  â†’ ç®€å•å¿«é€Ÿ

å°å›¢é˜Ÿ/å†…éƒ¨å·¥å…·:
  â†’ FastAPI + Docker + Nginx
  â†’ è´Ÿè½½å‡è¡¡

ç”Ÿäº§ç¯å¢ƒ (äº‘ç«¯):
  â†’ vLLM + Kubernetes
  â†’ é«˜æ€§èƒ½ + è‡ªåŠ¨æ‰©å±•

è¾¹ç¼˜éƒ¨ç½²:
  â†’ TensorRT + INT8
  â†’ æè‡´ä¼˜åŒ–

ä¼ä¸šçº§:
  â†’ Triton Inference Server
  â†’ å¤šæ¨¡å‹ã€å¤šæ¡†æ¶
```

### ğŸš€ å®Œæ•´æµç¨‹

```python
1. å¼€å‘
   è®­ç»ƒæ¨¡å‹ â†’ éªŒè¯æ•ˆæœ
   
2. ä¼˜åŒ–
   é‡åŒ– â†’ æµ‹è¯• â†’ é€‰æ‹©æœ€ä½³é…ç½®
   
3. å°è£…
   åˆ›å»ºAPI â†’ å®¹å™¨åŒ– â†’ æµ‹è¯•
   
4. éƒ¨ç½²
   é€‰æ‹©å¹³å° â†’ éƒ¨ç½² â†’ éªŒè¯
   
5. ç›‘æ§
   æ—¥å¿— â†’ æŒ‡æ ‡ â†’ å‘Šè­¦
   
6. è¿­ä»£
   æ”¶é›†åé¦ˆ â†’ ä¼˜åŒ– â†’ æ›´æ–°
```

---

**è®°ä½ï¼š**

> éƒ¨ç½²ä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯èµ·ç‚¹ã€‚
> ä»ç®€å•å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–ã€‚
> ç›‘æ§æ˜¯å…³é”®ï¼Œç”¨æ•°æ®é©±åŠ¨å†³ç­–ã€‚
>
> ä¸€ä¸ªå¥½çš„éƒ¨ç½²æ–¹æ¡ˆï¼Œåº”è¯¥ï¼š
> - å¿«é€Ÿè¿­ä»£
> - ç¨³å®šå¯é   
> - æ˜“äºç›‘æ§
> - æˆæœ¬å¯æ§

ğŸ‰ æ­å–œä½ å®Œæˆäº†ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´å­¦ä¹ ï¼
