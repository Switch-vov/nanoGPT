# ç¬¬04ç« ï¼šå®Œæ•´æŒ‡å—ä¸å®éªŒ - ä»ç†è®ºåˆ°å®è·µ

> **å­¦ä¹ ç›®æ ‡**ï¼šé€šè¿‡ç³»ç»Ÿæ€§å®éªŒæŒæ¡GPTè®­ç»ƒçš„å®Œæ•´æµç¨‹  
> **éš¾åº¦ç­‰çº§**ï¼šğŸŒ¿ è¿›é˜¶  
> **é¢„è®¡æ—¶é—´**ï¼š40-50åˆ†é’Ÿï¼ˆä¸å«å®éªŒæ—¶é—´ï¼‰  
> **å‰ç½®çŸ¥è¯†**ï¼šç¬¬01-03ç« 

---

## ğŸ¯ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

å­¦å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… æŒæ¡å®Œæ•´çš„è®­ç»ƒæµç¨‹å’Œå…³é”®æ­¥éª¤
- âœ… è®¾è®¡å’Œæ‰§è¡Œç³»ç»Ÿæ€§å®éªŒ
- âœ… åˆ†æå®éªŒç»“æœï¼Œç»˜åˆ¶lossæ›²çº¿
- âœ… è¯Šæ–­å’Œè§£å†³å¸¸è§è®­ç»ƒé—®é¢˜
- âœ… ä¼˜åŒ–è®­ç»ƒé€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½
- âœ… å®Œæˆç¬¬ä¸€æ¬¡æˆåŠŸçš„æ¨¡å‹è®­ç»ƒ

---

## ğŸ’­ å¼€å§‹ä¹‹å‰ï¼šä¸ºä»€ä¹ˆè¦åšå®éªŒï¼Ÿ

æƒ³è±¡ä½ åœ¨å­¦å¼€è½¦ï¼š

```
âŒ åªçœ‹ç†è®ºä¹¦ï¼š
  çŸ¥é“æ²¹é—¨ã€åˆ¹è½¦ã€æ–¹å‘ç›˜çš„ä½œç”¨
  ä½†ä»æ¥æ²¡å¼€è¿‡è½¦
  â†’ ä¸Šè·¯å°±æ…Œ

âœ… ç†è®º+å®è·µï¼š
  çœ‹ä¹¦å­¦ç†è®º
  ç»ƒä¹ åœºåå¤ç»ƒä¹ 
  ä¸åŒè·¯å†µéƒ½è¯•è¿‡
  â†’ çœŸæ­£ä¼šå¼€è½¦ï¼
```

**å­¦ä¹ AIä¹Ÿä¸€æ ·ï¼šç†è®º+å®éªŒæ‰èƒ½çœŸæ­£æŒæ¡ï¼**

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥ï¼ˆåŸºç¡€ï¼‰

### ğŸŒ± 1.1 æ•°æ®æµåŠ¨å…¨æ™¯å›¾

```
å®Œæ•´çš„æ•°æ®æµåŠ¨ï¼š

åŸå§‹æ–‡æœ¬
  "To be or not to be"
        â†“ ç¼–ç 
  [5, 10, 15, 20, 25, 10, 30]
        â†“ ä¿å­˜
  train.bin (äºŒè¿›åˆ¶æ–‡ä»¶)
        â†“ get_batch()
  X: [4, 8] å¼ é‡
  Y: [4, 8] å¼ é‡
        â†“ æ¨¡å‹
  logits: [4, 8, 65]
  loss: 2.45
        â†“ åå‘ä¼ æ’­
  æ¢¯åº¦
        â†“ ä¼˜åŒ–å™¨
  æ›´æ–°å‚æ•°
```

---

### ğŸŒ± 1.2 è®­ç»ƒä¸€æ¬¡è¿­ä»£çš„å®Œæ•´æµç¨‹

```python
# ä¼ªä»£ç ï¼ˆç®€åŒ–çš„train.pyæ ¸å¿ƒï¼‰

for iter_num in range(max_iters):
    
    # ========== é˜¶æ®µ1: å­¦ä¹ ç‡è°ƒåº¦ ==========
    lr = get_lr(iter_num)  # è®¡ç®—å½“å‰å­¦ä¹ ç‡
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    
    # ========== é˜¶æ®µ2: æ¢¯åº¦ç´¯ç§¯ ==========
    optimizer.zero_grad()  # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ¢¯åº¦
    
    for micro_step in range(gradient_accumulation_steps):
        X, Y = get_batch('train')  # åŠ è½½æ•°æ® [4, 8]
        
        with ctx:  # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            logits, loss = model(X, Y)  # å‰å‘ä¼ æ’­
            loss = loss / gradient_accumulation_steps  # å…³é”®ï¼
        
        scaler.scale(loss).backward()  # åå‘ä¼ æ’­ï¼Œæ¢¯åº¦ç´¯åŠ 
    
    # ========== é˜¶æ®µ3: æ¢¯åº¦è£å‰ªå’Œå‚æ•°æ›´æ–° ==========
    if grad_clip != 0.0:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
    
    scaler.step(optimizer)  # æ›´æ–°å‚æ•°
    scaler.update()  # æ›´æ–°ç¼©æ”¾å› å­
    
    # ========== é˜¶æ®µ4: å®šæœŸè¯„ä¼°å’Œä¿å­˜ ==========
    if iter_num % eval_interval == 0:
        val_loss = evaluate()
        if val_loss < best_val_loss:
            save_checkpoint()
    
    # ========== é˜¶æ®µ5: è®°å½•æ—¥å¿— ==========
    if iter_num % log_interval == 0:
        print(f"step {iter_num}: train loss {loss:.4f}")
```

---

### ğŸŒ± 1.3 é‡è¦å…¬å¼é€ŸæŸ¥

#### äº¤å‰ç†µæŸå¤±ï¼ˆCross Entropyï¼‰

```python
loss = -log(P(æ­£ç¡®ç­”æ¡ˆ))

ç›´è§‰ç†è§£ï¼š
  P(æ­£ç¡®) = 0.99 â†’ loss = -log(0.99) = 0.01  âœ… éå¸¸å¥½
  P(æ­£ç¡®) = 0.90 â†’ loss = -log(0.90) = 0.11  âœ… å¾ˆå¥½
  P(æ­£ç¡®) = 0.50 â†’ loss = -log(0.50) = 0.69  ğŸ¤” ä¸€èˆ¬
  P(æ­£ç¡®) = 0.10 â†’ loss = -log(0.10) = 2.30  âŒ å¾ˆå·®
  P(æ­£ç¡®) = 0.01 â†’ loss = -log(0.01) = 4.61  âŒ éå¸¸å·®

åˆå§‹lossï¼ˆéšæœºçŒœæµ‹ï¼‰ï¼š
  loss_random = -log(1/vocab_size)
              = -log(1/65)
              = 4.17
```

#### æ¢¯åº¦ä¸‹é™æ›´æ–°

```python
# æ ‡å‡†SGD
w_new = w_old - learning_rate Ã— gradient

# ä¾‹å­
w_new = 0.5 - 0.001 Ã— (-2.3)
      = 0.5 + 0.0023
      = 0.5023

# AdamWï¼ˆå®é™…ä½¿ç”¨ï¼‰
m = Î²1 Ã— m + (1-Î²1) Ã— gradient
v = Î²2 Ã— v + (1-Î²2) Ã— gradientÂ²
w = w - lr Ã— (m / âˆšv + weight_decay Ã— w)
```

#### å­¦ä¹ ç‡è°ƒåº¦

```python
# Warmupé˜¶æ®µ (step < warmup_iters)
lr = max_lr Ã— (step / warmup_iters)

# æ­£å¸¸è®­ç»ƒé˜¶æ®µ
lr = max_lr

# Cosine Decayé˜¶æ®µ (step > lr_decay_iters)
progress = (step - lr_decay_iters) / (max_iters - lr_decay_iters)
lr = min_lr + (max_lr - min_lr) Ã— 0.5 Ã— (1 + cos(Ï€ Ã— progress))
```

---

## ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šç³»ç»Ÿæ€§å®éªŒï¼ˆå®æˆ˜ï¼‰

### ğŸŒ¿ 2.1 å®éªŒ1ï¼šç†è§£é…ç½®å‚æ•°çš„å½±å“ â­

**ç›®æ ‡**ï¼šç›´è§‚æ„Ÿå—ä¸åŒå‚æ•°å¯¹è®­ç»ƒçš„å½±å“

**å®éªŒè®¾è®¡**ï¼š

```bash
# åˆ›å»ºå®éªŒç›®å½•
mkdir -p experiments

# åŸºçº¿å®éªŒ
python train.py config/train_shakespeare_char.py \
  --max_iters=1000 \
  --out_dir=experiments/exp1_baseline \
  2>&1 | tee experiments/exp1_baseline.log

# å®éªŒ1A: æ›´å¤§çš„å­¦ä¹ ç‡
python train.py config/train_shakespeare_char.py \
  --max_iters=1000 \
  --learning_rate=5e-3 \
  --out_dir=experiments/exp1_large_lr \
  2>&1 | tee experiments/exp1_large_lr.log

# å®éªŒ1B: æ›´å°çš„å­¦ä¹ ç‡
python train.py config/train_shakespeare_char.py \
  --max_iters=1000 \
  --learning_rate=1e-4 \
  --out_dir=experiments/exp1_small_lr \
  2>&1 | tee experiments/exp1_small_lr.log

# å®éªŒ1C: æ›´å°çš„batch
python train.py config/train_shakespeare_char.py \
  --max_iters=1000 \
  --batch_size=16 \
  --out_dir=experiments/exp1_small_batch \
  2>&1 | tee experiments/exp1_small_batch.log

# å®éªŒ1D: æ›´å¤§çš„æ¨¡å‹
python train.py config/train_shakespeare_char.py \
  --max_iters=1000 \
  --n_layer=8 \
  --n_embd=512 \
  --out_dir=experiments/exp1_large_model \
  2>&1 | tee experiments/exp1_large_model.log
```

**è§‚å¯Ÿé‡ç‚¹**ï¼š

| å®éªŒ | è§‚å¯ŸæŒ‡æ ‡ | é¢„æœŸç»“æœ |
|------|---------|---------|
| åŸºçº¿ | lossä¸‹é™é€Ÿåº¦ | ç¨³å®šä¸‹é™ |
| å¤§å­¦ä¹ ç‡ | æ˜¯å¦æ”¶æ•› | å¯èƒ½éœ‡è¡ |
| å°å­¦ä¹ ç‡ | æ”¶æ•›é€Ÿåº¦ | å¾ˆæ…¢ |
| å°batch | è®­ç»ƒç¨³å®šæ€§ | å™ªå£°å¤§ |
| å¤§æ¨¡å‹ | æœ€ç»ˆloss | æ›´ä½ |

**åˆ†ææ¨¡æ¿**ï¼š

```markdown
## å®éªŒ1ç»“æœåˆ†æ

### å®éªŒ1A (å¤§å­¦ä¹ ç‡ 5e-3):
- åˆæœŸlossä¸‹é™: [å¿«/æ…¢]
- æ˜¯å¦æ”¶æ•›: [æ˜¯/å¦/éœ‡è¡]
- æœ€ç»ˆval loss: [æ•°å€¼]
- è®­ç»ƒæ—¶é—´: [æ—¶é—´]
- ç»“è®º: [...]

### å®éªŒ1B (å°å­¦ä¹ ç‡ 1e-4):
- åˆæœŸlossä¸‹é™: [å¿«/æ…¢]
- æ˜¯å¦æ”¶æ•›: [æ˜¯/å¦]
- æœ€ç»ˆval loss: [æ•°å€¼]
- è®­ç»ƒæ—¶é—´: [æ—¶é—´]
- ç»“è®º: [...]

### å¯¹æ¯”æ€»ç»“:
- æœ€ä½³å­¦ä¹ ç‡: [...]
- åŸå› åˆ†æ: [...]
```

---

### ğŸŒ¿ 2.2 å®éªŒ2ï¼šæ¢¯åº¦ç´¯ç§¯éªŒè¯ â­â­

**ç›®æ ‡**ï¼šéªŒè¯æ¢¯åº¦ç´¯ç§¯ç¡®å®ç­‰ä»·äºå¤§batch

**ç†è®ºé¢„æµ‹**ï¼š

```python
æœ‰æ•ˆbatch_size = batch_size Ã— gradient_accumulation_steps

æ–¹æ³•A: batch_size=64, grad_accum=1 â†’ æœ‰æ•ˆbatch=64
æ–¹æ³•B: batch_size=16, grad_accum=4 â†’ æœ‰æ•ˆbatch=64

é¢„æœŸï¼šä¸¤è€…çš„lossæ›²çº¿åº”è¯¥å‡ ä¹ç›¸åŒï¼
```

**å®éªŒè®¾è®¡**ï¼š

```bash
# æ–¹æ³•A: ç›´æ¥ç”¨å¤§batch
python train.py config/train_shakespeare_char.py \
  --batch_size=64 \
  --gradient_accumulation_steps=1 \
  --max_iters=500 \
  --out_dir=experiments/exp2_large_batch \
  2>&1 | tee experiments/exp2_large_batch.log

# æ–¹æ³•B: å°batch + æ¢¯åº¦ç´¯ç§¯
python train.py config/train_shakespeare_char.py \
  --batch_size=16 \
  --gradient_accumulation_steps=4 \
  --max_iters=500 \
  --out_dir=experiments/exp2_grad_accum \
  2>&1 | tee experiments/exp2_grad_accum.log
```

**éªŒè¯æ–¹æ³•**ï¼š

```python
# ç»˜åˆ¶å¯¹æ¯”å›¾
import matplotlib.pyplot as plt

# è§£æä¸¤ä¸ªå®éªŒçš„loss
steps_a, _, val_loss_a = parse_log('experiments/exp2_large_batch.log')
steps_b, _, val_loss_b = parse_log('experiments/exp2_grad_accum.log')

plt.figure(figsize=(10, 6))
plt.plot(steps_a, val_loss_a, label='Large Batch (64)', marker='o')
plt.plot(steps_b, val_loss_b, label='Grad Accum (16Ã—4)', marker='s')
plt.xlabel('Steps')
plt.ylabel('Validation Loss')
plt.title('Gradient Accumulation Verification')
plt.legend()
plt.grid(True)
plt.savefig('experiments/exp2_comparison.png')

# è®¡ç®—å·®å¼‚
import numpy as np
diff = np.abs(np.array(val_loss_a) - np.array(val_loss_b))
print(f"å¹³å‡å·®å¼‚: {diff.mean():.6f}")
print(f"æœ€å¤§å·®å¼‚: {diff.max():.6f}")
# é¢„æœŸï¼šå·®å¼‚åº”è¯¥å¾ˆå°ï¼ˆ<0.01ï¼‰
```

---

### ğŸŒ¿ 2.3 å®éªŒ3ï¼šè¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ– â­â­

**ç›®æ ‡**ï¼šç†è§£è¿‡æ‹Ÿåˆï¼Œå­¦ä¼šä½¿ç”¨dropoutå’Œweight_decay

**è¯†åˆ«è¿‡æ‹Ÿåˆ**ï¼š

```python
å¥åº·çš„è®­ç»ƒï¼š
  train_loss = 1.4
  val_loss = 1.5
  å·®è· = 0.1  âœ… å¾ˆå¥½

è½»å¾®è¿‡æ‹Ÿåˆï¼š
  train_loss = 1.2
  val_loss = 1.5
  å·®è· = 0.3  ğŸ¤” å¯æ¥å—

ä¸¥é‡è¿‡æ‹Ÿåˆï¼š
  train_loss = 0.8
  val_loss = 1.8
  å·®è· = 1.0  âŒ éœ€è¦æ­£åˆ™åŒ–
```

**å®éªŒè®¾è®¡**ï¼š

```bash
# åŸºçº¿ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.0 \
  --weight_decay=0.0 \
  --out_dir=experiments/exp3_overfit

# åŠ dropout
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.2 \
  --weight_decay=0.0 \
  --out_dir=experiments/exp3_dropout

# åŠ weight_decay
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.0 \
  --weight_decay=0.1 \
  --out_dir=experiments/exp3_weight_decay

# ä¸¤è€…éƒ½åŠ 
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.2 \
  --weight_decay=0.1 \
  --out_dir=experiments/exp3_both
```

**åˆ†ææŒ‡æ ‡**ï¼š

```python
# è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦
def calculate_overfitting(train_loss, val_loss):
    gap = val_loss - train_loss
    ratio = val_loss / train_loss
    
    if gap < 0.1:
        return "å¥åº·"
    elif gap < 0.3:
        return "è½»å¾®è¿‡æ‹Ÿåˆ"
    elif gap < 0.5:
        return "ä¸­åº¦è¿‡æ‹Ÿåˆ"
    else:
        return "ä¸¥é‡è¿‡æ‹Ÿåˆ"

# å¯¹æ¯ä¸ªå®éªŒè®¡ç®—
experiments = ['overfit', 'dropout', 'weight_decay', 'both']
for exp in experiments:
    # è·å–æœ€ç»ˆçš„train_losså’Œval_loss
    status = calculate_overfitting(train_loss, val_loss)
    print(f"{exp}: {status}")
```

---

### ğŸŒ³ 2.4 å®éªŒ4ï¼šå­¦ä¹ ç‡è°ƒåº¦ â­â­â­

**ç›®æ ‡**ï¼šç†è§£warmupå’Œdecayçš„ä½œç”¨

**ç†è®ºèƒŒæ™¯**ï¼š

```
Warmupçš„ä½œç”¨ï¼š
  - åˆæœŸå‚æ•°éšæœºï¼Œæ¢¯åº¦ä¸ç¨³å®š
  - å°å­¦ä¹ ç‡è®©æ¨¡å‹"çƒ­èº«"
  - é¿å…åˆæœŸçš„å¤§å¹…éœ‡è¡

Decayçš„ä½œç”¨ï¼š
  - åæœŸæ¥è¿‘æœ€ä¼˜ç‚¹
  - å°å­¦ä¹ ç‡ç²¾ç»†è°ƒæ•´
  - é¿å…åœ¨æœ€ä¼˜ç‚¹é™„è¿‘éœ‡è¡
```

**å®éªŒè®¾è®¡**ï¼š

```bash
# æ— warmup, æ— decay
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=0 \
  --decay_lr=False \
  --out_dir=experiments/exp4_no_schedule

# åªæœ‰warmup
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=100 \
  --decay_lr=False \
  --out_dir=experiments/exp4_warmup_only

# åªæœ‰decay
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=0 \
  --decay_lr=True \
  --lr_decay_iters=2000 \
  --out_dir=experiments/exp4_decay_only

# ä¸¤è€…éƒ½æœ‰ï¼ˆæ¨èï¼‰âœ…
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=100 \
  --decay_lr=True \
  --lr_decay_iters=2000 \
  --out_dir=experiments/exp4_full_schedule
```

**è§‚å¯Ÿé‡ç‚¹**ï¼š

```python
# ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
def plot_lr_schedule(max_iters, warmup_iters, decay_lr):
    lrs = []
    for step in range(max_iters):
        lr = get_lr(step, max_iters, warmup_iters, decay_lr)
        lrs.append(lr)
    
    plt.plot(lrs)
    plt.xlabel('Steps')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedule')
    plt.grid(True)
    plt.show()

# å…³æ³¨ï¼š
# - å‰100æ­¥ï¼šwarmupçš„å½±å“
# - æœ€å200æ­¥ï¼šdecayçš„å½±å“
# - æ•´ä½“æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§
```

---

### ğŸŒ³ 2.5 å®éªŒ5ï¼šæ¨¡å‹è§„æ¨¡å®éªŒ â­â­â­

**ç›®æ ‡**ï¼šç†è§£æ¨¡å‹å®¹é‡ä¸æ€§èƒ½çš„å…³ç³»

**å‚æ•°é‡è®¡ç®—**ï¼š

```python
# ç²—ç•¥ä¼°ç®—å…¬å¼
params â‰ˆ n_layer Ã— 16 Ã— n_embdÂ² + vocab_size Ã— n_embd

# å…·ä½“è®¡ç®—
Tiny:   2å±‚ Ã— 16 Ã— 128Â²  + 65 Ã— 128  â‰ˆ 532K
Small:  4å±‚ Ã— 16 Ã— 256Â²  + 65 Ã— 256  â‰ˆ 4.2M
Medium: 6å±‚ Ã— 16 Ã— 384Â²  + 65 Ã— 384  â‰ˆ 14.2M
Large:  8å±‚ Ã— 16 Ã— 512Â²  + 65 Ã— 512  â‰ˆ 33.6M
XLarge: 12å±‚ Ã— 16 Ã— 768Â² + 65 Ã— 768  â‰ˆ 113M
```

**å®éªŒè®¾è®¡**ï¼š

```bash
# è¶…å°æ¨¡å‹
python train.py config/train_shakespeare_char.py \
  --n_layer=2 --n_head=2 --n_embd=128 \
  --max_iters=3000 \
  --out_dir=experiments/exp5_tiny

# å°æ¨¡å‹
python train.py config/train_shakespeare_char.py \
  --n_layer=4 --n_head=4 --n_embd=256 \
  --max_iters=3000 \
  --out_dir=experiments/exp5_small

# ä¸­æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
python train.py config/train_shakespeare_char.py \
  --n_layer=6 --n_head=6 --n_embd=384 \
  --max_iters=3000 \
  --out_dir=experiments/exp5_medium

# å¤§æ¨¡å‹
python train.py config/train_shakespeare_char.py \
  --n_layer=8 --n_head=8 --n_embd=512 \
  --max_iters=3000 \
  --out_dir=experiments/exp5_large
```

**åˆ†æç»´åº¦**ï¼š

| æ¨¡å‹ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æœ€ç»ˆloss | ç”Ÿæˆè´¨é‡ | æ˜¾å­˜å ç”¨ |
|------|--------|---------|---------|---------|---------|
| Tiny | 532K | åŸºå‡†Ã—0.5 | 1.8 | è¾ƒå·® | 500MB |
| Small | 4.2M | åŸºå‡†Ã—1.0 | 1.5 | ä¸€èˆ¬ | 1GB |
| Medium | 14.2M | åŸºå‡†Ã—2.0 | 1.3 | è‰¯å¥½ | 2GB |
| Large | 33.6M | åŸºå‡†Ã—4.0 | 1.2 | ä¼˜ç§€ | 4GB |

**å…³é”®å‘ç°**ï¼š

```
Scaling Lawï¼ˆç»éªŒè§„å¾‹ï¼‰ï¼š
  - å‚æ•°é‡å¢åŠ 10å€ â†’ lossé™ä½çº¦0.2
  - ä½†è®­ç»ƒæ—¶é—´ä¹Ÿå¢åŠ çº¦4å€
  - å­˜åœ¨æ”¶ç›Šé€’å‡æ•ˆåº”

æœ€ä½³é€‰æ‹©ï¼š
  - å­¦ä¹ /å®éªŒï¼šSmall (4M)
  - é«˜è´¨é‡ç”Ÿæˆï¼šMedium (14M)
  - è¿½æ±‚æè‡´ï¼šLarge (33M)
```

---

## ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»“æœåˆ†æå·¥å…·ï¼ˆå®ç”¨ï¼‰

### ğŸŒ³ 3.1 å·¥å…·1ï¼šç»˜åˆ¶Lossæ›²çº¿

åˆ›å»ºæ–‡ä»¶ `tools/plot_loss.py`ï¼š

```python
#!/usr/bin/env python3
"""
è®­ç»ƒç»“æœå¯è§†åŒ–å·¥å…·
"""
import matplotlib.pyplot as plt
import re
import sys

def parse_log(log_file):
    """è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–lossæ•°æ®"""
    steps = []
    train_losses = []
    val_losses = []
    
    with open(log_file, 'r') as f:
        for line in f:
            # åŒ¹é…æ ¼å¼ï¼š"step 0: train loss 4.1234, val loss 4.2345"
            match = re.search(
                r'step (\d+): train loss ([\d.]+), val loss ([\d.]+)', 
                line
            )
            if match:
                steps.append(int(match.group(1)))
                train_losses.append(float(match.group(2)))
                val_losses.append(float(match.group(3)))
    
    return steps, train_losses, val_losses

def plot_single_experiment(log_file, output_file='loss_curve.png'):
    """ç»˜åˆ¶å•ä¸ªå®éªŒçš„lossæ›²çº¿"""
    steps, train_loss, val_loss = parse_log(log_file)
    
    plt.figure(figsize=(12, 6))
    plt.plot(steps, train_loss, label='Train Loss', linewidth=2)
    plt.plot(steps, val_loss, label='Val Loss', linewidth=2)
    plt.xlabel('Steps', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Training Progress', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_file, dpi=150)
    print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    plt.show()

def plot_comparison(experiments, output_file='comparison.png'):
    """æ¯”è¾ƒå¤šä¸ªå®éªŒ"""
    plt.figure(figsize=(14, 7))
    
    for name, log_file in experiments.items():
        steps, _, val_loss = parse_log(log_file)
        plt.plot(steps, val_loss, label=name, linewidth=2, marker='o', markersize=3)
    
    plt.xlabel('Steps', fontsize=12)
    plt.ylabel('Validation Loss', fontsize=12)
    plt.title('Experiment Comparison', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_file, dpi=150)
    print(f"å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_file}")
    plt.show()

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python plot_loss.py <log_file>")
        print("æˆ–è€…: python plot_loss.py compare exp1.log exp2.log ...")
        sys.exit(1)
    
    if sys.argv[1] == 'compare':
        # æ¯”è¾ƒæ¨¡å¼
        experiments = {}
        for log_file in sys.argv[2:]:
            name = log_file.split('/')[-1].replace('.log', '')
            experiments[name] = log_file
        plot_comparison(experiments)
    else:
        # å•ä¸ªå®éªŒæ¨¡å¼
        plot_single_experiment(sys.argv[1])
```

**ä½¿ç”¨æ–¹æ³•**ï¼š

```bash
# å•ä¸ªå®éªŒ
python tools/plot_loss.py experiments/exp1_baseline.log

# æ¯”è¾ƒå¤šä¸ªå®éªŒ
python tools/plot_loss.py compare \
  experiments/exp1_baseline.log \
  experiments/exp1_large_lr.log \
  experiments/exp1_small_lr.log
```

---

### ğŸŒ³ 3.2 å·¥å…·2ï¼šå®éªŒç»“æœæ±‡æ€»

åˆ›å»ºæ–‡ä»¶ `tools/summarize_experiments.py`ï¼š

```python
#!/usr/bin/env python3
"""
å®éªŒç»“æœæ±‡æ€»å·¥å…·
"""
import os
import re
from tabulate import tabulate

def extract_final_loss(log_file):
    """æå–æœ€ç»ˆçš„trainå’Œval loss"""
    with open(log_file, 'r') as f:
        lines = f.readlines()
    
    # ä»åå¾€å‰æ‰¾æœ€åä¸€æ¬¡è¯„ä¼°
    for line in reversed(lines):
        match = re.search(r'train loss ([\d.]+), val loss ([\d.]+)', line)
        if match:
            return float(match.group(1)), float(match.group(2))
    
    return None, None

def extract_training_time(log_file):
    """æå–è®­ç»ƒæ—¶é—´"""
    with open(log_file, 'r') as f:
        content = f.read()
    
    # æŸ¥æ‰¾ç±»ä¼¼ "training took 123.45 seconds" çš„è¡Œ
    match = re.search(r'training took ([\d.]+)', content)
    if match:
        return float(match.group(1))
    return None

def summarize_experiments(exp_dir='experiments'):
    """æ±‡æ€»æ‰€æœ‰å®éªŒç»“æœ"""
    results = []
    
    for exp_name in sorted(os.listdir(exp_dir)):
        exp_path = os.path.join(exp_dir, exp_name)
        if not os.path.isdir(exp_path):
            continue
        
        log_file = os.path.join(exp_path, 'training.log')
        if not os.path.exists(log_file):
            log_file = f"{exp_path}.log"
            if not os.path.exists(log_file):
                continue
        
        train_loss, val_loss = extract_final_loss(log_file)
        training_time = extract_training_time(log_file)
        
        if train_loss is not None:
            results.append([
                exp_name,
                f"{train_loss:.4f}",
                f"{val_loss:.4f}",
                f"{val_loss - train_loss:.4f}",
                f"{training_time:.1f}s" if training_time else "N/A"
            ])
    
    # æ‰“å°è¡¨æ ¼
    headers = ["å®éªŒåç§°", "Train Loss", "Val Loss", "å·®è·", "è®­ç»ƒæ—¶é—´"]
    print("\n" + "="*80)
    print("å®éªŒç»“æœæ±‡æ€»")
    print("="*80)
    print(tabulate(results, headers=headers, tablefmt="grid"))
    print("="*80 + "\n")

if __name__ == '__main__':
    summarize_experiments()
```

---

## ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šè°ƒè¯•æŠ€å·§ï¼ˆè¿›é˜¶ï¼‰

### ğŸŒ³ 4.1 æ£€æŸ¥æ•°æ®åŠ è½½

```python
# åœ¨train.pyå¼€å¤´æ·»åŠ ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ï¼‰
def debug_data_loading():
    """è°ƒè¯•æ•°æ®åŠ è½½"""
    print("\n" + "="*60)
    print("æ•°æ®åŠ è½½è°ƒè¯•")
    print("="*60)
    
    X, Y = get_batch('train')
    print(f"âœ… X shape: {X.shape}")
    print(f"âœ… Y shape: {Y.shape}")
    print(f"âœ… X dtype: {X.dtype}")
    print(f"âœ… Y dtype: {Y.dtype}")
    print(f"âœ… X device: {X.device}")
    
    # æ£€æŸ¥æ•°å€¼èŒƒå›´
    print(f"âœ… X range: [{X.min().item()}, {X.max().item()}]")
    print(f"âœ… Y range: [{Y.min().item()}, {Y.max().item()}]")
    
    # è§£ç æŸ¥çœ‹å®é™…æ–‡æœ¬
    if meta_vocab_size:
        with open(meta_path, 'rb') as f:
            meta = pickle.load(f)
        itos = meta['itos']
        
        print(f"\næ ·æœ¬0:")
        x_text = ''.join([itos[int(i)] for i in X[0]])
        y_text = ''.join([itos[int(i)] for i in Y[0]])
        print(f"  X: '{x_text}'")
        print(f"  Y: '{y_text}'")
        
        # éªŒè¯Yæ˜¯Xå³ç§»1ä½
        assert x_text[1:] == y_text[:-1], "âŒ Yä¸æ˜¯Xå³ç§»1ä½ï¼"
        print(f"  âœ… éªŒè¯é€šè¿‡ï¼šY = Xå³ç§»1ä½")
    
    print("="*60 + "\n")

# åœ¨è®­ç»ƒå¼€å§‹å‰è°ƒç”¨
if iter_num == 0:
    debug_data_loading()
```

---

### ğŸŒ³ 4.2 ç›‘æ§æ¢¯åº¦å¥åº·åº¦

```python
def monitor_gradients(model, iter_num):
    """ç›‘æ§æ¢¯åº¦ç»Ÿè®¡"""
    if iter_num % 100 != 0:
        return
    
    total_norm = 0
    max_grad = 0
    min_grad = float('inf')
    grad_stats = []
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2).item()
            total_norm += param_norm ** 2
            
            grad_max = param.grad.abs().max().item()
            grad_min = param.grad.abs().min().item()
            
            max_grad = max(max_grad, grad_max)
            min_grad = min(min_grad, grad_min)
            
            grad_stats.append({
                'name': name,
                'norm': param_norm,
                'max': grad_max,
                'mean': param.grad.mean().item()
            })
    
    total_norm = total_norm ** 0.5
    
    print(f"\næ¢¯åº¦ç»Ÿè®¡ (step {iter_num}):")
    print(f"  æ€»èŒƒæ•°: {total_norm:.4f}")
    print(f"  æœ€å¤§æ¢¯åº¦: {max_grad:.6f}")
    print(f"  æœ€å°æ¢¯åº¦: {min_grad:.6f}")
    
    # è­¦å‘Šæ£€æŸ¥
    if total_norm > 10:
        print(f"  âš ï¸  è­¦å‘Šï¼šæ¢¯åº¦å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦é™ä½å­¦ä¹ ç‡æˆ–å¯ç”¨æ¢¯åº¦è£å‰ª")
    if total_norm < 0.001:
        print(f"  âš ï¸  è­¦å‘Šï¼šæ¢¯åº¦å¾ˆå°ï¼Œå¯èƒ½éœ€è¦å¢å¤§å­¦ä¹ ç‡")
    if max_grad > 100:
        print(f"  âš ï¸  è­¦å‘Šï¼šå­˜åœ¨æå¤§æ¢¯åº¦ï¼Œå¯èƒ½ä¼šå¯¼è‡´NaN")

# åœ¨åå‘ä¼ æ’­åè°ƒç”¨
loss.backward()
monitor_gradients(model, iter_num)
```

---

### ğŸŒ³ 4.3 æ£€æŸ¥æ¨¡å‹è¾“å‡º

```python
def check_model_output(model, vocab_size):
    """æ£€æŸ¥æ¨¡å‹åˆå§‹è¾“å‡ºæ˜¯å¦æ­£å¸¸"""
    print("\n" + "="*60)
    print("æ¨¡å‹è¾“å‡ºæ£€æŸ¥")
    print("="*60)
    
    model.eval()
    with torch.no_grad():
        X_test, Y_test = get_batch('val')
        logits, loss = model(X_test, Y_test)
        
        print(f"âœ… Logits shape: {logits.shape}")
        print(f"âœ… Loss: {loss.item():.4f}")
        
        # ç†è®ºæœ€å¤§lossï¼ˆéšæœºçŒœæµ‹ï¼‰
        random_guess_loss = -math.log(1.0 / vocab_size)
        print(f"âœ… Random guess loss: {random_guess_loss:.4f}")
        
        # åˆå§‹lossåº”è¯¥æ¥è¿‘random_guess_loss
        diff = abs(loss.item() - random_guess_loss)
        if diff < 0.5:
            print(f"âœ… åˆå§‹lossæ­£å¸¸ï¼ˆå·®å¼‚: {diff:.4f}ï¼‰")
        else:
            print(f"âš ï¸  åˆå§‹losså¼‚å¸¸ï¼ˆå·®å¼‚: {diff:.4f}ï¼‰")
        
        # æ£€æŸ¥logitsçš„æ•°å€¼èŒƒå›´
        print(f"âœ… Logits range: [{logits.min().item():.2f}, {logits.max().item():.2f}]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if torch.isnan(logits).any():
            print(f"âŒ LogitsåŒ…å«NaNï¼")
        if torch.isinf(logits).any():
            print(f"âŒ LogitsåŒ…å«Infï¼")
    
    model.train()
    print("="*60 + "\n")

# åœ¨è®­ç»ƒå¼€å§‹å‰è°ƒç”¨
if iter_num == 0:
    check_model_output(model, vocab_size)
```

---

## ğŸ“ æ€»ç»“ä¸æ£€æŸ¥

### âœ… çŸ¥è¯†æ£€æŸ¥æ¸…å•

å®Œæˆå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€èƒ½åŠ›ï¼ˆå¿…é¡»æŒæ¡ï¼‰**
- [ ] èƒ½ç‹¬ç«‹è¿è¡Œä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒ
- [ ] ç†è§£è®­ç»ƒå¾ªç¯çš„æ¯ä¸ªæ­¥éª¤
- [ ] ä¼šæŸ¥çœ‹å’Œåˆ†ælossæ›²çº¿
- [ ] çŸ¥é“å¦‚ä½•è°ƒæ•´åŸºæœ¬å‚æ•°
- [ ] èƒ½è¯†åˆ«è®­ç»ƒæ˜¯å¦æ­£å¸¸

**è¿›é˜¶èƒ½åŠ›ï¼ˆå»ºè®®æŒæ¡ï¼‰**
- [ ] èƒ½è®¾è®¡ç³»ç»Ÿæ€§å®éªŒ
- [ ] ä¼šä½¿ç”¨å·¥å…·åˆ†æç»“æœ
- [ ] èƒ½è¯Šæ–­å¸¸è§è®­ç»ƒé—®é¢˜
- [ ] ç†è§£è¿‡æ‹Ÿåˆå’Œæ­£åˆ™åŒ–
- [ ] ä¼šä¼˜åŒ–è®­ç»ƒé€Ÿåº¦

**é«˜çº§èƒ½åŠ›ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰**
- [ ] èƒ½ç‹¬ç«‹å®Œæˆæ¨¡å‹è°ƒä¼˜
- [ ] ä¼šç¼–å†™è°ƒè¯•å’Œåˆ†æå·¥å…·
- [ ] èƒ½è§£å†³å¤æ‚çš„è®­ç»ƒé—®é¢˜
- [ ] ç†è§£ä¸åŒå‚æ•°çš„ç›¸äº’å½±å“
- [ ] èƒ½æŒ‡å¯¼ä»–äººè¿›è¡Œè®­ç»ƒ

### ğŸ“Š å®éªŒå®Œæˆæ£€æŸ¥è¡¨

```
â–¡ å®éªŒ1ï¼šå‚æ•°å½±å“å®éªŒ
  â–¡ åŸºçº¿å®éªŒ
  â–¡ å¤§å­¦ä¹ ç‡å®éªŒ
  â–¡ å°å­¦ä¹ ç‡å®éªŒ
  â–¡ å°batchå®éªŒ
  â–¡ å¤§æ¨¡å‹å®éªŒ
  â–¡ ç»˜åˆ¶å¯¹æ¯”å›¾
  â–¡ åˆ†æç»“è®º

â–¡ å®éªŒ2ï¼šæ¢¯åº¦ç´¯ç§¯éªŒè¯
  â–¡ å¤§batchå®éªŒ
  â–¡ æ¢¯åº¦ç´¯ç§¯å®éªŒ
  â–¡ éªŒè¯ç­‰ä»·æ€§
  â–¡ åˆ†æå·®å¼‚

â–¡ å®éªŒ3ï¼šè¿‡æ‹Ÿåˆå®éªŒ
  â–¡ æ— æ­£åˆ™åŒ–å®éªŒ
  â–¡ dropoutå®éªŒ
  â–¡ weight_decayå®éªŒ
  â–¡ ç»„åˆå®éªŒ
  â–¡ åˆ†ææ•ˆæœ

â–¡ å®éªŒ4ï¼šå­¦ä¹ ç‡è°ƒåº¦
  â–¡ æ— è°ƒåº¦å®éªŒ
  â–¡ warmupå®éªŒ
  â–¡ decayå®éªŒ
  â–¡ å®Œæ•´è°ƒåº¦å®éªŒ
  â–¡ å¯¹æ¯”åˆ†æ

â–¡ å®éªŒ5ï¼šæ¨¡å‹è§„æ¨¡
  â–¡ è¶…å°æ¨¡å‹
  â–¡ å°æ¨¡å‹
  â–¡ ä¸­æ¨¡å‹
  â–¡ å¤§æ¨¡å‹
  â–¡ æ€§èƒ½å¯¹æ¯”

â–¡ å·¥å…·ä½¿ç”¨
  â–¡ å®‰è£…plot_loss.py
  â–¡ ç»˜åˆ¶lossæ›²çº¿
  â–¡ å®éªŒç»“æœæ±‡æ€»
  â–¡ è°ƒè¯•å·¥å…·ä½¿ç”¨
```

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

ç°åœ¨ä½ å·²ç»æŒæ¡äº†å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼æ¥ä¸‹æ¥åº”è¯¥ï¼š

1. **05_model_architecture_deep_dive.md** - æ·±å…¥ç†è§£Transformeræ¶æ„
2. **ç»§ç»­å®éªŒ** - å°è¯•æ›´å¤šå‚æ•°ç»„åˆ
3. **ä¼˜åŒ–æ¨¡å‹** - è¿½æ±‚æ›´å¥½çš„æ€§èƒ½

### ğŸ’¡ å®è·µå»ºè®®

1. **å®Œæˆè‡³å°‘3ä¸ªå®éªŒ**ï¼šäº²è‡ªåŠ¨æ‰‹æ‰èƒ½çœŸæ­£ç†è§£
2. **è®°å½•å®éªŒæ—¥å¿—**ï¼šå»ºç«‹è‡ªå·±çš„å®éªŒç¬”è®°
3. **åˆ†äº«ç»éªŒ**ï¼šæ•™åˆ«äººæ˜¯æœ€å¥½çš„å­¦ä¹ æ–¹å¼
4. **æŒç»­ä¼˜åŒ–**ï¼šä¸æ–­å°è¯•æ–°çš„æƒ³æ³•

---

## ğŸ“š æ¨èèµ„æº

### ğŸ“– å»¶ä¼¸é˜…è¯»
- [æ·±åº¦å­¦ä¹ è°ƒå‚æŠ€å·§](https://karpathy.github.io/2019/04/25/recipe/)
- [å¦‚ä½•è°ƒè¯•ç¥ç»ç½‘ç»œ](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)
- [å®éªŒè®¾è®¡æœ€ä½³å®è·µ](https://arxiv.org/abs/1909.05858)

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- [Andrej Karpathy: è®­ç»ƒæŠ€å·§](https://www.youtube.com/watch?v=P6sfmUTpUmc)

### ğŸ”§ å®ç”¨å·¥å…·
- [TensorBoard](https://www.tensorflow.org/tensorboard) - å¯è§†åŒ–è®­ç»ƒ
- [Weights & Biases](https://wandb.ai/) - å®éªŒç®¡ç†
- [Optuna](https://optuna.org/) - è¶…å‚æ•°ä¼˜åŒ–

---

## ğŸ› å¸¸è§é—®é¢˜ FAQ

### Q1: Lossä¸€ç›´ä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

**è¯Šæ–­æµç¨‹**ï¼š
```python
1. æ£€æŸ¥æ•°æ®
   â–¡ Xå’ŒYæ˜¯å¦å¯¹åº”ï¼Ÿ
   â–¡ æ•°æ®èŒƒå›´æ˜¯å¦æ­£å¸¸ï¼Ÿ
   â–¡ æ˜¯å¦æœ‰é‡å¤æ•°æ®ï¼Ÿ

2. æ£€æŸ¥å­¦ä¹ ç‡
   â–¡ æ˜¯å¦å¤ªå°ï¼Ÿè¯•è¯•å¢å¤§10å€
   â–¡ æ˜¯å¦å¤ªå¤§ï¼Ÿè¯•è¯•å‡å°10å€
   â–¡ å°è¯•ï¼š1e-4, 1e-3, 1e-2

3. æ£€æŸ¥æ¨¡å‹
   â–¡ æ¨¡å‹æ˜¯å¦å¤ªå°ï¼Ÿ
   â–¡ å‚æ•°æ˜¯å¦åˆå§‹åŒ–æ­£å¸¸ï¼Ÿ
   â–¡ æ˜¯å¦æœ‰æ¢¯åº¦ï¼Ÿ

4. æ£€æŸ¥ä»£ç 
   â–¡ loss.backward()æ˜¯å¦è°ƒç”¨ï¼Ÿ
   â–¡ optimizer.step()æ˜¯å¦è°ƒç”¨ï¼Ÿ
   â–¡ æ¢¯åº¦æ˜¯å¦è¢«æ¸…ç©ºï¼Ÿ
```

### Q2: Losså˜æˆNaNæ€ä¹ˆåŠï¼Ÿ

**åŸå› å’Œè§£å†³**ï¼š
```python
åŸå› 1: å­¦ä¹ ç‡å¤ªå¤§
  ç°è±¡ï¼šå‰å‡ æ­¥å°±NaN
  è§£å†³ï¼šå‡å°learning_rateåˆ°1/10

åŸå› 2: æ¢¯åº¦çˆ†ç‚¸
  ç°è±¡ï¼šè®­ç»ƒä¸€æ®µæ—¶é—´åNaN
  è§£å†³ï¼šå¯ç”¨grad_clip=1.0

åŸå› 3: æ•°å€¼ä¸ç¨³å®š
  ç°è±¡ï¼šéšæœºå‡ºç°NaN
  è§£å†³ï¼š
    - ä½¿ç”¨float32è€Œä¸æ˜¯float16
    - æ£€æŸ¥é™¤é›¶æ“ä½œ
    - æ£€æŸ¥log(0)æ“ä½œ

åŸå› 4: æ•°æ®é—®é¢˜
  ç°è±¡ï¼šç‰¹å®šbatchå¯¼è‡´NaN
  è§£å†³ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰å¼‚å¸¸å€¼
```

### Q3: è¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ

**è¯†åˆ«å’Œè§£å†³**ï¼š
```python
è¯†åˆ«ï¼š
  train_loss << val_loss
  ä¾‹ï¼štrain=1.2, val=1.8 (å·®è·0.6)

è§£å†³æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
  1. å¢åŠ dropout (0.0 â†’ 0.2)
  2. å¢åŠ weight_decay (0.0 â†’ 0.1)
  3. å‡å°æ¨¡å‹ (n_layer -= 2)
  4. è·å–æ›´å¤šæ•°æ®
  5. æ•°æ®å¢å¼º
  6. æ—©åœï¼ˆearly stoppingï¼‰

éªŒè¯ï¼š
  val_lossåº”è¯¥æ¥è¿‘train_loss
```

### Q4: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**åŠ é€ŸæŠ€å·§ï¼ˆæŒ‰æ•ˆæœæ’åºï¼‰**ï¼š
```python
1. å¯ç”¨compile=True
   æ•ˆæœï¼š1.5-2xåŠ é€Ÿ
   ä»£ä»·ï¼šé¦–æ¬¡ç¼–è¯‘éœ€è¦1-2åˆ†é’Ÿ

2. å¢å¤§batch_size
   æ•ˆæœï¼šGPUåˆ©ç”¨ç‡æé«˜
   ä»£ä»·ï¼šéœ€è¦æ›´å¤šæ˜¾å­˜

3. ä½¿ç”¨float16
   æ•ˆæœï¼šæ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦æå‡30-50%
   ä»£ä»·ï¼šå¯èƒ½æœ‰ç²¾åº¦æŸå¤±

4. å‡å°eval_interval
   æ•ˆæœï¼šå‡å°‘è¯„ä¼°æ—¶é—´
   ä»£ä»·ï¼šçœ‹ä¸åˆ°å®æ—¶è¿›åº¦

5. å¤šGPUè®­ç»ƒ
   æ•ˆæœï¼šæ¥è¿‘çº¿æ€§åŠ é€Ÿ
   ä»£ä»·ï¼šéœ€è¦å¤šå¼ GPU

6. å‡å°æ¨¡å‹
   æ•ˆæœï¼šè®­ç»ƒæ›´å¿«
   ä»£ä»·ï¼šæ€§èƒ½å¯èƒ½ä¸‹é™
```

---

**æ­å–œä½ å®Œæˆç¬¬04ç« ï¼** ğŸ‰

ä½ ç°åœ¨å·²ç»æŒæ¡äº†ï¼š
- âœ… å®Œæ•´çš„è®­ç»ƒæµç¨‹
- âœ… ç³»ç»Ÿæ€§å®éªŒè®¾è®¡
- âœ… ç»“æœåˆ†ææ–¹æ³•
- âœ… è°ƒè¯•å’Œä¼˜åŒ–æŠ€å·§

**ä½ å·²ç»å¯ä»¥ç‹¬ç«‹è®­ç»ƒGPTæ¨¡å‹äº†ï¼** ğŸš€

**å‡†å¤‡å¥½æ·±å…¥ç†è§£æ¨¡å‹æ¶æ„äº†å—ï¼Ÿ** â†’ [05_model_architecture_deep_dive.md](05_model_architecture_deep_dive.md)
