# Train.py å®Œå…¨è§£æ - æ€»ç»“ä¸å®éªŒæŒ‡å—

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥è¡¨

### 1. æ•°æ®æµåŠ¨

```
åŸå§‹æ–‡æœ¬ â†’ ç¼–ç  â†’ .binæ–‡ä»¶ â†’ get_batch() â†’ æ¨¡å‹ â†’ loss
  "To be"    [5,10]   train.bin    å¼ é‡[4,8]    å‰å‘   2.45
```

### 2. è®­ç»ƒä¸€æ¬¡è¿­ä»£çš„å®Œæ•´æµç¨‹

```python
# ä¼ªä»£ç ï¼ˆç®€åŒ–çš„train.pyæ ¸å¿ƒï¼‰

for iter_num in range(max_iters):
    
    # ========== é˜¶æ®µ1: å‡†å¤‡ ==========
    lr = get_lr(iter_num)  # è®¡ç®—å½“å‰å­¦ä¹ ç‡
    
    # ========== é˜¶æ®µ2: æ¢¯åº¦ç´¯ç§¯ ==========
    optimizer.zero_grad()
    for micro_step in range(4):  # ç´¯ç§¯4æ¬¡
        X, Y = get_batch('train')  # [4, 8]
        logits, loss = model(X, Y)  # å‰å‘ä¼ æ’­
        loss = loss / 4  # å…³é”®ï¼šé™¤ä»¥ç´¯ç§¯æ­¥æ•°
        loss.backward()  # åå‘ä¼ æ’­ï¼Œæ¢¯åº¦ç´¯åŠ 
    
    # ========== é˜¶æ®µ3: æ›´æ–°å‚æ•° ==========
    clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
    optimizer.step()  # æ›´æ–°å‚æ•°
    
    # ========== é˜¶æ®µ4: è®°å½•å’Œè¯„ä¼° ==========
    if iter_num % 2000 == 0:
        val_loss = evaluate()
        save_checkpoint()
```

### 3. é‡è¦å…¬å¼

#### æŸå¤±å‡½æ•°ï¼ˆCross Entropyï¼‰
```
loss = -log(P(æ­£ç¡®ç­”æ¡ˆ))

ä¾‹å­ï¼š
P(æ­£ç¡®) = 0.9  â†’ loss = -log(0.9) = 0.11  âœ… å¥½
P(æ­£ç¡®) = 0.5  â†’ loss = -log(0.5) = 0.69  ğŸ¤” ä¸€èˆ¬
P(æ­£ç¡®) = 0.1  â†’ loss = -log(0.1) = 2.30  âŒ å·®
```

#### æ¢¯åº¦ä¸‹é™æ›´æ–°
```
å‚æ•°(æ–°) = å‚æ•°(æ—§) - å­¦ä¹ ç‡ Ã— æ¢¯åº¦

ä¾‹å­ï¼š
w_new = 0.5 - 0.001 Ã— (-2.3) = 0.5 + 0.0023 = 0.5023
```

#### å­¦ä¹ ç‡è°ƒåº¦
```
Warmupé˜¶æ®µ (step < 2000):
  lr = max_lr Ã— (step / warmup_steps)

æ­£å¸¸è®­ç»ƒé˜¶æ®µ:
  lr = max_lr

Decayé˜¶æ®µ:
  lr = min_lr + (max_lr - min_lr) Ã— cos_decay
```

---

## ğŸ§ª å®éªŒåˆ—è¡¨ï¼ˆä»æ˜“åˆ°éš¾ï¼‰

### å®éªŒ1: ç†è§£é…ç½®å‚æ•°çš„å½±å“ â­

**ç›®æ ‡**ï¼šç›´è§‚æ„Ÿå—ä¸åŒå‚æ•°å¯¹è®­ç»ƒçš„å½±å“

```bash
# åŸºçº¿
python train.py config/train_shakespeare_char.py --max_iters=1000 --out_dir=exp1_baseline

# å®éªŒ1A: æ›´å¤§çš„å­¦ä¹ ç‡
python train.py config/train_shakespeare_char.py --max_iters=1000 --learning_rate=5e-3 --out_dir=exp1_large_lr

# å®éªŒ1B: æ›´å°çš„å­¦ä¹ ç‡  
python train.py config/train_shakespeare_char.py --max_iters=1000 --learning_rate=1e-4 --out_dir=exp1_small_lr

# å®éªŒ1C: æ›´å°çš„batch
python train.py config/train_shakespeare_char.py --max_iters=1000 --batch_size=16 --out_dir=exp1_small_batch

# å®éªŒ1D: æ›´å¤§çš„æ¨¡å‹
python train.py config/train_shakespeare_char.py --max_iters=1000 --n_layer=8 --n_embd=512 --out_dir=exp1_large_model
```

**è§‚å¯Ÿé‡ç‚¹**ï¼š
- lossä¸‹é™é€Ÿåº¦
- æœ€ç»ˆval loss
- è®­ç»ƒæ—¶é—´
- ç”Ÿæˆè´¨é‡

**åˆ†ææ¨¡æ¿**ï¼š
```
å®éªŒ1A (å¤§å­¦ä¹ ç‡):
  - åˆæœŸlossä¸‹é™: [å¿«/æ…¢]
  - æ˜¯å¦æ”¶æ•›: [æ˜¯/å¦/éœ‡è¡]
  - æœ€ç»ˆval loss: [æ•°å€¼]
  - ç»“è®º: [...]
```

---

### å®éªŒ2: æ¢¯åº¦ç´¯ç§¯éªŒè¯ â­â­

**ç›®æ ‡**ï¼šéªŒè¯æ¢¯åº¦ç´¯ç§¯ç¡®å®ç­‰ä»·äºå¤§batch

```bash
# æ–¹æ³•A: ç›´æ¥ç”¨å¤§batch
python train.py config/train_shakespeare_char.py \
  --batch_size=64 \
  --gradient_accumulation_steps=1 \
  --max_iters=500 \
  --out_dir=exp2_large_batch

# æ–¹æ³•B: å°batch + æ¢¯åº¦ç´¯ç§¯
python train.py config/train_shakespeare_char.py \
  --batch_size=16 \
  --gradient_accumulation_steps=4 \
  --max_iters=500 \
  --out_dir=exp2_grad_accum
```

**è§‚å¯Ÿé‡ç‚¹**ï¼š
- ä¸¤è€…çš„lossæ›²çº¿åº”è¯¥éå¸¸ç›¸ä¼¼
- è®­ç»ƒæ—¶é—´å¯èƒ½ç•¥æœ‰å·®å¼‚
- æœ€ç»ˆæ¨¡å‹æ€§èƒ½åº”è¯¥å‡ ä¹ç›¸åŒ

**ç†è®ºé¢„æµ‹**ï¼š
```
æœ‰æ•ˆbatch_size = batch_size Ã— gradient_accumulation_steps

æ–¹æ³•A: 64 Ã— 1 = 64
æ–¹æ³•B: 16 Ã— 4 = 64  âœ… ç›¸åŒï¼
```

---

### å®éªŒ3: è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ– â­â­

**ç›®æ ‡**ï¼šç†è§£è¿‡æ‹Ÿåˆï¼Œå­¦ä¼šä½¿ç”¨dropoutå’Œweight_decay

```bash
# åŸºçº¿ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.0 \
  --weight_decay=0.0 \
  --out_dir=exp3_overfit

# åŠ dropout
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.2 \
  --weight_decay=0.0 \
  --out_dir=exp3_dropout

# åŠ weight_decay
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.0 \
  --weight_decay=0.1 \
  --out_dir=exp3_weight_decay

# ä¸¤è€…éƒ½åŠ 
python train.py config/train_shakespeare_char.py \
  --max_iters=5000 \
  --dropout=0.2 \
  --weight_decay=0.1 \
  --out_dir=exp3_both
```

**è§‚å¯Ÿé‡ç‚¹**ï¼š
```
å…³æ³¨train_losså’Œval_lossçš„å·®è·ï¼š

è¿‡æ‹Ÿåˆè¿¹è±¡ï¼š
  train_loss = 1.2
  val_loss = 1.8  â† å·®è·å¤§ï¼
  
æ³›åŒ–è‰¯å¥½ï¼š
  train_loss = 1.4
  val_loss = 1.5  â† å·®è·å° âœ…
```

---

### å®éªŒ4: å­¦ä¹ ç‡è°ƒåº¦ â­â­â­

**ç›®æ ‡**ï¼šç†è§£warmupå’Œdecayçš„ä½œç”¨

```bash
# æ— warmup, æ— decay
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=0 \
  --decay_lr=False \
  --out_dir=exp4_no_schedule

# åªæœ‰warmup
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=100 \
  --decay_lr=False \
  --out_dir=exp4_warmup_only

# åªæœ‰decay
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=0 \
  --decay_lr=True \
  --lr_decay_iters=2000 \
  --out_dir=exp4_decay_only

# ä¸¤è€…éƒ½æœ‰ï¼ˆæ¨èï¼‰
python train.py config/train_shakespeare_char.py \
  --max_iters=2000 \
  --warmup_iters=100 \
  --decay_lr=True \
  --lr_decay_iters=2000 \
  --out_dir=exp4_full_schedule
```

**è§‚å¯Ÿé‡ç‚¹**ï¼š
- å‰100æ­¥çš„losså˜åŒ–ï¼ˆwarmupå½±å“ï¼‰
- æœ€å200æ­¥çš„losså˜åŒ–ï¼ˆdecayå½±å“ï¼‰
- æ•´ä½“æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§

---

### å®éªŒ5: æ¨¡å‹è§„æ¨¡å®éªŒ â­â­â­

**ç›®æ ‡**ï¼šç†è§£æ¨¡å‹å®¹é‡ä¸æ€§èƒ½çš„å…³ç³»

```bash
# è¶…å°æ¨¡å‹
python train.py config/train_shakespeare_char.py \
  --n_layer=2 --n_head=2 --n_embd=128 \
  --max_iters=3000 \
  --out_dir=exp5_tiny

# å°æ¨¡å‹
python train.py config/train_shakespeare_char.py \
  --n_layer=4 --n_head=4 --n_embd=256 \
  --max_iters=3000 \
  --out_dir=exp5_small

# ä¸­æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
python train.py config/train_shakespeare_char.py \
  --n_layer=6 --n_head=6 --n_embd=384 \
  --max_iters=3000 \
  --out_dir=exp5_medium

# å¤§æ¨¡å‹
python train.py config/train_shakespeare_char.py \
  --n_layer=8 --n_head=8 --n_embd=512 \
  --max_iters=3000 \
  --out_dir=exp5_large
```

**è®¡ç®—å‚æ•°é‡**ï¼š
```python
# ç²—ç•¥ä¼°ç®—
params â‰ˆ n_layer Ã— (4 Ã— n_embdÂ² + 12 Ã— n_embdÂ²) + vocab_size Ã— n_embd
       â‰ˆ n_layer Ã— 16 Ã— n_embdÂ² + vocab_size Ã— n_embd

Tiny:   2 Ã— 16 Ã— 128Â² + 65 Ã— 128 â‰ˆ 532K
Small:  4 Ã— 16 Ã— 256Â² + 65 Ã— 256 â‰ˆ 4.2M
Medium: 6 Ã— 16 Ã— 384Â² + 65 Ã— 384 â‰ˆ 14.2M
Large:  8 Ã— 16 Ã— 512Â² + 65 Ã— 512 â‰ˆ 33.6M
```

**åˆ†æè¦ç‚¹**ï¼š
- æ¨¡å‹è¶Šå¤§ï¼Œval lossè¶Šä½ï¼ˆä½†æ”¶ç›Šé€’å‡ï¼‰
- è®­ç»ƒæ—¶é—´å¢é•¿
- æ˜¾å­˜éœ€æ±‚å¢é•¿
- ç”Ÿæˆè´¨é‡æå‡

---

## ğŸ“Š ç»“æœåˆ†æå·¥å…·

### å·¥å…·1: ç»˜åˆ¶Lossæ›²çº¿

åˆ›å»ºæ–‡ä»¶ `plot_loss.py`ï¼š

```python
import matplotlib.pyplot as plt
import re

def parse_log(log_file):
    """è§£æè®­ç»ƒæ—¥å¿—"""
    steps = []
    train_losses = []
    val_losses = []
    
    with open(log_file, 'r') as f:
        for line in f:
            # è§£æç±»ä¼¼ "step 0: train loss 4.1234, val loss 4.2345"
            match = re.search(r'step (\d+): train loss ([\d.]+), val loss ([\d.]+)', line)
            if match:
                steps.append(int(match.group(1)))
                train_losses.append(float(match.group(2)))
                val_losses.append(float(match.group(3)))
    
    return steps, train_losses, val_losses

# ä½¿ç”¨
steps, train_loss, val_loss = parse_log('training.log')

plt.figure(figsize=(10, 6))
plt.plot(steps, train_loss, label='Train Loss')
plt.plot(steps, val_loss, label='Val Loss')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training Progress')
plt.legend()
plt.grid(True)
plt.savefig('loss_curve.png')
plt.show()
```

### å·¥å…·2: æ¯”è¾ƒå¤šä¸ªå®éªŒ

```python
import matplotlib.pyplot as plt

experiments = {
    'Baseline': 'exp1_baseline/training.log',
    'Large LR': 'exp1_large_lr/training.log',
    'Small LR': 'exp1_small_lr/training.log',
}

plt.figure(figsize=(12, 6))

for name, log_file in experiments.items():
    steps, _, val_loss = parse_log(log_file)
    plt.plot(steps, val_loss, label=name)

plt.xlabel('Steps')
plt.ylabel('Validation Loss')
plt.title('Experiment Comparison')
plt.legend()
plt.grid(True)
plt.savefig('comparison.png')
plt.show()
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. æ£€æŸ¥æ•°æ®åŠ è½½

```python
# åœ¨train.pyå¼€å¤´æ·»åŠ 
X, Y = get_batch('train')
print(f"X shape: {X.shape}")
print(f"Y shape: {Y.shape}")
print(f"X sample: {X[0]}")
print(f"Y sample: {Y[0]}")

# è§£ç æŸ¥çœ‹å®é™…æ–‡æœ¬
if meta_vocab_size:
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    itos = meta['itos']
    x_text = ''.join([itos[int(i)] for i in X[0]])
    y_text = ''.join([itos[int(i)] for i in Y[0]])
    print(f"X text: {x_text}")
    print(f"Y text: {y_text}")
```

### 2. ç›‘æ§æ¢¯åº¦

```python
# åœ¨åå‘ä¼ æ’­åæ·»åŠ 
if iter_num % 100 == 0:
    total_norm = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    print(f"Gradient norm: {total_norm:.4f}")
    
    # å¦‚æœæ¢¯åº¦è¿‡å¤§ï¼Œè¯´æ˜å¯èƒ½æœ‰é—®é¢˜
    if total_norm > 10:
        print("âš ï¸  è­¦å‘Šï¼šæ¢¯åº¦å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦é™ä½å­¦ä¹ ç‡æˆ–å¯ç”¨æ¢¯åº¦è£å‰ª")
```

### 3. æ£€æŸ¥æ¨¡å‹è¾“å‡º

```python
# è®­ç»ƒå‰æµ‹è¯•ä¸€ä¸‹
model.eval()
with torch.no_grad():
    X_test, Y_test = get_batch('val')
    logits, loss = model(X_test, Y_test)
    print(f"Initial loss: {loss.item():.4f}")
    
    # ç†è®ºæœ€å¤§lossï¼ˆéšæœºçŒœï¼‰
    random_guess_loss = -math.log(1.0 / vocab_size)
    print(f"Random guess loss: {random_guess_loss:.4f}")
    
    # åˆå§‹lossåº”è¯¥æ¥è¿‘random_guess_loss
model.train()
```

---

## ğŸ’¡ å¸¸è§é—®é¢˜è§£ç­”

### Q1: Lossä¸€ç›´ä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

**æ£€æŸ¥æ¸…å•**ï¼š
1. âœ… å­¦ä¹ ç‡æ˜¯å¦å¤ªå°ï¼Ÿè¯•è¯•å¢å¤§10å€
2. âœ… æ•°æ®æ˜¯å¦æ­£ç¡®ï¼Ÿæ£€æŸ¥Xå’ŒYæ˜¯å¦å¯¹åº”
3. âœ… æ¨¡å‹æ˜¯å¦å¤ªå°ï¼Ÿè¯•è¯•å¢å¤§n_layeræˆ–n_embd
4. âœ… æ˜¯å¦æœ‰æ¢¯åº¦ï¼Ÿæ‰“å°grad_normæ£€æŸ¥

### Q2: Losså˜æˆNaNæ€ä¹ˆåŠï¼Ÿ

**åŸå› å’Œè§£å†³**ï¼š
```
åŸå› 1: å­¦ä¹ ç‡å¤ªå¤§
  è§£å†³: å‡å°learning_rateåˆ°1/10

åŸå› 2: æ¢¯åº¦çˆ†ç‚¸
  è§£å†³: å¯ç”¨grad_clip=1.0

åŸå› 3: æ•°å€¼ä¸ç¨³å®š
  è§£å†³: ä½¿ç”¨float32è€Œä¸æ˜¯float16
```

### Q3: è¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ

**è¯†åˆ«**ï¼štrain_loss << val_loss

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å¢åŠ dropout (0.1 â†’ 0.2)
2. å¢åŠ weight_decay (0 â†’ 0.1)
3. å‡å°æ¨¡å‹ (n_layer -= 2)
4. è·å–æ›´å¤šæ•°æ®
5. æ—©åœï¼ˆval_lossä¸å†ä¸‹é™å°±åœæ­¢ï¼‰

### Q4: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**åŠ é€ŸæŠ€å·§**ï¼š
1. å¯ç”¨compile=Trueï¼ˆæé€Ÿ2xï¼‰
2. å¢å¤§batch_sizeï¼ˆGPUåˆ©ç”¨ç‡æé«˜ï¼‰
3. å‡å°eval_intervalï¼ˆå°‘åšè¯„ä¼°ï¼‰
4. ä½¿ç”¨float16ï¼ˆæ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦æå‡ï¼‰
5. å¤šGPUè®­ç»ƒï¼ˆçº¿æ€§åŠ é€Ÿï¼‰

---

## ğŸ¯ è‡ªæµ‹é¢˜

å®Œæˆä»¥ä¸‹é—®é¢˜ï¼Œæ£€éªŒä½ çš„ç†è§£ï¼š

1. **æ¢¯åº¦ç´¯ç§¯**ï¼š
   - batch_size=8, gradient_accumulation_steps=4
   - ç­‰æ•ˆbatch_sizeæ˜¯å¤šå°‘ï¼Ÿ [ç­”æ¡ˆ: 32]
   - ä¸ºä»€ä¹ˆlossè¦é™¤ä»¥4ï¼Ÿ

2. **å­¦ä¹ ç‡**ï¼š
   - å¦‚æœlosséœ‡è¡ï¼Œåº”è¯¥å¢å¤§è¿˜æ˜¯å‡å°lrï¼Ÿ [ç­”æ¡ˆ: å‡å°]
   - warmupçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

3. **è¿‡æ‹Ÿåˆ**ï¼š
   - train_loss=1.2, val_loss=1.8ï¼Œè¿™æ˜¯è¿‡æ‹Ÿåˆå—ï¼Ÿ [ç­”æ¡ˆ: æ˜¯]
   - å¦‚ä½•è§£å†³ï¼Ÿ

4. **æ¢¯åº¦è£å‰ª**ï¼š
   - grad_clip=1.0çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
   - ä»€ä¹ˆæ—¶å€™éœ€è¦æ¢¯åº¦è£å‰ªï¼Ÿ

5. **æ•°æ®åŠ è½½**ï¼š
   - ä¸ºä»€ä¹ˆYæ¯”Xå‘å³ç§»ä¸€ä½ï¼Ÿ
   - ä¸ºä»€ä¹ˆè¦éšæœºé‡‡æ ·è€Œä¸æ˜¯é¡ºåºè¯»å–ï¼Ÿ

---

## ğŸš€ è¿›é˜¶æŒ‘æˆ˜

å®Œæˆè¿™äº›æŒ‘æˆ˜ï¼Œæˆä¸ºçœŸæ­£çš„é«˜æ‰‹ï¼š

1. **å®ç°ä½ è‡ªå·±çš„get_batch()**
   - æ”¯æŒåºåˆ—æ‰“åŒ…
   - æ”¯æŒåŠ¨æ€padding

2. **å®ç°learning rate finder**
   - è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å­¦ä¹ ç‡
   - å‚è€ƒï¼šLeslie Smithçš„è®ºæ–‡

3. **å®ç°æ—©åœï¼ˆEarly Stoppingï¼‰**
   - å½“val_lossè¿ç»­Næ­¥ä¸é™å°±åœæ­¢
   - ä¿å­˜æœ€ä½³æ¨¡å‹

4. **å®ç°æ¢¯åº¦å¯è§†åŒ–**
   - æ¯å±‚çš„æ¢¯åº¦åˆ†å¸ƒ
   - è¯†åˆ«æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

5. **å®ç°æ•°æ®å¢å¼º**
   - éšæœºmaskéƒ¨åˆ†token
   - å‚è€ƒBERTçš„MLM

---

## ğŸ“š æ¨èé˜…è¯»

1. **ä¼˜åŒ–å™¨**ï¼š
   - Adamè®ºæ–‡ï¼šhttps://arxiv.org/abs/1412.6980
   - AdamWè®ºæ–‡ï¼šhttps://arxiv.org/abs/1711.05101

2. **å­¦ä¹ ç‡è°ƒåº¦**ï¼š
   - Cosine Annealingï¼šhttps://arxiv.org/abs/1608.03983
   - Learning Rate Finderï¼šhttps://arxiv.org/abs/1506.01186

3. **æ­£åˆ™åŒ–**ï¼š
   - Dropoutï¼šhttps://jmlr.org/papers/v15/srivastava14a.html
   - Weight Decay vs L2ï¼šhttps://arxiv.org/abs/1711.05101

4. **æ¢¯åº¦**ï¼š
   - åå‘ä¼ æ’­è¯¦è§£ï¼šhttp://cs231n.github.io/optimization-2/
   - æ¢¯åº¦è£å‰ªï¼šhttps://arxiv.org/abs/1211.5063

---

**æ­å–œä½ å®Œæˆtrain.pyçš„æ·±åº¦å­¦ä¹ ï¼ğŸ‰**

ç°åœ¨ä½ å·²ç»ç†è§£äº†ï¼š
- âœ… å®Œæ•´çš„è®­ç»ƒå¾ªç¯æµç¨‹
- âœ… æ¯ä¸ªè¶…å‚æ•°çš„ä½œç”¨
- âœ… å„ç§è®­ç»ƒæŠ€å·§
- âœ… å¦‚ä½•è°ƒè¯•å’Œä¼˜åŒ–

**å‡†å¤‡å¥½å­¦ä¹ model.pyäº†å—ï¼Ÿ** é‚£é‡Œæ‰æ˜¯GPTçœŸæ­£çš„"å¤§è„‘"æ‰€åœ¨ï¼
